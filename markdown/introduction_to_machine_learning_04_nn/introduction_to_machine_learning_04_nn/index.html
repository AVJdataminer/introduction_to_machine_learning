



<!DOCTYPE html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="University of Wroclaw, Department of Physics and Astronomy">
      
      
        <link rel="canonical" href="https://tomaszgolan.github.io/introduction_to_machine_learning/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/">
      
      
        <meta name="author" content="Tomasz Golan">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-0.17.2, mkdocs-material-2.2.3">
    
    
      
        <title>Neural Networks - Introduction to Machine Learning</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/application.bcabdff3.css">
      
        <link rel="stylesheet" href="../../../assets/stylesheets/application-palette.792431c1.css">
      
    
    
    
      <script src="../../../assets/javascripts/modernizr.1aa3b519.js"></script>
    
    <script src="../../../js/require.min.js"></script>

    
      
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
    
    
    
  </head>
  
    
    
    
      <body data-md-color-primary="blue-grey" data-md-color-accent="blue">
    
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="drawer">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="search">
    <label class="md-overlay" data-md-component="overlay" for="drawer"></label>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://tomaszgolan.github.io/introduction_to_machine_learning/" title="Introduction to Machine Learning" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                Introduction to Machine Learning
              </span>
              <span class="md-header-nav__topic">
                Neural Networks
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="search"></label>
  <div class="md-search__inner">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" required placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query">
      <label class="md-icon md-search__icon" for="search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset">&#xE5CD;</button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  


  <a href="https://github.com/TomaszGolan/introduction_to_machine_learning/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      IML @ GitHub
    </div>
  </a>

          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="drawer">
    <span class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </span>
    Introduction to Machine Learning
  </label>
  
    <div class="md-nav__source">
      


  


  <a href="https://github.com/TomaszGolan/introduction_to_machine_learning/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      IML @ GitHub
    </div>
  </a>

    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../../.." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/" title="Introduction" class="md-nav__link">
      Introduction
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/" title="k-Nearest Neighbors" class="md-nav__link">
      k-Nearest Neighbors
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/" title="Decision Trees" class="md-nav__link">
      Decision Trees
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/" title="Support Vector Machine" class="md-nav__link">
      Support Vector Machine
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="toc">
        Neural Networks
      </label>
    
    <a href="./" title="Neural Networks" class="md-nav__link md-nav__link--active">
      Neural Networks
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#linear-regression" title="Linear regression" class="md-nav__link">
    Linear regression
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#normal-equation" title="Normal equation" class="md-nav__link">
    Normal equation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-descent" title="Gradient descent" class="md-nav__link">
    Gradient descent
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example" title="Example" class="md-nav__link">
    Example
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#logistic-regression" title="Logistic regression" class="md-nav__link">
    Logistic regression
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hypothesis" title="Hypothesis" class="md-nav__link">
    Hypothesis
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cost-function" title="Cost function" class="md-nav__link">
    Cost function
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#quick-proof" title="Quick proof" class="md-nav__link">
    Quick proof
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example_1" title="Example" class="md-nav__link">
    Example
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multinominal-logistic-regression" title="Multinominal logistic regression" class="md-nav__link">
    Multinominal logistic regression
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#logit-approach" title="Logit approach" class="md-nav__link">
    Logit approach
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#check-binary-case" title="Check binary case" class="md-nav__link">
    Check binary case
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax-approach" title="Softmax approach" class="md-nav__link">
    Softmax approach
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#check-binary-case_1" title="Check binary case" class="md-nav__link">
    Check binary case
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cost-function_1" title="Cost function" class="md-nav__link">
    Cost function
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gradient" title="Gradient" class="md-nav__link">
    Gradient
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example_2" title="Example" class="md-nav__link">
    Example
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dataset" title="Dataset" class="md-nav__link">
    Dataset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-preparation" title="Data preparation" class="md-nav__link">
    Data preparation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training" title="Training" class="md-nav__link">
    Training
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#validation" title="Validation" class="md-nav__link">
    Validation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax-visualization" title="Softmax visualization" class="md-nav__link">
    Softmax visualization
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#neural-networks" title="Neural Networks" class="md-nav__link">
    Neural Networks
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#neuron" title="Neuron" class="md-nav__link">
    Neuron
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#neural-networks_1" title="Neural networks" class="md-nav__link">
    Neural networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backpropagation" title="Backpropagation" class="md-nav__link">
    Backpropagation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#and-or-vs-xor" title="AND, OR vs XOR" class="md-nav__link">
    AND, OR vs XOR
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#single-neuron-approach" title="Single neuron approach" class="md-nav__link">
    Single neuron approach
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#neural-network-approach" title="Neural network approach" class="md-nav__link">
    Neural network approach
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#again-the-same-but-with-tensorflow" title="Again the same, but with tensorflow" class="md-nav__link">
    Again the same, but with tensorflow
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simple-regression-with-nn" title="Simple regression with NN" class="md-nav__link">
    Simple regression with NN
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#more-examples" title="More examples" class="md-nav__link">
    More examples
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mnist" title="MNIST" class="md-nav__link">
    MNIST
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gradient-descent-variations" title="Gradient descent variations" class="md-nav__link">
    Gradient descent variations
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sgd-on-mnist" title="SGD on MNIST" class="md-nav__link">
    SGD on MNIST
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#momentum" title="Momentum" class="md-nav__link">
    Momentum
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptive-models" title="Adaptive models" class="md-nav__link">
    Adaptive models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#adagrad" title="Adagrad" class="md-nav__link">
    Adagrad
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adadelta" title="Adadelta" class="md-nav__link">
    Adadelta
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adam" title="Adam" class="md-nav__link">
    Adam
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regularization" title="Regularization" class="md-nav__link">
    Regularization
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#to-regularize-or-not-to-regularize" title="To regularize or not to regularize" class="md-nav__link">
    To regularize or not to regularize
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" title="Summary" class="md-nav__link">
    Summary
  </a>
  
</li>
      
      
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../introduction_to_machine_learning_05_dl/introduction_to_machine_learning_05_dl/" title="Deep Learning" class="md-nav__link">
      Deep Learning
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#linear-regression" title="Linear regression" class="md-nav__link">
    Linear regression
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#normal-equation" title="Normal equation" class="md-nav__link">
    Normal equation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-descent" title="Gradient descent" class="md-nav__link">
    Gradient descent
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example" title="Example" class="md-nav__link">
    Example
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#logistic-regression" title="Logistic regression" class="md-nav__link">
    Logistic regression
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hypothesis" title="Hypothesis" class="md-nav__link">
    Hypothesis
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cost-function" title="Cost function" class="md-nav__link">
    Cost function
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#quick-proof" title="Quick proof" class="md-nav__link">
    Quick proof
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example_1" title="Example" class="md-nav__link">
    Example
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multinominal-logistic-regression" title="Multinominal logistic regression" class="md-nav__link">
    Multinominal logistic regression
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#logit-approach" title="Logit approach" class="md-nav__link">
    Logit approach
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#check-binary-case" title="Check binary case" class="md-nav__link">
    Check binary case
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax-approach" title="Softmax approach" class="md-nav__link">
    Softmax approach
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#check-binary-case_1" title="Check binary case" class="md-nav__link">
    Check binary case
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cost-function_1" title="Cost function" class="md-nav__link">
    Cost function
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gradient" title="Gradient" class="md-nav__link">
    Gradient
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example_2" title="Example" class="md-nav__link">
    Example
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dataset" title="Dataset" class="md-nav__link">
    Dataset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-preparation" title="Data preparation" class="md-nav__link">
    Data preparation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training" title="Training" class="md-nav__link">
    Training
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#validation" title="Validation" class="md-nav__link">
    Validation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax-visualization" title="Softmax visualization" class="md-nav__link">
    Softmax visualization
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#neural-networks" title="Neural Networks" class="md-nav__link">
    Neural Networks
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#neuron" title="Neuron" class="md-nav__link">
    Neuron
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#neural-networks_1" title="Neural networks" class="md-nav__link">
    Neural networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backpropagation" title="Backpropagation" class="md-nav__link">
    Backpropagation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#and-or-vs-xor" title="AND, OR vs XOR" class="md-nav__link">
    AND, OR vs XOR
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#single-neuron-approach" title="Single neuron approach" class="md-nav__link">
    Single neuron approach
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#neural-network-approach" title="Neural network approach" class="md-nav__link">
    Neural network approach
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#again-the-same-but-with-tensorflow" title="Again the same, but with tensorflow" class="md-nav__link">
    Again the same, but with tensorflow
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simple-regression-with-nn" title="Simple regression with NN" class="md-nav__link">
    Simple regression with NN
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#more-examples" title="More examples" class="md-nav__link">
    More examples
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mnist" title="MNIST" class="md-nav__link">
    MNIST
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gradient-descent-variations" title="Gradient descent variations" class="md-nav__link">
    Gradient descent variations
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sgd-on-mnist" title="SGD on MNIST" class="md-nav__link">
    SGD on MNIST
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#momentum" title="Momentum" class="md-nav__link">
    Momentum
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptive-models" title="Adaptive models" class="md-nav__link">
    Adaptive models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#adagrad" title="Adagrad" class="md-nav__link">
    Adagrad
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adadelta" title="Adadelta" class="md-nav__link">
    Adadelta
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adam" title="Adam" class="md-nav__link">
    Adam
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regularization" title="Regularization" class="md-nav__link">
    Regularization
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#to-regularize-or-not-to-regularize" title="To regularize or not to regularize" class="md-nav__link">
    To regularize or not to regularize
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" title="Summary" class="md-nav__link">
    Summary
  </a>
  
</li>
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/TomaszGolan/introduction_to_machine_learning/edit/master/docs/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="neural-network">Neural Network<a class="headerlink" href="#neural-network" title="Permanent link">&para;</a></h1>
<ul>
<li>
<p>Artificial neural network (in particular deep NN) is the most popular machine learning method these days</p>
</li>
<li>
<p>They are inspired by human brains (at least initially)</p>
</li>
<li>
<p>Artifical neuron is a mathematical function</p>
</li>
<li>
<p>Neurons are connected with each other (kind of synapses)</p>
</li>
<li>
<p>Usually connections have some weights</p>
</li>
<li>
<p>Today, feedforward neural networks (multilayer perceptrons) will be discussed</p>
</li>
<li>
<p>However, before we go there, lets start with linear and logistic regression</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="c1"># our standard imports: matplotlib and numpy</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="c1"># just to overwrite default colab style</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;default&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;seaborn-talk&#39;</span><span class="p">)</span>
</pre></div>


<h2 id="linear-regression">Linear regression<a class="headerlink" href="#linear-regression" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p>Lets consider dataset <span><span class="MathJax_Preview">\left\{(\vec x^{(i)}, y^{(i)}); i = 1,\cdots, N\right\}</span><script type="math/tex">\left\{(\vec x^{(i)}, y^{(i)}); i = 1,\cdots, N\right\}</script></span></p>
</li>
<li>
<p>where explanatory variables (<strong>features</strong>) <span><span class="MathJax_Preview">\vec x^{(i)} = (x^{(i)}_1,\cdots, x^{(i)}_n) \in \mathcal{R}^n</span><script type="math/tex">\vec x^{(i)} = (x^{(i)}_1,\cdots, x^{(i)}_n) \in \mathcal{R}^n</script></span></p>
</li>
<li>
<p>and dependent variables (<strong>targets</strong>) <span><span class="MathJax_Preview">y^{(i)} \in \mathcal{R}</span><script type="math/tex">y^{(i)} \in \mathcal{R}</script></span></p>
</li>
<li>
<p>Lets define the <strong>hypothesis</strong>: <p align="center"><br><span><span class="MathJax_Preview">h(\vec x) = w_0 + w_1x_1 + \cdots w_nx_n</span><script type="math/tex">h(\vec x) = w_0 + w_1x_1 + \cdots w_nx_n</script></span></p><br></p>
</li>
<li>
<p>In other words we claim that <span><span class="MathJax_Preview">y^{(i)}</span><script type="math/tex">y^{(i)}</script></span> can be calculated from <span><span class="MathJax_Preview">h(\vec x^{(i)})</span><script type="math/tex">h(\vec x^{(i)})</script></span>, if we know <strong>weights</strong> <span><span class="MathJax_Preview">w_i</span><script type="math/tex">w_i</script></span></p>
</li>
<li>
<p>For convenience lets set <span><span class="MathJax_Preview">x^{(i)}_0 = 1</span><script type="math/tex">x^{(i)}_0 = 1</script></span>, so we can rewrite the hypothesis: <p align="center"><br><span><span class="MathJax_Preview">h(\vec x) = \sum_{i=0}^nw_ix_i = \vec w \cdot \vec x</span><script type="math/tex">h(\vec x) = \sum_{i=0}^nw_ix_i = \vec w \cdot \vec x</script></span></p><br></p>
</li>
</ul>
<h3 id="normal-equation">Normal equation<a class="headerlink" href="#normal-equation" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>One could find weights using normal equation</p>
</li>
<li>
<p>Let <span><span class="MathJax_Preview">Y^T = (y^{(1)}, \cdots, y^{(N)})</span><script type="math/tex">Y^T = (y^{(1)}, \cdots, y^{(N)})</script></span>, <span><span class="MathJax_Preview">X^T = \left((\vec x^{(1)})^T, \cdots, (\vec x^{(N)})^T\right)</span><script type="math/tex">X^T = \left((\vec x^{(1)})^T, \cdots, (\vec x^{(N)})^T\right)</script></span> and <span><span class="MathJax_Preview">W^T = (w_0, \cdots, w_n)</span><script type="math/tex">W^T = (w_0, \cdots, w_n)</script></span></p>
</li>
<li>
<p>Then we can write a matrix equation: <p align="center"><br><span><span class="MathJax_Preview">Y = XW</span><script type="math/tex">Y = XW</script></span></p><br></p>
</li>
<li>
<p>The normal equation (minimizing the sum of the square differences between left and right sides): <p align="center"><br><span><span class="MathJax_Preview">X^TY = X^TXW</span><script type="math/tex">X^TY = X^TXW</script></span></p><br></p>
</li>
<li>
<p>Thus, one could find weights by calculating: <p align="center"><br><span><span class="MathJax_Preview">W = (X^TX)^{-1}X^TY</span><script type="math/tex">W = (X^TX)^{-1}X^TY</script></span></p><br></p>
</li>
<li>
<p>Doable, but computational expensive</p>
</li>
</ul>
<h3 id="gradient-descent">Gradient descent<a class="headerlink" href="#gradient-descent" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>Gradient descent is an iterative algorithm for finding the minimum</p>
</li>
<li>
<p>For linear regression the <strong>cost function</strong> (or <strong>loss function</strong>) is given by mean squared error: <p align="center"><br><span><span class="MathJax_Preview">L(\vec w) = \frac{1}{2N}\sum\limits_{i=1}^N\left(h(\vec x^{(i)}) - y^{(i)}\right)^2</span><script type="math/tex">L(\vec w) = \frac{1}{2N}\sum\limits_{i=1}^N\left(h(\vec x^{(i)}) - y^{(i)}\right)^2</script></span></p><br></p>
</li>
<li>
<p>It measures the quality of given set of parameters / weights</p>
</li>
<li>
<p>Please note, that <span><span class="MathJax_Preview">\frac{1}{2}</span><script type="math/tex">\frac{1}{2}</script></span> is added for convenience to MSE definition</p>
</li>
<li>
<p>In gradient descent method weights are updated w.r.t. the gradient of cost function: <p align="center"><br><span><span class="MathJax_Preview">w_j \rightarrow w_j - \alpha\frac{\partial L(\vec w)}{\partial w_j} = w_j - \frac{\alpha}{N}\sum\limits_{i=1}^{N}\left(h(\vec x^{(i)}) - y^{(i)}\right)x^{(i)}_j</span><script type="math/tex">w_j \rightarrow w_j - \alpha\frac{\partial L(\vec w)}{\partial w_j} = w_j - \frac{\alpha}{N}\sum\limits_{i=1}^{N}\left(h(\vec x^{(i)}) - y^{(i)}\right)x^{(i)}_j</script></span></p><br></p>
</li>
<li>
<p>Where <span><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> is training rate</p>
</li>
</ul>
<h3 id="example">Example<a class="headerlink" href="#example" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>Lets generate some fake data according to <span><span class="MathJax_Preview">y = ax + b</span><script type="math/tex">y = ax + b</script></span> for given slope and intercept</p>
</li>
<li>
<p>And add some noise to <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span></p>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="c1">### SETTINGS ###</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># number of samples</span>

<span class="n">a</span> <span class="o">=</span> <span class="mf">0.50</span> <span class="c1"># slope</span>
<span class="n">b</span> <span class="o">=</span> <span class="mf">0.50</span> <span class="c1"># y-intercept</span>
<span class="n">s</span> <span class="o">=</span> <span class="mf">0.25</span> <span class="c1"># sigma</span>

<span class="c1">### GENERATE SAMPLES ###</span>

<span class="n">X</span> <span class="o">=</span> <span class="p">(</span><span class="mf">10.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">N</span><span class="p">))</span>                               <span class="c1"># features</span>
<span class="n">Y</span> <span class="o">=</span> <span class="p">[(</span><span class="n">a</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)]</span> <span class="c1"># targets</span>

<span class="c1">### PLOT SAMPLES ###</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Feature&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Target&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">);</span>
</pre></div>


<p><img alt="png" src="../output_11_0.png" /></p>
<ul>
<li>It is time to learn about new framework</li>
</ul>
<blockquote>
<p>Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. It can use GPUs and perform efficient symbolic differentiation.</p>
</blockquote>
<div class="codehilite"><pre><span></span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">theano</span>
</pre></div>


<ul>
<li>
<p>In this example the hypothesis is given by: <p align="center"><br><span><span class="MathJax_Preview">h(x) = \vec w \cdot \vec x = ax + b</span><script type="math/tex">h(x) = \vec w \cdot \vec x = ax + b</script></span></p><br></p>
</li>
<li>
<p>where <span><span class="MathJax_Preview">\vec w = (b, a)</span><script type="math/tex">\vec w = (b, a)</script></span> and <span><span class="MathJax_Preview">\vec x = (1, x)</span><script type="math/tex">\vec x = (1, x)</script></span></p>
</li>
<li>
<p>Lets first create symbolic variable for:</p>
<ul>
<li>
<p>feature vector <span><span class="MathJax_Preview">X = (x_1, \cdots, x_N)</span><script type="math/tex">X = (x_1, \cdots, x_N)</script></span></p>
</li>
<li>
<p>target <span><span class="MathJax_Preview">Y = (y_1, \cdots, y_N)</span><script type="math/tex">Y = (y_1, \cdots, y_N)</script></span></p>
</li>
<li>
<p>weights <span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> and <span><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span> (or <span><span class="MathJax_Preview">w_1</span><script type="math/tex">w_1</script></span> and <span><span class="MathJax_Preview">w_0</span><script type="math/tex">w_0</script></span>)</p>
</li>
</ul>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">theano</span>
<span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="kn">as</span> <span class="nn">T</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">vector</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span> <span class="c1"># feature vector</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">vector</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span> <span class="c1"># target vector</span>

<span class="c1"># weights initialized randomly</span>
<span class="c1">#a = theano.shared(np.random.randn(), name = &#39;w&#39;)</span>
<span class="c1">#b = theano.shared(np.random.randn(), name = &#39;b&#39;)</span>

<span class="c1"># initial weights by hand for demonstration (random may be to close)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">)</span>
</pre></div>


<ul>
<li>
<p>Having that, we can define:</p>
<ul>
<li>
<p>hypothesis</p>
</li>
<li>
<p>cost function</p>
</li>
<li>
<p>gradients</p>
</li>
</ul>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">pred</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>                 <span class="c1"># hyphothesis</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">N</span>   <span class="c1"># cost function</span>
<span class="n">grad_a</span><span class="p">,</span> <span class="n">grad_b</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>  <span class="c1"># gradients</span>
</pre></div>


<ul>
<li>And finally, we define gradient descent method (which also returns the value of the cost function)</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.005</span> <span class="c1"># learning rate</span>

<span class="c1"># at each training step we update weights:</span>
<span class="c1"># w -&gt; w - alpha * grad_w and b -&gt; b - alpha * grad_b</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">],</span>
                        <span class="n">outputs</span> <span class="o">=</span> <span class="n">cost</span><span class="p">,</span>
                        <span class="n">updates</span> <span class="o">=</span> <span class="p">((</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_a</span><span class="p">),</span>
                                   <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">b</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_b</span><span class="p">)))</span>
</pre></div>


<ul>
<li>Each training step involves the full cycle on training data (<strong>epoch</strong>)</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">1000</span>  <span class="c1"># number of training steps / epochs</span>
<span class="n">costs</span> <span class="o">=</span> <span class="p">[]</span>       <span class="c1"># to keep track on the value of cost function on each step</span>
<span class="n">weights</span> <span class="o">=</span> <span class="p">[]</span>     <span class="c1"># to store few set of weights</span>

<span class="n">keep</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>  <span class="c1"># save result for some epochs passed</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">keep</span><span class="p">:</span>
    <span class="n">weights</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">a</span><span class="o">.</span><span class="n">get_value</span><span class="p">(),</span> <span class="n">b</span><span class="o">.</span><span class="n">get_value</span><span class="p">()))</span>

  <span class="n">costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">))</span>
</pre></div>


<ul>
<li>Finally, we can visualize the results</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>
<span class="n">n_rows</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">n_cols</span> <span class="o">=</span> <span class="mi">2</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">a_</span><span class="p">,</span> <span class="n">b_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">weights</span><span class="p">):</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">n_rows</span><span class="p">,</span> <span class="n">n_cols</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Epoch </span><span class="si">%i</span><span class="s1">: y = </span><span class="si">%.2f</span><span class="s1"> x + </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">keep</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">a_</span><span class="p">,</span> <span class="n">b_</span><span class="p">))</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Feature&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Target&#39;</span><span class="p">)</span>

  <span class="n">x_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">a_</span><span class="o">*</span><span class="n">x_</span> <span class="o">+</span> <span class="n">b_</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C1&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">n_rows</span><span class="p">,</span> <span class="n">n_cols</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Cost function&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;L&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.2</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">costs</span><span class="p">)),</span> <span class="n">costs</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>


<p><img alt="png" src="../output_23_0.png" /></p>
<h2 id="logistic-regression">Logistic regression<a class="headerlink" href="#logistic-regression" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p>Logistic regression is used when dependent variable (target) is categorical</p>
</li>
<li>
<p>Lets consider dataset <span><span class="MathJax_Preview">\left\{(\vec x^{(i)}, y^{(i)}); i = 1,\cdots, N\right\}</span><script type="math/tex">\left\{(\vec x^{(i)}, y^{(i)}); i = 1,\cdots, N\right\}</script></span></p>
</li>
<li>
<p>where independent variables <span><span class="MathJax_Preview">\vec x^{(i)} = (1, x^{(i)}_1,\cdots, x^{(i)}_n)</span><script type="math/tex">\vec x^{(i)} = (1, x^{(i)}_1,\cdots, x^{(i)}_n)</script></span></p>
</li>
<li>
<p>and dependent variables (we start with binary case) <span><span class="MathJax_Preview">y^{(i)} \in \{0, 1\}</span><script type="math/tex">y^{(i)} \in \{0, 1\}</script></span></p>
</li>
</ul>
<h3 id="hypothesis">Hypothesis<a class="headerlink" href="#hypothesis" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>The dependent variable follows Bernoulli distribution (<span><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span> with probability <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span>, <span><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span> with probability <span><span class="MathJax_Preview">1-p</span><script type="math/tex">1-p</script></span>)</p>
</li>
<li>
<p>We want to link the independent variable with Bernoulli distribution</p>
</li>
<li>
<p>The <strong>logit function</strong> <em>translates</em> a linear combination <span><span class="MathJax_Preview">\vec w \cdot \vec x</span><script type="math/tex">\vec w \cdot \vec x</script></span> (which can result in any value) into probability distribution: <p align="center"><br><span><span class="MathJax_Preview">logit(p) = \ln(odds) = \ln\left(\frac{p}{1-p}\right) = \vec w \cdot \vec x</span><script type="math/tex">logit(p) = \ln(odds) = \ln\left(\frac{p}{1-p}\right) = \vec w \cdot \vec x</script></span></p><br></p>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">p_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Logit function&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;p&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$\ln(p/(1-p))$&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_</span><span class="p">)));</span>
</pre></div>


<p><img alt="png" src="../output_28_0.png" /></p>
<ul>
<li>
<p>In logistic regression <strong>hypothesis</strong> is defined as the inverse function of logit - <strong>logistic function</strong>: <p align="center"><br><span><span class="MathJax_Preview">h(\vec x) = \frac{1}{1 + e^{-\vec w \cdot \vec x}}</span><script type="math/tex">h(\vec x) = \frac{1}{1 + e^{-\vec w \cdot \vec x}}</script></span></p><br></p>
</li>
<li>
<p>Thus, the probability of <span><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span> is given by <span><span class="MathJax_Preview">P(y = 1~|~\vec x, \vec w) = h(\vec x)</span><script type="math/tex">P(y = 1~|~\vec x, \vec w) = h(\vec x)</script></span></p>
</li>
<li>
<p>and the probability of <span><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span> is given by <span><span class="MathJax_Preview">P(y = 0~|~\vec x, \vec w) = 1 - h(\vec x)</span><script type="math/tex">P(y = 0~|~\vec x, \vec w) = 1 - h(\vec x)</script></span></p>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">x_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Logistic function&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$1/(1 + e^{-x})$&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x_</span><span class="p">)));</span>
</pre></div>


<p><img alt="png" src="../output_30_0.png" /></p>
<h3 id="cost-function">Cost function<a class="headerlink" href="#cost-function" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>The probablity mass function (PMS) for <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> (for given <span><span class="MathJax_Preview">\vec w</span><script type="math/tex">\vec w</script></span>): <p align="center"><br><span><span class="MathJax_Preview">p(y~|~\vec x, \vec w) = h(\vec x)^y\cdot \left(1 - h(\vec x)\right)^{1 - y}</span><script type="math/tex">p(y~|~\vec x, \vec w) = h(\vec x)^y\cdot \left(1 - h(\vec x)\right)^{1 - y}</script></span></p><br></p>
</li>
<li>
<p>The <strong>likelihood function</strong> is PMS considered as a function of <span><span class="MathJax_Preview">\vec w</span><script type="math/tex">\vec w</script></span> (for fixed <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>)</p>
</li>
<li>
<p>Thus, for a single data point <span><span class="MathJax_Preview">(\vec x^{(i)}, y^{(i)})</span><script type="math/tex">(\vec x^{(i)}, y^{(i)})</script></span>: <p align="center"><br><span><span class="MathJax_Preview">l(\vec w) = h(\vec x^{(i)})^{y^{(i)}}\cdot \left(1 - h(\vec x^{(i)})\right)^{1 - y^{(i)}}</span><script type="math/tex">l(\vec w) = h(\vec x^{(i)})^{y^{(i)}}\cdot \left(1 - h(\vec x^{(i)})\right)^{1 - y^{(i)}}</script></span></p><br></p>
</li>
<li>
<p>And for the whole dataset: <p align="center"><br><span><span class="MathJax_Preview">l(\vec w) = \prod\limits_{i=1}^N h(\vec x^{(i)})^{y^{(i)}}\cdot \left(1 - h(\vec x^{(i)})\right)^{1 - y^{(i)}}</span><script type="math/tex">l(\vec w) = \prod\limits_{i=1}^N h(\vec x^{(i)})^{y^{(i)}}\cdot \left(1 - h(\vec x^{(i)})\right)^{1 - y^{(i)}}</script></span></p><br></p>
</li>
<li>
<p>The goal is to maximize likelihood w.r.t to <span><span class="MathJax_Preview">\vec w</span><script type="math/tex">\vec w</script></span>, which is the same as maximizing <strong>log-likelihood</strong>: <p align="center"><br><span><span class="MathJax_Preview">\ln\left(l(\vec w)\right) = \sum\limits_{i=1}^N\left[y^{(i)}\ln\left(h(\vec x^{(i)})\right) + \left(1 - y^{(i)}\right)\ln\left(1 - h(\vec x^{(i)})\right)\right]</span><script type="math/tex">\ln\left(l(\vec w)\right) = \sum\limits_{i=1}^N\left[y^{(i)}\ln\left(h(\vec x^{(i)})\right) + \left(1 - y^{(i)}\right)\ln\left(1 - h(\vec x^{(i)})\right)\right]</script></span></p><br></p>
</li>
<li>
<p>Which is the same as minimizing the cost function <span><span class="MathJax_Preview">L(\vec w) = -\frac{1}{N}\ln\left(l(\vec w)\right)</span><script type="math/tex">L(\vec w) = -\frac{1}{N}\ln\left(l(\vec w)\right)</script></span></p>
</li>
<li>
<p>Once again using gradient descent method we can update weights using: <p align="center"><br><span><span class="MathJax_Preview">w_j \rightarrow w_j - \alpha\frac{\partial L(\vec w)}{\partial w_j} = w_j - \frac{\alpha}{N}\sum\limits_{i=1}^{N}\left(y^{(i)} - h(\vec x^{(i)})\right)x^{(i)}_j</span><script type="math/tex">w_j \rightarrow w_j - \alpha\frac{\partial L(\vec w)}{\partial w_j} = w_j - \frac{\alpha}{N}\sum\limits_{i=1}^{N}\left(y^{(i)} - h(\vec x^{(i)})\right)x^{(i)}_j</script></span></p><br></p>
</li>
</ul>
<h4 id="quick-proof">Quick proof<a class="headerlink" href="#quick-proof" title="Permanent link">&para;</a></h4>
<ul>
<li>First, lets consider the derivative of <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span></li>
</ul>
<p align="center"><br>$\begin{eqnarray}
\frac{\partial h(\vec x)}{\partial w_j} & = & \frac{\partial}{\partial w_j}\left(1 + e^{-\vec w \cdot \vec x}\right)^{-1} \\
                                        & = & e^{-\vec w \cdot \vec x}\left(1 + e^{-\vec w \cdot \vec x}\right)^{-2}x_j \\
                                        & = & (1 + e^{-\vec w \cdot \vec x} - 1)\left(1 + e^{-\vec w \cdot \vec x}\right)^{-2}x_j \\
                                        & = & \left(1 + e^{-\vec w \cdot \vec x}\right)^{-1}x_j - \left(1 + e^{-\vec w \cdot \vec x}\right)^{-2}x_j\\
                                        & = & h(\vec x)x_j - h^2(\vec x)x_j \\
                                        & = & h(\vec x)\left(1 - h(\vec x)\right)x_j
\end{eqnarray}$</p>

<p><br></p>
<ul>
<li>
<p>Thus, <span><span class="MathJax_Preview">\frac{\partial}{\partial w_j}\ln\left(h(\vec x)\right) = \left(1 - h(\vec x)\right)x_j</span><script type="math/tex">\frac{\partial}{\partial w_j}\ln\left(h(\vec x)\right) = \left(1 - h(\vec x)\right)x_j</script></span></p>
</li>
<li>
<p>and <span><span class="MathJax_Preview">\frac{\partial}{\partial w_j}\ln\left(1 - h(\vec x)\right) = -h(\vec x)x_j</span><script type="math/tex">\frac{\partial}{\partial w_j}\ln\left(1 - h(\vec x)\right) = -h(\vec x)x_j</script></span></p>
</li>
<li>
<p>Finally, we have <p align="center"><br><span><span class="MathJax_Preview">\frac{\partial}{\partial w_j}\left[y\ln\left(h(\vec x)\right) + (1 - y)\ln\left(1 - h(\vec x)\right)\right] = \left[y\left(1 - h(\vec x)\right) - (1 - y)h(\vec x)\right]x_j = \left[y - h(\vec x)\right]x_j</span><script type="math/tex">\frac{\partial}{\partial w_j}\left[y\ln\left(h(\vec x)\right) + (1 - y)\ln\left(1 - h(\vec x)\right)\right] = \left[y\left(1 - h(\vec x)\right) - (1 - y)h(\vec x)\right]x_j = \left[y - h(\vec x)\right]x_j</script></span></p><br></p>
</li>
</ul>
<h3 id="example_1">Example<a class="headerlink" href="#example_1" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>Lets consider the following dataset</p>
<ul>
<li>
<p><span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> - number of hours spent studying machine learning</p>
</li>
<li>
<p><span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> - student passed (1) or failed (0) the exam</p>
</li>
</ul>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># number of students per class</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">N</span><span class="p">))</span> <span class="o">*</span> <span class="mi">35</span><span class="p">,</span>
                   <span class="mi">30</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">N</span><span class="p">))</span> <span class="o">*</span> <span class="mi">25</span><span class="p">))</span>

<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(([</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">N</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">N</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Study time [h]&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Success&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">);</span>
</pre></div>


<p><img alt="png" src="../output_37_0.png" /></p>
<ul>
<li>Once again lets use <code>theano</code></li>
</ul>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">theano</span>
<span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="kn">as</span> <span class="nn">T</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">vector</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span> <span class="c1"># feature vector</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">vector</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span> <span class="c1"># target vector</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(),</span> <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="c1"># weights initialized randomly</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(),</span> <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">)</span>

<span class="n">hypo</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">T</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="o">-</span> <span class="n">b</span><span class="p">))</span>              <span class="c1"># hyphothesis</span>
<span class="n">xent</span> <span class="o">=</span> <span class="o">-</span> <span class="n">y</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">hypo</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">hypo</span><span class="p">)</span>  <span class="c1"># cross-entropy loss function</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">xent</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>                                     <span class="c1"># cost function</span>
<span class="n">grad_a</span><span class="p">,</span> <span class="n">grad_b</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>                 <span class="c1"># gradients</span>

<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="c1"># learning rate</span>

<span class="c1"># at each training step we update weights:</span>
<span class="c1"># w -&gt; w - alpha * grad_w and b -&gt; b - alpha * grad_b</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">],</span>
                        <span class="n">outputs</span> <span class="o">=</span> <span class="n">cost</span><span class="p">,</span>
                        <span class="n">updates</span> <span class="o">=</span> <span class="p">((</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_a</span><span class="p">),</span>
                                   <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">b</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_b</span><span class="p">)))</span>
</pre></div>


<ul>
<li>For the training we will scale features to <span><span class="MathJax_Preview">[0, 1]</span><script type="math/tex">[0, 1]</script></span>, which helps gradient descent to converge faster</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">x_min</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">x_max</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">s</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_min</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">x_max</span> <span class="o">-</span> <span class="n">x_min</span><span class="p">)</span>  <span class="c1"># scale</span>
</pre></div>


<ul>
<li>Now, we train the model on normalized data</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="p">[</span><span class="n">train</span><span class="p">(</span><span class="n">s</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">Y</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">)]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Study time [h]&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Success&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

<span class="n">h_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">h_</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">s</span><span class="p">(</span><span class="n">h_</span><span class="p">)</span><span class="o">*</span><span class="n">a</span><span class="o">.</span><span class="n">get_value</span><span class="p">()</span> <span class="o">-</span> <span class="n">b</span><span class="o">.</span><span class="n">get_value</span><span class="p">())),</span> <span class="s1">&#39;C1&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">60</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="s1">&#39;C2--&#39;</span><span class="p">);</span>
</pre></div>


<p><img alt="png" src="../output_43_0.png" /></p>
<ul>
<li>
<p>The orange line gives the probability of success as a function of study time</p>
</li>
<li>
<p>To classify a student one can make a cut at <span><span class="MathJax_Preview">0.5</span><script type="math/tex">0.5</script></span></p>
</li>
</ul>
<h2 id="multinominal-logistic-regression">Multinominal logistic regression<a class="headerlink" href="#multinominal-logistic-regression" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p>So far, we considered dependent variable to be binary - it is time to generalize LR to <span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> possible classes</p>
</li>
<li>
<p>Lets consider dataset <span><span class="MathJax_Preview">\left\{(\vec x^{(i)}, y^{(i)}); i = 1,\cdots, N\right\}</span><script type="math/tex">\left\{(\vec x^{(i)}, y^{(i)}); i = 1,\cdots, N\right\}</script></span></p>
</li>
<li>
<p>where independent variables <span><span class="MathJax_Preview">\vec x^{(i)} = (1, x^{(i)}_1,\cdots, x^{(i)}_n)</span><script type="math/tex">\vec x^{(i)} = (1, x^{(i)}_1,\cdots, x^{(i)}_n)</script></span></p>
</li>
<li>
<p>and dependent variables <span><span class="MathJax_Preview">y^{(i)} \in \{1, \cdots, K\}</span><script type="math/tex">y^{(i)} \in \{1, \cdots, K\}</script></span></p>
</li>
</ul>
<h3 id="logit-approach">Logit approach<a class="headerlink" href="#logit-approach" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>The one way to go is to prepare <span><span class="MathJax_Preview">K - 1</span><script type="math/tex">K - 1</script></span> binary classifiers w.r.t. to a chosen class as a pivot</p>
</li>
<li>
<p>The odds of <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> being <span><span class="MathJax_Preview">j \in \left\{1, \cdots, K-1\right\}</span><script type="math/tex">j \in \left\{1, \cdots, K-1\right\}</script></span> (chosing <span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> as a pivot) is given by: <span><span class="MathJax_Preview">\frac{P(y = j)}{P(y = K)}</span><script type="math/tex">\frac{P(y = j)}{P(y = K)}</script></span></p>
</li>
<li>
<p>which leads to <span><span class="MathJax_Preview">K-1</span><script type="math/tex">K-1</script></span> equations with different weights for each possible outcome:</p>
</li>
</ul>
<p align="center"><br>$\begin{eqnarray}
\ln\frac{P(y = 1)}{P(y = K)} = \vec w_1 \cdot \vec x & \Rightarrow & P(y=1) = P(y=K)e^{\vec w_1 \cdot \vec x} \\
\ln\frac{P(y = 2)}{P(y = K)} = \vec w_2 \cdot \vec x & \Rightarrow & P(y=2) = P(y=K)e^{\vec w_2 \cdot \vec x} \\
                                                     & ... & \\
\ln\frac{P(y = K-1)}{P(y = K)} = \vec w_{K-1} \cdot \vec x & \Rightarrow & P(y=K-1) = P(y=K)e^{\vec w_{K-1} \cdot \vec x} \\
\end{eqnarray}$</p>

<p><br></p>
<ul>
<li>
<p>And as they have to sum to <span><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span> we get: <p align="center"><br><span><span class="MathJax_Preview">P(y=K) = 1 - \sum\limits_{i=1}^{K-1}P(y=K)e^{\vec w_i \cdot \vec x} \Rightarrow P(y = K) = \frac{1}{1 + \sum\limits_{i=1}^{K-1}e^{\vec w_i \cdot \vec x}}</span><script type="math/tex">P(y=K) = 1 - \sum\limits_{i=1}^{K-1}P(y=K)e^{\vec w_i \cdot \vec x} \Rightarrow P(y = K) = \frac{1}{1 + \sum\limits_{i=1}^{K-1}e^{\vec w_i \cdot \vec x}}</script></span></p><br></p>
</li>
<li>
<p>Thus, the probability of <span><span class="MathJax_Preview">y = j</span><script type="math/tex">y = j</script></span>, for <span><span class="MathJax_Preview">j \in \left\{1, \cdots, K-1\right\}</span><script type="math/tex">j \in \left\{1, \cdots, K-1\right\}</script></span> is given by: <p align="center"><br><span><span class="MathJax_Preview">P(y=j) = \frac{e^{\vec w_j \cdot \vec x}}{1 + \sum\limits_{i=1}^{K-1}e^{\vec w_i \cdot \vec x}}</span><script type="math/tex">P(y=j) = \frac{e^{\vec w_j \cdot \vec x}}{1 + \sum\limits_{i=1}^{K-1}e^{\vec w_i \cdot \vec x}}</script></span></p><br></p>
</li>
</ul>
<h4 id="check-binary-case">Check binary case<a class="headerlink" href="#check-binary-case" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p>Lets consider binary classification with <span><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span> being a pivot</p>
</li>
<li>
<p>Then we have: <p align="center"><br><span><span class="MathJax_Preview">p = P(y=1) = \frac{e^{\vec w \cdot \vec x}}{1 + e^{\vec w \cdot \vec x}} = \frac{1}{1 + e^{-\vec w \cdot \vec x}}</span><script type="math/tex">p = P(y=1) = \frac{e^{\vec w \cdot \vec x}}{1 + e^{\vec w \cdot \vec x}} = \frac{1}{1 + e^{-\vec w \cdot \vec x}}</script></span></p><br></p>
</li>
<li>
<p>so the same result as before</p>
</li>
</ul>
<h3 id="softmax-approach">Softmax approach<a class="headerlink" href="#softmax-approach" title="Permanent link">&para;</a></h3>
<ul>
<li>An alternative approach (which works for any number of classes) is to consider each class separately with its own parameters set and include the normalization factor ensuring that we get a probability distribution:</li>
</ul>
<p align="center"><br>$\begin{eqnarray}
\ln P(y = 1) = \vec w_1 \cdot \vec x - \ln Z & \Rightarrow & P(y=1) = \frac{1}{Z}e^{\vec w_1 \cdot \vec x} \\
\ln P(y = 2) = \vec w_2 \cdot \vec x - \ln Z & \Rightarrow & P(y=2) = \frac{1}{Z}e^{\vec w_2 \cdot \vec x} \\
                                                     & ... & \\
\ln P(y = K) = \vec w_K \cdot \vec x - \ln Z & \Rightarrow & P(y=K) = \frac{1}{Z}e^{\vec w_K \cdot \vec x} \\
\end{eqnarray}$</p>

<p><br></p>
<ul>
<li>
<p>As they have to sum to <span><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span>: <p align="center"><br><span><span class="MathJax_Preview">\frac{1}{Z}\sum\limits_{i=1}^Ke^{\vec w_i \cdot \vec x} = 1 \Rightarrow Z = \sum\limits_{i=1}^Ke^{\vec w_i \cdot \vec x}</span><script type="math/tex">\frac{1}{Z}\sum\limits_{i=1}^Ke^{\vec w_i \cdot \vec x} = 1 \Rightarrow Z = \sum\limits_{i=1}^Ke^{\vec w_i \cdot \vec x}</script></span></p><br></p>
</li>
<li>
<p>Thus, the probability of <span><span class="MathJax_Preview">y = j</span><script type="math/tex">y = j</script></span>, for <span><span class="MathJax_Preview">j \in \left\{1, \cdots, K\right\}</span><script type="math/tex">j \in \left\{1, \cdots, K\right\}</script></span> is given by: <p align="center"><br><span><span class="MathJax_Preview">P(y=j) = \frac{e^{\vec w_j \cdot \vec x}}{\sum\limits_{i=1}^{K}e^{\vec w_i \cdot \vec x}}</span><script type="math/tex">P(y=j) = \frac{e^{\vec w_j \cdot \vec x}}{\sum\limits_{i=1}^{K}e^{\vec w_i \cdot \vec x}}</script></span></p><br></p>
</li>
<li>
<p>which is called <strong>softmax function</strong></p>
</li>
</ul>
<h4 id="check-binary-case_1">Check binary case<a class="headerlink" href="#check-binary-case_1" title="Permanent link">&para;</a></h4>
<ul>
<li>For <span><span class="MathJax_Preview">y \in \left\{0, 1\right\}</span><script type="math/tex">y \in \left\{0, 1\right\}</script></span> we have:</li>
</ul>
<p align="center"><br>$\begin{eqnarray}
P(y = 0) & = & \frac{e^{\vec w_0 \cdot \vec x}}{e^{\vec w_0 \cdot \vec x} + e^{\vec w_1 \cdot \vec x}} \\
P(y = 1) & = & \frac{e^{\vec w_1 \cdot \vec x}}{e^{\vec w_0 \cdot \vec x} + e^{\vec w_1 \cdot \vec x}}
\end{eqnarray}$</p>

<p><br></p>
<ul>
<li>
<p>Please note, that this model is overspecified! <span><span class="MathJax_Preview">\rightarrow</span><script type="math/tex">\rightarrow</script></span> <span><span class="MathJax_Preview">P(y = 0) + P(y = 1) = 1</span><script type="math/tex">P(y = 0) + P(y = 1) = 1</script></span> (always)</p>
</li>
<li>
<p>That means, that once we have one probability the other is given, so we can choose one of <span><span class="MathJax_Preview">\vec w_i</span><script type="math/tex">\vec w_i</script></span> arbitrary - lets choose <span><span class="MathJax_Preview">\vec w_0 = 0</span><script type="math/tex">\vec w_0 = 0</script></span> (and <span><span class="MathJax_Preview">\vec w_1 \equiv \vec w</span><script type="math/tex">\vec w_1 \equiv \vec w</script></span>): <p align="center"><br><span><span class="MathJax_Preview">P(y = 1) = \frac{e^{\vec w \cdot \vec x}}{1 + e^{\vec w\cdot x}} = \frac{1}{1 + e^{-\vec w \cdot x}}</span><script type="math/tex">P(y = 1) = \frac{e^{\vec w \cdot \vec x}}{1 + e^{\vec w\cdot x}} = \frac{1}{1 + e^{-\vec w \cdot x}}</script></span></p><br></p>
</li>
</ul>
<h3 id="cost-function_1">Cost function<a class="headerlink" href="#cost-function_1" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>Lets recall that in our notation we have:</p>
<ul>
<li>
<p>dataset: <span><span class="MathJax_Preview">\left\{(\vec x^{(i)}, y^{(i)}); i = 1,\cdots, N\right\}</span><script type="math/tex">\left\{(\vec x^{(i)}, y^{(i)}); i = 1,\cdots, N\right\}</script></span></p>
</li>
<li>
<p>features: <span><span class="MathJax_Preview">\vec x^{(i)} = (1, x^{(i)}_1,\cdots, x^{(i)}_n)</span><script type="math/tex">\vec x^{(i)} = (1, x^{(i)}_1,\cdots, x^{(i)}_n)</script></span></p>
</li>
<li>
<p>targets: <span><span class="MathJax_Preview">y^{(i)} \in \{1, \cdots, K\}</span><script type="math/tex">y^{(i)} \in \{1, \cdots, K\}</script></span></p>
</li>
</ul>
</li>
<li>
<p>For every possible outcome we have a corresponding vector of weights <span><span class="MathJax_Preview">\vec w_j</span><script type="math/tex">\vec w_j</script></span> (for <span><span class="MathJax_Preview">j = 1, \cdots, K</span><script type="math/tex">j = 1, \cdots, K</script></span>) - so in fact we have a matrix of parameters (<span><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span>)</p>
</li>
<li>
<p>The hypothesis is given by a vector: <p align="center"><br><span><span class="MathJax_Preview">h(\vec x) = \left[\begin{array}{c}\frac{e^{\vec w_1 \cdot \vec x}}{\sum\limits_{i=1}^{K}e^{\vec w_i \cdot \vec x}} \\ \frac{e^{\vec w_2 \cdot \vec x}}{\sum\limits_{i=1}^{K}e^{\vec w_i \cdot \vec x}} \\ ... \\ \frac{e^{\vec w_K \cdot \vec x}}{\sum\limits_{i=1}^{K}e^{\vec w_i \cdot \vec x}}\end{array}\right]</span><script type="math/tex">h(\vec x) = \left[\begin{array}{c}\frac{e^{\vec w_1 \cdot \vec x}}{\sum\limits_{i=1}^{K}e^{\vec w_i \cdot \vec x}} \\ \frac{e^{\vec w_2 \cdot \vec x}}{\sum\limits_{i=1}^{K}e^{\vec w_i \cdot \vec x}} \\ ... \\ \frac{e^{\vec w_K \cdot \vec x}}{\sum\limits_{i=1}^{K}e^{\vec w_i \cdot \vec x}}\end{array}\right]</script></span></p><br></p>
</li>
<li>
<p>The prediction for unseen sample is done using <code>argmax</code> function</p>
</li>
<li>
<p>As before we define the cost function as the negative log-likelihood: <p align="center"><br><span><span class="MathJax_Preview">L(W) = -\frac{1}{N}\sum\limits_{i=1}^N\ln\left[\frac{e^{\vec w_{y^{(i)}} \cdot \vec x^{(i)}}}{\sum\limits_{j=1}^{K}e^{\vec w_j \cdot \vec x^{(i)}}}\right] = -\frac{1}{N}\sum\limits_{i=1}^N\left[\vec w_{y^{(i)}} \cdot \vec x^{(i)} - \ln\sum\limits_{j=1}^{K}e^{\vec w_j \cdot \vec x^{(i)}}\right]</span><script type="math/tex">L(W) = -\frac{1}{N}\sum\limits_{i=1}^N\ln\left[\frac{e^{\vec w_{y^{(i)}} \cdot \vec x^{(i)}}}{\sum\limits_{j=1}^{K}e^{\vec w_j \cdot \vec x^{(i)}}}\right] = -\frac{1}{N}\sum\limits_{i=1}^N\left[\vec w_{y^{(i)}} \cdot \vec x^{(i)} - \ln\sum\limits_{j=1}^{K}e^{\vec w_j \cdot \vec x^{(i)}}\right]</script></span></p><br></p>
</li>
</ul>
<h4 id="gradient">Gradient<a class="headerlink" href="#gradient" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p>We need to calculate the partial derivative for each parameter <span><span class="MathJax_Preview">w_{ab}</span><script type="math/tex">w_{ab}</script></span> </p>
<ul>
<li>
<p><span><span class="MathJax_Preview">a = 1, \cdots, K</span><script type="math/tex">a = 1, \cdots, K</script></span> (possible outcome <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>)</p>
</li>
<li>
<p><span><span class="MathJax_Preview">b = 0, \cdots, n</span><script type="math/tex">b = 0, \cdots, n</script></span> (<span><span class="MathJax_Preview">\vec x</span><script type="math/tex">\vec x</script></span> coordinate)</p>
</li>
</ul>
</li>
<li>
<p>For the first term we have: <p align="center"><br><span><span class="MathJax_Preview">\frac{\partial}{\partial w_{ab}} \vec w_{y^{(i)}} \cdot \vec x^{(i)} = [y^{(i)} = a]x^{(i)}_b</span><script type="math/tex">\frac{\partial}{\partial w_{ab}} \vec w_{y^{(i)}} \cdot \vec x^{(i)} = [y^{(i)} = a]x^{(i)}_b</script></span></p><br></p>
</li>
<li>
<p>For the second term we have:</p>
</li>
</ul>
<p align="center"><br>$\begin{eqnarray}
\frac{\partial}{\partial w_{ab}} \ln\sum\limits_{j=1}^{K}e^{\vec w_j \cdot \vec x^{(i)}} & = & \frac{\sum\limits_{j=1}^{K}e^{\vec w_j \cdot \vec x^{(i)}} \cdot [y^{(i)} = a] \cdot x^{(i)}_b}{\sum\limits_{j=1}^{K}e^{\vec w_j \cdot \vec x^{(i)}}} \\ & = & \sum\limits_{j=1}^{K}\left[\frac{e^{\vec w_j \cdot \vec x^{(i)}}}{\sum\limits_{j=1}^{K}e^{\vec w_j \cdot \vec x^{(i)}}}\cdot [y^{(i)} = a] \cdot x^{(i)}_b\right]\\ & = & \sum\limits_{j=1}^{K}\left[P(y = j~|~\vec x^{(i)})\cdot [y^{(i)} = a] \cdot x^{(i)}_b\right] \\ & = & P(y=a~|~\vec x^{(i)})x^{(i)}_b
\end{eqnarray}$</p>

<p><br></p>
<ul>
<li>
<p>Finally, the gradient of the cost function is given by: <p align="center"><br><span><span class="MathJax_Preview">\frac{\partial L(W)}{\partial w_{ab}} = -\frac{1}{N}\sum\limits_{i=1}^N\left[[y^{(i)}=a]x^{(i)}_b - P(y=a~|~\vec x^{(i)})x^{(i)}_b\right]</span><script type="math/tex">\frac{\partial L(W)}{\partial w_{ab}} = -\frac{1}{N}\sum\limits_{i=1}^N\left[[y^{(i)}=a]x^{(i)}_b - P(y=a~|~\vec x^{(i)})x^{(i)}_b\right]</script></span></p><br></p>
</li>
<li>
<p>And for every iteration of the gradient descent method weights are updated according to: <p align="center"><br><span><span class="MathJax_Preview">w_{ab} \rightarrow w_{ab} - \alpha\frac{\partial L(W)}{\partial w_{ab}}</span><script type="math/tex">w_{ab} \rightarrow w_{ab} - \alpha\frac{\partial L(W)}{\partial w_{ab}}</script></span></p><br></p>
</li>
</ul>
<h3 id="example_2">Example<a class="headerlink" href="#example_2" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>Let a student have two features:</p>
<ul>
<li>
<p>initial knowledge: <span><span class="MathJax_Preview">x_1 \in [0, 100]</span><script type="math/tex">x_1 \in [0, 100]</script></span></p>
</li>
<li>
<p>hours spent studying: <span><span class="MathJax_Preview">x_2 \in [0, 50]</span><script type="math/tex">x_2 \in [0, 50]</script></span></p>
</li>
</ul>
</li>
<li>
<p>And, based on these two features, a grade can be assigned to a student:</p>
<ul>
<li>target: <span><span class="MathJax_Preview">y \in \left\{2, 3, 4, 5\right\}</span><script type="math/tex">y \in \left\{2, 3, 4, 5\right\}</script></span></li>
</ul>
</li>
</ul>
<h4 id="dataset">Dataset<a class="headerlink" href="#dataset" title="Permanent link">&para;</a></h4>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">grade</span><span class="p">(</span><span class="n">init_know</span><span class="p">,</span> <span class="n">study_time</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Arbitrary grading system.&quot;&quot;&quot;</span>
  <span class="n">score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">init_know</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">study_time</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">score</span> <span class="o">&gt;</span> <span class="mi">90</span><span class="p">:</span> <span class="k">return</span> <span class="mi">3</span>    <span class="c1"># bdb</span>
  <span class="k">elif</span> <span class="n">score</span> <span class="o">&gt;</span> <span class="mi">70</span><span class="p">:</span> <span class="k">return</span> <span class="mi">2</span>  <span class="c1"># db</span>
  <span class="k">elif</span> <span class="n">score</span> <span class="o">&gt;</span> <span class="mi">50</span><span class="p">:</span> <span class="k">return</span> <span class="mi">1</span>  <span class="c1"># dst</span>
  <span class="k">else</span><span class="p">:</span> <span class="k">return</span> <span class="mi">0</span>             <span class="c1"># ndst</span>
</pre></div>


<ul>
<li>The training set</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">1000</span>  <span class="c1"># number of students</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">50</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">grade</span><span class="p">(</span><span class="o">*</span><span class="n">student</span><span class="p">)</span> <span class="k">for</span> <span class="n">student</span> <span class="ow">in</span> <span class="n">X</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int32&#39;</span><span class="p">)</span>
</pre></div>


<div class="codehilite"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Initial knowledge&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Study time&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">student</span><span class="p">,</span> <span class="n">g</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">student</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">g</span><span class="p">),</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="../output_65_0.png" /></p>
<h4 id="data-preparation">Data preparation<a class="headerlink" href="#data-preparation" title="Permanent link">&para;</a></h4>
<ul>
<li>For the training process we scale features to <span><span class="MathJax_Preview">[0, 1]</span><script type="math/tex">[0, 1]</script></span> - otherwise <em>initial knowledge</em> would weight more!</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="o">/</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">50</span><span class="p">]))</span>
</pre></div>


<ul>
<li>Lets add 1 for bias term to the dataset</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">X_train</span><span class="p">))</span>
</pre></div>


<ul>
<li>How does it look?</li>
</ul>
<div class="codehilite"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s2">&quot;Original:&quot;</span><span class="p">,</span> <span class="n">X</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="s2">&quot;Preprocessed:&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">sep</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>


<div class="codehilite"><pre><span></span>Original:

[[ 7.47319311 25.86221876]
 [21.66198394 33.34522121]
 [69.87399886  4.53759853]
 [75.83844581 22.05176574]
 [17.38539272 47.02800299]]

Preprocessed:

[[1.         0.07473193 0.51724438]
 [1.         0.21661984 0.66690442]
 [1.         0.69873999 0.09075197]
 [1.         0.75838446 0.44103531]
 [1.         0.17385393 0.94056006]]
</pre></div>


<h4 id="training">Training<a class="headerlink" href="#training" title="Permanent link">&para;</a></h4>
<ul>
<li>The implementation of MLR in <code>theano</code></li>
</ul>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">theano</span>
<span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="kn">as</span> <span class="nn">T</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>  <span class="c1"># feature vectors</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">ivector</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span> <span class="c1"># target vector</span>

<span class="n">W</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>  <span class="c1"># weight matrix (2 features + bias,</span>
                                          <span class="c1">#                4 possible outcomes)</span>

<span class="n">hypo</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">W</span><span class="p">))</span>                     <span class="c1"># hyphothesis</span>
<span class="n">cost</span> <span class="o">=</span> <span class="o">-</span><span class="n">T</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">hypo</span><span class="p">)[</span><span class="n">T</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">y</span><span class="p">])</span>  <span class="c1"># cost function</span>
<span class="n">grad_W</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">cost</span><span class="o">=</span><span class="n">cost</span><span class="p">,</span> <span class="n">wrt</span><span class="o">=</span><span class="n">W</span><span class="p">)</span>                     <span class="c1"># gradients</span>

<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># learning rate</span>

<span class="c1"># define a training step</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">],</span>
                        <span class="n">outputs</span> <span class="o">=</span> <span class="n">cost</span><span class="p">,</span>
                        <span class="n">updates</span> <span class="o">=</span> <span class="p">[(</span><span class="n">W</span><span class="p">,</span> <span class="n">W</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_W</span><span class="p">)]</span>
                       <span class="p">)</span>

<span class="c1"># predict a class label</span>
<span class="n">predict</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">T</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">hypo</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</pre></div>


<ul>
<li>The training process on normalized data</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">acc_train</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># accuracy on training dataset</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
  <span class="c1"># do a single step of gradient descent</span>
  <span class="n">train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
  <span class="c1"># calculate accuracy with current set of weights</span>
  <span class="n">acc_train</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">Y</span> <span class="o">==</span> <span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>


<div class="codehilite"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Cost&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">acc_train</span><span class="p">)),</span> <span class="n">acc_train</span><span class="p">);</span>
</pre></div>


<p><img alt="png" src="../output_78_0.png" /></p>
<h4 id="validation">Validation<a class="headerlink" href="#validation" title="Permanent link">&para;</a></h4>
<ul>
<li>First we need <strong>unseen</strong> data for testing</li>
</ul>
<div class="codehilite"><pre><span></span><span class="c1"># another set of students</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">50</span><span class="p">]</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">grade</span><span class="p">(</span><span class="o">*</span><span class="n">student</span><span class="p">)</span> <span class="k">for</span> <span class="n">student</span> <span class="ow">in</span> <span class="n">X_test</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int32&#39;</span><span class="p">)</span>

<span class="c1"># normalize and add bias </span>
<span class="n">X_test_normalized</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="o">/</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">50</span><span class="p">]))</span>
<span class="n">X_test_normalized</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">X_test_normalized</span><span class="p">))</span>
</pre></div>


<ul>
<li>To predict a grade we use the function <code>predict</code> defined earlier</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">Y_pred</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">X_test_normalized</span><span class="p">)</span>
</pre></div>


<ul>
<li>We can visualize the prediction</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Initial knowledge&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Study time&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">student</span><span class="p">,</span> <span class="n">g</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">):</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">student</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">g</span><span class="p">),</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="../output_85_0.png" /></p>
<ul>
<li>and calculate the accuracy</li>
</ul>
<div class="codehilite"><pre><span></span><span class="p">(</span><span class="n">Y_test</span> <span class="o">==</span> <span class="n">Y_pred</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">Y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>


<div class="codehilite"><pre><span></span>0.906
</pre></div>


<h4 id="softmax-visualization">Softmax visualization<a class="headerlink" href="#softmax-visualization" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p>Lets visualize what the model has just learned</p>
</li>
<li>
<p>First we need an easy way to calculate softmax output for a student</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">softmax</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">hypo</span><span class="p">)</span>
</pre></div>


<ul>
<li>Now, we can use it on the validation dataset</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">probs</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">X_test_normalized</span><span class="p">)</span>
</pre></div>


<ul>
<li>For every sample softmax returns an array of the probabilities of belonging to each class</li>
</ul>
<div class="codehilite"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">probs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>


<div class="codehilite"><pre><span></span>(1000, 4)
</pre></div>


<ul>
<li>We can plot each class separately</li>
</ul>
<div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>

<span class="n">grades</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;ndst&quot;</span><span class="p">,</span> <span class="s2">&quot;dst&quot;</span><span class="p">,</span> <span class="s2">&quot;db&quot;</span><span class="p">,</span> <span class="s2">&quot;bdb&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
  <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>

  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Initial knowledge&quot;</span><span class="p">,</span>  <span class="n">labelpad</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Study time&quot;</span><span class="p">,</span>  <span class="n">labelpad</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Grade: &quot;</span> <span class="o">+</span> <span class="n">grades</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

  <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_test</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">probs</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="../output_96_0.png" /></p>
<p><img alt="png" src="../output_96_1.png" /></p>
<p><img alt="png" src="../output_96_2.png" /></p>
<p><img alt="png" src="../output_96_3.png" /></p>
<h2 id="neural-networks">Neural Networks<a class="headerlink" href="#neural-networks" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p>Here are some helpful functions to draw neural networks</p>
</li>
<li>
<p>Lets just skip them - it is just bunch of matplotlib</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="c1"># the functions below grabbed from http://www.astroml.org/book_figures/appendix/fig_neural_network.html</span>

<span class="n">radius</span> <span class="o">=</span> <span class="mf">0.3</span>

<span class="n">arrow_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">head_width</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">draw_connecting_arrow</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">circ1</span><span class="p">,</span> <span class="n">rad1</span><span class="p">,</span> <span class="n">circ2</span><span class="p">,</span> <span class="n">rad2</span><span class="p">):</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arctan2</span><span class="p">(</span><span class="n">circ2</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">circ1</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                       <span class="n">circ2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">circ1</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">starting_point</span> <span class="o">=</span> <span class="p">(</span><span class="n">circ1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">rad1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span>
                      <span class="n">circ1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">rad1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>

    <span class="n">length</span> <span class="o">=</span> <span class="p">(</span><span class="n">circ2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">circ1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">rad1</span> <span class="o">+</span> <span class="mf">1.4</span> <span class="o">*</span> <span class="n">rad2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span>
              <span class="n">circ2</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">circ1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">rad1</span> <span class="o">+</span> <span class="mf">1.4</span> <span class="o">*</span> <span class="n">rad2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">starting_point</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">starting_point</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
             <span class="n">length</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">length</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">**</span><span class="n">arrow_kwargs</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">draw_circle</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">center</span><span class="p">,</span> <span class="n">radius</span><span class="p">):</span>
    <span class="n">circ</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">(</span><span class="n">center</span><span class="p">,</span> <span class="n">radius</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">circ</span><span class="p">)</span>
</pre></div>


<div class="codehilite"><pre><span></span><span class="c1"># based on borrowed function we can create a new one to draw NN</span>

<span class="k">def</span> <span class="nf">draw_net</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">hidden_layers</span><span class="o">=</span><span class="p">[],</span> <span class="n">w</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Draw a network&quot;&quot;&quot;</span>
  <span class="n">x</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># initial layer position</span>

  <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span> <span class="o">+</span> <span class="n">w</span><span class="p">])</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="n">h</span> <span class="o">/</span> <span class="mi">2</span> <span class="p">,</span> <span class="n">h</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>

  <span class="c1"># set y position  </span>
  <span class="n">y_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">input_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="n">y_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">output_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">output_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="n">y_hidden</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">hidden_layers</span><span class="p">]</span>

  <span class="c1"># draw input layer</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s2">&quot;Input</span><span class="se">\n</span><span class="s2">Layer&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;top&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">y_input</span><span class="p">):</span>
    <span class="n">draw_circle</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">radius</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mf">0.9</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;$x_</span><span class="si">%i</span><span class="s1">$&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">input_size</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">i</span><span class="p">),</span>
            <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">draw_connecting_arrow</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mf">0.9</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="mf">0.1</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">radius</span><span class="p">)</span>

  <span class="n">last_layer</span> <span class="o">=</span> <span class="n">y_input</span>  <span class="c1"># last layer y positions</span>

  <span class="c1"># draw hidden layers</span>
  <span class="k">for</span> <span class="n">ys</span> <span class="ow">in</span> <span class="n">y_hidden</span><span class="p">:</span>
    <span class="c1"># shift x</span>
    <span class="n">x</span> <span class="o">+=</span> <span class="mi">2</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s2">&quot;Hidden</span><span class="se">\n</span><span class="s2">Layer&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;top&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

    <span class="c1"># draw neurons for each hidden layer</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">y1</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ys</span><span class="p">):</span>
      <span class="n">draw_circle</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">),</span> <span class="n">radius</span><span class="p">)</span>

      <span class="c1"># connect a neuron with all neurons from previous layer</span>
      <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span> <span class="c1"># skip bias</span>
        <span class="k">for</span> <span class="n">y2</span> <span class="ow">in</span> <span class="n">last_layer</span><span class="p">:</span>
          <span class="n">draw_connecting_arrow</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="n">y2</span><span class="p">),</span> <span class="n">radius</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">),</span> <span class="n">radius</span><span class="p">)</span>

    <span class="c1"># update last layer</span>
    <span class="n">last_layer</span> <span class="o">=</span> <span class="n">ys</span>

  <span class="n">x</span> <span class="o">+=</span> <span class="mi">2</span>  <span class="c1"># update position for output layer</span>

  <span class="c1"># draw output layer</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s2">&quot;Output</span><span class="se">\n</span><span class="s2">Layer&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;top&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">y1</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">y_output</span><span class="p">):</span>
    <span class="n">draw_circle</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">),</span> <span class="n">radius</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="s1">&#39;Output&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">draw_connecting_arrow</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">),</span> <span class="n">radius</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">y1</span><span class="p">),</span> <span class="mf">0.1</span><span class="p">)</span>

    <span class="c1"># connect each output neuron with all neurons from previous layer</span>
    <span class="k">for</span> <span class="n">y2</span> <span class="ow">in</span> <span class="n">last_layer</span><span class="p">:</span>
      <span class="n">draw_connecting_arrow</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="n">y2</span><span class="p">),</span> <span class="n">radius</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">),</span> <span class="n">radius</span><span class="p">)</span>
</pre></div>


<h3 id="neuron">Neuron<a class="headerlink" href="#neuron" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>As mentioned at the beginning, we are going to discuss feedforward neural networks</p>
</li>
<li>
<p>Lets start with a single neuron</p>
</li>
<li>
<p>What we were doing so far</p>
<ul>
<li>
<p>We had some training samples with <span><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span> features</p>
</li>
<li>
<p>We assumed linear model</p>
</li>
<li>
<p>We connected features with outcomes using linear, logistic or softmax function</p>
</li>
</ul>
</li>
<li>
<p>Thus, we considered somthing like this:</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">draw_net</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="../output_103_0.png" /></p>
<ul>
<li>
<p><em>Output circle</em> represents a neuron</p>
</li>
<li>
<p>Arrows represent connections (weights)</p>
</li>
<li>
<p>A neuron is defined by an activation function, e.g.:</p>
<ul>
<li>
<p>binary step</p>
</li>
<li>
<p>logistic function</p>
</li>
<li>
<p>hyperbolic tangent</p>
</li>
<li>
<p>rectified linear unit (ReLU)</p>
</li>
</ul>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>

<span class="k">def</span> <span class="nf">binary_step</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">1</span>

<span class="k">def</span> <span class="nf">logistic</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">x</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="n">bs</span> <span class="o">=</span> <span class="p">[</span><span class="n">binary_step</span><span class="p">(</span><span class="n">x_</span><span class="p">)</span> <span class="k">for</span> <span class="n">x_</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
<span class="n">lf</span> <span class="o">=</span> <span class="p">[</span><span class="n">logistic</span><span class="p">(</span><span class="n">x_</span><span class="p">)</span> <span class="k">for</span> <span class="n">x_</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
<span class="n">th</span> <span class="o">=</span> <span class="p">[</span><span class="n">tanh</span><span class="p">(</span><span class="n">x_</span><span class="p">)</span> <span class="k">for</span> <span class="n">x_</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
<span class="n">re</span> <span class="o">=</span> <span class="p">[</span><span class="n">relu</span><span class="p">(</span><span class="n">x_</span><span class="p">)</span> <span class="k">for</span> <span class="n">x_</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>

<span class="n">_</span><span class="p">,</span> <span class="p">((</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">),</span> <span class="p">(</span><span class="n">ax3</span><span class="p">,</span> <span class="n">ax4</span><span class="p">))</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Binary step&quot;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;TanH&quot;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Logistic&quot;</span><span class="p">)</span>
<span class="n">ax4</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;ReLU&quot;</span><span class="p">)</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">bs</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lf</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">th</span><span class="p">)</span>
<span class="n">ax4</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">re</span><span class="p">);</span>
</pre></div>


<p><img alt="png" src="../output_105_0.png" /></p>
<h3 id="neural-networks_1">Neural networks<a class="headerlink" href="#neural-networks_1" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>Imagine that the output of a neuron is an input for another neuron</p>
</li>
<li>
<p>This way we can create another layer of neurons (hidden layer) which would be an input for the output layer</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">draw_net</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">w</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="../output_108_0.png" /></p>
<ul>
<li>Or we could get carried away</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">draw_net</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">w</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="../output_110_0.png" /></p>
<ul>
<li>
<p>This way we can solve non-linear problems</p>
</li>
<li>
<p>In general, the more the problem is complex the more neurons we need</p>
</li>
<li>
<p>The numbers of hidden layers and hidden neurons are <strong>hyperparameters</strong> of the model</p>
<ul>
<li>
<p>If the NN is too small - underfitting</p>
</li>
<li>
<p>It the NN is too large - overfitting</p>
</li>
</ul>
</li>
<li>
<p>Plase note, that we may have also many possible outcomes through e.g. softmax</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">draw_net</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">w</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="../output_112_0.png" /></p>
<ul>
<li>How to train this monster?</li>
</ul>
<h3 id="backpropagation">Backpropagation<a class="headerlink" href="#backpropagation" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>We can use the gradient descent method, but now we need to propagate the error through all layers:</p>
<ul>
<li>
<p>Input -&gt; forward propagation -&gt; error</p>
</li>
<li>
<p>Backpropagate the error -&gt; update weights</p>
</li>
</ul>
</li>
<li>
<p>Lets see how it works on a simple example</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">draw_net</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">w</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="../output_117_0.png" /></p>
<ul>
<li>
<p>The input is given by: <span><span class="MathJax_Preview">x_0 = 1</span><script type="math/tex">x_0 = 1</script></span>, <span><span class="MathJax_Preview">x_1</span><script type="math/tex">x_1</script></span>, <span><span class="MathJax_Preview">x_2</span><script type="math/tex">x_2</script></span></p>
</li>
<li>
<p>There are 2 hidden neurons + bias: <span><span class="MathJax_Preview">h_0 = 1</span><script type="math/tex">h_0 = 1</script></span>, <span><span class="MathJax_Preview">h_1</span><script type="math/tex">h_1</script></span>, <span><span class="MathJax_Preview">h_2</span><script type="math/tex">h_2</script></span></p>
</li>
<li>
<p>And two possible outcomes: <span><span class="MathJax_Preview">o_1</span><script type="math/tex">o_1</script></span>, <span><span class="MathJax_Preview">o_2</span><script type="math/tex">o_2</script></span></p>
</li>
<li>
<p>The input layer is connected with the hidden layer by weights: <span><span class="MathJax_Preview">w^{(1)}_{ij}</span><script type="math/tex">w^{(1)}_{ij}</script></span>, where <span><span class="MathJax_Preview">i = 0, 1, 2</span><script type="math/tex">i = 0, 1, 2</script></span> and <span><span class="MathJax_Preview">j = 1, 2</span><script type="math/tex">j = 1, 2</script></span>, e.g.</p>
<ul>
<li><span><span class="MathJax_Preview">w^{(1)}_{12}</span><script type="math/tex">w^{(1)}_{12}</script></span> is the weight connecting <span><span class="MathJax_Preview">x_1</span><script type="math/tex">x_1</script></span> and <span><span class="MathJax_Preview">h_2</span><script type="math/tex">h_2</script></span></li>
</ul>
</li>
<li>
<p>The hidden layer is connected with the output layer by weights: <span><span class="MathJax_Preview">w^{(2)}_{ij}</span><script type="math/tex">w^{(2)}_{ij}</script></span></p>
</li>
<li>
<p>The input for a neuron <span><span class="MathJax_Preview">h_k</span><script type="math/tex">h_k</script></span> is given by: <p align="center"><br><span><span class="MathJax_Preview">h_{k, in} = w^{(1)}_{0k} + w^{(1)}_{1k} \cdot x_1 + w^{(1)}_{2k} \cdot x_2</span><script type="math/tex">h_{k, in} = w^{(1)}_{0k} + w^{(1)}_{1k} \cdot x_1 + w^{(1)}_{2k} \cdot x_2</script></span></p><br></p>
</li>
<li>
<p>And the output: <p align="center"><br><span><span class="MathJax_Preview">h_{k, out} = \left(1 + \exp(-h_{k, in})\right)^{-1}</span><script type="math/tex">h_{k, out} = \left(1 + \exp(-h_{k, in})\right)^{-1}</script></span></p><br></p>
</li>
<li>
<p>The input for the outcome <span><span class="MathJax_Preview">o_k</span><script type="math/tex">o_k</script></span> is given by: <p align="center"><br><span><span class="MathJax_Preview">o_{k, in} = w^{(2)}_{0k} + w^{(2)}_{1k} \cdot h_{1, out} + w^{(2)}_{2k} \cdot h_{2, out}</span><script type="math/tex">o_{k, in} = w^{(2)}_{0k} + w^{(2)}_{1k} \cdot h_{1, out} + w^{(2)}_{2k} \cdot h_{2, out}</script></span></p><br></p>
</li>
<li>
<p>And the output: <p align="center"><br><span><span class="MathJax_Preview">o_{k, out} = \left(1 + \exp(-o_{k, in})\right)^{-1}</span><script type="math/tex">o_{k, out} = \left(1 + \exp(-o_{k, in})\right)^{-1}</script></span></p><br></p>
</li>
<li>
<p>Lets define the cost function: <p align="center"><br><span><span class="MathJax_Preview">L(w) = \frac{1}{2}\left(o_{1, true} - o_{1, out}\right)^2 + \frac{1}{2}\left(o_{2, true} - o_{2, out}\right)^2</span><script type="math/tex">L(w) = \frac{1}{2}\left(o_{1, true} - o_{1, out}\right)^2 + \frac{1}{2}\left(o_{2, true} - o_{2, out}\right)^2</script></span></p><br></p>
</li>
<li>
<p>To update weights using the gradient descent method we need to calculate <span><span class="MathJax_Preview">\partial L(w) / \partial w^{(a)}_{ij}</span><script type="math/tex">\partial L(w) / \partial w^{(a)}_{ij}</script></span></p>
</li>
<li>
<p>As an example, let consider updating <span><span class="MathJax_Preview">w^{(2)}_{11}</span><script type="math/tex">w^{(2)}_{11}</script></span>: <p align="center"><br><span><span class="MathJax_Preview">\frac{\partial L(w)}{\partial w^{(2)}_{11}} = \frac{\partial L(w)}{\partial o_{1, out}}\cdot\frac{\partial o_{1, out}}{\partial o_{1, in}}\cdot\frac{\partial o_{1, in}}{\partial w^{(2)}_{11}}</span><script type="math/tex">\frac{\partial L(w)}{\partial w^{(2)}_{11}} = \frac{\partial L(w)}{\partial o_{1, out}}\cdot\frac{\partial o_{1, out}}{\partial o_{1, in}}\cdot\frac{\partial o_{1, in}}{\partial w^{(2)}_{11}}</script></span></p><br></p>
</li>
</ul>
<h2 id="and-or-vs-xor">AND, OR vs XOR<a class="headerlink" href="#and-or-vs-xor" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p>I will assume that basic logic gates do not need introduction</p>
</li>
<li>
<p>The point here is that AND and OR are linearly separable, and XOR is not</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">Y_and</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">Y_or</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">Y_xor</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

<span class="n">titles</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;AND&quot;</span><span class="p">,</span> <span class="s2">&quot;OR&quot;</span><span class="p">,</span> <span class="s2">&quot;XOR&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">Y</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">Y_and</span><span class="p">,</span> <span class="n">Y_or</span><span class="p">,</span> <span class="n">Y_xor</span><span class="p">]):</span>
  <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span>

  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>

  <span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">titles</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">X</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
  <span class="k">elif</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">])</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;?&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="../output_121_0.png" /></p>
<h3 id="single-neuron-approach">Single neuron approach<a class="headerlink" href="#single-neuron-approach" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span><span class="n">draw_net</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="../output_123_0.png" /></p>
<ul>
<li>
<p>Let logistic function be our hypothesis: <p align="center"><br><span><span class="MathJax_Preview">h(x_1, x_2) = \left(1 + \exp(-w_0 - w_1 \cdot x_1 - w_2 \cdot x_2)\right)^{-1}</span><script type="math/tex">h(x_1, x_2) = \left(1 + \exp(-w_0 - w_1 \cdot x_1 - w_2 \cdot x_2)\right)^{-1}</script></span></p><br></p>
</li>
<li>
<p>For AND gate we want <span><span class="MathJax_Preview">h(0, 0) = h(0, 1) = h(1, 0) = 0</span><script type="math/tex">h(0, 0) = h(0, 1) = h(1, 0) = 0</script></span> and <span><span class="MathJax_Preview">h(1, 1) = 1</span><script type="math/tex">h(1, 1) = 1</script></span></p>
</li>
<li>
<p>The example is so simple, that we could guess weights:</p>
<ul>
<li>
<p><span><span class="MathJax_Preview">w_0 &lt;&lt; 0</span><script type="math/tex">w_0 << 0</script></span></p>
</li>
<li>
<p><span><span class="MathJax_Preview">w_0 + w_1 &lt;&lt; 0</span><script type="math/tex">w_0 + w_1 << 0</script></span></p>
</li>
<li>
<p><span><span class="MathJax_Preview">w_0 + w_2 &lt;&lt; 0</span><script type="math/tex">w_0 + w_2 << 0</script></span></p>
</li>
<li>
<p><span><span class="MathJax_Preview">w_0 + w_1 + w_2 &gt;&gt; 0</span><script type="math/tex">w_0 + w_1 + w_2 >> 0</script></span></p>
</li>
</ul>
</li>
<li>
<p>But lets build a neuron</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">theano</span>
<span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="kn">as</span> <span class="nn">T</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span> <span class="c1"># feature vector</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">vector</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span> <span class="c1"># target vector</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="c1"># weights initialized randomly</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(),</span> <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">)</span>  <span class="c1"># bias term</span>

<span class="n">hypo</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">T</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">-</span> <span class="n">b</span><span class="p">))</span>              <span class="c1"># hyphothesis</span>
<span class="n">xent</span> <span class="o">=</span> <span class="o">-</span> <span class="n">y</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">hypo</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">hypo</span><span class="p">)</span>  <span class="c1"># cross-entropy loss function</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">xent</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>                                     <span class="c1"># cost function</span>
<span class="n">grad_w</span><span class="p">,</span> <span class="n">grad_b</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="p">[</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>                 <span class="c1"># gradients</span>

<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="c1"># learning rate</span>

<span class="c1"># at each training step we update weights:</span>
<span class="c1"># w -&gt; w - alpha * grad_w and b -&gt; b - alpha * grad_b</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">],</span>
                        <span class="n">outputs</span> <span class="o">=</span> <span class="n">cost</span><span class="p">,</span>
                        <span class="n">updates</span> <span class="o">=</span> <span class="p">((</span><span class="n">w</span><span class="p">,</span> <span class="n">w</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_w</span><span class="p">),</span>
                                   <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">b</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_b</span><span class="p">)))</span>

<span class="n">predict</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">hypo</span><span class="p">)</span>
</pre></div>


<ul>
<li>Train for all gates and save prediction</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">gates</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;AND&quot;</span><span class="p">,</span> <span class="s2">&quot;OR&quot;</span><span class="p">,</span> <span class="s2">&quot;XOR&quot;</span><span class="p">)</span>
<span class="n">gates_pred</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">gate</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">gates</span><span class="p">,</span> <span class="p">(</span><span class="n">Y_and</span><span class="p">,</span> <span class="n">Y_or</span><span class="p">,</span> <span class="n">Y_xor</span><span class="p">)):</span>
  <span class="c1"># reset weights</span>
  <span class="n">w</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
  <span class="n">b</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">())</span>

  <span class="c1"># train neuron</span>
  <span class="p">[</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)]</span>
  <span class="n">gates_pred</span><span class="p">[</span><span class="n">gate</span><span class="p">]</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>


<ul>
<li>Lets see the result</li>
</ul>
<div class="codehilite"><pre><span></span><span class="k">for</span> <span class="n">gate</span> <span class="ow">in</span> <span class="n">gates</span><span class="p">:</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;{} {} {} -&gt; {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">gate</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">gates_pred</span><span class="p">[</span><span class="n">gate</span><span class="p">][</span><span class="n">i</span><span class="p">]))</span>
  <span class="k">print</span><span class="p">()</span>
</pre></div>


<div class="codehilite"><pre><span></span>0 AND 0 -&gt; 0.00018774340087294284
0 AND 1 -&gt; 0.04835388458963121
1 AND 0 -&gt; 0.04834664667043076
1 AND 1 -&gt; 0.9321880416392287

0 OR 0 -&gt; 0.05096342740264947
0 OR 1 -&gt; 0.9795405137359009
1 OR 0 -&gt; 0.9798688809365104
1 OR 1 -&gt; 0.9999769570570585

0 XOR 0 -&gt; 0.49999999640124027
0 XOR 1 -&gt; 0.49999999943423534
1 XOR 0 -&gt; 0.4999999994362604
1 XOR 1 -&gt; 0.5000000024692554
</pre></div>


<ul>
<li>As one could / should expect a linear classifier can not work for non-linear problems (like XOR gate)</li>
</ul>
<h3 id="neural-network-approach">Neural network approach<a class="headerlink" href="#neural-network-approach" title="Permanent link">&para;</a></h3>
<ul>
<li>Please note, that XOR is not linear, but it can be expressed in terms or linear problems combination</li>
</ul>
<div class="codehilite"><pre><span></span>x XOR y = (x AND NOT y) OR (y AND NOT x)
</pre></div>


<ul>
<li>Lets consider NN with two hidden neurons</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">draw_net</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">w</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="../output_133_0.png" /></p>
<ul>
<li>Lets use <code>theano</code> last time to build the network</li>
</ul>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">theano</span>
<span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="kn">as</span> <span class="nn">T</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span> <span class="c1"># feature vector</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">vector</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span> <span class="c1"># target vector</span>

<span class="c1"># first layer&#39;s weights (including bias)</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;w1&#39;</span><span class="p">)</span>
<span class="c1"># second layer&#39;s weights (including bias)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;w2&#39;</span><span class="p">)</span>

<span class="n">h</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w1</span><span class="p">[:</span><span class="mi">2</span><span class="p">,])</span> <span class="o">+</span> <span class="n">w1</span><span class="p">[</span><span class="mi">2</span><span class="p">,])</span>  <span class="c1"># hidden layer</span>
<span class="n">o</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w2</span><span class="p">[:</span><span class="mi">2</span><span class="p">,])</span> <span class="o">+</span> <span class="n">w2</span><span class="p">[</span><span class="mi">2</span><span class="p">,])</span>  <span class="c1"># output layer</span>

<span class="n">xent</span> <span class="o">=</span> <span class="o">-</span> <span class="n">y</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">o</span><span class="p">)</span>  <span class="c1"># cross-entropy loss function</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">xent</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>                               <span class="c1"># cost function</span>
<span class="n">grad_w1</span><span class="p">,</span> <span class="n">grad_w2</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="p">[</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">])</span>       <span class="c1"># gradients</span>

<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="c1"># learning rate</span>

<span class="c1"># at each training step we update weights:</span>
<span class="c1"># w -&gt; w - alpha * grad_w and b -&gt; b - alpha * grad_b</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">],</span>
                        <span class="n">outputs</span> <span class="o">=</span> <span class="n">cost</span><span class="p">,</span>
                        <span class="n">updates</span> <span class="o">=</span> <span class="p">((</span><span class="n">w1</span><span class="p">,</span> <span class="n">w1</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_w1</span><span class="p">),</span>
                                   <span class="p">(</span><span class="n">w2</span><span class="p">,</span> <span class="n">w2</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_w2</span><span class="p">)))</span>

<span class="n">predict</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">o</span><span class="p">)</span>
</pre></div>


<ul>
<li>Train on XOR and print prediction</li>
</ul>
<div class="codehilite"><pre><span></span><span class="p">[</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y_xor</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">)]</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
  <span class="k">print</span><span class="p">(</span><span class="s2">&quot;{} XOR {} -&gt; {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">prediction</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</pre></div>


<div class="codehilite"><pre><span></span>0 XOR 0 -&gt; 0.005114740815278212
0 XOR 1 -&gt; 0.9932581408757912
1 XOR 0 -&gt; 0.9931902289336407
1 XOR 1 -&gt; 0.004574250556087016
</pre></div>


<h3 id="again-the-same-but-with-tensorflow">Again the same, but with tensorflow<a class="headerlink" href="#again-the-same-but-with-tensorflow" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>Lets try to solve XOR gate using <code>tensorflow</code></p>
</li>
<li>
<p>In comments there is <code>theano</code> code</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>

<span class="c1"># x = T.matrix(&#39;x&#39;) # feature vector</span>
<span class="c1"># y = T.vector(&#39;y&#39;) # target vector</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="c1"># w1 = theano.shared(np.random.randn(3,2), name = &#39;w1&#39;)</span>
<span class="c1"># w2 = theano.shared(np.random.randn(3), name = &#39;w2&#39;)</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;w1&#39;</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;w2&#39;</span><span class="p">)</span>

<span class="c1"># h = T.nnet.sigmoid(T.dot(x, w1[:2,]) + w1[2,])</span>
<span class="c1"># o = T.nnet.sigmoid(T.dot(h, w2[:2,]) + w2[2,])</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w1</span><span class="p">[:</span><span class="mi">2</span><span class="p">,]),</span> <span class="n">w1</span><span class="p">[</span><span class="mi">2</span><span class="p">,]))</span>
<span class="n">o</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w2</span><span class="p">[:</span><span class="mi">2</span><span class="p">,]),</span> <span class="n">w2</span><span class="p">[</span><span class="mi">2</span><span class="p">,]))</span>

<span class="c1">#xent = - y * tf.log(o) - (1 - y) * tf.log(1 - o)</span>
<span class="n">xent</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">log_loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">o</span><span class="p">)</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xent</span><span class="p">)</span>

<span class="n">opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
</pre></div>


<div class="codehilite"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">Y_xor</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]]</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
  <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>

  <span class="p">[</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">Y_xor</span><span class="p">})</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">)]</span>

  <span class="k">print</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">X</span><span class="p">}))</span>
</pre></div>


<div class="codehilite"><pre><span></span>[[0.01071231]
 [0.98791456]
 [0.98790956]
 [0.01878399]]
</pre></div>


<h2 id="simple-regression-with-nn">Simple regression with NN<a class="headerlink" href="#simple-regression-with-nn" title="Permanent link">&para;</a></h2>
<ul>
<li>Lets consider a dataset generated from <em>noised</em> sinus distribution</li>
</ul>
<div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">sin</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">pi</span><span class="p">,</span> <span class="n">exp</span>

<span class="k">def</span> <span class="nf">get_dataset</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Generate N training samples&quot;&quot;&quot;</span>
  <span class="c1"># X is a set of random points from [-1, 1]</span>
  <span class="n">X</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
  <span class="c1"># Y are corresponding target values (with noise included)</span>
  <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">sin</span><span class="p">(</span><span class="n">pi</span><span class="o">*</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">])</span>

  <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span>

<span class="c1"># plot a sample</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">get_dataset</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">)</span>

<span class="n">x_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x_</span><span class="p">),</span> <span class="s1">&#39;C0--&#39;</span><span class="p">);</span>
</pre></div>


<p><img alt="png" src="../output_144_0.png" /></p>
<ul>
<li>
<p>We represent hidden neurons via logistic function (3 should be enough)</p>
</li>
<li>
<p>The output is just a sum of three hidden neurons outputs and bias term</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">draw_net</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">w</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="../output_146_0.png" /></p>
<ul>
<li>Lets implement above network in <code>tensorflow</code></li>
</ul>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">w1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;w1&#39;</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;w2&#39;</span><span class="p">)</span>

<span class="n">b1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="mi">3</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;b1&#39;</span><span class="p">)</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;b2&#39;</span><span class="p">)</span>

<span class="n">h</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w1</span><span class="p">),</span> <span class="n">b1</span><span class="p">))</span>
<span class="n">o</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w2</span><span class="p">),</span> <span class="n">b2</span><span class="p">)</span>

<span class="n">xent</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">o</span><span class="p">)</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xent</span><span class="p">)</span>

<span class="n">opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
</pre></div>


<ul>
<li>We need to reshape out training data</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">Y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Original&quot;</span><span class="p">,</span> <span class="n">X</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">Y</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Reshaped&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">Y_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>


<div class="codehilite"><pre><span></span>Original

[ 0.52226483  0.80798072  0.15974077 -0.50709742  0.78635136]

[ 0.97538575  0.83448142  0.4117488  -0.89203221  0.38857674]

Reshaped

[[ 0.52226483]
 [ 0.80798072]
 [ 0.15974077]
 [-0.50709742]
 [ 0.78635136]]

[[ 0.97538575]
 [ 0.83448142]
 [ 0.4117488 ]
 [-0.89203221]
 [ 0.38857674]]
</pre></div>


<ul>
<li>And we can train the model</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
  <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>

  <span class="p">[</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">Y_train</span><span class="p">})</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">)]</span>

  <span class="n">prediction</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">X_test</span><span class="p">})</span>
</pre></div>


<div class="codehilite"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C2&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;NN&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C1&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x_</span><span class="p">),</span> <span class="s1">&#39;C0--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Truth&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>


<p><img alt="png" src="../output_153_0.png" /></p>
<h2 id="more-examples">More examples<a class="headerlink" href="#more-examples" title="Permanent link">&para;</a></h2>
<ul>
<li>The lecturer was too lazy to prepare more examples, but we can play in <a href="https://playground.tensorflow.org/">tensorflow playground</a>, which has awesome visualizations</li>
</ul>
<h2 id="mnist">MNIST<a class="headerlink" href="#mnist" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="http://yann.lecun.com/exdb/mnist/">THE MNIST DATABASE of handwritten digits</a></li>
</ul>
<blockquote>
<p>The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.</p>
<p>It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.</p>
</blockquote>
<ul>
<li>We can grab MNIST dataset using <code>tensorflow.examples.tutorials.mnist</code></li>
</ul>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.examples.tutorials.mnist</span> <span class="kn">import</span> <span class="n">input_data</span>

<span class="c1"># to avoid warnings printed in the notebook</span>
<span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">set_verbosity</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>

<span class="c1"># one hot -&gt; label 0-9 -&gt; 0...01, 0...10, ...</span>
<span class="n">mnist</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">read_data_sets</span><span class="p">(</span><span class="s2">&quot;/tmp/&quot;</span><span class="p">,</span> <span class="n">one_hot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>


<div class="codehilite"><pre><span></span>Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.
Extracting /tmp/train-images-idx3-ubyte.gz
Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.
Extracting /tmp/train-labels-idx1-ubyte.gz
Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.
Extracting /tmp/t10k-images-idx3-ubyte.gz
Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.
Extracting /tmp/t10k-labels-idx1-ubyte.gz
</pre></div>


<ul>
<li>Lets see how data looks like</li>
</ul>
<div class="codehilite"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>


<div class="codehilite"><pre><span></span>(55000, 784)
</pre></div>


<div class="codehilite"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">221</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span>

  <span class="c1"># random training sample</span>
  <span class="n">index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">images</span><span class="p">))</span>

  <span class="c1"># train.images contains images in a form of a vector</span>
  <span class="c1"># so we reshape it back to 28x28</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>

  <span class="c1"># train.labels contains labels in one hot format</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>


<p><img alt="png" src="../output_162_0.png" /></p>
<ul>
<li>
<p>Today we solve this classification problem with <code>softmax</code></p>
</li>
<li>
<p>During next lecture we apply convolutional NN on MNIST dataset</p>
</li>
<li>
<p>Lets start building the model</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">784</span><span class="p">])</span>  <span class="c1"># img -&gt; 28x28 -&gt; 784</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>   <span class="c1"># 10 classes</span>

<span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">784</span><span class="p">,</span> <span class="mi">10</span><span class="p">]))</span>  <span class="c1"># weights</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">10</span><span class="p">]))</span>       <span class="c1"># bias</span>

<span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
</pre></div>


<ul>
<li>Define the loss function and optimizer</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">cross_entropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">out</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">)</span>
</pre></div>


<ul>
<li>Train the model</li>
</ul>
<div class="codehilite"><pre><span></span><span class="c1"># create a session</span>
<span class="n">sess</span> <span class="o">=</span>  <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="c1"># initialize weights</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
  <span class="c1"># here instead of updating weights after the whole training set</span>
  <span class="c1"># we use batch size 100 (more about that in the next section)</span>
  <span class="n">batch_xs</span><span class="p">,</span> <span class="n">batch_ys</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">next_batch</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>

  <span class="c1"># train_step is minimizing cross_entropy with learning rate 0.5 using GD</span>
  <span class="c1"># we pass small batches to placeholders x and y</span>
  <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_step</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">batch_xs</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">batch_ys</span><span class="p">})</span>
</pre></div>


<ul>
<li>Validate the model</li>
</ul>
<div class="codehilite"><pre><span></span><span class="c1"># argmax returns the index of the heighest index in a tensor</span>
<span class="c1"># equal returns True / False if prediction is equal/not equal to true label</span>
<span class="c1"># cast would convert True/False to 1/0, so we can calculate the average</span>
<span class="n">correct_prediction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct_prediction</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">images</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">labels</span><span class="p">}))</span>

<span class="n">sess</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>


<div class="codehilite"><pre><span></span>0.9241
</pre></div>


<ul>
<li>
<p>It is pretty crazy what you can do with modern ML/DL frameworks</p>
</li>
<li>
<p>We have just learned our computer to recognize handwritten digits with a few lines of code</p>
</li>
<li>
<p>During next lecture we will improve the accuracy using deep neural networks</p>
</li>
</ul>
<h2 id="gradient-descent-variations">Gradient descent variations<a class="headerlink" href="#gradient-descent-variations" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p>In the <strong>gradient descent</strong> (GD) method weights are updated after a full loop over training data: <p align="center"><br><span><span class="MathJax_Preview">W \rightarrow W - \alpha\cdot\nabla_W L(W)</span><script type="math/tex">W \rightarrow W - \alpha\cdot\nabla_W L(W)</script></span></p><br></p>
</li>
<li>
<p>In the <strong>stochastic gradient descent</strong> (SGD) method weights are updated for each training sample: <p align="center"><br><span><span class="MathJax_Preview">W \rightarrow W - \alpha\cdot\nabla_W L(W; x^{(i)}; y^{(i)})</span><script type="math/tex">W \rightarrow W - \alpha\cdot\nabla_W L(W; x^{(i)}; y^{(i)})</script></span></p><br></p>
</li>
<li>
<p>Note, that SGD is also called <strong>online learning</strong></p>
</li>
<li>
<p>For the large dataset it is likely that GD would recompute gradients for similar examples before an update</p>
</li>
<li>
<p>SGD perform frequent updates but with a high variance, so objective function fluctuates</p>
</li>
<li>
<p>It may help to escape local minima</p>
</li>
<li>
<p>The common method, which is somehow between GD and SGD, is <strong>mini-batch gradient descent</strong> (MBGD) - the one we used for MNIST example: <p align="center"><br><span><span class="MathJax_Preview">W \rightarrow W - \alpha\cdot\nabla_W L(W; x^{(i; i+n)}; y^{(i; i+n)})</span><script type="math/tex">W \rightarrow W - \alpha\cdot\nabla_W L(W; x^{(i; i+n)}; y^{(i; i+n)})</script></span></p><br></p>
</li>
</ul>
<h3 id="sgd-on-mnist">SGD on MNIST<a class="headerlink" href="#sgd-on-mnist" title="Permanent link">&para;</a></h3>
<ul>
<li>Lets see how the loss function looks like when applying SGD for training the network on MNIST data</li>
</ul>
<div class="codehilite"><pre><span></span><span class="c1"># create a session</span>
<span class="n">sess</span> <span class="o">=</span>  <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="c1"># initialize weights</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>

<span class="n">test_loss</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># placeholder for loss value per iteration</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
  <span class="c1"># SGD -&gt; batch size = 1</span>
  <span class="n">batch_xs</span><span class="p">,</span> <span class="n">batch_ys</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">next_batch</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  
  <span class="c1"># update weights</span>
  <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_step</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">batch_xs</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">batch_ys</span><span class="p">})</span>
  <span class="c1"># calculate loss funtion on test samples</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">,</span>
                  <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">images</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">labels</span><span class="p">})</span>
  <span class="c1"># save it  </span>
  <span class="n">test_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>


<div class="codehilite"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">test_loss</span><span class="p">);</span>
</pre></div>


<p><img alt="png" src="../output_177_0.png" /></p>
<h3 id="momentum">Momentum<a class="headerlink" href="#momentum" title="Permanent link">&para;</a></h3>
<ul>
<li>Online learning may help to escape local minima</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">a</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">5</span><span class="o">*</span><span class="n">a</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="o">-</span><span class="mf">1.45</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.75</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">300</span><span class="p">);</span>
</pre></div>


<p><img alt="png" src="../output_180_0.png" /></p>
<ul>
<li>Another technique that can help escape local minima is to use a momentum term</li>
</ul>
<p align="center"><br>$\begin{eqnarray}
v_t & = & \gamma\cdot v_{t-1} + \alpha\cdot\nabla_W L(W; x^{(i)}; y^{(i)}) \\
W &\rightarrow& W - v_t
\end{eqnarray}$</p>

<p><br></p>
<ul>
<li>
<p>It is like pushing a ball down a hill and ball accumulates momentum</p>
</li>
<li>
<p><span><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> is another hyperparameter, usually set around <span><span class="MathJax_Preview">0.9</span><script type="math/tex">0.9</script></span></p>
</li>
<li>
<p>One may want a smarter ball, which can predict its future position</p>
</li>
<li>
<p>The future position is approximate by <span><span class="MathJax_Preview">W - \gamma\cdot v_{t-1}</span><script type="math/tex">W - \gamma\cdot v_{t-1}</script></span></p>
</li>
<li>
<p><strong>Nesterov accelerated gradient</strong> calculates gradient <strong>not</strong> w.r.t current weights, but w.r.t approximated future  values:</p>
</li>
</ul>
<p align="center"><br>$\begin{eqnarray}
v_t & = & \gamma\cdot v_{t-1} + \alpha\cdot\nabla_W L(W - \gamma\cdot v_{t-1} ; x^{(i)}; y^{(i)}) \\
W &\rightarrow& W - v_t
\end{eqnarray}$</p>

<p><br></p>
<h3 id="adaptive-models">Adaptive models<a class="headerlink" href="#adaptive-models" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>A drawback of <em>regular</em> gradient descent methods is a constant learning rate, which needs to be tuned by hand</p>
<ul>
<li>
<p>if too small the training process is long</p>
</li>
<li>
<p>if too large the minimum may be skipped</p>
</li>
</ul>
</li>
<li>
<p>There are several algorithms which adapt the learning rate during training</p>
</li>
<li>
<p>Adagrad, Adadelta, Adam are presented here, but please note there are more available</p>
</li>
</ul>
<h4 id="adagrad">Adagrad<a class="headerlink" href="#adagrad" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p>Let <span><span class="MathJax_Preview">g_{ti}</span><script type="math/tex">g_{ti}</script></span> be the gradient of the objective function w.r.t the weight <span><span class="MathJax_Preview">W_i</span><script type="math/tex">W_i</script></span> at time step <span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>: <p align="center"><br><span><span class="MathJax_Preview">g_{ti} = \nabla_WL(W_{t,i})</span><script type="math/tex">g_{ti} = \nabla_WL(W_{t,i})</script></span></p><br></p>
</li>
<li>
<p>In this notation, SGD step for a parameter <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> can be written as:  <p align="center"><br><span><span class="MathJax_Preview">W_{t+1, i} = W_{t,i} - \alpha g_{ti}</span><script type="math/tex">W_{t+1, i} = W_{t,i} - \alpha g_{ti}</script></span></p><br></p>
</li>
<li>
<p>Adagrad modifies the learning rate <span><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> based on previous gradients: <p align="center"><br><span><span class="MathJax_Preview">W_{t+1, i} = W_{t,i} - \frac{\alpha}{\sqrt{G_{t,ii} + \varepsilon}} g_{ti}</span><script type="math/tex">W_{t+1, i} = W_{t,i} - \frac{\alpha}{\sqrt{G_{t,ii} + \varepsilon}} g_{ti}</script></span></p><br></p>
</li>
<li>
<p>Where <span><span class="MathJax_Preview">G_{t}</span><script type="math/tex">G_{t}</script></span> is a diagonal matrix, where <span><span class="MathJax_Preview">i, i</span><script type="math/tex">i, i</script></span> elements are equal to the sum of all gradients w.r.t <span><span class="MathJax_Preview">W_i</span><script type="math/tex">W_i</script></span> up to step <span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span></p>
</li>
<li>
<p><span><span class="MathJax_Preview">\varepsilon</span><script type="math/tex">\varepsilon</script></span> is just to ensure denominator not equal zero</p>
</li>
<li>
<p>Since <span><span class="MathJax_Preview">G_t</span><script type="math/tex">G_t</script></span> is diagonal, we can write the general formula using element-wise product: <p align="center"><br><span><span class="MathJax_Preview">W_{t+1} = W_{t}- \frac{\alpha}{\sqrt{G_{t} + \varepsilon}} \odot g_{t}</span><script type="math/tex">W_{t+1} = W_{t}- \frac{\alpha}{\sqrt{G_{t} + \varepsilon}} \odot g_{t}</script></span></p><br></p>
</li>
<li>
<p>Note, that each weight has now its own training rate; they just share initial value <span><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span></p>
</li>
<li>
<p>There is no need to tune <span><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> anymore; usually one set it up to be around <span><span class="MathJax_Preview">0.01</span><script type="math/tex">0.01</script></span> and let Adagrad do the job</p>
</li>
<li>
<p>The disadvantage of Adagrad is that it accumulates squares of gradients in the denominator - at some point training rate is so small that the model is unable to learn anythng new</p>
</li>
</ul>
<h4 id="adadelta">Adadelta<a class="headerlink" href="#adadelta" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p>Adadelta is an extension of Adagrad</p>
</li>
<li>
<p>Its goal is to reduce the speed of training rate decreasing</p>
</li>
<li>
<p>The idea is to accumulate just a few last gradients instead all of them</p>
</li>
<li>
<p>Since it is highly inefficient, Adagrad implements this as an exponentially decaying average of all the squared gradients</p>
</li>
<li>
<p>Let <span><span class="MathJax_Preview">E[g^2]_t</span><script type="math/tex">E[g^2]_t</script></span> be the average of the squared greadients up to step <span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span></p>
</li>
<li>
<p>Let <span><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> be decay constant (similar to momentum), then: <p align="center"><br><span><span class="MathJax_Preview">E[g^2]_t = \gamma\cdot E[g^2]_{t-1} + (1 - \gamma)\cdot g_t^2</span><script type="math/tex">E[g^2]_t = \gamma\cdot E[g^2]_{t-1} + (1 - \gamma)\cdot g_t^2</script></span></p><br></p>
</li>
<li>
<p>And the training step is given by: <p align="center"><br><span><span class="MathJax_Preview">W_{t+1} = W_{t}- \frac{\alpha}{\sqrt{E[g^2]_t + \varepsilon}}  g_{t}</span><script type="math/tex">W_{t+1} = W_{t}- \frac{\alpha}{\sqrt{E[g^2]_t + \varepsilon}}  g_{t}</script></span></p><br></p>
</li>
<li>
<p>Please note, that the denominator is just root mean squared (RMS) of the gradient:  <p align="center"><br><span><span class="MathJax_Preview">W_{t+1} = W_{t}- \frac{\alpha}{RMS[g]_t}  g_{t}</span><script type="math/tex">W_{t+1} = W_{t}- \frac{\alpha}{RMS[g]_t}  g_{t}</script></span></p><br></p>
</li>
<li>
<p>The author also note that in GD-like methods the update has different hypothetical units than weights itself: <p align="center"><br><span><span class="MathJax_Preview">units~of~\Delta w \sim units~of~g \sim units~of~\frac{\partial L}{\partial w} \sim \frac{1}{units~of~w}</span><script type="math/tex">units~of~\Delta w \sim units~of~g \sim units~of~\frac{\partial L}{\partial w} \sim \frac{1}{units~of~w}</script></span></p><br></p>
</li>
<li>
<p>Inspired by second order optimization method that using Hessian information (like Newton's method): <p align="center"><br><span><span class="MathJax_Preview">units~of~\Delta w \sim units~of~H^{-1}g \sim units~of~\frac{\frac{\partial L}{\partial w}}{\frac{\partial^2L}{\partial w^2}} \sim units~of~w</span><script type="math/tex">units~of~\Delta w \sim units~of~H^{-1}g \sim units~of~\frac{\frac{\partial L}{\partial w}}{\frac{\partial^2L}{\partial w^2}} \sim units~of~w</script></span></p><br></p>
</li>
<li>
<p>the nominator is replaced by RMS of of weights updates (assuming diagonal Hessian): <p align="center"><br><span><span class="MathJax_Preview">\Delta w = \frac{\frac{\partial L}{\partial w}}{\frac{\partial^2 L}{\partial w^2}} \Rightarrow \frac{1}{\frac{\partial^2 L}{\partial w^2}} = \frac{\Delta w}{\frac{\partial L}{\partial w}}</span><script type="math/tex">\Delta w = \frac{\frac{\partial L}{\partial w}}{\frac{\partial^2 L}{\partial w^2}} \Rightarrow \frac{1}{\frac{\partial^2 L}{\partial w^2}} = \frac{\Delta w}{\frac{\partial L}{\partial w}}</script></span></p><br></p>
</li>
<li>
<p>Since current update is not known until step is done, the following approximation is used: <p align="center"><br><span><span class="MathJax_Preview">\Delta W_t = - \frac{RMS[\Delta W]_{t-1}}{RMS[g]_t}  g_{t}</span><script type="math/tex">\Delta W_t = - \frac{RMS[\Delta W]_{t-1}}{RMS[g]_t}  g_{t}</script></span></p><br></p>
</li>
<li>
<p>Please note, that Adagrad does not require initial training rate!</p>
</li>
<li>
<p>Although, it is still not parameter-free model as one need to set up <span><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> and <span><span class="MathJax_Preview">\varepsilon</span><script type="math/tex">\varepsilon</script></span></p>
</li>
</ul>
<h4 id="adam">Adam<a class="headerlink" href="#adam" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p>Similar to Adedelta Adam uses exponentially decaying average of past sqaured gradients <span><span class="MathJax_Preview">v_t</span><script type="math/tex">v_t</script></span> (notation from the original paper): <p align="center"><br><span><span class="MathJax_Preview">v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2)\cdot g_t^2</span><script type="math/tex">v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2)\cdot g_t^2</script></span></p><br></p>
</li>
<li>
<p>And similar to SGD with momentum Adam also keeps information about past gradients: <p align="center"><br><span><span class="MathJax_Preview">m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1)\cdot g_t</span><script type="math/tex">m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1)\cdot g_t</script></span></p><br></p>
</li>
<li>
<p><span><span class="MathJax_Preview">m_t</span><script type="math/tex">m_t</script></span> and <span><span class="MathJax_Preview">v_t</span><script type="math/tex">v_t</script></span> are initialized with zeros</p>
</li>
<li>
<p>For <span><span class="MathJax_Preview">\beta_{1,2}\sim 1</span><script type="math/tex">\beta_{1,2}\sim 1</script></span> training is slow at the beginning</p>
</li>
<li>
<p>Thus, bias-corrected first and second moments are introduced:</p>
</li>
</ul>
<p align="center"><br>$\begin{eqnarray}
\hat m_t & = & \frac{m_t}{1 - \beta_1^t} \\
\hat v_t & = & \frac{v_t}{1 - \beta_2^t}
\end{eqnarray}$</p>

<p><br></p>
<ul>
<li>
<p>And the update rule is given by: <p align="center"><br><span><span class="MathJax_Preview">\Delta W_{t+1} = W_t - \frac{\alpha}{\sqrt{\hat v_t} + \varepsilon}\hat m_t</span><script type="math/tex">\Delta W_{t+1} = W_t - \frac{\alpha}{\sqrt{\hat v_t} + \varepsilon}\hat m_t</script></span></p><br></p>
</li>
<li>
<p>The authors suggest <span><span class="MathJax_Preview">\beta_1 \approx 0.9</span><script type="math/tex">\beta_1 \approx 0.9</script></span>, <span><span class="MathJax_Preview">\beta_2 \approx 0.999</span><script type="math/tex">\beta_2 \approx 0.999</script></span>, and <span><span class="MathJax_Preview">\varepsilon\approx 10^{-8}</span><script type="math/tex">\varepsilon\approx 10^{-8}</script></span></p>
</li>
</ul>
<h2 id="regularization">Regularization<a class="headerlink" href="#regularization" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p>As mentioned during last lecture, regularization is any modification to learning algorithm made to prevent overfitting</p>
</li>
<li>
<p>The common method is to add <strong>regularization term</strong> (<strong>regularizer</strong>) to a loss function <span><span class="MathJax_Preview">L(W)</span><script type="math/tex">L(W)</script></span>: <p align="center"><br><span><span class="MathJax_Preview">L(W) \rightarrow L(W) + \alpha R(W)</span><script type="math/tex">L(W) \rightarrow L(W) + \alpha R(W)</script></span></p><br></p>
</li>
<li>
<p>Two common regularizer are <span><span class="MathJax_Preview">L1</span><script type="math/tex">L1</script></span> or <span><span class="MathJax_Preview">L2</span><script type="math/tex">L2</script></span> norms</p>
</li>
<li>
<p><strong>Ridge</strong> regression uses <span><span class="MathJax_Preview">L2</span><script type="math/tex">L2</script></span> regularization: <span><span class="MathJax_Preview">R(W) = \sum\limits_i W_i^2</span><script type="math/tex">R(W) = \sum\limits_i W_i^2</script></span></p>
</li>
<li>
<p><strong>Lasso</strong> regression uses <span><span class="MathJax_Preview">L1</span><script type="math/tex">L1</script></span> regularization: <span><span class="MathJax_Preview">R(W) = \sum\limits_i |W_i|</span><script type="math/tex">R(W) = \sum\limits_i |W_i|</script></span></p>
</li>
<li>
<p>Why to penalize the magnitude of weights?</p>
</li>
<li>
<p>Lets consider a simple example</p>
</li>
</ul>
<h3 id="to-regularize-or-not-to-regularize">To regularize or not to regularize<a class="headerlink" href="#to-regularize-or-not-to-regularize" title="Permanent link">&para;</a></h3>
<ul>
<li>Lets consider again <em>sinus</em> dataset</li>
</ul>
<div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">sin</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">pi</span><span class="p">,</span> <span class="n">exp</span>

<span class="k">def</span> <span class="nf">get_dataset</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Generate N training samples&quot;&quot;&quot;</span>
  <span class="c1"># X is a set of random points from [-1, 1]</span>
  <span class="n">X</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
  <span class="c1"># Y are corresponding target values (with noise included)</span>
  <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">sin</span><span class="p">(</span><span class="n">pi</span><span class="o">*</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">])</span>

  <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span>

<span class="c1"># plot a sample</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">get_dataset</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>

<span class="n">x_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x_</span><span class="p">),</span> <span class="s1">&#39;C0--&#39;</span><span class="p">);</span>
</pre></div>


<p><img alt="png" src="../output_194_0.png" /></p>
<ul>
<li>Lets fit data to polynomial of order 20</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># polynomial order</span>

<span class="c1"># add powers of x </span>
<span class="n">X_train</span> <span class="o">=</span> <span class="p">[[</span><span class="n">x</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">)]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">]</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="p">);</span>
</pre></div>


<ul>
<li>And plot prediction together with training data</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="n">x</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">N</span><span class="p">)]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X_test</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="s1">&#39;C0&#39;</span><span class="p">);</span>
</pre></div>


<p><img alt="png" src="../output_198_0.png" /></p>
<ul>
<li>
<p>It is clearly overfitted</p>
</li>
<li>
<p>Lets do the same using Ridge regression</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>

<span class="n">reg_l2</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">reg_l2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

<span class="n">Y_test</span> <span class="o">=</span> <span class="n">reg_l2</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="n">x</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">N</span><span class="p">)]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X_test</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="s1">&#39;C0&#39;</span><span class="p">);</span>
</pre></div>


<p><img alt="png" src="../output_200_0.png" /></p>
<ul>
<li>
<p>Play on your own with <span><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> to see how it affects the result</p>
</li>
<li>
<p>Now, take a look at coefficients without regularizer</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span>
</pre></div>


<div class="codehilite"><pre><span></span>array([ 2.60931184e+00, -1.34651207e+00,  2.04999890e+01,  9.00515668e+00,
       -4.18472051e+02, -1.27117676e+02,  3.14310368e+03,  1.10582631e+03,
       -1.24897897e+04, -4.36716920e+03,  2.86984815e+04,  8.89018057e+03,
       -3.94741356e+04, -9.78295086e+03,  3.21343881e+04,  5.54204833e+03,
       -1.43111414e+04, -1.26843444e+03,  2.69454755e+03])
</pre></div>


<ul>
<li>
<p>Note, that coefficients increase drastically for large powers of <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span></p>
</li>
<li>
<p>High weights mean that a model put a lot of emphasis on a given features, which leads to overfitting</p>
</li>
<li>
<p>That is why we need to put some constraints on the magnitude of weights</p>
</li>
<li>
<p>Now, lets see how coefficients look like with L2 regularizer</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">reg_l2</span><span class="o">.</span><span class="n">coef_</span>
</pre></div>


<div class="codehilite"><pre><span></span>array([ 2.3350277 , -0.00324186, -1.57606849,  0.04982775, -1.0117878 ,
       -0.025911  , -0.4412411 , -0.04283921, -0.10340078, -0.03155621,
        0.07582327, -0.01138888,  0.16245585,  0.01121341,  0.19718257,
        0.03442763,  0.20359299,  0.05769045,  0.19518384])
</pre></div>


<ul>
<li>
<p>Ridge regression causes coefficient shrinkage and reduces model complexity</p>
</li>
<li>
<p>Lets repeat the same for Lasso regression</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>

<span class="n">reg_l1</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">reg_l1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

<span class="n">Y_test</span> <span class="o">=</span> <span class="n">reg_l1</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="n">x</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">N</span><span class="p">)]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X_test</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="s1">&#39;C0&#39;</span><span class="p">);</span>
</pre></div>


<p><img alt="png" src="../output_206_0.png" /></p>
<ul>
<li>
<p>Once again we got a nice fit</p>
</li>
<li>
<p>But there is a difference - see the coefficients</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">reg_l1</span><span class="o">.</span><span class="n">coef_</span>
</pre></div>


<div class="codehilite"><pre><span></span>array([ 2.779238  , -0.        , -3.20859903, -0.        , -0.        ,
       -0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.1980417 ,  0.        ,  0.22056433,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ])
</pre></div>


<ul>
<li>
<p>Note, that in opposite to Ridge not all features are taken into account as some weights ended up being zero</p>
</li>
<li>
<p>Thus, Lasso regression additionally performs features selection, which is useful for data with many features</p>
</li>
</ul>
<h2 id="summary">Summary<a class="headerlink" href="#summary" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p>Neural networks are one of the most popular machine learning method these days</p>
</li>
<li>
<p>Note, that NN has much more hyperparameters than models described so far:</p>
<ul>
<li>
<p>number of hidden layers</p>
</li>
<li>
<p>number of neurons</p>
</li>
<li>
<p>activation functions</p>
</li>
<li>
<p>learning algorithm settings (model, learning rate, momentum etc)</p>
</li>
<li>
<p>regularization method and its parameters</p>
</li>
<li>
<p>batch size in mini-batch gradient descent</p>
</li>
</ul>
</li>
<li>
<p>It is crucial to use cross-validation for hyperparameter tuning</p>
</li>
<li>
<p>Next week "we need to go deeper"</p>
</li>
</ul>
                
                  
                
              
              
                
              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../../introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/" title="Support Vector Machine" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Support Vector Machine
              </span>
            </div>
          </a>
        
        
          <a href="../../introduction_to_machine_learning_05_dl/introduction_to_machine_learning_05_dl/" title="Deep Learning" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Deep Learning
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="http://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
  <div class="md-footer-social">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    
      <a href="https://github.com/TomaszGolan/introduction_to_machine_learning" class="md-footer-social__link fa fa-github"></a>
    
  </div>

      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../../assets/javascripts/application.6cdc17f0.js"></script>
      
      <script>app.initialize({version:"0.17.2",url:{base:"../../.."}})</script>
      
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
    
      
    
  </body>
</html>