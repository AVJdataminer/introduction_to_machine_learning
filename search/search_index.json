{
    "docs": [
        {
            "location": "/",
            "text": "Introduction to Machine Learning\n\u00b6\n\n\n\n\nNote\n\n\nThe content of this website was automatically generated from Jupyter notebooks.\n\n\nHopefully, nothing has been broken.\n\n\n\n\nLectures notes are prepared in \nColaboratory\n, which is a Jupyter notebook environment running in the cloud and storing notebooks in Google Drive. It makes it easy to collaborate on a project in Jupyter notebooks and provides free computing power.\n\n\nThis website was created for your convenience. Feel free to use Jupyter notebooks though:\n\n\n\n\n\n\nIntroduction (\nColaboratory\n or \nGitHub\n)\n\n\n\n\n\n\nk-Nearest Neighbors (\nColaboratory\n or \nGitHub\n)\n\n\n\n\n\n\nFirst ML problem\n\n\n\n\n\n\nNearest Neighbor algorithm\n\n\n\n\n\n\nk-Nearest Neighbors algorithm\n\n\n\n\n\n\nHyperparameters\n\n\n\n\n\n\nIris dataset\n\n\n\n\n\n\nMNIST\n\n\n\n\n\n\nRegression with kNN\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\nDecision trees (\nColaboratory\n or \nGitHub\n)\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\nID3 and C4.5 algorithms\n\n\n\n\n\n\nCART\n\n\n\n\n\n\nBias-variance trade-off\n\n\n\n\n\n\nEnsemble learning\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\nSupport vector machine (\nColaboratory\n or \nGitHub\n)\n\n\n\n\n\n\nLinear SVM\n\n\n\n\n\n\nLagrange multipliers\n\n\n\n\n\n\nOptimal margin\n\n\n\n\n\n\nNon-linear SVM\n\n\n\n\n\n\nSoft margin\n\n\n\n\n\n\nSMO algorithm\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\nMulticlass classification\n\n\n\n\n\n\nSVM regression\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\nNeural networks (\nColaboratory\n or \nGitHub\n)\n\n\n\n\n\n\nLinear regression\n\n\n\n\n\n\nLogistic regression\n\n\n\n\n\n\nMultinominal logistic regression\n\n\n\n\n\n\nNeural networks\n\n\n\n\n\n\nAND, OR, XOR\n\n\n\n\n\n\nSimple regression with NN\n\n\n\n\n\n\nMNIST with softmax\n\n\n\n\n\n\nGradient descent variations\n\n\n\n\n\n\nRegularization\n\n\n\n\n\n\nSummary",
            "title": "Home"
        },
        {
            "location": "/#introduction-to-machine-learning",
            "text": "Note  The content of this website was automatically generated from Jupyter notebooks.  Hopefully, nothing has been broken.   Lectures notes are prepared in  Colaboratory , which is a Jupyter notebook environment running in the cloud and storing notebooks in Google Drive. It makes it easy to collaborate on a project in Jupyter notebooks and provides free computing power.  This website was created for your convenience. Feel free to use Jupyter notebooks though:    Introduction ( Colaboratory  or  GitHub )    k-Nearest Neighbors ( Colaboratory  or  GitHub )    First ML problem    Nearest Neighbor algorithm    k-Nearest Neighbors algorithm    Hyperparameters    Iris dataset    MNIST    Regression with kNN    Summary      Decision trees ( Colaboratory  or  GitHub )    Introduction    ID3 and C4.5 algorithms    CART    Bias-variance trade-off    Ensemble learning    Summary      Support vector machine ( Colaboratory  or  GitHub )    Linear SVM    Lagrange multipliers    Optimal margin    Non-linear SVM    Soft margin    SMO algorithm    Examples    Multiclass classification    SVM regression    Summary      Neural networks ( Colaboratory  or  GitHub )    Linear regression    Logistic regression    Multinominal logistic regression    Neural networks    AND, OR, XOR    Simple regression with NN    MNIST with softmax    Gradient descent variations    Regularization    Summary",
            "title": "Introduction to Machine Learning"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/",
            "text": "Introduction to Machine Learning\n\u00b6\n\n\nLecturer details\n\u00b6\n\n\n\n\n\n\nTomasz Golan\n\n\n\n\n\n\nemail: \ntomasz.golan@uwr.edu.pl\n\n\n\n\n\n\nroom@ift: 438\n\n\n\n\n\n\nphone: +48 71 375-9405\n\n\n\n\n\n\nconsultations:\n\n\n\n\n\n\nMonday 11-12\n\n\n\n\n\n\nThursday 16-17 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture details\n\u00b6\n\n\nPlan\n\u00b6\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\nk-Nearest Neighbors\n\n\n\n\n\n\nDecision Tree\n\n\n\n\n\n\nSupport Vector Machine\n\n\n\n\n\n\nMultilayer Perceptron\n\n\n\n\n\n\nDeep Learning\n\n\n\n\n\n\nLiterature\n\u00b6\n\n\n\n\n\n\n\u201cDeep Learning\u201d by Ian Goodfellow, Yoshua Bengio, Aaron Courville\n\n\n\n\n\n\n\u201cPattern Recognition and Machine Learning\u201d by Christopher Bishop\n\n\n\n\n\n\nRecommended prerequisite knowledge\n\u00b6\n\n\n\n\n\n\nLinear algebra\n\n\n\n\n\n\nCalculus\n\n\n\n\n\n\nPython\n\n\n\n\n\n\nExam\n\u00b6\n\n\n\n\n\n\nIn the form of the presentation\n\n\n\n\n\n\nIndividual or group project\n\n\n\n\n\n\nAt least one machine learning algorithm must be used\n\n\n\n\n\n\nWith the model description included\n\n\n\n\n\n\nuseful (but not interesting) functions\n\u00b6\n\n\n\n\n\n\nHere, I just define some functions used for making demo plots during the introduction.\n\n\n\n\n\n\nFeel free to look at them later (especially if you are not familiar with \nnumpy\n and \nmatplotlib\n).\n\n\n\n\n\n\nBut now let's skip them.\n\n\n\n\n\n\n# numpy and matplotlib will be used a lot during the lecture\n\n\n# if you are familiar with these libraries you may skip this part\n\n\n# if not - extended comments were added to make it easier to understand\n\n\n\n# it is kind of standard to import numpy as np and pyplot as plt\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\nimport\n \nmatplotlib.pyplot\n \nas\n \nplt\n\n\n\n# used later to apply different colors in for loops\n\n\nmpl_colors\n \n=\n \n(\n'r'\n,\n \n'b'\n,\n \n'g'\n,\n \n'c'\n,\n \n'm'\n,\n \n'y'\n,\n \n'k'\n,\n \n'w'\n)\n\n\n\n# just to overwrite default colab style\n\n\nplt\n.\nstyle\n.\nuse\n(\n'default'\n)\n\n\nplt\n.\nstyle\n.\nuse\n(\n'seaborn-talk'\n)\n\n\n\n\ndef\n \ngenerate_random_points\n(\nsize\n=\n10\n,\n \nlow\n=\n0\n,\n \nhigh\n=\n1\n):\n\n  \n\"\"\"Generate a set of random 2D points\n\n\n\n  size -- number of points to generate\n\n\n  low  -- min value\n\n\n  high -- max value\n\n\n  \"\"\"\n\n  \n# random_sample([size]) returns random numbers with shape defined by size\n\n  \n# e.g.\n\n  \n# >>> np.random.random_sample((2, 3))\n\n  \n#\n\n  \n# array([[ 0.44013807,  0.77358569,  0.64338619],\n\n  \n#        [ 0.54363868,  0.31855232,  0.16791031]])\n\n  \n#\n\n  \nreturn\n \n(\nhigh\n \n-\n \nlow\n)\n \n*\n \nnp\n.\nrandom\n.\nrandom_sample\n((\nsize\n,\n \n2\n))\n \n+\n \nlow\n\n\n\n\ndef\n \ninit_plot\n(\nx_range\n=\nNone\n,\n \ny_range\n=\nNone\n,\n \nx_label\n=\n\"$x_1$\"\n,\n \ny_label\n=\n\"$x_2$\"\n):\n\n  \n\"\"\"Set axes limits and labels\n\n\n\n  x_range -- [min x, max x]\n\n\n  y_range -- [min y, max y]\n\n\n  x_label -- string\n\n\n  y_label -- string\n\n\n  \"\"\"\n\n\n  \n# subplots returns figure and axes\n\n  \n# (in general you may want many axes on one figure)\n\n  \n# we do not need fig here\n\n  \n# but we will apply changes (including adding points) to axes\n\n  \n_\n,\n \nax\n \n=\n \nplt\n.\nsubplots\n(\ndpi\n=\n70\n)\n\n\n  \n# set grid style and color\n\n  \nax\n.\ngrid\n(\nc\n=\n'0.70'\n,\n \nlinestyle\n=\n':'\n)\n\n\n  \n# set axes limits (x_range and y_range is a list with two elements)\n\n  \nax\n.\nset_xlim\n(\nx_range\n)\n \n  \nax\n.\nset_ylim\n(\ny_range\n)\n\n\n  \n# set axes labels\n\n  \nax\n.\nset_xlabel\n(\nx_label\n)\n\n  \nax\n.\nset_ylabel\n(\ny_label\n)\n\n\n  \n# return axes so we can continue modyfing them later\n\n  \nreturn\n \nax\n\n\n\n\ndef\n \nplot_random_points\n(\nstyle\n=\nNone\n,\n \ncolor\n=\nNone\n):\n\n  \n\"\"\"Generate and plot two (separated) sets of random points\n\n\n\n  style -- latter group points style (default as first)\n\n\n  color -- latter group color (default as first)\n\n\n  \"\"\"\n\n\n  \n# create a plot with x and y ranges from 0 to 2.5\n\n  \nax\n \n=\n \ninit_plot\n([\n0\n,\n \n2.5\n],\n \n[\n0\n,\n \n2.5\n])\n\n\n  \n# add two different sets of random points\n\n  \n# first set = 5 points from [0.5, 1.0]x[0.5, 1.0]\n\n  \n# second set = 5 points from [1.5, 2.0]x[1.5, 2.0]\n\n  \n# generate_random_points return a numpy array in the format like\n\n  \n# [[x1, y1], [x2, y2], ..., [xn, yn]]\n\n  \n# pyplot.plt take separately arrays with X and Y, like\n\n  \n# plot([x1, x2, x3], [y1, y2, y3])\n\n  \n# thus, we transpose numpy array to the format\n\n  \n# [[x1, x2, ..., xn], [y1, y2, ..., yn]]\n\n  \n# and unpack it with *\n\n  \nax\n.\nplot\n(\n*\ngenerate_random_points\n(\n5\n,\n \n0.5\n,\n \n1.0\n)\n.\nT\n,\n \n'ro'\n)\n\n  \nax\n.\nplot\n(\n*\ngenerate_random_points\n(\n5\n,\n \n1.5\n,\n \n2.0\n)\n.\nT\n,\n \nstyle\n \nor\n \n'ro'\n)\n\n\n  \nreturn\n \nax\n\n\n\n\ndef\n \nplot_an_example\n(\nstyle\n=\nNone\n,\n \ncolor\n=\nNone\n,\n \nlabel\n=\n\"Class\"\n):\n\n  \n\"\"\"Plot an example of supervised or unsupervised learning\"\"\"\n\n  \nax\n \n=\n \nplot_random_points\n(\nstyle\n,\n \ncolor\n)\n\n\n  \n# circle areas related to each set of points\n\n  \n# pyplot.Circle((x, y), r); (x, y) - the center of a circle; r - radius\n\n  \n# lw - line width\n\n  \nax\n.\nadd_artist\n(\nplt\n.\nCircle\n((\n0.75\n,\n \n0.75\n),\n \n0.5\n,\n \nfill\n=\n0\n,\n \ncolor\n=\n'r'\n,\n \nlw\n=\n2\n))\n\n  \nax\n.\nadd_artist\n(\nplt\n.\nCircle\n((\n1.75\n,\n \n1.75\n),\n \n0.5\n,\n \nfill\n=\n0\n,\n \ncolor\n=\ncolor\n \nor\n \n'r'\n,\n \nlw\n=\n2\n))\n\n\n  \n# put group labels\n\n  \n# pyplot.text just put arbitrary text in given coordinates\n\n  \nax\n.\ntext\n(\n0.65\n,\n \n1.4\n,\n \nlabel\n \n+\n \n\" I\"\n,\n \nfontdict\n=\n{\n'color'\n:\n \n'r'\n})\n\n  \nax\n.\ntext\n(\n1.65\n,\n \n1.1\n,\n \nlabel\n \n+\n \n\" II\"\n,\n \nfontdict\n=\n{\n'color'\n:\n \ncolor\n \nor\n \n'r'\n})\n\n\n\n\n\n\nIntroduction\n\u00b6\n\n\nWhat is machine learning?\n\u00b6\n\n\n+-------------------------------------------------------------------------+\n|                                                                         |\n|  Any technique which enables                                            |\n|  computers to mimic human                      Artificial Intelligence  |\n|  intelligence                                                           |\n|                                                                         |\n|     +-------------------------------------------------------------------+\n|     |                                                                   |\n|     |   Statistical techniques which                                    |\n|     |   enable computers to improve               Machine Learning      |\n|     |   with experience (subset of AI)                                  |\n|     |                                                                   |\n|     |       +-----------------------------------------------------------+\n|     |       |                                                           |\n|     |       |  Subset of ML which makes                                 |\n|     |       |  the computations using              Deep Learning        |\n|     |       |  multi-layer neural networks                              |\n|     |       |                                                           |\n+-----+-------+-----------------------------------------------------------+\n\n\n\n\n\nSupervised learning\n\u00b6\n\n\n\n\n\n\nProblems: classification, regression\n\n\n\n\n\n\nLet \n\\vec x_i \\in X\n\\vec x_i \\in X\n be feature vectors\n\n\n\n\n\n\nLet \ny_i \\in Y\ny_i \\in Y\n be class labels\n\n\n\n\n\n\nLet \nh: X \\rightarrow Y\nh: X \\rightarrow Y\n be hypothesis\n\n\n\n\n\n\nFind \nh(\\vec x)\nh(\\vec x)\n given \nN\nN\n training examples \n\\left\\{(\\vec x_1, y_1), ..., (\\vec x_N, y_N)\\right\\}\n\\left\\{(\\vec x_1, y_1), ..., (\\vec x_N, y_N)\\right\\}\n\n\n\n\n\n\nplot_an_example\n(\nstyle\n=\n'bs'\n,\n \ncolor\n=\n'b'\n);\n\n\n\n\n\n\n\n\nUnsupervised learning\n\u00b6\n\n\n\n\n\n\nIn opposite to supervised learning data is not labeled\n\n\n\n\n\n\nProblems: clustering, association\n\n\n\n\n\n\nFor example: k-means clustering, self-organizing maps\n\n\n\n\n\n\nplot_an_example\n(\nlabel\n=\n\"Cluster\"\n);\n\n\n\n\n\n\n\n\nExample: Supervised vs Unsupervised\n\u00b6\n\n\n\n\n\n\nHaving \nN\nN\n photos of different animals\n\n\n\n\n\n\nSupervised task (requires labeled data)\n\n\n\n\n\n\n\n\nTrain an algorithm to recognise given species on a photo.\n\n\nOutput: There is X on a photo.\n\n\n\n\n\n\nUnsupervised task\n\n\n\n\n\n\nTrain an algorithm to group animals with similar features.\n\n\nOutput: No idea what it is, but it looks similar to these animals.\n\n\n\n\nReinforcement learning\n\u00b6\n\n\n                +---------+\n                |         |\n       +--------+  AGENT  | <------+\n       |        |         |        |\n       |        +---------+        |\n       |                           | Observation\nAction |                           |\n       |                           | Reward\n       |     +---------------+     |\n       |     |               |     |\n       +---> |  ENVIRONMENT  +-----+\n             |               |\n             +---------------+\n\n\n\n\n\nML applications\n\u00b6\n\n\n\n\n\n\nImage recognition\n\n\n\n\n\n\nGoogle Maps\n - finding licence plates and faces; extracting street names and building numbers\n\n\n\n\n\n\nFacebook\n - recognising similar faces\n\n\n\n\n\n\n\n\n\n\nSpeech recognition\n\n\n\n\n\n\nMicrosoft\n - Cortana\n\n\n\n\n\n\nApple\n - Siri\n\n\n\n\n\n\n\n\n\n\nNatural Language Processing\n\n\n\n\n\n\nGoogle Translate\n - machine translation\n\n\n\n\n\n\nNext Game of Thrones Book\n - language modeling\n\n\n\n\n\n\n\n\n\n\nMisc\n\n\n\n\n\n\nPayPal\n - fraud alert\n\n\n\n\n\n\nNetflix\n, \nAmazon\n - recommendation system\n\n\n\n\n\n\nArt\n\n\n\n\n\n\n\n\n\n\nAlphaGo\n\n\n\n\n\n\n\n\n\n\nML Fails\n\u00b6\n\n\n\n\n\n\nAmazon's Alexa - TV broadcast caused many orders around San Diego when presenter said \nI love the little girl, saying 'Alexa ordered me a dollhouse'.\n\n\n\n\n\n\nAmazon's Alexa - when a kid asked for his favorite song \nDigger, Digger\n Alexa's respond was: \nYou want to hear a station for porn detected \u2026 hot chick amateur girl sexy.\n\n\n\n\n\n\nMicrosoft's Tay chatbot learned from tweets how to be racist\n\n\n\n\n\n\n\n\n\n\nPassport checker rejects Asian's photo because \neyes are closed\n\n\n\n\n\n\n\n\nSo make sure you can not relate to this\n\n\n\n\n\n\nML Frameworks\n\u00b6\n\n\n\n\n\n\nTensorflow\n by Google - Python (and somewhat in C/C++)\n\n\n\n\n\n\nCaffe\n by Berkeley Vision and Learning Center - C/C++, Python, MATLAB, Command line interface\n\n\n\n\n\n\nTorch\n by many - Lua and C/C++\n\n\n\n\n\n\nTheano\n by University of Montreal - Python (development stopped in 2017)\n\n\n\n\n\n\nscikit-learn\n by many - Python\n\n\n\n\n\n\nand many others",
            "title": "Introduction"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#introduction-to-machine-learning",
            "text": "",
            "title": "Introduction to Machine Learning"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#lecturer-details",
            "text": "Tomasz Golan    email:  tomasz.golan@uwr.edu.pl    room@ift: 438    phone: +48 71 375-9405    consultations:    Monday 11-12    Thursday 16-17",
            "title": "Lecturer details"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#lecture-details",
            "text": "",
            "title": "Lecture details"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#plan",
            "text": "Introduction    k-Nearest Neighbors    Decision Tree    Support Vector Machine    Multilayer Perceptron    Deep Learning",
            "title": "Plan"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#literature",
            "text": "\u201cDeep Learning\u201d by Ian Goodfellow, Yoshua Bengio, Aaron Courville    \u201cPattern Recognition and Machine Learning\u201d by Christopher Bishop",
            "title": "Literature"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#recommended-prerequisite-knowledge",
            "text": "Linear algebra    Calculus    Python",
            "title": "Recommended prerequisite knowledge"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#exam",
            "text": "In the form of the presentation    Individual or group project    At least one machine learning algorithm must be used    With the model description included",
            "title": "Exam"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#useful-but-not-interesting-functions",
            "text": "Here, I just define some functions used for making demo plots during the introduction.    Feel free to look at them later (especially if you are not familiar with  numpy  and  matplotlib ).    But now let's skip them.    # numpy and matplotlib will be used a lot during the lecture  # if you are familiar with these libraries you may skip this part  # if not - extended comments were added to make it easier to understand  # it is kind of standard to import numpy as np and pyplot as plt  import   numpy   as   np  import   matplotlib.pyplot   as   plt  # used later to apply different colors in for loops  mpl_colors   =   ( 'r' ,   'b' ,   'g' ,   'c' ,   'm' ,   'y' ,   'k' ,   'w' )  # just to overwrite default colab style  plt . style . use ( 'default' )  plt . style . use ( 'seaborn-talk' )  def   generate_random_points ( size = 10 ,   low = 0 ,   high = 1 ): \n   \"\"\"Generate a set of random 2D points    size -- number of points to generate    low  -- min value    high -- max value    \"\"\" \n   # random_sample([size]) returns random numbers with shape defined by size \n   # e.g. \n   # >>> np.random.random_sample((2, 3)) \n   # \n   # array([[ 0.44013807,  0.77358569,  0.64338619], \n   #        [ 0.54363868,  0.31855232,  0.16791031]]) \n   # \n   return   ( high   -   low )   *   np . random . random_sample (( size ,   2 ))   +   low  def   init_plot ( x_range = None ,   y_range = None ,   x_label = \"$x_1$\" ,   y_label = \"$x_2$\" ): \n   \"\"\"Set axes limits and labels    x_range -- [min x, max x]    y_range -- [min y, max y]    x_label -- string    y_label -- string    \"\"\" \n\n   # subplots returns figure and axes \n   # (in general you may want many axes on one figure) \n   # we do not need fig here \n   # but we will apply changes (including adding points) to axes \n   _ ,   ax   =   plt . subplots ( dpi = 70 ) \n\n   # set grid style and color \n   ax . grid ( c = '0.70' ,   linestyle = ':' ) \n\n   # set axes limits (x_range and y_range is a list with two elements) \n   ax . set_xlim ( x_range )  \n   ax . set_ylim ( y_range ) \n\n   # set axes labels \n   ax . set_xlabel ( x_label ) \n   ax . set_ylabel ( y_label ) \n\n   # return axes so we can continue modyfing them later \n   return   ax  def   plot_random_points ( style = None ,   color = None ): \n   \"\"\"Generate and plot two (separated) sets of random points    style -- latter group points style (default as first)    color -- latter group color (default as first)    \"\"\" \n\n   # create a plot with x and y ranges from 0 to 2.5 \n   ax   =   init_plot ([ 0 ,   2.5 ],   [ 0 ,   2.5 ]) \n\n   # add two different sets of random points \n   # first set = 5 points from [0.5, 1.0]x[0.5, 1.0] \n   # second set = 5 points from [1.5, 2.0]x[1.5, 2.0] \n   # generate_random_points return a numpy array in the format like \n   # [[x1, y1], [x2, y2], ..., [xn, yn]] \n   # pyplot.plt take separately arrays with X and Y, like \n   # plot([x1, x2, x3], [y1, y2, y3]) \n   # thus, we transpose numpy array to the format \n   # [[x1, x2, ..., xn], [y1, y2, ..., yn]] \n   # and unpack it with * \n   ax . plot ( * generate_random_points ( 5 ,   0.5 ,   1.0 ) . T ,   'ro' ) \n   ax . plot ( * generate_random_points ( 5 ,   1.5 ,   2.0 ) . T ,   style   or   'ro' ) \n\n   return   ax  def   plot_an_example ( style = None ,   color = None ,   label = \"Class\" ): \n   \"\"\"Plot an example of supervised or unsupervised learning\"\"\" \n   ax   =   plot_random_points ( style ,   color ) \n\n   # circle areas related to each set of points \n   # pyplot.Circle((x, y), r); (x, y) - the center of a circle; r - radius \n   # lw - line width \n   ax . add_artist ( plt . Circle (( 0.75 ,   0.75 ),   0.5 ,   fill = 0 ,   color = 'r' ,   lw = 2 )) \n   ax . add_artist ( plt . Circle (( 1.75 ,   1.75 ),   0.5 ,   fill = 0 ,   color = color   or   'r' ,   lw = 2 )) \n\n   # put group labels \n   # pyplot.text just put arbitrary text in given coordinates \n   ax . text ( 0.65 ,   1.4 ,   label   +   \" I\" ,   fontdict = { 'color' :   'r' }) \n   ax . text ( 1.65 ,   1.1 ,   label   +   \" II\" ,   fontdict = { 'color' :   color   or   'r' })",
            "title": "useful (but not interesting) functions"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#introduction",
            "text": "",
            "title": "Introduction"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#what-is-machine-learning",
            "text": "+-------------------------------------------------------------------------+\n|                                                                         |\n|  Any technique which enables                                            |\n|  computers to mimic human                      Artificial Intelligence  |\n|  intelligence                                                           |\n|                                                                         |\n|     +-------------------------------------------------------------------+\n|     |                                                                   |\n|     |   Statistical techniques which                                    |\n|     |   enable computers to improve               Machine Learning      |\n|     |   with experience (subset of AI)                                  |\n|     |                                                                   |\n|     |       +-----------------------------------------------------------+\n|     |       |                                                           |\n|     |       |  Subset of ML which makes                                 |\n|     |       |  the computations using              Deep Learning        |\n|     |       |  multi-layer neural networks                              |\n|     |       |                                                           |\n+-----+-------+-----------------------------------------------------------+",
            "title": "What is machine learning?"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#supervised-learning",
            "text": "Problems: classification, regression    Let  \\vec x_i \\in X \\vec x_i \\in X  be feature vectors    Let  y_i \\in Y y_i \\in Y  be class labels    Let  h: X \\rightarrow Y h: X \\rightarrow Y  be hypothesis    Find  h(\\vec x) h(\\vec x)  given  N N  training examples  \\left\\{(\\vec x_1, y_1), ..., (\\vec x_N, y_N)\\right\\} \\left\\{(\\vec x_1, y_1), ..., (\\vec x_N, y_N)\\right\\}    plot_an_example ( style = 'bs' ,   color = 'b' );",
            "title": "Supervised learning"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#unsupervised-learning",
            "text": "In opposite to supervised learning data is not labeled    Problems: clustering, association    For example: k-means clustering, self-organizing maps    plot_an_example ( label = \"Cluster\" );",
            "title": "Unsupervised learning"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#example-supervised-vs-unsupervised",
            "text": "Having  N N  photos of different animals    Supervised task (requires labeled data)     Train an algorithm to recognise given species on a photo.  Output: There is X on a photo.    Unsupervised task    Train an algorithm to group animals with similar features.  Output: No idea what it is, but it looks similar to these animals.",
            "title": "Example: Supervised vs Unsupervised"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#reinforcement-learning",
            "text": "+---------+\n                |         |\n       +--------+  AGENT  | <------+\n       |        |         |        |\n       |        +---------+        |\n       |                           | Observation\nAction |                           |\n       |                           | Reward\n       |     +---------------+     |\n       |     |               |     |\n       +---> |  ENVIRONMENT  +-----+\n             |               |\n             +---------------+",
            "title": "Reinforcement learning"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#ml-applications",
            "text": "Image recognition    Google Maps  - finding licence plates and faces; extracting street names and building numbers    Facebook  - recognising similar faces      Speech recognition    Microsoft  - Cortana    Apple  - Siri      Natural Language Processing    Google Translate  - machine translation    Next Game of Thrones Book  - language modeling      Misc    PayPal  - fraud alert    Netflix ,  Amazon  - recommendation system    Art      AlphaGo",
            "title": "ML applications"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#ml-fails",
            "text": "Amazon's Alexa - TV broadcast caused many orders around San Diego when presenter said  I love the little girl, saying 'Alexa ordered me a dollhouse'.    Amazon's Alexa - when a kid asked for his favorite song  Digger, Digger  Alexa's respond was:  You want to hear a station for porn detected \u2026 hot chick amateur girl sexy.    Microsoft's Tay chatbot learned from tweets how to be racist      Passport checker rejects Asian's photo because  eyes are closed     So make sure you can not relate to this",
            "title": "ML Fails"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#ml-frameworks",
            "text": "Tensorflow  by Google - Python (and somewhat in C/C++)    Caffe  by Berkeley Vision and Learning Center - C/C++, Python, MATLAB, Command line interface    Torch  by many - Lua and C/C++    Theano  by University of Montreal - Python (development stopped in 2017)    scikit-learn  by many - Python    and many others",
            "title": "ML Frameworks"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/",
            "text": "k-Nearest Neighbors\n\u00b6\n\n\nuseful (but not interesting) functions\n\u00b6\n\n\n\n\n\n\nHere, I just define some functions used for making demo plots during the introduction.\n\n\n\n\n\n\nFeel free to look at them later (especially if you are not familiar with \nnumpy\n and \nmatplotlib\n).\n\n\n\n\n\n\nBut now let's skip them.\n\n\n\n\n\n\n# numpy and matplotlib will be used a lot during the lecture\n\n\n# if you are familiar with these libraries you may skip this part\n\n\n# if not - extended comments were added to make it easier to understand\n\n\n\n# it is kind of standard to import numpy as np and pyplot as plt\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\nimport\n \nmatplotlib.pyplot\n \nas\n \nplt\n\n\n\n# used later to apply different colors in for loops\n\n\nmpl_colors\n \n=\n \n(\n'r'\n,\n \n'b'\n,\n \n'g'\n,\n \n'c'\n,\n \n'm'\n,\n \n'y'\n,\n \n'k'\n,\n \n'w'\n)\n\n\n\n# just to overwrite default colab style\n\n\nplt\n.\nstyle\n.\nuse\n(\n'default'\n)\n\n\nplt\n.\nstyle\n.\nuse\n(\n'seaborn-talk'\n)\n\n\n\n\ndef\n \ngenerate_random_points\n(\nsize\n=\n10\n,\n \nlow\n=\n0\n,\n \nhigh\n=\n1\n):\n\n  \n\"\"\"Generate a set of random 2D points\n\n\n\n  size -- number of points to generate\n\n\n  low  -- min value\n\n\n  high -- max value\n\n\n  \"\"\"\n\n  \n# random_sample([size]) returns random numbers with shape defined by size\n\n  \n# e.g.\n\n  \n# >>> np.random.random_sample((2, 3))\n\n  \n#\n\n  \n# array([[ 0.44013807,  0.77358569,  0.64338619],\n\n  \n#        [ 0.54363868,  0.31855232,  0.16791031]])\n\n  \n#\n\n  \nreturn\n \n(\nhigh\n \n-\n \nlow\n)\n \n*\n \nnp\n.\nrandom\n.\nrandom_sample\n((\nsize\n,\n \n2\n))\n \n+\n \nlow\n\n\n\n\ndef\n \ninit_plot\n(\nx_range\n=\nNone\n,\n \ny_range\n=\nNone\n,\n \nx_label\n=\n\"$x_1$\"\n,\n \ny_label\n=\n\"$x_2$\"\n):\n\n  \n\"\"\"Set axes limits and labels\n\n\n\n  x_range -- [min x, max x]\n\n\n  y_range -- [min y, max y]\n\n\n  x_label -- string\n\n\n  y_label -- string\n\n\n  \"\"\"\n\n\n  \n# subplots returns figure and axes\n\n  \n# (in general you may want many axes on one figure)\n\n  \n# we do not need fig here\n\n  \n# but we will apply changes (including adding points) to axes\n\n  \n_\n,\n \nax\n \n=\n \nplt\n.\nsubplots\n(\ndpi\n=\n70\n)\n\n\n  \n# set grid style and color\n\n  \nax\n.\ngrid\n(\nc\n=\n'0.70'\n,\n \nlinestyle\n=\n':'\n)\n\n\n  \n# set axes limits (x_range and y_range is a list with two elements)\n\n  \nax\n.\nset_xlim\n(\nx_range\n)\n \n  \nax\n.\nset_ylim\n(\ny_range\n)\n\n\n  \n# set axes labels\n\n  \nax\n.\nset_xlabel\n(\nx_label\n)\n\n  \nax\n.\nset_ylabel\n(\ny_label\n)\n\n\n  \n# return axes so we can continue modyfing them later\n\n  \nreturn\n \nax\n\n\n\n\ndef\n \nplot_random_points\n(\nstyle\n=\nNone\n,\n \ncolor\n=\nNone\n):\n\n  \n\"\"\"Generate and plot two (separated) sets of random points\n\n\n\n  style -- latter group points style (default as first)\n\n\n  color -- latter group color (default as first)\n\n\n  \"\"\"\n\n\n  \n# create a plot with x and y ranges from 0 to 2.5\n\n  \nax\n \n=\n \ninit_plot\n([\n0\n,\n \n2.5\n],\n \n[\n0\n,\n \n2.5\n])\n\n\n  \n# add two different sets of random points\n\n  \n# first set = 5 points from [0.5, 1.0]x[0.5, 1.0]\n\n  \n# second set = 5 points from [1.5, 2.0]x[1.5, 2.0]\n\n  \n# generate_random_points return a numpy array in the format like\n\n  \n# [[x1, y1], [x2, y2], ..., [xn, yn]]\n\n  \n# pyplot.plt take separately arrays with X and Y, like\n\n  \n# plot([x1, x2, x3], [y1, y2, y3])\n\n  \n# thus, we transpose numpy array to the format\n\n  \n# [[x1, x2, ..., xn], [y1, y2, ..., yn]]\n\n  \n# and unpack it with *\n\n  \nax\n.\nplot\n(\n*\ngenerate_random_points\n(\n5\n,\n \n0.5\n,\n \n1.0\n)\n.\nT\n,\n \n'ro'\n)\n\n  \nax\n.\nplot\n(\n*\ngenerate_random_points\n(\n5\n,\n \n1.5\n,\n \n2.0\n)\n.\nT\n,\n \nstyle\n \nor\n \n'ro'\n)\n\n\n  \nreturn\n \nax\n\n\n\n\ndef\n \nplot_an_example\n(\nstyle\n=\nNone\n,\n \ncolor\n=\nNone\n,\n \nlabel\n=\n\"Class\"\n):\n\n  \n\"\"\"Plot an example of supervised or unsupervised learning\"\"\"\n\n  \nax\n \n=\n \nplot_random_points\n(\nstyle\n,\n \ncolor\n)\n\n\n  \n# circle areas related to each set of points\n\n  \n# pyplot.Circle((x, y), r); (x, y) - the center of a circle; r - radius\n\n  \n# lw - line width\n\n  \nax\n.\nadd_artist\n(\nplt\n.\nCircle\n((\n0.75\n,\n \n0.75\n),\n \n0.5\n,\n \nfill\n=\n0\n,\n \ncolor\n=\n'r'\n,\n \nlw\n=\n2\n))\n\n  \nax\n.\nadd_artist\n(\nplt\n.\nCircle\n((\n1.75\n,\n \n1.75\n),\n \n0.5\n,\n \nfill\n=\n0\n,\n \ncolor\n=\ncolor\n \nor\n \n'r'\n,\n \nlw\n=\n2\n))\n\n\n  \n# put group labels\n\n  \n# pyplot.text just put arbitrary text in given coordinates\n\n  \nax\n.\ntext\n(\n0.65\n,\n \n1.4\n,\n \nlabel\n \n+\n \n\" I\"\n,\n \nfontdict\n=\n{\n'color'\n:\n \n'r'\n})\n\n  \nax\n.\ntext\n(\n1.65\n,\n \n1.1\n,\n \nlabel\n \n+\n \n\" II\"\n,\n \nfontdict\n=\n{\n'color'\n:\n \ncolor\n \nor\n \n'r'\n})\n\n\n\n\n\n\nOur first ML problem\n\u00b6\n\n\n\n\n\n\nTwo classes: red circles and blue squares (\ntraining\n samples)\n\n\n\n\n\n\nWhere does the green triangle (\ntest\n sample) belong?\n\n\n\n\n\n\nX1\n \n=\n \ngenerate_random_points\n(\n20\n,\n \n0\n,\n \n1\n)\n\n\nX2\n \n=\n \ngenerate_random_points\n(\n20\n,\n \n1\n,\n \n2\n)\n\n\n\nnew_point\n \n=\n \ngenerate_random_points\n(\n1\n,\n \n0\n,\n \n2\n)\n\n\n\nplot\n \n=\n \ninit_plot\n([\n0\n,\n \n2\n],\n \n[\n0\n,\n \n2\n])\n  \n# [0, 2] x [0, 2]\n\n\n\nplot\n.\nplot\n(\n*\nX1\n.\nT\n,\n \n'ro'\n,\n \n*\nX2\n.\nT\n,\n \n'bs'\n,\n \n*\nnew_point\n.\nT\n,\n \n'g^'\n);\n\n\n\n\n\n\n\n\nNearest Neighbor\n\u00b6\n\n\n\n\n\n\nThe nearest neigbor classifier \ncompares\n a test sample with all training samples to predict a label (class).\n\n\n\n\n\n\nHow to compare two samples?\n\n\n\n\n\n\nL1 distance: \nd(\\vec x_1, \\vec x_2) = \\sum\\limits_i |x_1^i - x_2^i|\nd(\\vec x_1, \\vec x_2) = \\sum\\limits_i |x_1^i - x_2^i|\n\n\n\n\n\n\nL2 distance: \nd(\\vec x_1, \\vec x_2) = \\sqrt{\\sum\\limits_i (x_1^i - x_2^i)^2}\nd(\\vec x_1, \\vec x_2) = \\sqrt{\\sum\\limits_i (x_1^i - x_2^i)^2}\n\n\n\n\n\n\nnote: in practice square root is ignored (becasue is monotonic function)\n\n\n\n\n\n\nL2 is less forgiving than L1 - prefers many small disagreements than one big one\n\n\n\n\n\n\n\n\n\n\ncosine distance (cosine similarity): \nd(\\vec x_1, \\vec x_2) = \\frac{x_1 \\cdot x_1}{||\\vec x_1|| \\cdot ||\\vec x_2||}\nd(\\vec x_1, \\vec x_2) = \\frac{x_1 \\cdot x_1}{||\\vec x_1|| \\cdot ||\\vec x_2||}\n\n\n\n\n\n\nChebyshev distance: \nd(\\vec x_1, \\vec x_2) = max_i(|x_1^i - x_2^i|)\nd(\\vec x_1, \\vec x_2) = max_i(|x_1^i - x_2^i|)\n\n\n\n\n\n\nand many others\n\n\n\n\n\n\n\n\n\n\nThe closest one determines the test sample label\n\n\n\n\n\n\nImplementation\n\u00b6\n\n\n\n\n\n\nThe implementation of nearest neighbor algorithm is pretty straightforward\n\n\n\n\n\n\nThere is no real training process here - we just need to remember all training feature vectors and corresponding labels\n\n\n\n\n\n\nTo predict a label for new sample we just need to find the label of the closest point from training samples\n\n\n\n\n\n\nclass\n \nNearestNeighbor\n():\n\n  \n\"\"\"Nearest Neighbor Classifier\"\"\"\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \ndistance\n=\n0\n):\n\n    \n\"\"\"Set distance definition: 0 - L1, 1 - L2\"\"\"\n\n    \nif\n \ndistance\n \n==\n \n0\n:\n\n      \nself\n.\ndistance\n \n=\n \nnp\n.\nabs\n     \n# absolute value\n\n    \nelif\n \ndistance\n \n==\n \n1\n:\n\n      \nself\n.\ndistance\n \n=\n \nnp\n.\nsquare\n  \n# square root\n\n    \nelse\n:\n\n      \nraise\n \nException\n(\n\"Distance not defined.\"\n)\n\n\n\n  \ndef\n \ntrain\n(\nself\n,\n \nx\n,\n \ny\n):\n\n    \n\"\"\"Train the classifier (here simply save training data)\n\n\n\n    x -- feature vectors (N x D)\n\n\n    y -- labels (N x 1)\n\n\n    \"\"\"\n\n    \nself\n.\nx_train\n \n=\n \nx\n\n    \nself\n.\ny_train\n \n=\n \ny\n\n\n\n  \ndef\n \npredict\n(\nself\n,\n \nx\n):\n\n    \n\"\"\"Predict and return labels for each feature vector from x\n\n\n\n    x -- feature vectors (N x D)\n\n\n    \"\"\"\n\n    \npredictions\n \n=\n \n[]\n  \n# placeholder for N labels\n\n\n    \n# loop over all test samples\n\n    \nfor\n \nx_test\n \nin\n \nx\n:\n\n      \n# array of distances between current test and all training samples\n\n      \ndistances\n \n=\n \nnp\n.\nsum\n(\nself\n.\ndistance\n(\nself\n.\nx_train\n \n-\n \nx_test\n),\n \naxis\n=\n1\n)\n\n\n      \n# get the closest one\n\n      \nmin_index\n \n=\n \nnp\n.\nargmin\n(\ndistances\n)\n\n\n      \n# add corresponding label\n\n      \npredictions\n.\nappend\n(\nself\n.\ny_train\n[\nmin_index\n])\n\n\n    \nreturn\n \npredictions\n\n\n\n\n\n\nThe magic of numpy\n\u00b6\n\n\n\n\n\n\nNumPy is irreplacable tool for numerical operations on arrays\n\n\n\n\n\n\nUsing numpy we could easily find all distances using one line\n\n\n\n\n\n\ndistances = np.sum(self.distance(self.x_train - x_test), axis=1)\n\n\n\n\n\n\n\nHere is how it works\n\n\n\n\n# let's create an array with 5x2 shape\n\n\na\n \n=\n \nnp\n.\nrandom\n.\nrandom_sample\n((\n5\n,\n \n2\n))\n\n\n\n# and another array with 1x2 shape\n\n\nb\n \n=\n \nnp\n.\narray\n([[\n1.\n,\n \n1.\n]])\n\n\n\nprint\n(\na\n,\n \nb\n,\n \nsep\n=\n\"\n\\n\\n\n\"\n)\n\n\n\n\n\n\n[[0.79036457 0.36571819]\n [0.76743991 0.08439684]\n [0.56876884 0.97967839]\n [0.77020776 0.21238365]\n [0.94235534 0.73884472]]\n\n[[1. 1.]]\n\n\n\n\n\n# subtract arguments (element-wise)\n\n\n# note, that at least one dimension must be the same \n\n\nprint\n(\na\n \n-\n \nb\n)\n\n\n\n\n\n\n[[-0.20963543 -0.63428181]\n [-0.23256009 -0.91560316]\n [-0.43123116 -0.02032161]\n [-0.22979224 -0.78761635]\n [-0.05764466 -0.26115528]]\n\n\n\n\n\n# numpy.abs calculates absolute value (element-wise)\n\n\nprint\n(\nnp\n.\nabs\n(\na\n \n-\n \nb\n))\n\n\n\n\n\n\n[[0.20963543 0.63428181]\n [0.23256009 0.91560316]\n [0.43123116 0.02032161]\n [0.22979224 0.78761635]\n [0.05764466 0.26115528]]\n\n\n\n\n\n# sum all elements\n\n\nnp\n.\nsum\n(\nnp\n.\nabs\n(\na\n \n-\n \nb\n))\n\n\n\n\n\n\n3.7798417848539096\n\n\n\n\n\n# sum elements over a given axis\n\n\nnp\n.\nsum\n(\nnp\n.\nabs\n(\na\n \n-\n \nb\n),\n \naxis\n=\n0\n)\n\n\n\n\n\n\narray([1.16086358, 2.61897821])\n\n\n\n\n\nnp\n.\nsum\n(\nnp\n.\nabs\n(\na\n \n-\n \nb\n),\n \naxis\n=\n1\n)\n\n\n\n\n\n\narray([0.84391724, 1.14816326, 0.45155276, 1.01740859, 0.31879994])\n\n\n\n\n\nAnalysis\n\u00b6\n\n\n\n\n\n\nBefore we start using \nNearestNeighbor\n let's create a simple mini-framework to apply NN and visualize results easily\n\n\n\n\n\n\nWe want to initilize \nNearestNeighbor\n with some feature vectors (and automatically assign labels for each class)\n\n\n\n\n\n\nWe want our test samples to be a grid of uniformly distributed points\n\n\n\n\n\n\nWe want methods to process test data and to make a plots with final results\n\n\n\n\n\n\nclass\n \nAnalysis\n():\n\n  \n\"\"\"Apply NearestNeighbor to generated (uniformly) test samples.\"\"\"\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \n*\nx\n,\n \ndistance\n):\n\n    \n\"\"\"Generate labels and initilize classifier\n\n\n\n    x -- feature vectors arrays\n\n\n    distance -- 0 for L1, 1 for L2    \n\n\n    \"\"\"\n\n    \n# get number of classes\n\n    \nself\n.\nnof_classes\n \n=\n \nlen\n(\nx\n)\n\n\n    \n# create lables array\n\n    \n# np.ones creates an array of given shape filled with 1 of given type\n\n    \n# we apply consecutive integer numbers as class labels\n\n    \n# ravel return flatten array\n\n    \ny\n \n=\n \n[\ni\n \n*\n \nnp\n.\nones\n(\n_x\n.\nshape\n[\n0\n],\n \ndtype\n=\nnp\n.\nint\n)\n \nfor\n \ni\n,\n \n_x\n \nin\n \nenumerate\n(\nx\n)]\n\n    \ny\n \n=\n \nnp\n.\narray\n(\ny\n)\n.\nravel\n()\n\n\n    \n# save training samples to plot them later\n\n    \nself\n.\nx_train\n \n=\n \nx\n\n\n    \n# merge feature vector arrays for NearestNeighbor\n\n    \nx\n \n=\n \nnp\n.\nconcatenate\n(\nx\n,\n \naxis\n=\n0\n)\n\n\n    \n# train classifier\n\n    \nself\n.\nnn\n \n=\n \nNearestNeighbor\n(\ndistance\n)\n\n    \nself\n.\nnn\n.\ntrain\n(\nx\n,\n \ny\n)\n\n\n\n  \ndef\n \nprepare_test_samples\n(\nself\n,\n \nlow\n=\n0\n,\n \nhigh\n=\n2\n,\n \nstep\n=\n0.01\n):\n\n    \n\"\"\"Generate a grid with test points (from low to high with step)\"\"\"\n\n    \n# remember range\n\n    \nself\n.\nrange\n \n=\n \n[\nlow\n,\n \nhigh\n]\n\n\n    \n# start with grid of points from [low, high] x [low, high]\n\n    \ngrid\n \n=\n \nnp\n.\nmgrid\n[\nlow\n:\nhigh\n+\nstep\n:\nstep\n,\n \nlow\n:\nhigh\n+\nstep\n:\nstep\n]\n\n\n    \n# convert to an array of 2D points\n\n    \nself\n.\nx_test\n \n=\n \nnp\n.\nvstack\n([\ngrid\n[\n0\n]\n.\nravel\n(),\n \ngrid\n[\n1\n]\n.\nravel\n()])\n.\nT\n\n\n\n  \ndef\n \nanalyse\n(\nself\n):\n\n    \n\"\"\"Run classifier on test samples and split them according to labels.\"\"\"\n\n\n    \n# find labels for test samples \n\n    \nself\n.\ny_test\n \n=\n \nself\n.\nnn\n.\npredict\n(\nself\n.\nx_test\n)\n\n\n    \nself\n.\nclassified\n \n=\n \n[]\n  \n# [class I test points, class II test ...]\n\n\n    \n# loop over available labels\n\n    \nfor\n \nlabel\n \nin\n \nrange\n(\nself\n.\nnof_classes\n):\n\n      \n# if i-th label == current label -> add test[i]\n\n      \nclass_i\n \n=\n \nnp\n.\narray\n([\nself\n.\nx_test\n[\ni\n]\n \\\n                          \nfor\n \ni\n,\n \nl\n \nin\n \nenumerate\n(\nself\n.\ny_test\n)\n \\\n                          \nif\n \nl\n \n==\n \nlabel\n])\n\n      \nself\n.\nclassified\n.\nappend\n(\nclass_i\n)\n\n\n\n  \ndef\n \nplot\n(\nself\n,\n \nt\n=\n''\n):\n\n    \n\"\"\"Visualize the result of classification\"\"\"\n\n    \nplot\n \n=\n \ninit_plot\n(\nself\n.\nrange\n,\n \nself\n.\nrange\n)\n\n    \nplot\n.\nset_title\n(\nt\n)\n\n    \nplot\n.\ngrid\n(\nFalse\n)\n\n\n    \n# plot training samples\n\n    \nfor\n \ni\n,\n \nx\n \nin\n \nenumerate\n(\nself\n.\nx_train\n):\n\n      \nplot\n.\nplot\n(\n*\nx\n.\nT\n,\n \nmpl_colors\n[\ni\n]\n \n+\n \n'o'\n)\n\n\n    \n# plot test samples\n\n    \nfor\n \ni\n,\n \nx\n \nin\n \nenumerate\n(\nself\n.\nclassified\n):\n\n      \nplot\n.\nplot\n(\n*\nx\n.\nT\n,\n \nmpl_colors\n[\ni\n]\n \n+\n \n','\n)\n\n\n\n\n\n\nL1 test\n\u00b6\n\n\nl1\n \n=\n \nAnalysis\n(\nX1\n,\n \nX2\n,\n \ndistance\n=\n0\n)\n\n\nl1\n.\nprepare_test_samples\n()\n\n\nl1\n.\nanalyse\n()\n\n\nl1\n.\nplot\n()\n\n\n\n\n\n\n\n\nL2 Test\n\u00b6\n\n\nl2\n \n=\n \nAnalysis\n(\nX1\n,\n \nX2\n,\n \ndistance\n=\n1\n)\n\n\nl2\n.\nprepare_test_samples\n()\n\n\nl2\n.\nanalyse\n()\n\n\nl2\n.\nplot\n()\n\n\n\n\n\n\n\n\nMulticlass classification\n\u00b6\n\n\n\n\n\n\nTraining samples from 4 squares:\n\n\n\n\n[0, 1] x [0, 1]\n\n\n[0, 1] x [1, 2]\n\n\n[1, 2] x [0, 1]\n\n\n[1, 2] x [1, 2]\n\n\n\n\n\n\n\n\nWe expect 4 squares created by test samples grid\n\n\n\n\n\n\nHow does it depend on the size of training samples?\n\n\n\n\n\n\ndef\n \ngenerate4\n(\nn\n=\n50\n):\n\n  \n\"\"\"Generate 4 sets of random points.\"\"\"\n\n\n  \n# points from [0, 1] x [0, 1]\n\n  \nX1\n \n=\n \ngenerate_random_points\n(\nn\n,\n \n0\n,\n \n1\n)\n\n  \n# points from [1, 2] x [1, 2]\n\n  \nX2\n \n=\n \ngenerate_random_points\n(\nn\n,\n \n1\n,\n \n2\n)\n\n  \n# points from [0, 1] x [1, 2]\n\n  \nX3\n \n=\n \nnp\n.\narray\n([[\nx\n,\n \ny\n+\n1\n]\n \nfor\n \nx\n,\ny\n \nin\n \ngenerate_random_points\n(\nn\n,\n \n0\n,\n \n1\n)])\n\n  \n# points from [1, 2] x [0, 1]\n\n  \nX4\n \n=\n \nnp\n.\narray\n([[\nx\n,\n \ny\n-\n1\n]\n \nfor\n \nx\n,\ny\n \nin\n \ngenerate_random_points\n(\nn\n,\n \n1\n,\n \n2\n)])\n\n\n  \nreturn\n \nX1\n,\n \nX2\n,\n \nX3\n,\n \nX4\n\n\n\n\n\n\n# loop over no. of training samples\n\n\nfor\n \nn\n \nin\n \n(\n5\n,\n \n10\n,\n \n50\n,\n \n100\n):\n\n  \n# generate 4 sets of random points (each one with n samples)\n\n  \n# unpack them when passing to Analysis\n\n  \nc4\n \n=\n \nAnalysis\n(\n*\ngenerate4\n(\nn\n),\n \ndistance\n=\n1\n)\n\n  \nc4\n.\nprepare_test_samples\n()\n\n  \nc4\n.\nanalyse\n()\n\n  \nc4\n.\nplot\n(\n\"No. of samples = {}\"\n.\nformat\n(\nn\n))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMessage 01: size matters!\n\n\nNoise\n\u00b6\n\n\n\n\n\n\nData are rarely perfect and you may expect some training samples to have unsual features\n\n\n\n\n\n\nFeatures shared by a majority of training samples are more important than a single occurrence\n\n\n\n\n\n\nLet's add some noise to our data and see how Nearest Neighbor deal with it\n\n\n\n\n\n\n# generate 4 classes of 2D points\n\n\nX1\n,\n \nX2\n,\n \nX3\n,\n \nX4\n \n=\n \ngenerate4\n()\n\n\n\n# add some noise by applying gaussian to every point coordinates\n\n\nnoise\n \n=\n \nlambda\n \nx\n,\n \ny\n:\n \n[\nnp\n.\nrandom\n.\nnormal\n(\nx\n,\n \n0.1\n),\n \nnp\n.\nrandom\n.\nnormal\n(\ny\n,\n \n0.1\n)]\n\n\n\nX1\n \n=\n \nnp\n.\narray\n([\nnoise\n(\nx\n,\n \ny\n)\n \nfor\n \nx\n,\n \ny\n \nin\n \nX1\n])\n\n\nX2\n \n=\n \nnp\n.\narray\n([\nnoise\n(\nx\n,\n \ny\n)\n \nfor\n \nx\n,\n \ny\n \nin\n \nX2\n])\n\n\nX3\n \n=\n \nnp\n.\narray\n([\nnoise\n(\nx\n,\n \ny\n)\n \nfor\n \nx\n,\n \ny\n \nin\n \nX3\n])\n\n\nX4\n \n=\n \nnp\n.\narray\n([\nnoise\n(\nx\n,\n \ny\n)\n \nfor\n \nx\n,\n \ny\n \nin\n \nX4\n])\n\n\n\n# perform analysis\n\n\nc4\n \n=\n \nAnalysis\n(\nX1\n,\n \nX2\n,\n \nX3\n,\n \nX4\n,\n \ndistance\n=\n1\n)\n\n\nc4\n.\nprepare_test_samples\n()\n\n\nc4\n.\nanalyse\n()\n\n\nc4\n.\nplot\n()\n\n\n\n\n\n\n\n\nOverfitting\n\u00b6\n\n\n\n\n\n\nThe above is an example of overfitting\n\n\n\n\nperfectly describe training data\n\n\nlose the generalization ability\n\n\n\n\n\n\n\n\nIn general you want to extract all common features from training samples, but neglect characteristic features of single sample\n\n\n\n\n\n\nMessage 02: avoid overfitting!\n\n\nAccuracy\n\u00b6\n\n\n\n\nAccuracy defines the fraction of (unseen) samples which are correctly classify by the algorithm \n\n\n\n\naccuracy\n \n=\n \n0\n\n\n\n# loop over (sample, reconstructed label)\n\n\nfor\n \nsample\n,\n \nlabel\n \nin\n \nzip\n(\nc4\n.\nx_test\n,\n \nc4\n.\ny_test\n):\n\n  \n# determine true label\n\n  \nif\n \nsample\n[\n0\n]\n \n<\n \n1\n \nand\n \nsample\n[\n1\n]\n \n<\n \n1\n:\n\n    \ntrue_label\n \n=\n \n0\n\n  \nelif\n \nsample\n[\n0\n]\n \n>\n \n1\n \nand\n \nsample\n[\n1\n]\n \n>\n \n1\n:\n\n    \ntrue_label\n \n=\n \n1\n\n  \nelif\n \nsample\n[\n0\n]\n \n<\n \n1\n \nand\n \nsample\n[\n1\n]\n \n>\n \n1\n:\n\n    \ntrue_label\n \n=\n \n2\n\n  \nelse\n:\n\n    \ntrue_label\n \n=\n \n3\n\n\n  \nif\n \ntrue_label\n \n==\n \nlabel\n:\n \naccuracy\n \n+=\n \n1\n\n\n\naccuracy\n \n/=\n \nlen\n(\nc4\n.\nx_test\n)\n\n\n\nprint\n(\naccuracy\n)\n\n\n\n\n\n\n0.924878097076805\n\n\n\n\n\n\n\n\n\nPlease note, that this is a toy model - in the case of real problems there is no way to determine true labels (otherwise there is no point to use ML methods...)\n\n\n\n\n\n\nTo measure accuracy of the model one usually splits data into:\n\n\n\n\n\n\ntraining samples (usually about 80%)\n\n\n\n\n\n\ntest samples (usually about 20%)\n\n\n\n\n\n\n\n\n\n\nAfter the model is trained on training samples, the accuracy is measured on test samples\n\n\n\n\n\n\nMessage 03: keep some data for testing!\n\n\nk-Nearest Neighbors\n\u00b6\n\n\n\n\nInstead of letting one closest neighbor to decide, let \nk\n nearest neghbors to vote\n\n\n\n\nImplementation\n\u00b6\n\n\n\n\n\n\nWe can base the implementation on \nNearestNeighbor\n, but\n\n\n\n\n\n\nThe \nconstructor\n has an extra parameter \nk\n\n\n\n\n\n\nand we need to override \npredict\n method\n\n\n\n\n\n\nclass\n \nkNearestNeighbors\n(\nNearestNeighbor\n):\n\n  \n\"\"\"k-Nearest Neighbor Classifier\"\"\"\n\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \nk\n=\n1\n,\n \ndistance\n=\n0\n):\n\n    \n\"\"\"Set distance definition: 0 - L1, 1 - L2\"\"\"\n\n    \nsuper\n()\n.\n__init__\n(\ndistance\n)\n\n    \nself\n.\nk\n \n=\n \nk\n\n\n\n  \ndef\n \npredict\n(\nself\n,\n \nx\n):\n\n    \n\"\"\"Predict and return labels for each feature vector from x\n\n\n\n    x -- feature vectors (N x D)\n\n\n    \"\"\"\n\n    \npredictions\n \n=\n \n[]\n  \n# placeholder for N labels\n\n\n    \n# no. of classes = max label (labels starts from 0)\n\n    \nnof_classes\n \n=\n \nnp\n.\namax\n(\nself\n.\ny_train\n)\n \n+\n \n1\n\n\n    \n# loop over all test samples\n\n    \nfor\n \nx_test\n \nin\n \nx\n:\n\n      \n# array of distances between current test and all training samples\n\n      \ndistances\n \n=\n \nnp\n.\nsum\n(\nself\n.\ndistance\n(\nself\n.\nx_train\n \n-\n \nx_test\n),\n \naxis\n=\n1\n)\n\n\n      \n# placeholder for labels votes\n\n      \nvotes\n \n=\n \nnp\n.\nzeros\n(\nnof_classes\n,\n \ndtype\n=\nnp\n.\nint\n)\n\n\n      \n# find k closet neighbors and vote\n\n      \n# argsort returns the indices that would sort an array\n\n      \n# so indices of nearest neighbors\n\n      \n# we take self.k first\n\n      \nfor\n \nneighbor_id\n \nin\n \nnp\n.\nargsort\n(\ndistances\n)[:\nself\n.\nk\n]:\n\n        \n# this is a label corresponding to one of the closest neighbor\n\n        \nneighbor_label\n \n=\n \nself\n.\ny_train\n[\nneighbor_id\n]\n\n        \n# which updates votes array\n\n        \nvotes\n[\nneighbor_label\n]\n \n+=\n \n1\n\n\n      \n# predicted label is the one with most votes\n\n      \npredictions\n.\nappend\n(\nnp\n.\nargmax\n(\nvotes\n))\n\n\n    \nreturn\n \npredictions\n\n\n\n\n\n\nkAnalysis\n\u00b6\n\n\n\n\nWe also create \nkAnalysis\n based on \nAnalysis\n for visualization of kNN results\n\n\n\n\nclass\n \nkAnalysis\n(\nAnalysis\n):\n\n  \n\"\"\"Apply kNearestNeighbor to generated (uniformly) test samples.\"\"\"\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \n*\nx\n,\n \nk\n=\n1\n,\n \ndistance\n=\n1\n):\n\n    \n\"\"\"Generate labels and initilize classifier\n\n\n\n    x -- feature vectors arrays\n\n\n    k -- number of nearest neighbors\n\n\n    distance -- 0 for L1, 1 for L2    \n\n\n    \"\"\"\n\n    \n# get number of classes\n\n    \nself\n.\nnof_classes\n \n=\n \nlen\n(\nx\n)\n\n\n    \n# create lables array\n\n    \ny\n \n=\n \n[\ni\n \n*\n \nnp\n.\nones\n(\n_x\n.\nshape\n[\n0\n],\n \ndtype\n=\nnp\n.\nint\n)\n \nfor\n \ni\n,\n \n_x\n \nin\n \nenumerate\n(\nx\n)]\n\n    \ny\n \n=\n \nnp\n.\narray\n(\ny\n)\n.\nravel\n()\n\n\n    \n# save training samples to plot them later\n\n    \nself\n.\nx_train\n \n=\n \nx\n\n\n    \n# merge feature vector arrays for NearestNeighbor\n\n    \nx\n \n=\n \nnp\n.\nconcatenate\n(\nx\n,\n \naxis\n=\n0\n)\n\n\n    \n# train classifier (knn this time)\n\n    \nself\n.\nnn\n \n=\n \nkNearestNeighbors\n(\nk\n,\n \ndistance\n)\n\n    \nself\n.\nnn\n.\ntrain\n(\nx\n,\n \ny\n)\n\n\n\n\n\n\nSanity check\n\u00b6\n\n\n\n\nk-Nearest Neighbor classifier with \nk = 1\n must give exactly the same results as Nearest Neighbor\n\n\n\n\n# apply kNN with k=1 on the same set of training samples\n\n\nknn\n \n=\n \nkAnalysis\n(\nX1\n,\n \nX2\n,\n \nX3\n,\n \nX4\n,\n \nk\n=\n1\n,\n \ndistance\n=\n1\n)\n\n\nknn\n.\nprepare_test_samples\n()\n\n\nknn\n.\nanalyse\n()\n\n\nknn\n.\nplot\n()\n\n\n\n\n\n\n\n\nk-Test\n\u00b6\n\n\n\n\n\n\nFor \nk = 1\n kNN is likely to overfit the problem\n\n\n\n\n\n\nAlthough, it does not mean that higher \nk\n is better!\n\n\n\n\n\n\nNow, let's see how different values of \nk\n affects the result\n\n\n\n\n\n\nLater, we will learn how to find optimal value of \nk\n for given problem\n\n\n\n\n\n\n# training size = 50\n\n\n# let's check a few values between 1 and 50\n\n\nfor\n \nk\n \nin\n \n(\n1\n,\n \n5\n,\n \n10\n,\n \n50\n):\n\n  \nknn\n \n=\n \nkAnalysis\n(\nX1\n,\n \nX2\n,\n \nX3\n,\n \nX4\n,\n \nk\n=\nk\n,\n \ndistance\n=\n1\n)\n\n  \nknn\n.\nprepare_test_samples\n()\n\n  \nknn\n.\nanalyse\n()\n\n  \nknn\n.\nplot\n(\n\"k = {}\"\n.\nformat\n(\nk\n))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHyperparameters\n\u00b6\n\n\n\n\n\n\nML model may have some hyperparameters - parameters set before training\n\n\n\n\n\n\nPlease note, ML algorithm may have also parameters which are set during training\n\n\n\n\n\n\nIn the case of kNN there are two hyperparameters:\n\n\n\n\n\n\nnumber of nearest neihgbors (\nk\n)\n\n\n\n\n\n\nthe definition of distance\n\n\n\n\n\n\n\n\n\n\nThe choice of hyperparameters values highly depends on a problem\n\n\n\n\n\n\nThe wrong choice of hyperparameters may lead to underfitting or overfitting\n\n\n\n\n\n\nOver-, under-fitting example\n\u00b6\n\n\n# generate random data from x^2 function (with some noise)\n\n\ndata\n \n=\n \nnp\n.\narray\n([[\nx\n,\n \nnp\n.\nrandom\n.\nnormal\n(\nx\n**\n2\n,\n \n0.1\n)]\n \\\n                 \nfor\n \nx\n \nin\n \n2\n*\nnp\n.\nrandom\n.\nrandom\n(\n10\n)\n \n-\n \n1\n])\n\n\n\nplot\n \n=\n \ninit_plot\n([\n-\n1\n,\n \n1\n],\n \n[\n-\n1\n,\n \n1\n])\n\n\nplot\n.\nplot\n(\n*\ndata\n.\nT\n,\n \n'o'\n);\n\n\n\n\n\n\n\n\n\n\n\n\nLet's try to fit this data to a polynomial\n\n\n\n\n\n\nThe degree is a hyperparamter (which defines number of coefficients)\n\n\n\n\n\n\n# loop over degrees of polynomial\n\n\n# data is x^2, so let's try degrees 1, 2, 10\n\n\nfor\n \nn\n \nin\n \n(\n1\n,\n \n2\n,\n \n10\n):\n\n  \n# polyfit returns an array with polynomial coefficients\n\n  \n# poly1d is a polynomial class\n\n  \nf\n \n=\n \nnp\n.\npoly1d\n(\nnp\n.\npolyfit\n(\n*\ndata\n.\nT\n,\n \nn\n))\n\n\n  \n# returns an array with 100 uniformly distributed numbers from -1 to 1\n\n  \nx\n \n=\n \nnp\n.\nlinspace\n(\n-\n1\n,\n \n1\n,\n \n100\n)\n\n\n  \nplot\n \n=\n \ninit_plot\n([\n-\n1\n,\n \n1\n],\n \n[\n-\n1\n,\n \n1\n])\n\n  \nplot\n.\nset_title\n(\n\"n = {}\"\n.\nformat\n(\nn\n))\n\n  \nplot\n.\nplot\n(\n*\ndata\n.\nT\n,\n \n'o'\n,\n \nx\n,\n \nf\n(\nx\n))\n\n\n\n\n\n\n/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RankWarning: Polyfit may be poorly conditioned\n  after removing the cwd from sys.path.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor \nn = 1\n we clearly underfit the data as we do not have enough parameters to describe the complexity of the problem\n\n\n\n\n\n\nFor \nn = 2\n we have appropriate capacity (as we actually generated data form \nx^2\nx^2\n function)\n\n\n\n\n\n\nFor \nn = 10\n we overfit the data - training samples are described perfectly, but we clearly lost the generalization ability\n\n\n\n\n\n\nMessage 04: right choice of hyperparameters is crucial!\n\n\nValidation dataset\n\u00b6\n\n\n\n\n\n\nOne splits data into training and test samples\n\n\n\n\n\n\ntraining samples are used to optimize model parameters\n\n\n\n\n\n\ntest samples are used to measure accuracy\n\n\n\n\n\n\nthere is no rule of thumb on how to split dataset\n\n\n\n\n\n\n\n\n\n\nIf a model has some hyperparameters the part of training set is used for valitation samples:\n\n\n\n\n\n\ntraining samples - tuning model parameters\n\n\n\n\n\n\nvalidation samples - tuning hyperparameters\n\n\n\n\n\n\n\n\n\n\n                  +---------------------+      +------------------------+\n+----------+      |                     |      |                        |\n|          |      | Measure accuracy on |      | Measure final accuracy |\n| Training | +--> |                     | +--> |                        |\n|          |      | validation samples  |      | on test samples        |\n+----------+      |                     |      |                        |\n     ^            +----------+----------+      +------------------------+\n     |                       |\n     |      Change           | \n     +-----------------------+\n         hyperparameters\n\n\n\n\n\nIris dataset\n\u00b6\n\n\n\n\n\n\nThe data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. \nsrc\n\n\n\n\n\n\nAttribute Information:\n\n\n\n\n\n\nsepal length in cm\n\n\n\n\n\n\nsepal width in cm\n\n\n\n\n\n\npetal length in cm\n\n\n\n\n\n\npetal width in cm\n\n\n\n\n\n\nclass: \n\n\n\n\n\n\nIris Setosa\n\n\n\n\n\n\nIris Versicolour\n\n\n\n\n\n\nIris Virginica\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoad dataset\n\u00b6\n\n\n\n\nWe use \npandas\n for data manipulation - it is super handy and supports many formats\n\n\n\n\nimport\n \npandas\n \nas\n \npd\n\n\n\n# columns names - can be used to access columns later\n\n\ncolumns\n \n=\n \n[\n\"Sepal Length\"\n,\n \n\"Sepal Width\"\n,\n\n           \n\"Petal Length\"\n,\n \n\"Petal Width\"\n,\n\n           \n\"Class\"\n]\n\n\n\n# iris.data is a csv file\n\n\nsrc\n \n=\n \n\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n\n\n\n# load the file with pandas.read_csv \n\n\n# it will name columns as defined in columns list\n\n\n# so one can access a column through index or name\n\n\niris_data\n \n=\n \npd\n.\nread_csv\n(\nsrc\n,\n \nheader\n=\nNone\n,\n \nnames\n=\ncolumns\n)\n\n\n\n\n\n\niris_data\n.\nhead\n()\n  \n# print a few first entries\n\n\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nSepal Length\n\n      \nSepal Width\n\n      \nPetal Length\n\n      \nPetal Width\n\n      \nClass\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n5.1\n\n      \n3.5\n\n      \n1.4\n\n      \n0.2\n\n      \nIris-setosa\n\n    \n\n    \n\n      \n1\n\n      \n4.9\n\n      \n3.0\n\n      \n1.4\n\n      \n0.2\n\n      \nIris-setosa\n\n    \n\n    \n\n      \n2\n\n      \n4.7\n\n      \n3.2\n\n      \n1.3\n\n      \n0.2\n\n      \nIris-setosa\n\n    \n\n    \n\n      \n3\n\n      \n4.6\n\n      \n3.1\n\n      \n1.5\n\n      \n0.2\n\n      \nIris-setosa\n\n    \n\n    \n\n      \n4\n\n      \n5.0\n\n      \n3.6\n\n      \n1.4\n\n      \n0.2\n\n      \nIris-setosa\n\n    \n\n  \n\n\n\n\n\n\n\nVisualize dataset\n\u00b6\n\n\n\n\n\n\npandas\n offers plotting through \nmatplotlib\n integration\n\n\n\n\n\n\nLet's visualize Iris data\n\n\n\n\n\n\nLet's keep the code short - sorry if it is hard to follow\n\n\n\n\n\n\n# to extract rows with class column == class_name\n\n\nextract\n \n=\n \nlambda\n \nclass_name\n:\n \niris_data\n.\nloc\n[\niris_data\n[\n'Class'\n]\n \n==\n \nclass_name\n]\n\n\n\n# axes settings - part = Sepal or Petal; x = Length, y = Width\n\n\nset_ax\n \n=\n \nlambda\n \npart\n:\n \n{\n\"x\"\n:\n \npart\n \n+\n \n\" Length\"\n,\n\n                       \n\"y\"\n:\n \npart\n \n+\n \n\" Width\"\n,\n\n                       \n\"kind\"\n:\n \n\"scatter\"\n}\n\n\n\n# add iris type / sepal or petal / color to existing axis\n\n\nplot\n \n=\n \nlambda\n \nclass_name\n,\n \npart\n,\n \ncolor\n,\n \naxis\n:\n \\\n  \nextract\n(\nclass_name\n)\n.\nplot\n(\n**\nset_ax\n(\npart\n),\n\n                           \ncolor\n=\ncolor\n,\n\n                           \nlabel\n=\nclass_name\n,\n\n                           \nax\n=\naxis\n)\n\n\n\n# plot all Iris types (sepal or petal) on existing axis\n\n\nplot_all\n \n=\n \nlambda\n \npart\n,\n \naxis\n:\n \\\n  \n[\nplot\n(\niris\n,\n \npart\n,\n \nmpl_colors\n[\ni\n],\n \naxis\n)\n \\\n   \nfor\n \ni\n,\n \niris\n \nin\n \nenumerate\n(\nset\n(\niris_data\n[\n'Class'\n]))]\n \n\n\n\n\n\n# with pyplot.subplots we can create many plots on one figure\n\n\n# here we create 2 plots - 1 row and 2 columns\n\n\n# thus, subplots returns figure, axes of 1st plot, axes for 2nd plot\n\n\n_\n,\n \n(\nax1\n,\n \nax2\n)\n \n=\n \nplt\n.\nsubplots\n(\n1\n,\n \n2\n,\n \nfigsize\n=\n(\n9\n,\n4\n))\n\n\n\n# using messy lambda we can plot all Iris types at once\n\n\n# Petal data on 1st plots and Sepal data on 2nd plot\n\n\nplot_all\n(\n\"Petal\"\n,\n \nax1\n)\n\n\nplot_all\n(\n\"Sepal\"\n,\n \nax2\n)\n\n\n\n# tight_layout adjust subplots params so they fit into figure ares\n\n\nplt\n.\ntight_layout\n()\n\n\n\n\n\n\n\n\nPrepare feature vectors and labels\n\u00b6\n\n\n\n\n\n\nFirst step is to prepare data - we need feature vectors with corresponding labels\n\n\n\n\n\n\nIn this case every sample's feature vector is 4D (sepal length, sepal width, petal length, petal width) and is labeled with one of three classes (Iris Setosa, Iris Versicolour, Iris Virginica)\n\n\n\n\n\n\n# every Iris has 4 features (forming our 4D feature vectors)\n\n\n# pandaoc.DataFrame.iloc allows us access data through indices\n\n\n# we create an array with feature vectors by taking all rows for first 4 columns\n\n\nX\n \n=\n \niris_data\n.\niloc\n[:,\n \n:\n4\n]\n\n\n\n# it is still pandoc.DataFrame object - pretty handy\n\n\nX\n.\nhead\n()\n\n\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nSepal Length\n\n      \nSepal Width\n\n      \nPetal Length\n\n      \nPetal Width\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n5.1\n\n      \n3.5\n\n      \n1.4\n\n      \n0.2\n\n    \n\n    \n\n      \n1\n\n      \n4.9\n\n      \n3.0\n\n      \n1.4\n\n      \n0.2\n\n    \n\n    \n\n      \n2\n\n      \n4.7\n\n      \n3.2\n\n      \n1.3\n\n      \n0.2\n\n    \n\n    \n\n      \n3\n\n      \n4.6\n\n      \n3.1\n\n      \n1.5\n\n      \n0.2\n\n    \n\n    \n\n      \n4\n\n      \n5.0\n\n      \n3.6\n\n      \n1.4\n\n      \n0.2\n\n    \n\n  \n\n\n\n\n\n\n\n\n\npandas.DataFrame\n object are handy to manipulate data, but at the end of the day we want to perform algebra with \nnumpy\n\n\n\n\n# create numpy array (matrix) for further processing\n\n\nX\n \n=\n \nnp\n.\narray\n(\nX\n)\n\n\n\n# print a few first entries\n\n\nprint\n(\nX\n[:\n5\n])\n\n\n\n\n\n\n[[5.1 3.5 1.4 0.2]\n [4.9 3.  1.4 0.2]\n [4.7 3.2 1.3 0.2]\n [4.6 3.1 1.5 0.2]\n [5.  3.6 1.4 0.2]]\n\n\n\n\n\n\n\nfrom the las column (\"Class\") we create our labels\n\n\n\n\n# as mentioned before, we can access DataFrame object through column labels\n\n\nY\n \n=\n \nnp\n.\narray\n(\niris_data\n[\n\"Class\"\n])\n\n\n\n# print a few first entries\n\n\nprint\n(\nY\n[:\n5\n])\n\n\n\n\n\n\n['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa']\n\n\n\n\n\nPrepare test dataset\n\u00b6\n\n\n\n\n\n\nLet's use 80% for training and 20% for testing\n\n\n\n\n\n\nWe, obviously, can not just take last 20% of samples for testing because our data is ordered\n\n\n\n\n\n\nBut we can randomly select 20% of samples\n\n\n\n\n\n\nEasy to do by hand, but let's start to use some ML frameworks\n\n\n\n\n\n\nfrom\n \nsklearn.model_selection\n \nimport\n \ntrain_test_split\n \nas\n \nsplit\n\n\n\n# train_test_split: Split arrays or matrices into random train and test subsets\n\n\nX_train\n,\n \nX_test\n,\n \nY_train\n,\n \nY_test\n \n=\n \nsplit\n(\nX\n,\n \nY\n,\n \ntest_size\n=\n0.2\n)\n\n\n\n# let's use 20% of training samples for validation\n\n\nX_train\n,\n \nX_valid\n,\n \nY_train\n,\n \nY_valid\n \n=\n \nsplit\n(\nX_train\n,\n \nY_train\n,\n \ntest_size\n=\n0.2\n)\n\n\n\n# check how many sample we have\n\n\nprint\n(\nX_train\n.\nshape\n[\n0\n],\n \nX_valid\n.\nshape\n[\n0\n],\n \nX_test\n.\nshape\n[\n0\n])\n\n\n\n\n\n\n96 24 30\n\n\n\n\n\nkNN from scikit-learn\n\u00b6\n\n\n\n\n\n\nscikit-learn\n has already implemented k-Nearest Neighbor algorithm (which is more flexible than the one implemented during this lecture)\n\n\n\n\n\n\nLet's see how \ncomplicated\n is using one of ML frameworks with Python\n\n\n\n\n\n\nfrom\n \nsklearn.neighbors\n \nimport\n \nKNeighborsClassifier\n\n\n\n# create knn classifier with k = 48\n\n\nknn\n \n=\n \nKNeighborsClassifier\n(\nn_neighbors\n=\n48\n)\n\n\n\n# train the model\n\n\nknn\n.\nfit\n(\nX_train\n,\n \nY_train\n)\n\n\n\n# predict labels for test samples\n\n\nY_pred\n \n=\n \nknn\n.\npredict\n(\nX_valid\n)\n\n\n\n\n\n\nAccuracy\n\u00b6\n\n\n\n\nFirst let's print true labels along with predicted ones\n\n\n\n\n# use bold if true != predicted\n\n\nfor\n \ntrue\n,\n \npred\n \nin\n \nzip\n(\nY_valid\n,\n \nY_pred\n):\n\n  \nif\n \npred\n \n==\n \ntrue\n:\n\n    \nprint\n(\n\"{}\n\\t\n -> {}\"\n.\nformat\n(\ntrue\n,\n \npred\n))\n\n  \nelse\n:\n\n    \nprint\n(\n\"\n\\033\n[1m{}\n\\t\n -> {}\n\\033\n[0m\"\n.\nformat\n(\ntrue\n,\n \npred\n))\n\n\n\n\n\n\nIris-setosa  -> Iris-setosa\nIris-versicolor  -> Iris-versicolor\nIris-setosa  -> Iris-setosa\nIris-versicolor  -> Iris-versicolor\nIris-virginica   -> Iris-virginica\nIris-virginica   -> Iris-virginica\n\u001b[1mIris-versicolor  -> Iris-virginica\u001b[0m\nIris-virginica   -> Iris-virginica\nIris-versicolor  -> Iris-versicolor\nIris-setosa  -> Iris-setosa\nIris-virginica   -> Iris-virginica\nIris-versicolor  -> Iris-versicolor\nIris-virginica   -> Iris-virginica\n\u001b[1mIris-virginica   -> Iris-versicolor\u001b[0m\nIris-virginica   -> Iris-virginica\nIris-virginica   -> Iris-virginica\nIris-versicolor  -> Iris-versicolor\nIris-setosa  -> Iris-setosa\nIris-setosa  -> Iris-setosa\nIris-virginica   -> Iris-virginica\n\u001b[1mIris-virginica   -> Iris-versicolor\u001b[0m\nIris-setosa  -> Iris-setosa\nIris-versicolor  -> Iris-versicolor\n\u001b[1mIris-virginica   -> Iris-versicolor\u001b[0m\n\n\n\n\n\n\n\nWe can easily calculate accuracy by hand as it is just a number of correctly predicted labels divided by no. of samples\n\n\n\n\n# Y_valid == Y_pred -> array of True/False (if two elements are equal or not)\n\n\n# (Y_valid == Y_pred).sum() -> number of Trues\n\n\n# Y_valid.shape[0] -> number of validation samples\n\n\naccuracy\n \n=\n \n(\nY_valid\n \n==\n \nY_pred\n)\n.\nsum\n()\n \n/\n \nY_valid\n.\nshape\n[\n0\n]\n\n\n\nprint\n(\naccuracy\n)\n\n\n\n\n\n\n0.8333333333333334\n\n\n\n\n\n\n\nBut we can also use \nscikit-learn\n function \naccuracy_score\n\n\n\n\nfrom\n \nsklearn.metrics\n \nimport\n \naccuracy_score\n\n\n\nprint\n(\naccuracy_score\n(\nY_valid\n,\n \nY_pred\n))\n\n\n\n\n\n\n0.8333333333333334\n\n\n\n\n\nk-dependence of the accuracy\n\u00b6\n\n\n\n\n\n\nLet's use validation set to determine the best hyperparameter \nk\n\n\n\n\n\n\nWe will run kNN for various values of \nk\n and measure accuracy\n\n\n\n\n\n\nThis will allow us to find the optimal value of \nk\n\n\n\n\n\n\nAnd check the accuracy on the test dataset\n\n\n\n\n\n\nscores\n \n=\n \n[]\n  \n# placeholder for accuracy\n\n\n\nmax_k\n \n=\n \n85\n  \n# maximum number of voters\n\n\n\n# loop over different values of k\n\n\nfor\n \nk\n \nin\n \nrange\n(\n1\n,\n \nmax_k\n):\n\n  \n# create knn classifier with k = k\n\n  \nknn\n \n=\n \nKNeighborsClassifier\n(\nn_neighbors\n=\nk\n)\n\n\n  \n# train the model\n\n  \nknn\n.\nfit\n(\nX_train\n,\n \nY_train\n)\n\n\n  \n# predict labels for test samples\n\n  \nY_pred\n \n=\n \nknn\n.\npredict\n(\nX_valid\n)\n\n\n  \n# add accuracy to score table\n\n  \nscores\n.\nappend\n(\naccuracy_score\n(\nY_valid\n,\n \nY_pred\n))\n\n\n\n\n\n\n\n\nNow, we can plot accuracy as a function of \nk\n\n\n\n\ndef\n \nk_accuracy_plot\n(\nmax_k\n=\n85\n):\n\n  \n\"\"\"Just plot settings\"\"\"\n\n  \nplt\n.\ngrid\n(\nTrue\n)\n\n  \nplt\n.\nxlabel\n(\n\"k\"\n)\n\n  \nplt\n.\nylabel\n(\n\"Accuracy\"\n)\n\n  \nplt\n.\nxlim\n([\n0\n,\n \nmax_k\n \n+\n \n5\n])\n\n  \nplt\n.\nylim\n([\n0\n,\n \n1\n])\n\n  \nplt\n.\nxticks\n(\nrange\n(\n0\n,\n \nmax_k\n \n+\n \n5\n,\n \n5\n))\n\n\n  \nreturn\n \nplt\n\n\n\nk_accuracy_plot\n()\n.\nplot\n(\nrange\n(\n1\n,\n \nmax_k\n),\n \nscores\n);\n\n\n\n\n\n\n\n\n\n\nAnd check the accuracy measured on the test samples\n\n\n\n\nknn\n \n=\n \nKNeighborsClassifier\n(\nn_neighbors\n=\n9\n)\n\n\nknn\n.\nfit\n(\nX_train\n,\n \nY_train\n)\n\n\nY_pred\n \n=\n \nknn\n.\npredict\n(\nX_test\n)\n\n\n\nprint\n(\naccuracy_score\n(\nY_test\n,\n \nY_pred\n))\n\n\n\n\n\n\n0.9666666666666667\n\n\n\n\n\n\n\n\n\nThe accuracy plot is not smooth\n\n\n\n\n\n\nIt is common if one does not have enough validation samples\n\n\n\n\n\n\nBut there is another way to measure accuracy dependence on hyperparameters\n\n\n\n\n\n\nCross-validation\n\u00b6\n\n\n         Split training samples into N folds\n\n +-------+   +-------+   +-------+         +-------+\n |       |   |       |   |       |         |       |\n |   1   |   |   2   |   |   3   |   ...   |   N   |\n |       |   |       |   |       |         |       |\n +-------+   +-------+   +-------+         +-------+\n\nTake one fold as validation set and train on N-1 folds\n\n +-------+   +-------+   +-------+         +-------+\n |*******|   |       |   |       |         |       |\n |*******|   |   2   |   |   3   |   ...   |   N   |\n |*******|   |       |   |       |         |       |\n +-------+   +-------+   +-------+         +-------+\n\n         Take the next one as validation set\n\n +-------+   +-------+   +-------+         +-------+\n |       |   |*******|   |       |         |       |\n |   1   |   |*******|   |   3   |   ...   |   N   |\n |       |   |*******|   |       |         |       |\n +-------+   +-------+   +-------+         +-------+\n\n          Repeat the procedure for all folds\n\n +-------+   +-------+   +-------+         +-------+\n |       |   |       |   |       |         |*******|\n |   1   |   |   2   |   |   3   |   ...   |*******|\n |       |   |       |   |       |         |*******|\n +-------+   +-------+   +-------+         +-------+\n\n            And average out the accuracy\n\n\n\n\n\n\n\nOnce again \nscikit-learn\n has already implemented the procedure we need\n\n\n\n\nfrom\n \nsklearn.model_selection\n \nimport\n \ncross_val_score\n\n\n\n# this time we do not create dedicated validation set\n\n\nX_train\n,\n \nX_test\n,\n \nY_train\n,\n \nY_test\n \n=\n \nsplit\n(\nX\n,\n \nY\n,\n \ntest_size\n=\n0.2\n)\n\n\n\navg_scores\n \n=\n \n[]\n  \n# average score for different k\n\n\n\nnof_folds\n \n=\n \n10\n\n\n\n# loop over different values of k\n\n\nfor\n \nk\n \nin\n \nrange\n(\n1\n,\n \nmax_k\n):\n\n  \n# create knn classifier with k = k\n\n  \nknn\n \n=\n \nKNeighborsClassifier\n(\nn_neighbors\n=\nk\n)\n\n\n  \n# cross-validate knn on our training sample with nof_folds\n\n  \nscores\n \n=\n \ncross_val_score\n(\nknn\n,\n \nX_train\n,\n \nY_train\n,\n\n                           \ncv\n=\nnof_folds\n,\n \nscoring\n=\n'accuracy'\n)\n\n\n  \n# add avg accuracy to score table\n\n  \navg_scores\n.\nappend\n(\nscores\n.\nmean\n())\n\n\n\n\n\n\nk_accuracy_plot\n()\n.\nplot\n(\nrange\n(\n1\n,\n \nmax_k\n),\n \navg_scores\n);\n\n\n\n\n\n\n\n\n\n\n\n\nIn theory, k-fold cross-validation is the way to go (especially if a dataset is small)\n\n\n\n\n\n\nIn practice, people tend to use a single validation split as it is not that computational expensive\n\n\n\n\n\n\nData normalization\n\u00b6\n\n\n\n\n\n\nSometimes there is a need to preprocess data before training\n\n\n\n\n\n\nLet's imagine Iris sepal data is in cm but petal data in mm\n\n\n\n\n\n\n# original data - both in cm\n\n\nprint\n(\nX\n[:\n5\n])\n\n\n\n\n\n\n[[5.1 3.5 1.4 0.2]\n [4.9 3.  1.4 0.2]\n [4.7 3.2 1.3 0.2]\n [4.6 3.1 1.5 0.2]\n [5.  3.6 1.4 0.2]]\n\n\n\n\n\n# make a copy of X\n\n\nXmm\n \n=\n \nX\n.\ncopy\n()\n\n\n\n# and multiply last two columns by 0.1\n\n\nXmm\n[:,\n2\n:]\n \n*=\n \n0.1\n\n\n\n# and we have our fake Iris data with petal length/width in mm\n\n\nprint\n(\nXmm\n[:\n5\n])\n\n\n\n\n\n\n[[5.1  3.5  0.14 0.02]\n [4.9  3.   0.14 0.02]\n [4.7  3.2  0.13 0.02]\n [4.6  3.1  0.15 0.02]\n [5.   3.6  0.14 0.02]]\n\n\n\n\n\n\n\nLet's compare result of the same classifier on both dataset\n\n\n\n\ndef\n \nget_accuracy\n(\nX\n,\n \nY\n,\n \nk\n=\n10\n):\n\n  \n\"\"\"Make training and test datasets and process through kNN\"\"\"\n\n\n  \n# prepare training / test samples\n\n  \nX_train\n,\n \nX_test\n,\n \nY_train\n,\n \nY_test\n \n=\n \nsplit\n(\nX\n,\n \nY\n,\n \ntest_size\n=\n0.2\n)\n\n\n  \n# create a kNN with k = k\n\n  \nknn\n \n=\n \nKNeighborsClassifier\n(\nn_neighbors\n=\nk\n)\n\n\n  \n# get prediction for original dataset\n\n  \nknn\n.\nfit\n(\nX_train\n,\n \nY_train\n)\n\n  \nY_pred\n \n=\n \nknn\n.\npredict\n(\nX_test\n)\n\n\n  \nreturn\n \naccuracy_score\n(\nY_test\n,\n \nY_pred\n)\n\n\n\ncm\n \n=\n \nget_accuracy\n(\nX\n,\n \nY\n)\n\n\nmm\n \n=\n \nget_accuracy\n(\nXmm\n,\n \nY\n)\n\n\n\nprint\n(\n\"Accuracy:\n\\n\\t\nboth in cm: {}\n\\n\\t\npetal in mm: {}\"\n.\nformat\n(\ncm\n,\n \nmm\n))\n\n\n\n\n\n\nAccuracy:\n    both in cm: 1.0\n    petal in mm: 0.7\n\n\n\n\n\n\n\n\n\nIt is kind of obvious here - petal information will barely contribute to the distance\n\n\n\n\n\n\nHowever, it is not always obvious if some features are not suppressed by the way data is normalized\n\n\n\n\n\n\nMessage 05: be aware of data normalization!\n\n\nMNIST\n\u00b6\n\n\n\n\n\n\nTHE MNIST DATABASE of handwritten digits\n\n\n\n\n\n\nThe MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n\n\n\n\n\n\nIt is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.\n\n\n\n\n\n\n\n\n\n\n\n\nTo make it simpler (and faster) let's use digits toy dataset which comes with \nscikit-learn\n \nsrc\n\n\n\n\n\n\nEach datapoint is a 8x8 image of a digit.\n\n\n\n\n\n\nAbout 180 samples per class (digit)\n\n\n\n\n\n\nTotal number of samples 1797\n\n\n\n\n\n\nLoad digits\n\u00b6\n\n\nfrom\n \nsklearn.datasets\n \nimport\n \nload_digits\n\n\n\ndigits\n \n=\n \nload_digits\n()\n\n\n\nprint\n(\ndigits\n.\ndata\n.\nshape\n)\n\n\n\n\n\n\n(1797, 64)\n\n\n\n\n\n\n\n\n\ndigits.images\n is a \nnumpy\n array with 1797 \nnumpy\n arrays 8x8 (feature vectors) representing digits\n\n\n\n\n\n\ndigits.target\n is a \nnumpy\n array with 1797 integer numbers (class labels)\n\n\n\n\n\n\nthe code below allow us to visualize a random digits from the dataset\n\n\n\n\n\n\n# set grayscale\n\n\nplt\n.\ngray\n()\n\n\n\n# get some random index from 0 to dataset size\n\n\nrandom_index\n \n=\n \nnp\n.\nrandom\n.\nrandint\n(\n1796\n)\n\n\n\n\n# draw random digit\n\n\nplt\n.\nmatshow\n(\ndigits\n.\nimages\n[\nrandom_index\n])\n\n\n\n# and print the matrix\n\n\nplt\n.\ntext\n(\n8\n,\n \n5\n,\n \ndigits\n.\nimages\n[\nrandom_index\n],\n\n         \nfontdict\n=\n{\n'family'\n:\n \n'monospace'\n,\n \n'size'\n:\n \n16\n})\n\n\n\n# and the label\n\n\nplt\n.\ntext\n(\n10\n,\n \n1\n,\n \n\"This is: {}\"\n.\nformat\n(\ndigits\n.\ntarget\n[\nrandom_index\n]),\n\n         \nfontdict\n=\n{\n'family'\n:\n \n'monospace'\n,\n \n'size'\n:\n \n16\n});\n\n\n\n\n\n\n<matplotlib.figure.Figure at 0x7faccc90a048>\n\n\n\n\n\n\n\nDistance between images\n\u00b6\n\n\n  TEST      TRAIN    PIXEL-WISE\n| 4 2 0     2 5 8 |   |2 3 8|\n| 5 3 9  -  2 8 1 | = |3 5 8|  ->  38\n| 0 2 3     1 4 9 |   |1 2 6|\n\n\n\n\n\nPrepare data\n\u00b6\n\n\n\n\n\n\nWe need to split dataset to training and test samples\n\n\n\n\n\n\nHowever, images are in 8x8 format and we have to flatten them first\n\n\n\n\n\n\n# the original shape of an image\n\n\nprint\n(\ndigits\n.\nimages\n.\nshape\n)\n\n\n\n\n\n\n(1797, 8, 8)\n\n\n\n\n\n# numpy.reshape is handy here\n\n\nprint\n(\ndigits\n.\nimages\n.\nreshape\n((\n1797\n,\n \n-\n1\n))\n.\nshape\n)\n\n\n\n\n\n\n(1797, 64)\n\n\n\n\n\n\n\n\n\nPlease note -1 in new shape\n\n\n\n\n\n\nnumpy.reshape\n allows us to pass one \nunknown\n dimension which can be determined automatically\n\n\n\n\n\n\nThus, the above is equivalent to\n\n\n\n\n\n\nprint\n(\ndigits\n.\nimages\n.\nreshape\n((\n1797\n,\n \n64\n))\n.\nshape\n)\n\n\n\n\n\n\n(1797, 64)\n\n\n\n\n\nprint\n(\ndigits\n.\nimages\n.\nreshape\n((\n-\n1\n,\n \n64\n))\n.\nshape\n)\n\n\n\n\n\n\n(1797, 64)\n\n\n\n\n\n\n\nAs before, we can split our dataset using \nsklearn.model_selection.train_test_split\n\n\n\n\ndata_train\n,\n \ndata_test\n,\n \nlabel_train\n,\n \nlabel_test\n \n=\n \\\n  \nsplit\n(\ndigits\n.\nimages\n.\nreshape\n((\n1797\n,\n \n-\n1\n)),\n \ndigits\n.\ntarget\n,\n \ntest_size\n=\n0.2\n)\n\n\n\n\n\n\nCross-validation\n\u00b6\n\n\n\n\nWe perform cross-validation on training samples to determine the best \nk\n (as for the Iris dataset)\n\n\n\n\navg_scores\n \n=\n \n[]\n  \n# average score for different k\n\n\n\nmax_k\n \n=\n \n50\n\n\nnof_folds\n \n=\n \n10\n\n\n\n# loop over different values of k\n\n\nfor\n \nk\n \nin\n \nrange\n(\n1\n,\n \nmax_k\n):\n\n  \n# create knn classifier with k = k\n\n  \nknn\n \n=\n \nKNeighborsClassifier\n(\nn_neighbors\n=\nk\n)\n\n\n  \n# cross-validate knn on our training sample with nof_folds\n\n  \nscores\n \n=\n \ncross_val_score\n(\nknn\n,\n \ndata_train\n,\n \nlabel_train\n,\n\n                           \ncv\n=\nnof_folds\n,\n \nscoring\n=\n'accuracy'\n)\n\n\n  \n# add avg accuracy to score table\n\n  \navg_scores\n.\nappend\n(\nscores\n.\nmean\n())\n\n\n\n\n\n\nplt\n.\ngrid\n(\nTrue\n)\n\n\nplt\n.\nxlabel\n(\n\"k\"\n)\n\n\nplt\n.\nylabel\n(\n\"Accuracy\"\n)\n\n\nplt\n.\nxlim\n([\n0\n,\n \nmax_k\n])\n\n\nplt\n.\nylim\n([\n0\n,\n \n1\n])\n\n\nplt\n.\nxticks\n(\nrange\n(\n0\n,\n \nmax_k\n,\n \n5\n))\n\n\n\nplt\n.\nplot\n(\nrange\n(\n1\n,\n \nmax_k\n),\n \navg_scores\n);\n\n\n\n\n\n\n\n\n\n\n\n\nWe used nearly the same procedure as for the Iris dataset\n\n\n\n\n\n\nNote, that digits toy dataset prefer different \nk\n\n\n\n\n\n\nThis is the idea of ML - the same algorithm can solve different problems if train on different data\n\n\n\n\n\n\nNowadays, in ML field \ndata is more important than algorithms\n (we have good algorithms already) \n\n\n\n\n\n\nFinal test\n\u00b6\n\n\n\n\nLet's take the bes \nk\n and check how the classifier works on test samples\n\n\n\n\nfrom\n \nsklearn.metrics\n \nimport\n \naccuracy_score\n\n\n\nknn\n \n=\n \nKNeighborsClassifier\n(\nn_neighbors\n=\n1\n)\n\n\nknn\n.\nfit\n(\ndata_train\n,\n \nlabel_train\n)\n\n\nprediction\n \n=\n \nknn\n.\npredict\n(\ndata_test\n)\n\n\n\nprint\n(\naccuracy_score\n(\nlabel_test\n,\n \nprediction\n))\n\n\n\n\n\n\n0.9888888888888889\n\n\n\n\n\n\n\nWe can take a look at misclassified digits\n\n\n\n\nfor\n \ni\n,\n \n(\ntrue\n,\n \npredict\n)\n \nin\n \nenumerate\n(\nzip\n(\nlabel_test\n,\n \nprediction\n)):\n\n  \nif\n \ntrue\n \n!=\n \npredict\n:\n\n    \ndigit\n \n=\n \ndata_test\n[\ni\n]\n.\nreshape\n((\n8\n,\n \n8\n))\n  \n# reshape again to 8x8\n\n    \nplt\n.\nmatshow\n(\ndigit\n)\n                    \n# for matshow\n\n    \nplt\n.\ntitle\n(\n\"{} predicted as {}\"\n.\nformat\n(\ntrue\n,\n \npredict\n))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression with kNN\n\u00b6\n\n\n\n\n\n\nIt is also possible to do regression using k-Nearest Neighbors\n\n\n\n\n\n\nfind \nk\n nearest neighbors from training samples\n\n\n\n\n\n\ncalculate the predicted value using inverse distance weighting method\n\n\n\n\n\n\n\n\ny_{pred}(\\vec x) = \\frac{\\sum\\limits_i w_i(\\vec x) y_{train, i}}{\\sum\\limits_i w_i(\\vec x_i)}\n\n\ny_{pred}(\\vec x) = \\frac{\\sum\\limits_i w_i(\\vec x) y_{train, i}}{\\sum\\limits_i w_i(\\vec x_i)}\n\n\n\n\n\n\n\n\nwhere \nw_i(\\vec x) = \\frac{1}{d(\\vec x, \\vec x_{train, i})}\nw_i(\\vec x) = \\frac{1}{d(\\vec x, \\vec x_{train, i})}\n\n\n\n\n\n\nNote, that \ny_{pred}(\\vec x) = y_{train, i}\ny_{pred}(\\vec x) = y_{train, i}\n if \nd(\\vec x, \\vec x_{train, i}) = 0\nd(\\vec x, \\vec x_{train, i}) = 0\n\n\n\n\n\n\n\n\n\n\nGenearate some fake data\n\u00b6\n\n\n\n\n\n\nLet's grab some random points from the sine function\n\n\n\n\n\n\nAnd add some noise to make it more like real data\n\n\n\n\n\n\ndata_size\n \n=\n \n50\n\n\n\n# generate and sort *data_size* numbers from 0 to 4pi \n\n\nx_train\n \n=\n \n4\n \n*\n \nnp\n.\npi\n \n*\n \nnp\n.\nsort\n(\nnp\n.\nrandom\n.\nrand\n(\ndata_size\n,\n \n1\n),\n \naxis\n=\n0\n)\n\n\n\n# let's fit to sine  \n\n\ny_train\n \n=\n \nnp\n.\nsin\n(\nx_train\n)\n.\nravel\n()\n\n\n\n# add some noise to the data\n\n\ny_train\n \n=\n \nnp\n.\narray\n([\nnp\n.\nrandom\n.\nnormal\n(\ny\n,\n \n0.05\n)\n \nfor\n \ny\n \nin\n \ny_train\n])\n\n\n\nplt\n.\nplot\n(\nx_train\n,\n \ny_train\n,\n \n'ro'\n);\n\n\n\n\n\n\n\n\nMake a fit\n\u00b6\n\n\n\n\n\n\nIn general, one should do cross-validation to determine the best \nk\n\n\n\n\n\n\nWe will skip this part during the lecture (feel free to check this at home though!)\n\n\n\n\n\n\nLet's just check how kNN fit works for a few different values of \nk\n\n\n\n\n\n\nComment on \nnumpy.newaxis\n\u00b6\n\n\n# let's create a 1D numpy array\n\n\nD1\n \n=\n \nnp\n.\narray\n([\n1\n,\n \n2\n,\n \n3\n,\n \n4\n])\n\n\n\nprint\n(\nD1\n)\n\n\n\n\n\n\n[1 2 3 4]\n\n\n\n\n\n# we can easily add another dimension using numpy.newaxis\n\n\nD2\n \n=\n \nD1\n[:,\n \nnp\n.\nnewaxis\n]\n\n\n\nprint\n(\nD2\n)\n\n\n\n\n\n\n[[1]\n [2]\n [3]\n [4]]\n\n\n\n\n\nAnd back to the task\n\u00b6\n\n\n\n\nWe use kNN regressor from \nscikit-learn\n (from intro: \nWhat I really do...\n)\n\n\n\n\nfrom\n \nsklearn.neighbors\n \nimport\n \nKNeighborsRegressor\n\n\n\n# first we need test sample\n\n\nx_test\n \n=\n \nnp\n.\nlinspace\n(\n0\n,\n \n4\n*\nnp\n.\npi\n,\n \n100\n)[:,\n \nnp\n.\nnewaxis\n]\n\n\n\nfor\n \ni\n,\n \nk\n \nin\n \nenumerate\n((\n1\n,\n \n5\n,\n \n10\n,\n \n20\n)):\n\n  \n# weights=distance - weight using distances\n\n  \nknn\n \n=\n \nKNeighborsRegressor\n(\nk\n,\n \nweights\n=\n'distance'\n)\n\n\n  \n# calculate y_test for all points in x_test\n\n  \ny_test\n \n=\n \nknn\n.\nfit\n(\nx_train\n,\n \ny_train\n)\n.\npredict\n(\nx_test\n)\n\n\n  \nplt\n.\nsubplot\n(\n2\n,\n \n2\n,\n \ni\n \n+\n \n1\n)\n\n\n  \nplt\n.\ntitle\n(\n\"k = {}\"\n.\nformat\n(\nk\n))\n\n\n  \nplt\n.\nplot\n(\nx_train\n,\n \ny_train\n,\n \n'ro'\n,\n \nx_test\n,\n \ny_test\n,\n \n'g'\n);\n\n\n\nplt\n.\ntight_layout\n()\n\n\n\n\n\n\n\n\nSummary\n\u00b6\n\n\n\n\n\n\nWe have learned first ML algorithm - k-Nearest Neighbors\n\n\n\n\n\n\nIt has some pros:\n\n\n\n\n\n\neasy to understand and implement\n\n\n\n\n\n\nno time needed for training - may be used for initial analysis before one reaches for some \nheavier\n tool\n\n\n\n\n\n\nsolves nonlinear problems \n\n\n\n\n\n\nlimited number of hyperparameters\n\n\n\n\n\n\nno parameters!\n\n\n\n\n\n\nat the end of this lecture we will deal with tens of hyperparameters and thousands of parameters\n\n\n\n\n\n\n\n\n\n\nAlthough cons make it hard to use in practice\n\n\n\n\n\n\ntraining data must be kept for the whole time (so called \nlazy training\n)\n\n\n\n\n\n\nimagine having GB of training samples and you want to make mobile app\n\n\n\n\n\n\nother algorithms allows to discard training samples once the model is trained (\neager learning\n) - usually it means long training process but super fast classification (which is what we really want)\n\n\n\n\n\n\n\n\n\n\ndistance-comparing is not suitable for all data - a picture of a cat on a blue background (e.g. sky) can be close to a ship on a sea (because background pixels vote too)\n\n\n\n\ne.g. for \nCIFAR-10\n (60k pictures, 10 classes, more about that later) vanilla kNN get less than 40% accuracy\n\n\n\n\n\n\n\n\nstill better than random guessing (10%), but convolutional neural networks get >95%\n\n\n\n\n\n\n\n\n\n\nStill, we have learned from kNN a few important things:\n\n\n\n\n\n\nData is important (both size and quality)\n\n\n\n\n\n\nSometimes data requires preprocessing\n\n\n\n\n\n\nWrong choice of hyperparameters may lead to under- or over-fitting\n\n\n\n\n\n\nUse validation samples to tune the model\n\n\n\n\n\n\nAnd \nDO NOT\n touch test samples until you are done!",
            "title": "k-Nearest Neighbors"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#k-nearest-neighbors",
            "text": "",
            "title": "k-Nearest Neighbors"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#useful-but-not-interesting-functions",
            "text": "Here, I just define some functions used for making demo plots during the introduction.    Feel free to look at them later (especially if you are not familiar with  numpy  and  matplotlib ).    But now let's skip them.    # numpy and matplotlib will be used a lot during the lecture  # if you are familiar with these libraries you may skip this part  # if not - extended comments were added to make it easier to understand  # it is kind of standard to import numpy as np and pyplot as plt  import   numpy   as   np  import   matplotlib.pyplot   as   plt  # used later to apply different colors in for loops  mpl_colors   =   ( 'r' ,   'b' ,   'g' ,   'c' ,   'm' ,   'y' ,   'k' ,   'w' )  # just to overwrite default colab style  plt . style . use ( 'default' )  plt . style . use ( 'seaborn-talk' )  def   generate_random_points ( size = 10 ,   low = 0 ,   high = 1 ): \n   \"\"\"Generate a set of random 2D points    size -- number of points to generate    low  -- min value    high -- max value    \"\"\" \n   # random_sample([size]) returns random numbers with shape defined by size \n   # e.g. \n   # >>> np.random.random_sample((2, 3)) \n   # \n   # array([[ 0.44013807,  0.77358569,  0.64338619], \n   #        [ 0.54363868,  0.31855232,  0.16791031]]) \n   # \n   return   ( high   -   low )   *   np . random . random_sample (( size ,   2 ))   +   low  def   init_plot ( x_range = None ,   y_range = None ,   x_label = \"$x_1$\" ,   y_label = \"$x_2$\" ): \n   \"\"\"Set axes limits and labels    x_range -- [min x, max x]    y_range -- [min y, max y]    x_label -- string    y_label -- string    \"\"\" \n\n   # subplots returns figure and axes \n   # (in general you may want many axes on one figure) \n   # we do not need fig here \n   # but we will apply changes (including adding points) to axes \n   _ ,   ax   =   plt . subplots ( dpi = 70 ) \n\n   # set grid style and color \n   ax . grid ( c = '0.70' ,   linestyle = ':' ) \n\n   # set axes limits (x_range and y_range is a list with two elements) \n   ax . set_xlim ( x_range )  \n   ax . set_ylim ( y_range ) \n\n   # set axes labels \n   ax . set_xlabel ( x_label ) \n   ax . set_ylabel ( y_label ) \n\n   # return axes so we can continue modyfing them later \n   return   ax  def   plot_random_points ( style = None ,   color = None ): \n   \"\"\"Generate and plot two (separated) sets of random points    style -- latter group points style (default as first)    color -- latter group color (default as first)    \"\"\" \n\n   # create a plot with x and y ranges from 0 to 2.5 \n   ax   =   init_plot ([ 0 ,   2.5 ],   [ 0 ,   2.5 ]) \n\n   # add two different sets of random points \n   # first set = 5 points from [0.5, 1.0]x[0.5, 1.0] \n   # second set = 5 points from [1.5, 2.0]x[1.5, 2.0] \n   # generate_random_points return a numpy array in the format like \n   # [[x1, y1], [x2, y2], ..., [xn, yn]] \n   # pyplot.plt take separately arrays with X and Y, like \n   # plot([x1, x2, x3], [y1, y2, y3]) \n   # thus, we transpose numpy array to the format \n   # [[x1, x2, ..., xn], [y1, y2, ..., yn]] \n   # and unpack it with * \n   ax . plot ( * generate_random_points ( 5 ,   0.5 ,   1.0 ) . T ,   'ro' ) \n   ax . plot ( * generate_random_points ( 5 ,   1.5 ,   2.0 ) . T ,   style   or   'ro' ) \n\n   return   ax  def   plot_an_example ( style = None ,   color = None ,   label = \"Class\" ): \n   \"\"\"Plot an example of supervised or unsupervised learning\"\"\" \n   ax   =   plot_random_points ( style ,   color ) \n\n   # circle areas related to each set of points \n   # pyplot.Circle((x, y), r); (x, y) - the center of a circle; r - radius \n   # lw - line width \n   ax . add_artist ( plt . Circle (( 0.75 ,   0.75 ),   0.5 ,   fill = 0 ,   color = 'r' ,   lw = 2 )) \n   ax . add_artist ( plt . Circle (( 1.75 ,   1.75 ),   0.5 ,   fill = 0 ,   color = color   or   'r' ,   lw = 2 )) \n\n   # put group labels \n   # pyplot.text just put arbitrary text in given coordinates \n   ax . text ( 0.65 ,   1.4 ,   label   +   \" I\" ,   fontdict = { 'color' :   'r' }) \n   ax . text ( 1.65 ,   1.1 ,   label   +   \" II\" ,   fontdict = { 'color' :   color   or   'r' })",
            "title": "useful (but not interesting) functions"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#our-first-ml-problem",
            "text": "Two classes: red circles and blue squares ( training  samples)    Where does the green triangle ( test  sample) belong?    X1   =   generate_random_points ( 20 ,   0 ,   1 )  X2   =   generate_random_points ( 20 ,   1 ,   2 )  new_point   =   generate_random_points ( 1 ,   0 ,   2 )  plot   =   init_plot ([ 0 ,   2 ],   [ 0 ,   2 ])    # [0, 2] x [0, 2]  plot . plot ( * X1 . T ,   'ro' ,   * X2 . T ,   'bs' ,   * new_point . T ,   'g^' );",
            "title": "Our first ML problem"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#nearest-neighbor",
            "text": "The nearest neigbor classifier  compares  a test sample with all training samples to predict a label (class).    How to compare two samples?    L1 distance:  d(\\vec x_1, \\vec x_2) = \\sum\\limits_i |x_1^i - x_2^i| d(\\vec x_1, \\vec x_2) = \\sum\\limits_i |x_1^i - x_2^i|    L2 distance:  d(\\vec x_1, \\vec x_2) = \\sqrt{\\sum\\limits_i (x_1^i - x_2^i)^2} d(\\vec x_1, \\vec x_2) = \\sqrt{\\sum\\limits_i (x_1^i - x_2^i)^2}    note: in practice square root is ignored (becasue is monotonic function)    L2 is less forgiving than L1 - prefers many small disagreements than one big one      cosine distance (cosine similarity):  d(\\vec x_1, \\vec x_2) = \\frac{x_1 \\cdot x_1}{||\\vec x_1|| \\cdot ||\\vec x_2||} d(\\vec x_1, \\vec x_2) = \\frac{x_1 \\cdot x_1}{||\\vec x_1|| \\cdot ||\\vec x_2||}    Chebyshev distance:  d(\\vec x_1, \\vec x_2) = max_i(|x_1^i - x_2^i|) d(\\vec x_1, \\vec x_2) = max_i(|x_1^i - x_2^i|)    and many others      The closest one determines the test sample label",
            "title": "Nearest Neighbor"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#implementation",
            "text": "The implementation of nearest neighbor algorithm is pretty straightforward    There is no real training process here - we just need to remember all training feature vectors and corresponding labels    To predict a label for new sample we just need to find the label of the closest point from training samples    class   NearestNeighbor (): \n   \"\"\"Nearest Neighbor Classifier\"\"\" \n\n   def   __init__ ( self ,   distance = 0 ): \n     \"\"\"Set distance definition: 0 - L1, 1 - L2\"\"\" \n     if   distance   ==   0 : \n       self . distance   =   np . abs       # absolute value \n     elif   distance   ==   1 : \n       self . distance   =   np . square    # square root \n     else : \n       raise   Exception ( \"Distance not defined.\" ) \n\n\n   def   train ( self ,   x ,   y ): \n     \"\"\"Train the classifier (here simply save training data)      x -- feature vectors (N x D)      y -- labels (N x 1)      \"\"\" \n     self . x_train   =   x \n     self . y_train   =   y \n\n\n   def   predict ( self ,   x ): \n     \"\"\"Predict and return labels for each feature vector from x      x -- feature vectors (N x D)      \"\"\" \n     predictions   =   []    # placeholder for N labels \n\n     # loop over all test samples \n     for   x_test   in   x : \n       # array of distances between current test and all training samples \n       distances   =   np . sum ( self . distance ( self . x_train   -   x_test ),   axis = 1 ) \n\n       # get the closest one \n       min_index   =   np . argmin ( distances ) \n\n       # add corresponding label \n       predictions . append ( self . y_train [ min_index ]) \n\n     return   predictions",
            "title": "Implementation"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#the-magic-of-numpy",
            "text": "NumPy is irreplacable tool for numerical operations on arrays    Using numpy we could easily find all distances using one line    distances = np.sum(self.distance(self.x_train - x_test), axis=1)   Here is how it works   # let's create an array with 5x2 shape  a   =   np . random . random_sample (( 5 ,   2 ))  # and another array with 1x2 shape  b   =   np . array ([[ 1. ,   1. ]])  print ( a ,   b ,   sep = \" \\n\\n \" )   [[0.79036457 0.36571819]\n [0.76743991 0.08439684]\n [0.56876884 0.97967839]\n [0.77020776 0.21238365]\n [0.94235534 0.73884472]]\n\n[[1. 1.]]  # subtract arguments (element-wise)  # note, that at least one dimension must be the same   print ( a   -   b )   [[-0.20963543 -0.63428181]\n [-0.23256009 -0.91560316]\n [-0.43123116 -0.02032161]\n [-0.22979224 -0.78761635]\n [-0.05764466 -0.26115528]]  # numpy.abs calculates absolute value (element-wise)  print ( np . abs ( a   -   b ))   [[0.20963543 0.63428181]\n [0.23256009 0.91560316]\n [0.43123116 0.02032161]\n [0.22979224 0.78761635]\n [0.05764466 0.26115528]]  # sum all elements  np . sum ( np . abs ( a   -   b ))   3.7798417848539096  # sum elements over a given axis  np . sum ( np . abs ( a   -   b ),   axis = 0 )   array([1.16086358, 2.61897821])  np . sum ( np . abs ( a   -   b ),   axis = 1 )   array([0.84391724, 1.14816326, 0.45155276, 1.01740859, 0.31879994])",
            "title": "The magic of numpy"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#analysis",
            "text": "Before we start using  NearestNeighbor  let's create a simple mini-framework to apply NN and visualize results easily    We want to initilize  NearestNeighbor  with some feature vectors (and automatically assign labels for each class)    We want our test samples to be a grid of uniformly distributed points    We want methods to process test data and to make a plots with final results    class   Analysis (): \n   \"\"\"Apply NearestNeighbor to generated (uniformly) test samples.\"\"\" \n\n   def   __init__ ( self ,   * x ,   distance ): \n     \"\"\"Generate labels and initilize classifier      x -- feature vectors arrays      distance -- 0 for L1, 1 for L2          \"\"\" \n     # get number of classes \n     self . nof_classes   =   len ( x ) \n\n     # create lables array \n     # np.ones creates an array of given shape filled with 1 of given type \n     # we apply consecutive integer numbers as class labels \n     # ravel return flatten array \n     y   =   [ i   *   np . ones ( _x . shape [ 0 ],   dtype = np . int )   for   i ,   _x   in   enumerate ( x )] \n     y   =   np . array ( y ) . ravel () \n\n     # save training samples to plot them later \n     self . x_train   =   x \n\n     # merge feature vector arrays for NearestNeighbor \n     x   =   np . concatenate ( x ,   axis = 0 ) \n\n     # train classifier \n     self . nn   =   NearestNeighbor ( distance ) \n     self . nn . train ( x ,   y ) \n\n\n   def   prepare_test_samples ( self ,   low = 0 ,   high = 2 ,   step = 0.01 ): \n     \"\"\"Generate a grid with test points (from low to high with step)\"\"\" \n     # remember range \n     self . range   =   [ low ,   high ] \n\n     # start with grid of points from [low, high] x [low, high] \n     grid   =   np . mgrid [ low : high + step : step ,   low : high + step : step ] \n\n     # convert to an array of 2D points \n     self . x_test   =   np . vstack ([ grid [ 0 ] . ravel (),   grid [ 1 ] . ravel ()]) . T \n\n\n   def   analyse ( self ): \n     \"\"\"Run classifier on test samples and split them according to labels.\"\"\" \n\n     # find labels for test samples  \n     self . y_test   =   self . nn . predict ( self . x_test ) \n\n     self . classified   =   []    # [class I test points, class II test ...] \n\n     # loop over available labels \n     for   label   in   range ( self . nof_classes ): \n       # if i-th label == current label -> add test[i] \n       class_i   =   np . array ([ self . x_test [ i ]  \\\n                           for   i ,   l   in   enumerate ( self . y_test )  \\\n                           if   l   ==   label ]) \n       self . classified . append ( class_i ) \n\n\n   def   plot ( self ,   t = '' ): \n     \"\"\"Visualize the result of classification\"\"\" \n     plot   =   init_plot ( self . range ,   self . range ) \n     plot . set_title ( t ) \n     plot . grid ( False ) \n\n     # plot training samples \n     for   i ,   x   in   enumerate ( self . x_train ): \n       plot . plot ( * x . T ,   mpl_colors [ i ]   +   'o' ) \n\n     # plot test samples \n     for   i ,   x   in   enumerate ( self . classified ): \n       plot . plot ( * x . T ,   mpl_colors [ i ]   +   ',' )",
            "title": "Analysis"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#l1-test",
            "text": "l1   =   Analysis ( X1 ,   X2 ,   distance = 0 )  l1 . prepare_test_samples ()  l1 . analyse ()  l1 . plot ()",
            "title": "L1 test"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#l2-test",
            "text": "l2   =   Analysis ( X1 ,   X2 ,   distance = 1 )  l2 . prepare_test_samples ()  l2 . analyse ()  l2 . plot ()",
            "title": "L2 Test"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#multiclass-classification",
            "text": "Training samples from 4 squares:   [0, 1] x [0, 1]  [0, 1] x [1, 2]  [1, 2] x [0, 1]  [1, 2] x [1, 2]     We expect 4 squares created by test samples grid    How does it depend on the size of training samples?    def   generate4 ( n = 50 ): \n   \"\"\"Generate 4 sets of random points.\"\"\" \n\n   # points from [0, 1] x [0, 1] \n   X1   =   generate_random_points ( n ,   0 ,   1 ) \n   # points from [1, 2] x [1, 2] \n   X2   =   generate_random_points ( n ,   1 ,   2 ) \n   # points from [0, 1] x [1, 2] \n   X3   =   np . array ([[ x ,   y + 1 ]   for   x , y   in   generate_random_points ( n ,   0 ,   1 )]) \n   # points from [1, 2] x [0, 1] \n   X4   =   np . array ([[ x ,   y - 1 ]   for   x , y   in   generate_random_points ( n ,   1 ,   2 )]) \n\n   return   X1 ,   X2 ,   X3 ,   X4   # loop over no. of training samples  for   n   in   ( 5 ,   10 ,   50 ,   100 ): \n   # generate 4 sets of random points (each one with n samples) \n   # unpack them when passing to Analysis \n   c4   =   Analysis ( * generate4 ( n ),   distance = 1 ) \n   c4 . prepare_test_samples () \n   c4 . analyse () \n   c4 . plot ( \"No. of samples = {}\" . format ( n ))       Message 01: size matters!",
            "title": "Multiclass classification"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#noise",
            "text": "Data are rarely perfect and you may expect some training samples to have unsual features    Features shared by a majority of training samples are more important than a single occurrence    Let's add some noise to our data and see how Nearest Neighbor deal with it    # generate 4 classes of 2D points  X1 ,   X2 ,   X3 ,   X4   =   generate4 ()  # add some noise by applying gaussian to every point coordinates  noise   =   lambda   x ,   y :   [ np . random . normal ( x ,   0.1 ),   np . random . normal ( y ,   0.1 )]  X1   =   np . array ([ noise ( x ,   y )   for   x ,   y   in   X1 ])  X2   =   np . array ([ noise ( x ,   y )   for   x ,   y   in   X2 ])  X3   =   np . array ([ noise ( x ,   y )   for   x ,   y   in   X3 ])  X4   =   np . array ([ noise ( x ,   y )   for   x ,   y   in   X4 ])  # perform analysis  c4   =   Analysis ( X1 ,   X2 ,   X3 ,   X4 ,   distance = 1 )  c4 . prepare_test_samples ()  c4 . analyse ()  c4 . plot ()",
            "title": "Noise"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#overfitting",
            "text": "The above is an example of overfitting   perfectly describe training data  lose the generalization ability     In general you want to extract all common features from training samples, but neglect characteristic features of single sample    Message 02: avoid overfitting!",
            "title": "Overfitting"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#accuracy",
            "text": "Accuracy defines the fraction of (unseen) samples which are correctly classify by the algorithm    accuracy   =   0  # loop over (sample, reconstructed label)  for   sample ,   label   in   zip ( c4 . x_test ,   c4 . y_test ): \n   # determine true label \n   if   sample [ 0 ]   <   1   and   sample [ 1 ]   <   1 : \n     true_label   =   0 \n   elif   sample [ 0 ]   >   1   and   sample [ 1 ]   >   1 : \n     true_label   =   1 \n   elif   sample [ 0 ]   <   1   and   sample [ 1 ]   >   1 : \n     true_label   =   2 \n   else : \n     true_label   =   3 \n\n   if   true_label   ==   label :   accuracy   +=   1  accuracy   /=   len ( c4 . x_test )  print ( accuracy )   0.924878097076805    Please note, that this is a toy model - in the case of real problems there is no way to determine true labels (otherwise there is no point to use ML methods...)    To measure accuracy of the model one usually splits data into:    training samples (usually about 80%)    test samples (usually about 20%)      After the model is trained on training samples, the accuracy is measured on test samples    Message 03: keep some data for testing!",
            "title": "Accuracy"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#k-nearest-neighbors_1",
            "text": "Instead of letting one closest neighbor to decide, let  k  nearest neghbors to vote",
            "title": "k-Nearest Neighbors"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#implementation_1",
            "text": "We can base the implementation on  NearestNeighbor , but    The  constructor  has an extra parameter  k    and we need to override  predict  method    class   kNearestNeighbors ( NearestNeighbor ): \n   \"\"\"k-Nearest Neighbor Classifier\"\"\" \n\n\n   def   __init__ ( self ,   k = 1 ,   distance = 0 ): \n     \"\"\"Set distance definition: 0 - L1, 1 - L2\"\"\" \n     super () . __init__ ( distance ) \n     self . k   =   k \n\n\n   def   predict ( self ,   x ): \n     \"\"\"Predict and return labels for each feature vector from x      x -- feature vectors (N x D)      \"\"\" \n     predictions   =   []    # placeholder for N labels \n\n     # no. of classes = max label (labels starts from 0) \n     nof_classes   =   np . amax ( self . y_train )   +   1 \n\n     # loop over all test samples \n     for   x_test   in   x : \n       # array of distances between current test and all training samples \n       distances   =   np . sum ( self . distance ( self . x_train   -   x_test ),   axis = 1 ) \n\n       # placeholder for labels votes \n       votes   =   np . zeros ( nof_classes ,   dtype = np . int ) \n\n       # find k closet neighbors and vote \n       # argsort returns the indices that would sort an array \n       # so indices of nearest neighbors \n       # we take self.k first \n       for   neighbor_id   in   np . argsort ( distances )[: self . k ]: \n         # this is a label corresponding to one of the closest neighbor \n         neighbor_label   =   self . y_train [ neighbor_id ] \n         # which updates votes array \n         votes [ neighbor_label ]   +=   1 \n\n       # predicted label is the one with most votes \n       predictions . append ( np . argmax ( votes )) \n\n     return   predictions",
            "title": "Implementation"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#kanalysis",
            "text": "We also create  kAnalysis  based on  Analysis  for visualization of kNN results   class   kAnalysis ( Analysis ): \n   \"\"\"Apply kNearestNeighbor to generated (uniformly) test samples.\"\"\" \n\n   def   __init__ ( self ,   * x ,   k = 1 ,   distance = 1 ): \n     \"\"\"Generate labels and initilize classifier      x -- feature vectors arrays      k -- number of nearest neighbors      distance -- 0 for L1, 1 for L2          \"\"\" \n     # get number of classes \n     self . nof_classes   =   len ( x ) \n\n     # create lables array \n     y   =   [ i   *   np . ones ( _x . shape [ 0 ],   dtype = np . int )   for   i ,   _x   in   enumerate ( x )] \n     y   =   np . array ( y ) . ravel () \n\n     # save training samples to plot them later \n     self . x_train   =   x \n\n     # merge feature vector arrays for NearestNeighbor \n     x   =   np . concatenate ( x ,   axis = 0 ) \n\n     # train classifier (knn this time) \n     self . nn   =   kNearestNeighbors ( k ,   distance ) \n     self . nn . train ( x ,   y )",
            "title": "kAnalysis"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#sanity-check",
            "text": "k-Nearest Neighbor classifier with  k = 1  must give exactly the same results as Nearest Neighbor   # apply kNN with k=1 on the same set of training samples  knn   =   kAnalysis ( X1 ,   X2 ,   X3 ,   X4 ,   k = 1 ,   distance = 1 )  knn . prepare_test_samples ()  knn . analyse ()  knn . plot ()",
            "title": "Sanity check"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#k-test",
            "text": "For  k = 1  kNN is likely to overfit the problem    Although, it does not mean that higher  k  is better!    Now, let's see how different values of  k  affects the result    Later, we will learn how to find optimal value of  k  for given problem    # training size = 50  # let's check a few values between 1 and 50  for   k   in   ( 1 ,   5 ,   10 ,   50 ): \n   knn   =   kAnalysis ( X1 ,   X2 ,   X3 ,   X4 ,   k = k ,   distance = 1 ) \n   knn . prepare_test_samples () \n   knn . analyse () \n   knn . plot ( \"k = {}\" . format ( k ))",
            "title": "k-Test"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#hyperparameters",
            "text": "ML model may have some hyperparameters - parameters set before training    Please note, ML algorithm may have also parameters which are set during training    In the case of kNN there are two hyperparameters:    number of nearest neihgbors ( k )    the definition of distance      The choice of hyperparameters values highly depends on a problem    The wrong choice of hyperparameters may lead to underfitting or overfitting",
            "title": "Hyperparameters"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#over-under-fitting-example",
            "text": "# generate random data from x^2 function (with some noise)  data   =   np . array ([[ x ,   np . random . normal ( x ** 2 ,   0.1 )]  \\\n                  for   x   in   2 * np . random . random ( 10 )   -   1 ])  plot   =   init_plot ([ - 1 ,   1 ],   [ - 1 ,   1 ])  plot . plot ( * data . T ,   'o' );      Let's try to fit this data to a polynomial    The degree is a hyperparamter (which defines number of coefficients)    # loop over degrees of polynomial  # data is x^2, so let's try degrees 1, 2, 10  for   n   in   ( 1 ,   2 ,   10 ): \n   # polyfit returns an array with polynomial coefficients \n   # poly1d is a polynomial class \n   f   =   np . poly1d ( np . polyfit ( * data . T ,   n )) \n\n   # returns an array with 100 uniformly distributed numbers from -1 to 1 \n   x   =   np . linspace ( - 1 ,   1 ,   100 ) \n\n   plot   =   init_plot ([ - 1 ,   1 ],   [ - 1 ,   1 ]) \n   plot . set_title ( \"n = {}\" . format ( n )) \n   plot . plot ( * data . T ,   'o' ,   x ,   f ( x ))   /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RankWarning: Polyfit may be poorly conditioned\n  after removing the cwd from sys.path.       For  n = 1  we clearly underfit the data as we do not have enough parameters to describe the complexity of the problem    For  n = 2  we have appropriate capacity (as we actually generated data form  x^2 x^2  function)    For  n = 10  we overfit the data - training samples are described perfectly, but we clearly lost the generalization ability    Message 04: right choice of hyperparameters is crucial!",
            "title": "Over-, under-fitting example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#validation-dataset",
            "text": "One splits data into training and test samples    training samples are used to optimize model parameters    test samples are used to measure accuracy    there is no rule of thumb on how to split dataset      If a model has some hyperparameters the part of training set is used for valitation samples:    training samples - tuning model parameters    validation samples - tuning hyperparameters                        +---------------------+      +------------------------+\n+----------+      |                     |      |                        |\n|          |      | Measure accuracy on |      | Measure final accuracy |\n| Training | +--> |                     | +--> |                        |\n|          |      | validation samples  |      | on test samples        |\n+----------+      |                     |      |                        |\n     ^            +----------+----------+      +------------------------+\n     |                       |\n     |      Change           | \n     +-----------------------+\n         hyperparameters",
            "title": "Validation dataset"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#iris-dataset",
            "text": "The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.  src    Attribute Information:    sepal length in cm    sepal width in cm    petal length in cm    petal width in cm    class:     Iris Setosa    Iris Versicolour    Iris Virginica",
            "title": "Iris dataset"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#load-dataset",
            "text": "We use  pandas  for data manipulation - it is super handy and supports many formats   import   pandas   as   pd  # columns names - can be used to access columns later  columns   =   [ \"Sepal Length\" ,   \"Sepal Width\" , \n            \"Petal Length\" ,   \"Petal Width\" , \n            \"Class\" ]  # iris.data is a csv file  src   =   \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"  # load the file with pandas.read_csv   # it will name columns as defined in columns list  # so one can access a column through index or name  iris_data   =   pd . read_csv ( src ,   header = None ,   names = columns )   iris_data . head ()    # print a few first entries    \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       Sepal Length \n       Sepal Width \n       Petal Length \n       Petal Width \n       Class \n     \n   \n   \n     \n       0 \n       5.1 \n       3.5 \n       1.4 \n       0.2 \n       Iris-setosa \n     \n     \n       1 \n       4.9 \n       3.0 \n       1.4 \n       0.2 \n       Iris-setosa \n     \n     \n       2 \n       4.7 \n       3.2 \n       1.3 \n       0.2 \n       Iris-setosa \n     \n     \n       3 \n       4.6 \n       3.1 \n       1.5 \n       0.2 \n       Iris-setosa \n     \n     \n       4 \n       5.0 \n       3.6 \n       1.4 \n       0.2 \n       Iris-setosa",
            "title": "Load dataset"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#visualize-dataset",
            "text": "pandas  offers plotting through  matplotlib  integration    Let's visualize Iris data    Let's keep the code short - sorry if it is hard to follow    # to extract rows with class column == class_name  extract   =   lambda   class_name :   iris_data . loc [ iris_data [ 'Class' ]   ==   class_name ]  # axes settings - part = Sepal or Petal; x = Length, y = Width  set_ax   =   lambda   part :   { \"x\" :   part   +   \" Length\" , \n                        \"y\" :   part   +   \" Width\" , \n                        \"kind\" :   \"scatter\" }  # add iris type / sepal or petal / color to existing axis  plot   =   lambda   class_name ,   part ,   color ,   axis :  \\\n   extract ( class_name ) . plot ( ** set_ax ( part ), \n                            color = color , \n                            label = class_name , \n                            ax = axis )  # plot all Iris types (sepal or petal) on existing axis  plot_all   =   lambda   part ,   axis :  \\\n   [ plot ( iris ,   part ,   mpl_colors [ i ],   axis )  \\\n    for   i ,   iris   in   enumerate ( set ( iris_data [ 'Class' ]))]    # with pyplot.subplots we can create many plots on one figure  # here we create 2 plots - 1 row and 2 columns  # thus, subplots returns figure, axes of 1st plot, axes for 2nd plot  _ ,   ( ax1 ,   ax2 )   =   plt . subplots ( 1 ,   2 ,   figsize = ( 9 , 4 ))  # using messy lambda we can plot all Iris types at once  # Petal data on 1st plots and Sepal data on 2nd plot  plot_all ( \"Petal\" ,   ax1 )  plot_all ( \"Sepal\" ,   ax2 )  # tight_layout adjust subplots params so they fit into figure ares  plt . tight_layout ()",
            "title": "Visualize dataset"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#prepare-feature-vectors-and-labels",
            "text": "First step is to prepare data - we need feature vectors with corresponding labels    In this case every sample's feature vector is 4D (sepal length, sepal width, petal length, petal width) and is labeled with one of three classes (Iris Setosa, Iris Versicolour, Iris Virginica)    # every Iris has 4 features (forming our 4D feature vectors)  # pandaoc.DataFrame.iloc allows us access data through indices  # we create an array with feature vectors by taking all rows for first 4 columns  X   =   iris_data . iloc [:,   : 4 ]  # it is still pandoc.DataFrame object - pretty handy  X . head ()    \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       Sepal Length \n       Sepal Width \n       Petal Length \n       Petal Width \n     \n   \n   \n     \n       0 \n       5.1 \n       3.5 \n       1.4 \n       0.2 \n     \n     \n       1 \n       4.9 \n       3.0 \n       1.4 \n       0.2 \n     \n     \n       2 \n       4.7 \n       3.2 \n       1.3 \n       0.2 \n     \n     \n       3 \n       4.6 \n       3.1 \n       1.5 \n       0.2 \n     \n     \n       4 \n       5.0 \n       3.6 \n       1.4 \n       0.2 \n     \n       pandas.DataFrame  object are handy to manipulate data, but at the end of the day we want to perform algebra with  numpy   # create numpy array (matrix) for further processing  X   =   np . array ( X )  # print a few first entries  print ( X [: 5 ])   [[5.1 3.5 1.4 0.2]\n [4.9 3.  1.4 0.2]\n [4.7 3.2 1.3 0.2]\n [4.6 3.1 1.5 0.2]\n [5.  3.6 1.4 0.2]]   from the las column (\"Class\") we create our labels   # as mentioned before, we can access DataFrame object through column labels  Y   =   np . array ( iris_data [ \"Class\" ])  # print a few first entries  print ( Y [: 5 ])   ['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa']",
            "title": "Prepare feature vectors and labels"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#prepare-test-dataset",
            "text": "Let's use 80% for training and 20% for testing    We, obviously, can not just take last 20% of samples for testing because our data is ordered    But we can randomly select 20% of samples    Easy to do by hand, but let's start to use some ML frameworks    from   sklearn.model_selection   import   train_test_split   as   split  # train_test_split: Split arrays or matrices into random train and test subsets  X_train ,   X_test ,   Y_train ,   Y_test   =   split ( X ,   Y ,   test_size = 0.2 )  # let's use 20% of training samples for validation  X_train ,   X_valid ,   Y_train ,   Y_valid   =   split ( X_train ,   Y_train ,   test_size = 0.2 )  # check how many sample we have  print ( X_train . shape [ 0 ],   X_valid . shape [ 0 ],   X_test . shape [ 0 ])   96 24 30",
            "title": "Prepare test dataset"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#knn-from-scikit-learn",
            "text": "scikit-learn  has already implemented k-Nearest Neighbor algorithm (which is more flexible than the one implemented during this lecture)    Let's see how  complicated  is using one of ML frameworks with Python    from   sklearn.neighbors   import   KNeighborsClassifier  # create knn classifier with k = 48  knn   =   KNeighborsClassifier ( n_neighbors = 48 )  # train the model  knn . fit ( X_train ,   Y_train )  # predict labels for test samples  Y_pred   =   knn . predict ( X_valid )",
            "title": "kNN from scikit-learn"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#accuracy_1",
            "text": "First let's print true labels along with predicted ones   # use bold if true != predicted  for   true ,   pred   in   zip ( Y_valid ,   Y_pred ): \n   if   pred   ==   true : \n     print ( \"{} \\t  -> {}\" . format ( true ,   pred )) \n   else : \n     print ( \" \\033 [1m{} \\t  -> {} \\033 [0m\" . format ( true ,   pred ))   Iris-setosa  -> Iris-setosa\nIris-versicolor  -> Iris-versicolor\nIris-setosa  -> Iris-setosa\nIris-versicolor  -> Iris-versicolor\nIris-virginica   -> Iris-virginica\nIris-virginica   -> Iris-virginica\n\u001b[1mIris-versicolor  -> Iris-virginica\u001b[0m\nIris-virginica   -> Iris-virginica\nIris-versicolor  -> Iris-versicolor\nIris-setosa  -> Iris-setosa\nIris-virginica   -> Iris-virginica\nIris-versicolor  -> Iris-versicolor\nIris-virginica   -> Iris-virginica\n\u001b[1mIris-virginica   -> Iris-versicolor\u001b[0m\nIris-virginica   -> Iris-virginica\nIris-virginica   -> Iris-virginica\nIris-versicolor  -> Iris-versicolor\nIris-setosa  -> Iris-setosa\nIris-setosa  -> Iris-setosa\nIris-virginica   -> Iris-virginica\n\u001b[1mIris-virginica   -> Iris-versicolor\u001b[0m\nIris-setosa  -> Iris-setosa\nIris-versicolor  -> Iris-versicolor\n\u001b[1mIris-virginica   -> Iris-versicolor\u001b[0m   We can easily calculate accuracy by hand as it is just a number of correctly predicted labels divided by no. of samples   # Y_valid == Y_pred -> array of True/False (if two elements are equal or not)  # (Y_valid == Y_pred).sum() -> number of Trues  # Y_valid.shape[0] -> number of validation samples  accuracy   =   ( Y_valid   ==   Y_pred ) . sum ()   /   Y_valid . shape [ 0 ]  print ( accuracy )   0.8333333333333334   But we can also use  scikit-learn  function  accuracy_score   from   sklearn.metrics   import   accuracy_score  print ( accuracy_score ( Y_valid ,   Y_pred ))   0.8333333333333334",
            "title": "Accuracy"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#k-dependence-of-the-accuracy",
            "text": "Let's use validation set to determine the best hyperparameter  k    We will run kNN for various values of  k  and measure accuracy    This will allow us to find the optimal value of  k    And check the accuracy on the test dataset    scores   =   []    # placeholder for accuracy  max_k   =   85    # maximum number of voters  # loop over different values of k  for   k   in   range ( 1 ,   max_k ): \n   # create knn classifier with k = k \n   knn   =   KNeighborsClassifier ( n_neighbors = k ) \n\n   # train the model \n   knn . fit ( X_train ,   Y_train ) \n\n   # predict labels for test samples \n   Y_pred   =   knn . predict ( X_valid ) \n\n   # add accuracy to score table \n   scores . append ( accuracy_score ( Y_valid ,   Y_pred ))    Now, we can plot accuracy as a function of  k   def   k_accuracy_plot ( max_k = 85 ): \n   \"\"\"Just plot settings\"\"\" \n   plt . grid ( True ) \n   plt . xlabel ( \"k\" ) \n   plt . ylabel ( \"Accuracy\" ) \n   plt . xlim ([ 0 ,   max_k   +   5 ]) \n   plt . ylim ([ 0 ,   1 ]) \n   plt . xticks ( range ( 0 ,   max_k   +   5 ,   5 )) \n\n   return   plt  k_accuracy_plot () . plot ( range ( 1 ,   max_k ),   scores );     And check the accuracy measured on the test samples   knn   =   KNeighborsClassifier ( n_neighbors = 9 )  knn . fit ( X_train ,   Y_train )  Y_pred   =   knn . predict ( X_test )  print ( accuracy_score ( Y_test ,   Y_pred ))   0.9666666666666667    The accuracy plot is not smooth    It is common if one does not have enough validation samples    But there is another way to measure accuracy dependence on hyperparameters",
            "title": "k-dependence of the accuracy"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#cross-validation",
            "text": "Split training samples into N folds\n\n +-------+   +-------+   +-------+         +-------+\n |       |   |       |   |       |         |       |\n |   1   |   |   2   |   |   3   |   ...   |   N   |\n |       |   |       |   |       |         |       |\n +-------+   +-------+   +-------+         +-------+\n\nTake one fold as validation set and train on N-1 folds\n\n +-------+   +-------+   +-------+         +-------+\n |*******|   |       |   |       |         |       |\n |*******|   |   2   |   |   3   |   ...   |   N   |\n |*******|   |       |   |       |         |       |\n +-------+   +-------+   +-------+         +-------+\n\n         Take the next one as validation set\n\n +-------+   +-------+   +-------+         +-------+\n |       |   |*******|   |       |         |       |\n |   1   |   |*******|   |   3   |   ...   |   N   |\n |       |   |*******|   |       |         |       |\n +-------+   +-------+   +-------+         +-------+\n\n          Repeat the procedure for all folds\n\n +-------+   +-------+   +-------+         +-------+\n |       |   |       |   |       |         |*******|\n |   1   |   |   2   |   |   3   |   ...   |*******|\n |       |   |       |   |       |         |*******|\n +-------+   +-------+   +-------+         +-------+\n\n            And average out the accuracy   Once again  scikit-learn  has already implemented the procedure we need   from   sklearn.model_selection   import   cross_val_score  # this time we do not create dedicated validation set  X_train ,   X_test ,   Y_train ,   Y_test   =   split ( X ,   Y ,   test_size = 0.2 )  avg_scores   =   []    # average score for different k  nof_folds   =   10  # loop over different values of k  for   k   in   range ( 1 ,   max_k ): \n   # create knn classifier with k = k \n   knn   =   KNeighborsClassifier ( n_neighbors = k ) \n\n   # cross-validate knn on our training sample with nof_folds \n   scores   =   cross_val_score ( knn ,   X_train ,   Y_train , \n                            cv = nof_folds ,   scoring = 'accuracy' ) \n\n   # add avg accuracy to score table \n   avg_scores . append ( scores . mean ())   k_accuracy_plot () . plot ( range ( 1 ,   max_k ),   avg_scores );      In theory, k-fold cross-validation is the way to go (especially if a dataset is small)    In practice, people tend to use a single validation split as it is not that computational expensive",
            "title": "Cross-validation"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#data-normalization",
            "text": "Sometimes there is a need to preprocess data before training    Let's imagine Iris sepal data is in cm but petal data in mm    # original data - both in cm  print ( X [: 5 ])   [[5.1 3.5 1.4 0.2]\n [4.9 3.  1.4 0.2]\n [4.7 3.2 1.3 0.2]\n [4.6 3.1 1.5 0.2]\n [5.  3.6 1.4 0.2]]  # make a copy of X  Xmm   =   X . copy ()  # and multiply last two columns by 0.1  Xmm [:, 2 :]   *=   0.1  # and we have our fake Iris data with petal length/width in mm  print ( Xmm [: 5 ])   [[5.1  3.5  0.14 0.02]\n [4.9  3.   0.14 0.02]\n [4.7  3.2  0.13 0.02]\n [4.6  3.1  0.15 0.02]\n [5.   3.6  0.14 0.02]]   Let's compare result of the same classifier on both dataset   def   get_accuracy ( X ,   Y ,   k = 10 ): \n   \"\"\"Make training and test datasets and process through kNN\"\"\" \n\n   # prepare training / test samples \n   X_train ,   X_test ,   Y_train ,   Y_test   =   split ( X ,   Y ,   test_size = 0.2 ) \n\n   # create a kNN with k = k \n   knn   =   KNeighborsClassifier ( n_neighbors = k ) \n\n   # get prediction for original dataset \n   knn . fit ( X_train ,   Y_train ) \n   Y_pred   =   knn . predict ( X_test ) \n\n   return   accuracy_score ( Y_test ,   Y_pred )  cm   =   get_accuracy ( X ,   Y )  mm   =   get_accuracy ( Xmm ,   Y )  print ( \"Accuracy: \\n\\t both in cm: {} \\n\\t petal in mm: {}\" . format ( cm ,   mm ))   Accuracy:\n    both in cm: 1.0\n    petal in mm: 0.7    It is kind of obvious here - petal information will barely contribute to the distance    However, it is not always obvious if some features are not suppressed by the way data is normalized    Message 05: be aware of data normalization!",
            "title": "Data normalization"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#mnist",
            "text": "THE MNIST DATABASE of handwritten digits    The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.    It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.       To make it simpler (and faster) let's use digits toy dataset which comes with  scikit-learn   src    Each datapoint is a 8x8 image of a digit.    About 180 samples per class (digit)    Total number of samples 1797",
            "title": "MNIST"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#load-digits",
            "text": "from   sklearn.datasets   import   load_digits  digits   =   load_digits ()  print ( digits . data . shape )   (1797, 64)    digits.images  is a  numpy  array with 1797  numpy  arrays 8x8 (feature vectors) representing digits    digits.target  is a  numpy  array with 1797 integer numbers (class labels)    the code below allow us to visualize a random digits from the dataset    # set grayscale  plt . gray ()  # get some random index from 0 to dataset size  random_index   =   np . random . randint ( 1796 )  # draw random digit  plt . matshow ( digits . images [ random_index ])  # and print the matrix  plt . text ( 8 ,   5 ,   digits . images [ random_index ], \n          fontdict = { 'family' :   'monospace' ,   'size' :   16 })  # and the label  plt . text ( 10 ,   1 ,   \"This is: {}\" . format ( digits . target [ random_index ]), \n          fontdict = { 'family' :   'monospace' ,   'size' :   16 });   <matplotlib.figure.Figure at 0x7faccc90a048>",
            "title": "Load digits"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#distance-between-images",
            "text": "TEST      TRAIN    PIXEL-WISE\n| 4 2 0     2 5 8 |   |2 3 8|\n| 5 3 9  -  2 8 1 | = |3 5 8|  ->  38\n| 0 2 3     1 4 9 |   |1 2 6|",
            "title": "Distance between images"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#prepare-data",
            "text": "We need to split dataset to training and test samples    However, images are in 8x8 format and we have to flatten them first    # the original shape of an image  print ( digits . images . shape )   (1797, 8, 8)  # numpy.reshape is handy here  print ( digits . images . reshape (( 1797 ,   - 1 )) . shape )   (1797, 64)    Please note -1 in new shape    numpy.reshape  allows us to pass one  unknown  dimension which can be determined automatically    Thus, the above is equivalent to    print ( digits . images . reshape (( 1797 ,   64 )) . shape )   (1797, 64)  print ( digits . images . reshape (( - 1 ,   64 )) . shape )   (1797, 64)   As before, we can split our dataset using  sklearn.model_selection.train_test_split   data_train ,   data_test ,   label_train ,   label_test   =  \\\n   split ( digits . images . reshape (( 1797 ,   - 1 )),   digits . target ,   test_size = 0.2 )",
            "title": "Prepare data"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#cross-validation_1",
            "text": "We perform cross-validation on training samples to determine the best  k  (as for the Iris dataset)   avg_scores   =   []    # average score for different k  max_k   =   50  nof_folds   =   10  # loop over different values of k  for   k   in   range ( 1 ,   max_k ): \n   # create knn classifier with k = k \n   knn   =   KNeighborsClassifier ( n_neighbors = k ) \n\n   # cross-validate knn on our training sample with nof_folds \n   scores   =   cross_val_score ( knn ,   data_train ,   label_train , \n                            cv = nof_folds ,   scoring = 'accuracy' ) \n\n   # add avg accuracy to score table \n   avg_scores . append ( scores . mean ())   plt . grid ( True )  plt . xlabel ( \"k\" )  plt . ylabel ( \"Accuracy\" )  plt . xlim ([ 0 ,   max_k ])  plt . ylim ([ 0 ,   1 ])  plt . xticks ( range ( 0 ,   max_k ,   5 ))  plt . plot ( range ( 1 ,   max_k ),   avg_scores );      We used nearly the same procedure as for the Iris dataset    Note, that digits toy dataset prefer different  k    This is the idea of ML - the same algorithm can solve different problems if train on different data    Nowadays, in ML field  data is more important than algorithms  (we have good algorithms already)",
            "title": "Cross-validation"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#final-test",
            "text": "Let's take the bes  k  and check how the classifier works on test samples   from   sklearn.metrics   import   accuracy_score  knn   =   KNeighborsClassifier ( n_neighbors = 1 )  knn . fit ( data_train ,   label_train )  prediction   =   knn . predict ( data_test )  print ( accuracy_score ( label_test ,   prediction ))   0.9888888888888889   We can take a look at misclassified digits   for   i ,   ( true ,   predict )   in   enumerate ( zip ( label_test ,   prediction )): \n   if   true   !=   predict : \n     digit   =   data_test [ i ] . reshape (( 8 ,   8 ))    # reshape again to 8x8 \n     plt . matshow ( digit )                      # for matshow \n     plt . title ( \"{} predicted as {}\" . format ( true ,   predict ))",
            "title": "Final test"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#regression-with-knn",
            "text": "It is also possible to do regression using k-Nearest Neighbors    find  k  nearest neighbors from training samples    calculate the predicted value using inverse distance weighting method     y_{pred}(\\vec x) = \\frac{\\sum\\limits_i w_i(\\vec x) y_{train, i}}{\\sum\\limits_i w_i(\\vec x_i)}  y_{pred}(\\vec x) = \\frac{\\sum\\limits_i w_i(\\vec x) y_{train, i}}{\\sum\\limits_i w_i(\\vec x_i)}     where  w_i(\\vec x) = \\frac{1}{d(\\vec x, \\vec x_{train, i})} w_i(\\vec x) = \\frac{1}{d(\\vec x, \\vec x_{train, i})}    Note, that  y_{pred}(\\vec x) = y_{train, i} y_{pred}(\\vec x) = y_{train, i}  if  d(\\vec x, \\vec x_{train, i}) = 0 d(\\vec x, \\vec x_{train, i}) = 0",
            "title": "Regression with kNN"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#genearate-some-fake-data",
            "text": "Let's grab some random points from the sine function    And add some noise to make it more like real data    data_size   =   50  # generate and sort *data_size* numbers from 0 to 4pi   x_train   =   4   *   np . pi   *   np . sort ( np . random . rand ( data_size ,   1 ),   axis = 0 )  # let's fit to sine    y_train   =   np . sin ( x_train ) . ravel ()  # add some noise to the data  y_train   =   np . array ([ np . random . normal ( y ,   0.05 )   for   y   in   y_train ])  plt . plot ( x_train ,   y_train ,   'ro' );",
            "title": "Genearate some fake data"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#make-a-fit",
            "text": "In general, one should do cross-validation to determine the best  k    We will skip this part during the lecture (feel free to check this at home though!)    Let's just check how kNN fit works for a few different values of  k",
            "title": "Make a fit"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#comment-on-numpynewaxis",
            "text": "# let's create a 1D numpy array  D1   =   np . array ([ 1 ,   2 ,   3 ,   4 ])  print ( D1 )   [1 2 3 4]  # we can easily add another dimension using numpy.newaxis  D2   =   D1 [:,   np . newaxis ]  print ( D2 )   [[1]\n [2]\n [3]\n [4]]",
            "title": "Comment on numpy.newaxis"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#and-back-to-the-task",
            "text": "We use kNN regressor from  scikit-learn  (from intro:  What I really do... )   from   sklearn.neighbors   import   KNeighborsRegressor  # first we need test sample  x_test   =   np . linspace ( 0 ,   4 * np . pi ,   100 )[:,   np . newaxis ]  for   i ,   k   in   enumerate (( 1 ,   5 ,   10 ,   20 )): \n   # weights=distance - weight using distances \n   knn   =   KNeighborsRegressor ( k ,   weights = 'distance' ) \n\n   # calculate y_test for all points in x_test \n   y_test   =   knn . fit ( x_train ,   y_train ) . predict ( x_test ) \n\n   plt . subplot ( 2 ,   2 ,   i   +   1 ) \n\n   plt . title ( \"k = {}\" . format ( k )) \n\n   plt . plot ( x_train ,   y_train ,   'ro' ,   x_test ,   y_test ,   'g' );  plt . tight_layout ()",
            "title": "And back to the task"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#summary",
            "text": "We have learned first ML algorithm - k-Nearest Neighbors    It has some pros:    easy to understand and implement    no time needed for training - may be used for initial analysis before one reaches for some  heavier  tool    solves nonlinear problems     limited number of hyperparameters    no parameters!    at the end of this lecture we will deal with tens of hyperparameters and thousands of parameters      Although cons make it hard to use in practice    training data must be kept for the whole time (so called  lazy training )    imagine having GB of training samples and you want to make mobile app    other algorithms allows to discard training samples once the model is trained ( eager learning ) - usually it means long training process but super fast classification (which is what we really want)      distance-comparing is not suitable for all data - a picture of a cat on a blue background (e.g. sky) can be close to a ship on a sea (because background pixels vote too)   e.g. for  CIFAR-10  (60k pictures, 10 classes, more about that later) vanilla kNN get less than 40% accuracy     still better than random guessing (10%), but convolutional neural networks get >95%      Still, we have learned from kNN a few important things:    Data is important (both size and quality)    Sometimes data requires preprocessing    Wrong choice of hyperparameters may lead to under- or over-fitting    Use validation samples to tune the model    And  DO NOT  touch test samples until you are done!",
            "title": "Summary"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/",
            "text": "Decision Trees\n\u00b6\n\n\nGraphviz\n\u00b6\n\n\nInstalling graphviz\n\u00b6\n\n\n!\napt\n \ninstall\n \n-\ny\n \ngraphviz\n\n\n!\npip\n \ninstall\n \ngraphviz\n\n\n\n\n\n\nA tree example\n\u00b6\n\n\nfrom\n \ngraphviz\n \nimport\n \nDigraph\n\n\n\nstyles\n \n=\n \n{\n\n    \n'top'\n:\n \n{\n'shape'\n:\n \n'ellipse'\n,\n \n'style'\n:\n \n'filled'\n,\n \n'color'\n:\n \n'lightblue'\n},\n\n    \n'no'\n:\n  \n{\n'shape'\n:\n \n'circle'\n,\n \n'style'\n:\n \n'filled'\n,\n \n'color'\n:\n \n'red'\n},\n\n    \n'yes'\n:\n \n{\n'shape'\n:\n \n'circle'\n,\n \n'style'\n:\n \n'filled'\n,\n \n'color'\n:\n \n'lightgreen'\n},\n\n    \n'qst'\n:\n \n{\n'shape'\n:\n \n'rect'\n}\n\n\n}\n\n\n\nexample_tree\n \n=\n \nDigraph\n()\n\n\n\nexample_tree\n.\nnode\n(\n'top'\n,\n \n'Should I attend the ML lecture?'\n,\n \nstyles\n[\n'top'\n])\n\n\nexample_tree\n.\nnode\n(\n'q1'\n,\n \n'Do I fulfill requirements?'\n,\n \nstyles\n[\n'qst'\n])\n\n\n\nexample_tree\n.\nnode\n(\n'q2'\n,\n \n'Do I like CS?'\n,\n \nstyles\n[\n'qst'\n])\n\n\nexample_tree\n.\nnode\n(\n'no1'\n,\n \n'No '\n,\n \nstyles\n[\n'no'\n])\n\n\n\nexample_tree\n.\nnode\n(\n'q3'\n,\n \n'Is the lecture early in the morning?'\n,\n \nstyles\n[\n'qst'\n])\n\n\nexample_tree\n.\nnode\n(\n'no2'\n,\n \n'No '\n,\n \nstyles\n[\n'no'\n])\n\n\n\nexample_tree\n.\nnode\n(\n'no3'\n,\n \n'No '\n,\n \nstyles\n[\n'no'\n])\n\n\nexample_tree\n.\nnode\n(\n'yes'\n,\n \n'Yes'\n,\n \nstyles\n[\n'yes'\n])\n\n\n\nexample_tree\n.\nedge\n(\n'top'\n,\n \n'q1'\n)\n\n\n\nexample_tree\n.\nedge\n(\n'q1'\n,\n \n'q2'\n,\n \n'Yes'\n)\n\n\nexample_tree\n.\nedge\n(\n'q1'\n,\n \n'no1'\n,\n \n'No'\n)\n\n\n\nexample_tree\n.\nedge\n(\n'q2'\n,\n \n'q3'\n,\n \n'Yes'\n)\n\n\nexample_tree\n.\nedge\n(\n'q2'\n,\n \n'no2'\n,\n \n'No'\n)\n\n\n\nexample_tree\n.\nedge\n(\n'q3'\n,\n \n'no3'\n,\n \n'Yes'\n)\n\n\nexample_tree\n.\nedge\n(\n'q3'\n,\n \n'yes'\n,\n \n'No'\n)\n\n\n\n\n\n\nexample_tree\n\n\n\n\n\n\n\n\nIntroduction\n\u00b6\n\n\nDecision trees\n\u00b6\n\n\n\n\n\n\nSupervised learning algorithm - training dataset with known labels\n\n\n\n\n\n\nEager learning - final model does not need training data to make prediction (all parameters are evaluated during learning step)\n\n\n\n\n\n\nIt can do both classification and regression\n\n\n\n\n\n\nA decision tree is built from:\n\n\n\n\ndecision nodes\n - correspond to features (attributes)\n\n\nleaf nodes\n - correspond to class labels\n\n\n\n\n\n\n\n\nThe \nroot\n of a tree is (should be) the best predictor (feature)\n\n\n\n\n\n\nExample\n\u00b6\n\n\nimport\n \nmatplotlib.pyplot\n \nas\n \nplt\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\n\n# just to overwrite default colab style\n\n\nplt\n.\nstyle\n.\nuse\n(\n'default'\n)\n\n\nplt\n.\nstyle\n.\nuse\n(\n'seaborn-talk'\n)\n\n\n\n\n\n\n# first define some points representing two classes\n\n\ngrid\n \n=\n \nnp\n.\nmgrid\n[\n0\n:\n10\n:\n2\n,\n \n0\n:\n10\n:\n2\n]\n\n\nset01\n \n=\n \nnp\n.\nvstack\n([\ngrid\n[\n0\n]\n.\nravel\n(),\n \ngrid\n[\n1\n]\n.\nravel\n()])\n.\nT\n\n\nset01\n \n=\n \nnp\n.\ndelete\n(\nset01\n,\n \n[\n17\n,\n \n18\n,\n \n19\n,\n \n22\n,\n \n24\n],\n \naxis\n=\n0\n)\n\n\n\ngrid\n \n=\n \nnp\n.\nmgrid\n[\n6\n:\n16\n:\n2\n,\n \n0\n:\n10\n:\n2\n]\n\n\nset02\n \n=\n \nnp\n.\nvstack\n([\ngrid\n[\n0\n]\n.\nravel\n(),\n \ngrid\n[\n1\n]\n.\nravel\n()])\n.\nT\n\n\nset02\n \n=\n \nnp\n.\ndelete\n(\nset02\n,\n \n[\n0\n,\n \n1\n,\n \n5\n,\n \n6\n,\n \n8\n],\n \naxis\n=\n0\n)\n\n\n\nplt\n.\nscatter\n(\n*\nset01\n.\nT\n)\n\n\nplt\n.\nscatter\n(\n*\nset02\n.\nT\n)\n\n\n\nplt\n.\ntext\n(\n15\n,\n \n4\n,\n \n\"There are two attributes: x and y\n\\n\\n\n\"\n\n                \n\"    * each decision node splits dataset based on one of the attributes\n\\n\\n\n\"\n\n                \n\"    * each leaf node defines a class label\"\n);\n\n\n\n\n\n\n\n\nplt\n.\nscatter\n(\n*\nset01\n.\nT\n)\n\n\nplt\n.\nscatter\n(\n*\nset02\n.\nT\n)\n\n\n\nplt\n.\nplot\n([\n5\n,\n \n5\n],\n \n[\n0\n,\n \n8\n],\n \n'r'\n)\n\n\nplt\n.\nplot\n([\n0\n,\n \n14\n],\n \n[\n3\n,\n \n3\n],\n \n'g'\n)\n\n\n\nplt\n.\ntext\n(\n15\n,\n \n3\n,\n \n\"We start with [20, 20] (blue, orange)\n\\n\\n\n\"\n\n                \n\"Red line splits dataset in [15, 0] (left) and [5, 20] (right)\n\\n\\n\n\"\n\n                \n\"Green line split dataset in [10, 6] (bottom) and [10, 14] (top)\n\\n\\n\n\"\n\n                \n\"Red line is a winner and should be the root of our tree\"\n);\n\n\n\n\n\n\n\n\ntree\n \n=\n \nDigraph\n()\n\n\n\ntree\n.\nedge\n(\n\"x > 5?\n\\n\n[20, 20]\"\n,\n \n\"blue\n\\n\n[15, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 5?\n\\n\n[20, 20]\"\n,\n \n\"y > 3?\n\\n\n[5, 20]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"y > 3?\n\\n\n[5, 20]\"\n,\n \n\"x > 9?\n\\n\n[4, 6]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"y > 3?\n\\n\n[5, 20]\"\n,\n \n\"almost orange\n\\n\n[1, 14]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"x > 9?\n\\n\n[4, 6]\"\n,\n \n\"blue\n\\n\n[4, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 9?\n\\n\n[4, 6]\"\n,\n \n\"orange\n\\n\n[0, 6]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"almost orange\n\\n\n[1, 14]\"\n,\n \n\"Should we continue?\n\\n\nOr would it be overfitting?\"\n)\n\n\n\ntree\n\n\n\n\n\n\n\n\n\n\n\n\nIt is important to start with good predictor\n\n\n\n\n\n\nOur choice of the root classifies 37.5% of points  in the first step\n\n\n\n\n\n\nNote, that we could also start with \nx > 9?\n\n\n\n\n\n\nHowever, if we started with \ny > 3\n we would never classify a point in the first step - does it mean that it is worse choice?\n\n\n\n\n\n\n\n\n\n\ntree\n \n=\n \nDigraph\n()\n\n\n\ntree\n.\nedge\n(\n\"y > 3?\n\\n\n[20, 20]\"\n,\n \n\"x > 9?\n\\n\n[10, 6]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"y > 3?\n\\n\n[20, 20]\"\n,\n \n\"x > 5?\n\\n\n[10, 14]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"x > 9?\n\\n\n[10, 6]\"\n,\n \n\"blue\n\\n\n[10, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 9?\n\\n\n[10, 6]\"\n,\n \n\"orange\n\\n\n[0, 6]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"x > 5?\n\\n\n[10, 14]\"\n,\n \n\"blue\n\\n\n[9, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 5?\n\\n\n[10, 14]\"\n,\n \n\"almost orange\n\\n\n[1, 14]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n\n\n\n\n\n\n\n\n\n\n\n\nIn this case we never have to make more than 2 checks\n\n\n\n\n\n\nThere are two open questions to answer:\n\n\n\n\n\n\nHow to automate the procees of chosing nodes?\n\n\n\n\n\n\nHow deep should we go?\n\n\n\n\n\n\n\n\n\n\nID3 and C4.5 algorithms\n\u00b6\n\n\n\n\n\n\nWe start with algorithms based on information theory\n\n\n\n\n\n\nID3 (Iterative Dichotomiser 3)\n\n\n\n\n\n\nC4.5 - extension of ID3 (why C4.5? C stands for programming language and 4.5 for version?)\n\n\n\n\n\n\nC5.0/See5 - improved C4.5 (commercial; single-threaded Linux version is available under GPL though)\n\n\n\n\n\n\n\n\n\n\nThe idea is to find nodes which maximize information gain\n\n\n\n\n\n\nInformation gain\n\u00b6\n\n\nSelf-information\n\u00b6\n\n\n\n\n\n\nLet \nX = (x_1, x_2, ..., x_n)\nX = (x_1, x_2, ..., x_n)\n be our \ninformation source\n (feature), e.g. weather condition: \nx_1\nx_1\n = sunny, \nx_2\nx_2\n = overcast, \nx_3\nx_3\n = rainy\n\n\n\n\n\n\nAnd let \nP = (p_1, p_2, ..., p_n)\nP = (p_1, p_2, ..., p_n)\n be corresponding probrability distribution (or more precisely - probability mass function)\n\n\n\n\n\n\nWe want some measure of information \nI\nI\n provided by an event. It should satisfy the following properties:\n\n\n\n\n\n\nI\nI\n depends only on the probability of \nx_i\nx_i\n, thus \nI \\equiv I(p_i)\nI \\equiv I(p_i)\n\n\n\n\n\n\nI\nI\n is continuous and deacreasing function of \np_i\np_i\n\n\n\n\n\n\nI\nI\n is non-negative and \nI(1) = 0\nI(1) = 0\n\n\n\n\n\n\nif \np_i = p_{i, 1} \\cdot p_{i, 2}\np_i = p_{i, 1} \\cdot p_{i, 2}\n (independent events) then \nI(p_i) = I(p_{i, 1}) + I(p_{i, 2})\nI(p_i) = I(p_{i, 1}) + I(p_{i, 2})\n\n\n\n\n\n\n\n\n\n\nLogarithmic function satisfies all above condition, so we define self-information as: \nI(p) = -\\log(p)\nI(p) = -\\log(p)\n\n\n\n\n\n\nThe most common log base is \n2\n and then information is in \nshannons (Sh)\n, also known as \nbits\n\n\n\n\n\n\nIn the case of \nnatural logarithm\n the unit is \nnat\n (natural unit of information)\n\n\n\n\n\n\nIn the case of base \n10\n the unit is \nhartley (Hart)\n, also known as \ndit\n\n\n\n\n\n\n\n\n\n\nx\n \n=\n \nnp\n.\narange\n(\n0.01\n,\n \n1.01\n,\n \n0.01\n)\n\n\n\nplt\n.\nxlabel\n(\n\"p\"\n)\n\n\nplt\n.\nylabel\n(\n\"I(p)\"\n)\n\n\n\nplt\n.\nplot\n(\nx\n,\n \n-\nnp\n.\nlog2\n(\nx\n),\n \nlabel\n=\n\"bit\"\n)\n\n\nplt\n.\nplot\n(\nx\n,\n \n-\nnp\n.\nlog\n(\nx\n),\n \nlabel\n=\n\"nat\"\n)\n\n\nplt\n.\nplot\n(\nx\n,\n \n-\nnp\n.\nlog10\n(\nx\n),\n \nlabel\n=\n\"dit\"\n)\n\n\n\nplt\n.\nlegend\n();\n\n\n\n\n\n\n\n\n\n\n\n\nLets X = (head, tail) with P = (0.5, 0.5)\n\n\n\n\nWe get 1 Sh of information\n\n\n\n\n\n\n\n\nLets X = (sunny, overcast, rainy) with P = (0.25, 0.75, 0.25)\n\n\n\n\n\n\nIf it is overcast, we get 0.415 Sh of information\n\n\n\n\n\n\nOtherwise, we get 2 Sh of information\n\n\n\n\n\n\n\n\n\n\nIf an event is more likely we learn less\n\n\n\n\n\n\nInformation entropy\n\u00b6\n\n\n\n\n\n\nAlso called Shannon entropy (after the father of intromation theory)\n\n\n\n\n\n\nUsually information entropy is denoted as \nH\nH\n\n\n\n\n\n\nH\nH\n is defined as the weighted average of the self-information of all possible outcomes \nH(X) = \\sum\\limits_{i=1}^N p_i \\cdot I(p_i) = -\\sum\\limits_{i=1}^N p_i\\cdot\\log(p_i)\nH(X) = \\sum\\limits_{i=1}^N p_i \\cdot I(p_i) = -\\sum\\limits_{i=1}^N p_i\\cdot\\log(p_i)\n\n\n\n\n\n\nLets consider two case scenario with \nP = (p, 1 - p)\nP = (p, 1 - p)\n, so entropy is given by \nH = -p \\log(p) - (1 - p) \\log(1 - p)\nH = -p \\log(p) - (1 - p) \\log(1 - p)\n\n\n\n\n\n\np\n \n=\n \nnp\n.\narange\n(\n0.01\n,\n \n1.0\n,\n \n0.01\n)\n\n\n\nplt\n.\nxlabel\n(\n\"p\"\n)\n\n\nplt\n.\nylabel\n(\n\"H\"\n)\n\n\n\nplt\n.\nannotate\n(\n'we are surprised'\n,\n \nxy\n=\n(\n0.5\n,\n \n1\n),\n \nxytext\n=\n(\n0.5\n,\n \n0.75\n),\n\n             \narrowprops\n=\ndict\n(\nfacecolor\n=\n'black'\n,\n \nshrink\n=\n0.1\n))\n\n\n\nplt\n.\nannotate\n(\n'we are not that surprised'\n,\n \nxy\n=\n(\n1\n,\n \n0.1\n),\n \nxytext\n=\n(\n0.5\n,\n \n0.25\n),\n\n             \narrowprops\n=\ndict\n(\nfacecolor\n=\n'black'\n,\n \nshrink\n=\n0.1\n))\n\n\n\nplt\n.\nplot\n(\np\n,\n \n-\np\n \n*\n \nnp\n.\nlog2\n(\np\n)\n \n-\n \n(\n1\n \n-\n \np\n)\n \n*\n \nnp\n.\nlog2\n(\n1\n \n-\n \np\n));\n\n\n\n\n\n\n\n\n\n\nLets consider three case scenario with \nP = (p, q, 1 - p - q)\nP = (p, q, 1 - p - q)\n, so entropy is given by \nH = -p \\log(p) - q\\log(q) - (1 - p - q) \\log(1 - p - q)\nH = -p \\log(p) - q\\log(q) - (1 - p - q) \\log(1 - p - q)\n\n\n\n\nfrom\n \nmpl_toolkits\n \nimport\n \nmplot3d\n\n\n\n# grid of p, q probabilities\n\n\np\n,\n \nq\n \n=\n \nnp\n.\nmeshgrid\n(\nnp\n.\narange\n(\n0.01\n,\n \n1.0\n,\n \n0.01\n),\n \nnp\n.\narange\n(\n0.01\n,\n \n1.0\n,\n \n0.01\n))\n\n\n\n# remove (set to 0) points which do not fulfill P <= 1\n\n\nidx\n \n=\n \np\n \n+\n \nq\n \n>\n \n1\n\n\np\n[\nidx\n]\n \n=\n \n0\n\n\nq\n[\nidx\n]\n \n=\n \n0\n\n\n\n# calculate entropy (disable warnings - we are aware of log(0))\n\n\nnp\n.\nwarnings\n.\nfilterwarnings\n(\n'ignore'\n)\n\n\nh\n \n=\n \n-\np\n \n*\n \nnp\n.\nlog2\n(\np\n)\n \n-\n \nq\n \n*\n \nnp\n.\nlog2\n(\nq\n)\n \n-\n \n(\n1\n \n-\n \np\n \n-\n \nq\n)\n \n*\n \nnp\n.\nlog2\n(\n1\n \n-\n \np\n \n-\n \nq\n)\n\n\n\n# make a plot\n\n\nplt\n.\naxes\n(\nprojection\n=\n'3d'\n)\n.\nplot_surface\n(\np\n,\n \nq\n,\n \nh\n);\n\n\n\n\n\n\n\n\nInformation gain\n\u00b6\n\n\n\n\n\n\nLet \nT\nT\n be the set of training samples with \nn\nn\n possible outcomes, thus \nT = \\{T_1, T_2, ..., T_n\\}\nT = \\{T_1, T_2, ..., T_n\\}\n\n\n\n\n\n\nThe entropy is given by \nH(T) = -\\sum\\limits_{i=1}^N p_i\\cdot\\log(p_i) = -\\sum\\limits_{i=1}^N \\frac{|T_i|}{|T|}\\cdot\\log(\\frac{|T_i|}{|T|})\nH(T) = -\\sum\\limits_{i=1}^N p_i\\cdot\\log(p_i) = -\\sum\\limits_{i=1}^N \\frac{|T_i|}{|T|}\\cdot\\log(\\frac{|T_i|}{|T|})\n\n\n\n\n\n\nWe can also calulate the entropy after \nT\nT\n was partitioned in \nT_i\nT_i\n with respect to some feature \nX\nX\n \nH(T, X) = \\sum\\limits_{i=1}^N p_i\\cdot H(T_i)\nH(T, X) = \\sum\\limits_{i=1}^N p_i\\cdot H(T_i)\n\n\n\n\n\n\nAnd the information gain is defined as \nG(X) = H(T) - H(T, X)\nG(X) = H(T) - H(T, X)\n\n\n\n\n\n\nExample\n\u00b6\n\n\n\n\n\n\nLets calculate some example step by step\n\n\n\n\n\n\nLets consider a fake dataset\n\n\n\n\n\n\ntwo classes: C01, C02\n\n\n\n\n\n\nthree features: X1, X2, X3\n\n\n\n\n\n\n\n\n\n\n   X1  ||  A  |  A  |  A  |  B  |  B  |  C  |  C  |  C  |  C  |\n---------------------------------------------------------------\n   X2  ||  0  |  0  |  1  |  1  |  0  |  1  |  1  |  1  |  0  |\n---------------------------------------------------------------\n   X3  || RED | GRN | GRN | BLU | RED | GRN | BLU | RED | GRN |\n===============================================================\n Class || C01 | C01 | C02 | C02 | C02 | C02 | C01 | C01 | C02 |\n\n\n\n\n\nfrom\n \nmath\n \nimport\n \nlog\n\n\n\ndef\n \nentropy\n(\n*\nprobs\n):\n\n  \n\"\"\"Calculate information entropy\"\"\"\n\n  \ntry\n:\n\n    \ntotal\n \n=\n \nsum\n(\nprobs\n)\n\n    \nreturn\n \nsum\n([\n-\np\n \n/\n \ntotal\n \n*\n \nlog\n(\np\n \n/\n \ntotal\n,\n \n2\n)\n \nfor\n \np\n \nin\n \nprobs\n])\n\n  \nexcept\n:\n\n    \nreturn\n \n0\n\n\n\nprint\n(\nentropy\n(\n4\n,\n \n5\n),\n \nentropy\n(\n2\n,\n \n1\n),\n \nentropy\n(\n2\n,\n \n2\n))\n\n\n\n\n\n\n0.9910760598382222 0.9182958340544896 1.0\n\n\n\n\n\n\n\n\n\nThe \nroot\n entropy\n\n\n\n\nWe have 9 samples: 4 belong to class C01 and 5 to C02 \nH(T) = -\\frac{4}{9}\\log(\\frac{4}{9}) - \\frac{5}{9}\\log(\\frac{5}{9}) = 0.99\nH(T) = -\\frac{4}{9}\\log(\\frac{4}{9}) - \\frac{5}{9}\\log(\\frac{5}{9}) = 0.99\n\n\n\n\n\n\n\n\nNow lets consider feature X1, which splits data into subsets \nT_1\nT_1\n, \nT_2\nT_2\n, and \nT_3\nT_3\n (with X1 value A, B, and C, respectively)\n\n\n\n\n\n\nWithin \nT_1\nT_1\n there are 3 samples: 2 from C01 and 1 from C02 \nH(T_1) = -\\frac{2}{3}\\log(\\frac{2}{3}) - \\frac{1}{3}\\log(\\frac{1}{3}) = 0.92\nH(T_1) = -\\frac{2}{3}\\log(\\frac{2}{3}) - \\frac{1}{3}\\log(\\frac{1}{3}) = 0.92\n\n\n\n\n\n\nWithin \nT_2\nT_2\n there are 2 samples: 0 from C01 and 2 from C02 \nH(T_2) = -\\frac{2}{2}\\log(\\frac{2}{2}) - \\frac{0}{2}\\log(\\frac{0}{2}) = 0.00\nH(T_2) = -\\frac{2}{2}\\log(\\frac{2}{2}) - \\frac{0}{2}\\log(\\frac{0}{2}) = 0.00\n\n\n\n\n\n\nWithin \nT_3\nT_3\n there are 4 samples: 2 from C01 and 2 from C02 \nH(T_3) = -\\frac{2}{4}\\log(\\frac{2}{4}) - \\frac{2}{4}\\log(\\frac{2}{4}) = 1.00\nH(T_3) = -\\frac{2}{4}\\log(\\frac{2}{4}) - \\frac{2}{4}\\log(\\frac{2}{4}) = 1.00\n\n\n\n\n\n\nThe resulting entropy is \nH(T, X1) = \\frac{3}{9}\\cdot H(T_1) + \\frac{2}{9}\\cdot H(T_2) + \\frac{4}{9}\\cdot H(T_3) = 0.75\nH(T, X1) = \\frac{3}{9}\\cdot H(T_1) + \\frac{2}{9}\\cdot H(T_2) + \\frac{4}{9}\\cdot H(T_3) = 0.75\n\n\n\n\n\n\nThus, infromation gain if the set is split according to X1 \nG(X1) = H(T) - H(T, X1) = 0.99 - 0.75 = 0.24 \\mbox{ Sh }\nG(X1) = H(T) - H(T, X1) = 0.99 - 0.75 = 0.24 \\mbox{ Sh }\n\n\n\n\n\n\n\n\n\n\nID3 algorithm\n\u00b6\n\n\n\n\n\n\nFor every attribute (feature) calculate the entropy\n\n\n\n\n\n\nSplit the training set using the one for which information gain is maximum\n\n\n\n\n\n\nContinue recursively on subsets using remaining features\n\n\n\n\n\n\nPlay Golf dataset\n\u00b6\n\n\n\n\n\n\nPopular dataset to explain decision trees\n\n\n\n\n\n\n4 features:\n\n\n\n\n\n\noutlook\n: \nrainy, overcast, sunny\n\n\n\n\n\n\ntemperature\n: \ncool, mild, hot\n\n\n\n\n\n\nhumidity\n: \nnormal, high\n\n\n\n\n\n\nwindy\n: \nfalse, true\n\n\n\n\n\n\n\n\n\n\nPossible outcomes (play golf?):\n\n\n\n\n\n\nfalse\n\n\n\n\n\n\ntrue\n\n\n\n\n\n\n\n\n\n\nimport\n \npandas\n \nas\n \npd\n\n\n\n# first row = headers\n\n\nsrc\n \n=\n \n\"http://chem-eng.utoronto.ca/~datamining/dmc/datasets/weather_nominal.csv\"\n\n\n\ngolf_data\n \n=\n \npd\n.\nread_csv\n(\nsrc\n)\n\n\n\n\n\n\ngolf_data\n\n\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nOutlook\n\n      \nTemperature\n\n      \nHumidity\n\n      \nWindy\n\n      \nPlay golf\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \nRainy\n\n      \nHot\n\n      \nHigh\n\n      \nFalse\n\n      \nNo\n\n    \n\n    \n\n      \n1\n\n      \nRainy\n\n      \nHot\n\n      \nHigh\n\n      \nTrue\n\n      \nNo\n\n    \n\n    \n\n      \n2\n\n      \nOvercast\n\n      \nHot\n\n      \nHigh\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n3\n\n      \nSunny\n\n      \nMild\n\n      \nHigh\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n4\n\n      \nSunny\n\n      \nCool\n\n      \nNormal\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n5\n\n      \nSunny\n\n      \nCool\n\n      \nNormal\n\n      \nTrue\n\n      \nNo\n\n    \n\n    \n\n      \n6\n\n      \nOvercast\n\n      \nCool\n\n      \nNormal\n\n      \nTrue\n\n      \nYes\n\n    \n\n    \n\n      \n7\n\n      \nRainy\n\n      \nMild\n\n      \nHigh\n\n      \nFalse\n\n      \nNo\n\n    \n\n    \n\n      \n8\n\n      \nRainy\n\n      \nCool\n\n      \nNormal\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n9\n\n      \nSunny\n\n      \nMild\n\n      \nNormal\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n10\n\n      \nRainy\n\n      \nMild\n\n      \nNormal\n\n      \nTrue\n\n      \nYes\n\n    \n\n    \n\n      \n11\n\n      \nOvercast\n\n      \nMild\n\n      \nHigh\n\n      \nTrue\n\n      \nYes\n\n    \n\n    \n\n      \n12\n\n      \nOvercast\n\n      \nHot\n\n      \nNormal\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n13\n\n      \nSunny\n\n      \nMild\n\n      \nHigh\n\n      \nTrue\n\n      \nNo\n\n    \n\n  \n\n\n\n\n\n\n\nPlay golf entropy\n\u00b6\n\n\nentropy\n(\n9\n,\n \n5\n)\n\n\n\n\n\n\n0.9402859586706309\n\n\n\n\n\n| Play golf |\n=============\n| yes | no  |  -> H(T) = 0.94\n-------------\n|  9  |  5  |\n\n\n\n\n\nPlay golf vs outlook\n\u00b6\n\n\n                   | Play golf |\n                   =============\n                   | yes | no  |\n        ------------------------\n        | sunny    |  3  |  2  |  5\noutlook | overcast |  4  |  0  |  4\n        | rainy    |  2  |  3  |  5\n        ------------------------\n                      9     5\n\n\n\n\n\nentropy\n(\n3\n,\n \n2\n),\n \n0\n,\n \nentropy\n(\n2\n,\n \n3\n)\n\n\n\n\n\n\n(0.9709505944546686, 0, 0.9709505944546686)\n\n\n\n\n\n\n\n\n\\begin{eqnarray}\n   H(\\mbox{sunny}) & = & 0.97 \\\\\n   H(\\mbox{rainy}) & = & 0.97 \\\\\nH(\\mbox{overcast}) & = & 0\n\\end{eqnarray}\n\n\n\n\n\n\n\n\n\n\n\n\\begin{eqnarray}\nH(T, \\mbox{outlook}) & = & P(\\mbox{sunny})\\cdot H(\\mbox{sunny}) + P(\\mbox{overcast})\\cdot H(\\mbox{overcast}) + P(\\mbox{rainy})\\cdot H(\\mbox{rainy}) \\\\\n                     & = & \\frac{5}{14}\\cdot 0.97 + \\frac{4}{14} \\cdot 0 + \\frac{5}{14}\\cdot 0.97 = 0.69\n\\end{eqnarray}\n\n\n\n\n\n\n\n\n\n\n\n\\begin{eqnarray}\nG(\\mbox{outlook}) & = & H(T) - H(T, \\mbox{outlook}) = 0.94 - 0.69 = 0.25\n\\end{eqnarray}\n\n\n\n\n\n\nResults for all features\n\u00b6\n\n\n                    | Play golf |                         | Play golf |\n                    =============                         =============\n                    | yes | no  |                         | yes | no  |\n         ------------------------                 --------------------\n         | sunny    |  3  |  2  |                 | hot   |  2  |  2  |\n outlook | overcast |  4  |  0  |     temperature | mild  |  4  |  2  |\n         | rainy    |  2  |  3  |                 | cool  |  3  |  1  |\n         ------------------------                 --------------------\n            Info. gain = 0.25                       Info gain = 0.03\n\n\n                    | Play golf |                         | Play golf |\n                    =============                         =============\n                    | yes | no  |                         | yes | no  |\n         ------------------------                 --------------------\n         | high     |  3  |  4  |                 | false |  6  |  2  |\nhumidity | normal   |  6  |  1  |           windy | true  |  3  |  3  |\n         ------------------------                 --------------------\n            Info. gain = 0.15                       Info gain = 0.05\n\n\n\n\n\nRoot of the tree\n\u00b6\n\n\n\n\n\n\nStart building a tree with the feature with the largest information gain: \noutlook\n\n\n\n\n\n\nA branch with \nentropy 0\n is a leaf node: \novercast\n\n\n\n\n\n\nOther branches must be spliited using other features \n\n\n\n\n\n\ntree\n \n=\n \nDigraph\n()\n\n\n\ntree\n.\nedge\n(\n\"outlook\"\n,\n \n\"sunny\"\n)\n\n\ntree\n.\nedge\n(\n\"outlook\"\n,\n \n\"overcast\"\n)\n\n\ntree\n.\nedge\n(\n\"outlook\"\n,\n \n\"rainy\"\n)\n\n\n\ntree\n.\nedge\n(\n\"overcast\"\n,\n \n\"yes\"\n)\n\n\n\ntree\n\n\n\n\n\n\n\n\nNext branch\n\u00b6\n\n\ngolf_data\n.\nloc\n[\ngolf_data\n[\n'Outlook'\n]\n \n==\n \n\"Sunny\"\n]\n\n\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nOutlook\n\n      \nTemperature\n\n      \nHumidity\n\n      \nWindy\n\n      \nPlay golf\n\n    \n\n  \n\n  \n\n    \n\n      \n3\n\n      \nSunny\n\n      \nMild\n\n      \nHigh\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n4\n\n      \nSunny\n\n      \nCool\n\n      \nNormal\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n5\n\n      \nSunny\n\n      \nCool\n\n      \nNormal\n\n      \nTrue\n\n      \nNo\n\n    \n\n    \n\n      \n9\n\n      \nSunny\n\n      \nMild\n\n      \nNormal\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n13\n\n      \nSunny\n\n      \nMild\n\n      \nHigh\n\n      \nTrue\n\n      \nNo\n\n    \n\n  \n\n\n\n\n\n\n\n\n\n\n\nIn general, one should calculate information gain for each feature for this subset\n\n\n\n\n\n\nIn this case it is clear that we can take \nwindy\n \n\n\n\n\n\n\ntree\n.\nedge\n(\n\"sunny\"\n,\n \n\"windy\"\n)\n\n\n\ntree\n.\nedge\n(\n\"windy\"\n,\n \n\"false\"\n)\n\n\ntree\n.\nedge\n(\n\"windy\"\n,\n \n\"true\"\n)\n\n\n\ntree\n.\nedge\n(\n\"false\"\n,\n \n\"yes\"\n)\n\n\ntree\n.\nedge\n(\n\"true\"\n,\n \n\"no\"\n)\n\n\n\ntree\n\n\n\n\n\n\n\n\nLast branch\n\u00b6\n\n\ngolf_data\n.\nloc\n[\ngolf_data\n[\n'Outlook'\n]\n \n==\n \n\"Rainy\"\n]\n\n\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nOutlook\n\n      \nTemperature\n\n      \nHumidity\n\n      \nWindy\n\n      \nPlay golf\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \nRainy\n\n      \nHot\n\n      \nHigh\n\n      \nFalse\n\n      \nNo\n\n    \n\n    \n\n      \n1\n\n      \nRainy\n\n      \nHot\n\n      \nHigh\n\n      \nTrue\n\n      \nNo\n\n    \n\n    \n\n      \n7\n\n      \nRainy\n\n      \nMild\n\n      \nHigh\n\n      \nFalse\n\n      \nNo\n\n    \n\n    \n\n      \n8\n\n      \nRainy\n\n      \nCool\n\n      \nNormal\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n10\n\n      \nRainy\n\n      \nMild\n\n      \nNormal\n\n      \nTrue\n\n      \nYes\n\n    \n\n  \n\n\n\n\n\n\n\ntree\n.\nedge\n(\n\"rainy\"\n,\n \n\"humidity\"\n)\n\n\n\ntree\n.\nedge\n(\n\"humidity\"\n,\n \n\"high\"\n)\n\n\ntree\n.\nedge\n(\n\"humidity\"\n,\n \n\"normal\"\n)\n\n\n\ntree\n.\nedge\n(\n\"normal\"\n,\n \n\"yes\"\n)\n\n\ntree\n.\nedge\n(\n\"high\"\n,\n \n\"no \"\n)\n\n\n\ntree\n\n\n\n\n\n\n\n\nSummary\n\u00b6\n\n\n\n\n\n\nWe got the final tree for Play Golf dataset using ID3 algorithm\n\n\n\n\n\n\nWe do not even use temperature attribute (for which information gain was 0.03)\n\n\n\n\n\n\nThe main problem is that the algorithm may overfit easily (tree does not stop growing until the whole training set is classified)\n\n\n\n\n\n\nImagine some crazy guys went playing on a \nrainy\n, \nwindy\n day with \nhigh humidity\n, beacaue it was still \nhot\n\n\n\n\n\n\nWith this extra data point we would have to create more branches\n\n\n\n\n\n\nIs one unique data sample worth to extend the whole tree?\n\n\n\n\n\n\n\n\n\n\nAnd there is more disadvantages:\n\n\n\n\n\n\nIt handles only discrete attributes\n\n\n\n\n\n\nThere is a strong bias for features with many possible outcomes\n\n\n\n\n\n\nAnd finally, it does not handle missing values\n\n\n\n\n\n\n\n\n\n\nC4.5 algorithm\n\u00b6\n\n\n\n\n\n\nC4.5 introduces some improvements to ID3:\n\n\n\n\n\n\ncontinuous values using threshold\n\n\n\n\n\n\ntree pruning to avoid overfitting\n\n\n\n\n\n\nnormalized information gain\n\n\n\n\n\n\nmissing values\n\n\n\n\n\n\n\n\n\n\nInformation gain ratio\n\u00b6\n\n\n\n\n\n\nTo avoid a bias in favor of features with a lot of different values C4.5 uses information gain ratio instead of information gain\n\n\n\n\n\n\nLets define intrinsic value \nV\nV\n of an attribute \nX\nX\n as \nV(X) = -\\sum\\limits_{i=1}^N \\frac{|T_i|}{|T|}\\cdot\\log(\\frac{|T_i|}{|T|})\nV(X) = -\\sum\\limits_{i=1}^N \\frac{|T_i|}{|T|}\\cdot\\log(\\frac{|T_i|}{|T|})\n\n\n\n\n\n\nwhere \nT_i\nT_i\n are samples corresponding to \ni\ni\n-th possible value of \nX\nX\n feature\n\n\n\n\n\n\nInformation gain ratio \nR(X)\nR(X)\n is defined as \nR(X) = \\frac{G(X)}{V(X)}\nR(X) = \\frac{G(X)}{V(X)}\n\n\n\n\n\n\nExample\n\u00b6\n\n\n\n\n\n\nLets consider a fake data set\n\n\n\n\n\n\nThe goal is to determine if someone plays or not video games\n\n\n\n\n\n\nWe have three features:\n\n\n\n\n\n\nname - mostly unique\n\n\n\n\n\n\nsex - 50% females and 50% males \n\n\n\n\n\n\nage - just old or young\n\n\n\n\n\n\n\n\n\n\nLooking at data we can say that\n\n\n\n\n\n\nmost young people play video games, why old people don't\n\n\n\n\n\n\nsex does not matter\n\n\n\n\n\n\nnames are almost distinct\n\n\n\n\n\n\n\n\n\n\n     name   ||  John  |  Mark  |  Anne  |  Adam  |  John  |  Alex  |  Alex  |  Xena  |  Tina  |  Lucy  |\n--------------------------------------------------------------------------------------------------------\n     sex    ||   M    |   M    |   F    |   M    |   M    |   F    |   M    |   F    |   F    |   F    |\n--------------------------------------------------------------------------------------------------------\n     age    ||  old   | young  |  old   | young  | young  | young  |  old   |  old   | young  | young  |\n========================================================================================================\n play games ||   N    |   Y    |   Y    |   Y    |   Y    |   N    |   N    |   N    |   Y    |   Y    |\n\n\n\n\n\n\n\nInformation gain for \nname\n\n\n\n\nh\n \n=\n \nentropy\n(\n4\n,\n \n6\n)\n  \n# dataset entropy H(T)\n\n\n\n# one John plays and the other one doesn't\n\n\n# in other cases entropy = 0\n\n\ng_name\n \n=\n \nh\n \n-\n \n2\n/\n10\n \n*\n \nentropy\n(\n1\n,\n \n1\n)\n\n\n\nprint\n(\ng_name\n)\n\n\n\n\n\n\n0.7709505944546686\n\n\n\n\n\n\n\nInformation gain for \nsex\n\n\n\n\n# 5 men - 3 play\n\n\n# 5 women - 3 play\n\n\ng_sex\n \n=\n \nh\n \n-\n \n5\n/\n10\n \n*\n \nentropy\n(\n2\n,\n \n3\n)\n \n-\n \n5\n/\n10\n \n*\n \nentropy\n(\n2\n,\n \n3\n)\n\n\n\nprint\n(\ng_sex\n)\n\n\n\n\n\n\n0.0\n\n\n\n\n\n\n\nInformation gain for \nage\n\n\n\n\n# 4 old people - 1 plays\n\n\n# 6 young people - 5 play\n\n\ng_age\n \n=\n \nh\n \n-\n \n4\n/\n10\n \n*\n \nentropy\n(\n1\n,\n \n3\n)\n \n-\n \n6\n/\n10\n \n*\n \nentropy\n(\n1\n,\n \n5\n)\n\n\n\nprint\n(\ng_age\n)\n\n\n\n\n\n\n0.256425891682003\n\n\n\n\n\n\n\n\n\nIn ID3 a feature with entropy = 0 is always a winner\n\n\n\n\nImagine having all distinct values (e.g. credit card numbers)\n\n\n\n\n\n\n\n\nIn this case we would choose \nname\n as the best predictor\n\n\n\n\n\n\nCreating a tree with 8 branches (from 10 samples)\n\n\n\n\n\n\nTraining data would be perfectly classify\n\n\n\n\n\n\nBut it is unlikely that the algorithm would be able to generalize for unseen data\n\n\n\n\n\n\n\n\n\n\nLets calculate information gain ratio and see how it changes the choice of the best feature\n\n\n\n\n\n\nInformation gain ratio for \nname\n\n\n\n\n\n\n# 2x John, 2x Alex, 6x unique name \n\n\ng_name\n \n/\n \nentropy\n(\n2\n,\n \n2\n,\n \n*\n[\n1\n]\n*\n6\n)\n\n\n\n\n\n\n0.26384995435159336\n\n\n\n\n\n\n\nInformation gain ratio for \nsex\n\n\n\n\n# 5 males and 5 females - zero stays zero though\n\n\ng_sex\n \n/\n \nentropy\n(\n5\n,\n \n5\n)\n\n\n\n\n\n\n0.0\n\n\n\n\n\n\n\nInformation gain ratio for \nage\n\n\n\n\n# 4x old and 6x young\n\n\ng_age\n \n/\n \nentropy\n(\n4\n,\n \n6\n)\n\n\n\n\n\n\n0.26409777505314147\n\n\n\n\n\n\n\n\n\nBased on information gain ratio we choose \nage\n as the best predictor\n\n\n\n\n\n\nBecause the denominator in a ratio penalizes features with many values\n\n\n\n\n\n\nprint\n(\n\"Two possible values:\n\\n\n\"\n)\n\n\n\nfor\n \ni\n \nin\n \nrange\n(\n0\n,\n \n11\n):\n\n  \nprint\n(\n\"\n\\t\n({}, {}) split -> entropy = {}\"\n.\nformat\n(\ni\n,\n \n10\n-\ni\n,\n \nentropy\n(\ni\n,\n \n10\n-\ni\n)))\n\n\n\nprint\n(\n\"\n\\n\n10 possible values:\"\n,\n \nentropy\n(\n*\n[\n1\n]\n*\n10\n))\n\n\n\n\n\n\nTwo possible values:\n\n    (0, 10) split -> entropy = 0\n    (1, 9) split -> entropy = 0.4689955935892812\n    (2, 8) split -> entropy = 0.7219280948873623\n    (3, 7) split -> entropy = 0.8812908992306927\n    (4, 6) split -> entropy = 0.9709505944546686\n    (5, 5) split -> entropy = 1.0\n    (6, 4) split -> entropy = 0.9709505944546686\n    (7, 3) split -> entropy = 0.8812908992306927\n    (8, 2) split -> entropy = 0.7219280948873623\n    (9, 1) split -> entropy = 0.4689955935892812\n    (10, 0) split -> entropy = 0\n\n10 possible values: 3.321928094887362\n\n\n\n\n\n\n\nThis datset was handcrafted to make a point, but I hope the message is still clear\n\n\n\n\nContinuous values\n\u00b6\n\n\n\n\n\n\nAttributes with continuous values must be first discretize\n\n\n\n\n\n\nThe best way is to find an optimal threshold which splits the set\n\n\n\n\n\n\nThe optimal threshold is the one which maximize the infromation gain\n\n\n\n\n\n\nExample\n\u00b6\n\n\n\n\n\n\nLets consider the same example as before\n\n\n\n\n\n\nBut this time age has numerical values\n\n\n\n\n\n\n     name   ||  John  |  Mark  |  Anne  |  Adam  |  John  |  Alex  |  Alex  |  Xena  |  Tina  |  Lucy  |\n--------------------------------------------------------------------------------------------------------\n     sex    ||   M    |   M    |   F    |   M    |   M    |   F    |   M    |   F    |   F    |   F    |\n--------------------------------------------------------------------------------------------------------\n     age    ||   50   |   18   |   65   |   24   |   31   |   18   |   50   |   50   |   24   |   31   |\n========================================================================================================\n play games ||   N    |   Y    |   Y    |   Y    |   Y    |   N    |   N    |   N    |   Y    |   Y    |\n\n\n\n\n\n\n\nThe possible thesholds are therefore \n\\{18, 24, 31, 50\\}\n\\{18, 24, 31, 50\\}\n\n\n\n\n# calculate entropy for all possible thresholds\n\n\ne18\n \n=\n \n2\n/\n10\n \n*\n \nentropy\n(\n1\n,\n \n1\n)\n \n+\n \n8\n/\n10\n \n*\n \nentropy\n(\n3\n,\n \n5\n)\n\n\ne24\n \n=\n \n4\n/\n10\n \n*\n \nentropy\n(\n1\n,\n \n3\n)\n \n+\n \n6\n/\n10\n \n*\n \nentropy\n(\n3\n,\n \n3\n)\n\n\ne31\n \n=\n \n6\n/\n10\n \n*\n \nentropy\n(\n1\n,\n \n5\n)\n \n+\n \n4\n/\n10\n \n*\n \nentropy\n(\n3\n,\n \n1\n)\n\n\ne50\n \n=\n \n9\n/\n10\n \n*\n \nentropy\n(\n4\n,\n \n5\n)\n \n+\n \n1\n/\n10\n \n*\n \nentropy\n(\n0\n,\n \n1\n)\n\n\n\nprint\n(\n\"With threshold = {}, entropy = {}\"\n.\nformat\n(\n18\n,\n \ne18\n))\n\n\nprint\n(\n\"With threshold = {}, entropy = {}\"\n.\nformat\n(\n24\n,\n \ne24\n))\n\n\nprint\n(\n\"With threshold = {}, entropy = {}\"\n.\nformat\n(\n31\n,\n \ne31\n))\n\n\nprint\n(\n\"With threshold = {}, entropy = {}\"\n.\nformat\n(\n50\n,\n \ne50\n))\n\n\n\n\n\n\nWith threshold = 18, entropy = 0.963547202339972\nWith threshold = 24, entropy = 0.9245112497836532\nWith threshold = 31, entropy = 0.7145247027726656\nWith threshold = 50, entropy = 0.8919684538544\n\n\n\n\n\n\n\n\n\nThe best test is \nif age > 31\n\n\n\n\nit splits the dataset to 6 samples (with 5 players) and 4 samples (with 3 non-players)\n\n\n\n\n\n\n\n\nPlease note, that the best threshold may change once a node is created\n\n\n\n\n\n\nUnknown parameters\n\u00b6\n\n\n\n\n\n\nIn the case some samples are incomplete one needs to correct the information gain\n\n\n\n\n\n\nThe information gain is calculated as before for samples with known attributes\n\n\n\n\n\n\nBut then it is normalized with respect to the probability that the given attribute has known values\n\n\n\n\n\n\nLets define the factor \nF\nF\n as the ratio of the number of samples with known value for a given feature to the number of all samples in a dataset\n\n\n\n\n\n\nThen information gain is defines as \nG(X) = F\\cdot (H(T) - H(T, X))\nG(X) = F\\cdot (H(T) - H(T, X))\n\n\n\n\n\n\nPlease note, that \nF = 1\nF = 1\n if all values are known\n\n\n\n\n\n\nOtherwise, information gain is scaled accordingly\n\n\n\n\n\n\nPruning\n\u00b6\n\n\n\n\n\n\nThe algorithm creates as many nodes as needed to classify all test samples\n\n\n\n\n\n\nIt may lead to overfitting and the resulting tree would fail to classify correctly unseen samples\n\n\n\n\n\n\nTo avoid this one can prune a tree\n\n\n\n\n\n\npre-pruning (early stopping)\n\n\n\n\n\n\nstop building a tree before leaves with few samples are produced\n\n\n\n\n\n\nhow to decide when it is good time to stop? e.g. using cross-validation on validation set (stop if the error does not increase significantly)\n\n\n\n\n\n\nunderfitting if stop to early\n\n\n\n\n\n\n\n\n\n\npost-pruning\n\n\n\n\n\n\nlet a tree grow completely\n\n\n\n\n\n\nthen go from bottom to top and try to replace a node with a leaf\n\n\n\n\n\n\nif there is improvement in accuracy - cut a tree\n\n\n\n\n\n\nif the accuracy stays the same - cut a tree (Occam's razor)\n\n\n\n\n\n\notherwise leave a node   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst example - step by step\n\u00b6\n\n\n\n\n\n\nLets consider the problem from the beginning of the lecture\n\n\n\n\n\n\nOur dataset has 20 blue points and 20 orange points\n\n\n\n\n\n\nEach point has two features (both are numerical)\n\n\n\n\n\n\nWe expect overfitting if pruning is not applied\n\n\n\n\n\n\nWe will calculate everything step by step (it is boring, but demonstrates how the algorithm works)\n\n\n\n\n\n\n# first define some points representing two classes\n\n\ngrid\n \n=\n \nnp\n.\nmgrid\n[\n0\n:\n10\n:\n2\n,\n \n0\n:\n10\n:\n2\n]\n\n\nset01\n \n=\n \nnp\n.\nvstack\n([\ngrid\n[\n0\n]\n.\nravel\n(),\n \ngrid\n[\n1\n]\n.\nravel\n()])\n.\nT\n\n\nset01\n \n=\n \nnp\n.\ndelete\n(\nset01\n,\n \n[\n17\n,\n \n18\n,\n \n19\n,\n \n22\n,\n \n24\n],\n \naxis\n=\n0\n)\n\n\n\ngrid\n \n=\n \nnp\n.\nmgrid\n[\n6\n:\n16\n:\n2\n,\n \n0\n:\n10\n:\n2\n]\n\n\nset02\n \n=\n \nnp\n.\nvstack\n([\ngrid\n[\n0\n]\n.\nravel\n(),\n \ngrid\n[\n1\n]\n.\nravel\n()])\n.\nT\n\n\nset02\n \n=\n \nnp\n.\ndelete\n(\nset02\n,\n \n[\n0\n,\n \n1\n,\n \n5\n,\n \n6\n,\n \n8\n],\n \naxis\n=\n0\n)\n\n\n\nplt\n.\nscatter\n(\n*\nset01\n.\nT\n)\n\n\nplt\n.\nscatter\n(\n*\nset02\n.\nT\n);\n\n\n\n\n\n\n\n\nValidation set\n\u00b6\n\n\n\n\n\n\nWe will use 10 points from the dataset for validation\n\n\n\n\n\n\nThis time selected manually to perform by hand calculations\n\n\n\n\n\n\nOn the plot below X denotes validation samples\n\n\n\n\n\n\n# split dataset to training and validation set\n\n\n# note, we should splt them randomly\n\n\n# but here we do this by hand\n\n\nvalid_idx\n \n=\n \n[\n3\n,\n \n7\n,\n \n10\n,\n \n14\n,\n \n18\n]\n\n\n\nblue_valid\n \n=\n \nset01\n[\nvalid_idx\n]\n\n\nblue_train\n \n=\n \nnp\n.\ndelete\n(\nset01\n,\n \nvalid_idx\n,\n \naxis\n=\n0\n)\n\n\n\norange_valid\n \n=\n \nset02\n[\nvalid_idx\n]\n\n\norange_train\n \n=\n \nnp\n.\ndelete\n(\nset02\n,\n \nvalid_idx\n,\n \naxis\n=\n0\n)\n\n\n\n# circles - training set\n\n\n# x - validation set\n\n\nplt\n.\nscatter\n(\n*\nblue_train\n.\nT\n)\n\n\nplt\n.\nscatter\n(\n*\nblue_valid\n.\nT\n,\n \ncolor\n=\n'C0'\n,\n \nmarker\n=\n'x'\n)\n\n\nplt\n.\nscatter\n(\n*\norange_train\n.\nT\n)\n\n\nplt\n.\nscatter\n(\n*\norange_valid\n.\nT\n,\n \ncolor\n=\n'C1'\n,\n \nmarker\n=\n'x'\n);\n\n\n\n\n\n\n\n\nThresholds finder\n\u00b6\n\n\n\n\n\n\nWhen building a tree we need to calculate information gain for every threshold in current subset\n\n\n\n\n\n\nEvery subset \nS\nS\n has \nN_b\nN_b\n blue samples and \nN_o\nN_o\n orange samples\n\n\n\n\n\n\nAfter split into accoring to some threshold we get two subsets\n\n\n\n\n\n\nn_b\nn_b\n of blue points and \nn_o\nn_o\n of orange points (\nS_1\nS_1\n)\n\n\n\n\n\n\nN_b - n_b\nN_b - n_b\n of blue points and \nN_o - n_o\nN_o - n_o\n of orange points (\nS_2\nS_2\n)\n\n\n\n\n\n\n\n\n\n\ndef\n \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \nnb\n,\n \nno\n):\n\n  \n\"\"\"Calculate information gain for given split\"\"\"\n\n  \nh\n \n=\n \nentropy\n(\nNb\n,\n \nNo\n)\n \n# H(S)\n\n  \ntotal\n \n=\n \nNb\n \n+\n \nNo\n     \n# total number of samples\n\n  \nsubtotal\n \n=\n \nnb\n \n+\n \nno\n  \n# number of samples in subset\n\n\n  \nreturn\n \nh\n \n-\n \nsubtotal\n \n/\n \ntotal\n \n*\n \nentropy\n(\nnb\n,\n \nno\n)\n \\\n           \n-\n \n(\ntotal\n \n-\n \nsubtotal\n)\n \n/\n \ntotal\n \n*\n \nentropy\n(\nNb\n \n-\n \nnb\n,\n \nNo\n \n-\n \nno\n)\n\n\n\n\n\n\nFeature X\n\u00b6\n\n\n\n\n\n\nWe need to calculate information gain ratio for the best threshold (the one that maximize information gain)\n\n\n\n\n\n\nPossible thresholds \n\\{0, 2, 4, 6, 8, 10, 12\\}\n\\{0, 2, 4, 6, 8, 10, 12\\}\n\n\n\n\n\n\nNb\n \n=\n \n15\n\n\nNo\n \n=\n \n15\n\n\n\nsplits\n \n=\n \n{\n\"0\"\n:\n \n(\n4\n,\n \n0\n),\n \n\"2 \"\n:\n \n(\n8\n,\n \n0\n),\n \n\"4\"\n:\n \n(\n11\n,\n \n0\n),\n \n\"6\"\n:\n \n(\n13\n,\n \n3\n),\n\n          \n\"8\"\n:\n \n(\n15\n,\n \n4\n),\n \n\"10\"\n:\n \n(\n15\n,\n \n8\n),\n \n\"12\"\n:\n \n(\n15\n,\n \n11\n)}\n\n\n\nfor\n \nthreshold\n,\n \n(\nno\n,\n \nnb\n)\n \nin\n \nsplits\n.\nitems\n():\n\n  \nprint\n(\n\"Threshold = {}\n\\t\n -> {}\"\n.\nformat\n(\nthreshold\n,\n \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \nno\n,\n \nnb\n)))\n\n\n\n\n\n\nThreshold = 0    -> 0.14818913558232172\nThreshold = 2    -> 0.33824492595034883\nThreshold = 4    -> 0.5297578726233217\nThreshold = 6    -> 0.3525728312615027\nThreshold = 8    -> 0.5297578726233217\nThreshold = 10   -> 0.28538113149388267\nThreshold = 12   -> 0.14818913558232172\n\n\n\n\n\n\n\n\n\nWe got the same cuts as predicted at the beginning of the lecture: \nx > 4\nx > 4\n or \nx > 8\nx > 8\n\n\n\n\n\n\nLets choose \nx > 4\nx > 4\n and calculate information gain ratio\n\n\n\n\n\n\n# 4 samples with x = 0, 4 samples with x = 2 etc\n\n\ninfo_gain\n(\nNb\n,\n \nNo\n,\n \n*\nsplits\n[\n\"4\"\n])\n \n/\n \nentropy\n(\n4\n,\n \n4\n,\n \n3\n,\n \n5\n,\n \n3\n,\n \n4\n,\n \n3\n,\n \n4\n)\n\n\n\n\n\n\n0.1779055922617179\n\n\n\n\n\nFeature Y\n\u00b6\n\n\n\n\n\n\nRepeat the procedure\n\n\n\n\n\n\nThis time possible thresholds = \n\\{0, 2, 4, 6\\}\n\\{0, 2, 4, 6\\}\n\n\n\n\n\n\nNb\n \n=\n \n15\n\n\nNo\n \n=\n \n15\n\n\n\nsplits\n \n=\n \n{\n\"0\"\n:\n \n(\n4\n,\n \n2\n),\n \n\"2\"\n:\n \n(\n8\n,\n \n5\n),\n \n\"4\"\n:\n \n(\n10\n,\n \n8\n),\n \n\"6\"\n:\n \n(\n13\n,\n \n11\n)}\n\n\n\nfor\n \nthreshold\n,\n \n(\nno\n,\n \nnb\n)\n \nin\n \nsplits\n.\nitems\n():\n\n  \nprint\n(\n\"Threshold = {}\n\\t\n -> {}\"\n.\nformat\n(\nthreshold\n,\n \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \nno\n,\n \nnb\n)))\n\n\n\n\n\n\nThreshold = 0    -> 0.02035297064032593\nThreshold = 2    -> 0.029594041354123246\nThreshold = 4    -> 0.013406861436605633\nThreshold = 6    -> 0.0203529706403259\n\n\n\n\n\n\n\n\n\nThe best cut is \ny > 2\ny > 2\n (as predicted before)\n\n\n\n\n\n\nLets calculate information gain ratio\n\n\n\n\n\n\ninfo_gain\n(\nNb\n,\n \nNo\n,\n \n*\nsplits\n[\n\"2\"\n])\n \n/\n \nentropy\n(\n6\n,\n \n7\n,\n \n5\n,\n \n6\n,\n \n6\n)\n\n\n\n\n\n\n0.01278981522839263\n\n\n\n\n\nThe root\n\u00b6\n\n\n\n\n\n\nAt the beginning we discussed the choice of \ny\ny\n as a root predictor\n\n\n\n\n\n\nID3 and C4.5 are greedy algorithms and select optimal solution at given stage\n\n\n\n\n\n\nWe can start to build the tree with the first best predictor\n\n\n\n\n\n\ntree\n \n=\n \nDigraph\n()\n\n\n\ntree\n.\nedge\n(\n\"x > 4?\n\\n\n[15, 15]\"\n,\n \n\"blue\n\\n\n[11, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 4?\n\\n\n[15, 15]\"\n,\n \n\"[4, 15]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n\n\n\n\n\n\n\n\nBranch x > 4\n\u00b6\n\n\n\n\n\n\nNow we have to repeat the procedure for the branch \n[4, 15]\n[4, 15]\n\n\n\n\n\n\nLets take a look what points are left\n\n\n\n\n\n\nplt\n.\nxlim\n([\n5.5\n,\n \n14.5\n])\n\n\n\nplt\n.\nscatter\n(\n*\nblue_train\n.\nT\n)\n\n\nplt\n.\nscatter\n(\n*\norange_train\n.\nT\n);\n\n\n\n\n\n\n\n\n\n\nCheck \nx\nx\n maximum information gain ratio\n\n\n\n\nNb\n \n=\n \n4\n\n\nNo\n \n=\n \n15\n\n\n\nsplits\n \n=\n \n{\n\"6\"\n:\n \n(\n2\n,\n \n3\n),\n \n\"8\"\n:\n \n(\n4\n,\n \n4\n),\n \n\"10\"\n:\n \n(\n4\n,\n \n8\n),\n \n\"12\"\n:\n \n(\n4\n,\n \n11\n)}\n\n\n\nfor\n \nthreshold\n,\n \n(\nno\n,\n \nnb\n)\n \nin\n \nsplits\n.\nitems\n():\n\n  \nprint\n(\n\"Threshold = {}\n\\t\n -> {}\"\n.\nformat\n(\nthreshold\n,\n \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \nno\n,\n \nnb\n)))\n\n\n\n\n\n\nThreshold = 6    -> 0.051004839414443226\nThreshold = 8    -> 0.32143493796317624\nThreshold = 10   -> 0.16251125329718286\nThreshold = 12   -> 0.08198172064120202\n\n\n\n\n\nprint\n(\n\"Information gain ratio with x > 8:\"\n,\n\n      \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \n*\nsplits\n[\n\"8\"\n])\n \n/\n \nentropy\n(\n5\n,\n \n3\n,\n \n4\n,\n \n3\n,\n \n4\n))\n\n\n\n\n\n\nInformation gain ratio with x > 8: 0.14010311259651076\n\n\n\n\n\n\n\nCheck \ny\ny\n maximum information gain ratio\n\n\n\n\nNb\n \n=\n \n4\n\n\nNo\n \n=\n \n15\n\n\n\nsplits\n \n=\n \n{\n\"0\"\n:\n \n(\n2\n,\n \n2\n),\n \n\"2\"\n:\n \n(\n3\n,\n \n5\n),\n \n\"4\"\n:\n \n(\n3\n,\n \n6\n),\n \n\"6\"\n:\n \n(\n4\n,\n \n9\n)}\n\n\n\nfor\n \nthreshold\n,\n \n(\nno\n,\n \nnb\n)\n \nin\n \nsplits\n.\nitems\n():\n\n  \nprint\n(\n\"Threshold = {}\n\\t\n -> {}\"\n.\nformat\n(\nthreshold\n,\n \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \nno\n,\n \nnb\n)))\n\n\n\n\n\n\nThreshold = 0    -> 0.08471690647404045\nThreshold = 2    -> 0.08617499693494635\nThreshold = 4    -> 0.06066554625879636\nThreshold = 6    -> 0.13320381570773476\n\n\n\n\n\nprint\n(\n\"Information gain ratio with y > 6:\"\n,\n\n      \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \n*\nsplits\n[\n\"6\"\n])\n \n/\n \nentropy\n(\n4\n,\n \n4\n,\n \n3\n,\n \n4\n,\n \n4\n))\n\n\n\n\n\n\nInformation gain ratio with y > 6: 0.05757775370755489\n\n\n\n\n\n\n\n\n\nOnce again \nx\nx\n is a winner\n\n\n\n\n\n\nAnd we have a new node\n\n\n\n\n\n\ntree\n \n=\n \nDigraph\n()\n\n\n\ntree\n.\nedge\n(\n\"x > 4?\n\\n\n[15, 15]\"\n,\n \n\"blue\n\\n\n[11, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 4?\n\\n\n[15, 15]\"\n,\n \n\"x > 8?\n\\n\n[4, 15]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"x > 8?\n\\n\n[4, 15]\"\n,\n \n\"[4, 4]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 8?\n\\n\n[4, 15]\"\n,\n \n\"orange\n\\n\n[0, 11]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n\n\n\n\n\n\n\n\nBranch x<= 8\n\u00b6\n\n\n\n\nWe will continue until the tree is fully grown\n\n\n\n\nplt\n.\nxlim\n([\n5.5\n,\n \n8.5\n])\n\n\n\nplt\n.\nscatter\n(\n*\nblue_train\n.\nT\n)\n\n\nplt\n.\nscatter\n(\n*\norange_train\n.\nT\n);\n\n\n\n\n\n\n\n\n\n\nAgain, the best cut may be pretty obvious, but lets check the math\n\n\nWe have one possible cut in \nx\nx\n\n\n\n\nNb\n \n=\n \n4\n\n\nNo\n \n=\n \n4\n\n\n\nprint\n(\n\"Information gain ratio with x > 6:\"\n,\n\n      \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \n2\n,\n \n3\n)\n \n/\n \nentropy\n(\n5\n,\n \n3\n))\n\n\n\n\n\n\nInformation gain ratio with x > 6: 0.05112447853477686\n\n\n\n\n\n\n\nAnd usual threshold candidates in \ny\ny\n\n\n\n\nsplits\n \n=\n \n{\n\"0\"\n:\n \n(\n2\n,\n \n0\n),\n \n\"2\"\n:\n \n(\n3\n,\n \n0\n),\n \n\"4\"\n:\n \n(\n3\n,\n \n1\n),\n \n\"6\"\n:\n \n(\n4\n,\n \n2\n)}\n\n\n\nfor\n \nthreshold\n,\n \n(\nno\n,\n \nnb\n)\n \nin\n \nsplits\n.\nitems\n():\n\n  \nprint\n(\n\"Threshold = {}\n\\t\n -> {}\"\n.\nformat\n(\nthreshold\n,\n \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \nno\n,\n \nnb\n)))\n\n\n\n\n\n\nThreshold = 0    -> 0.31127812445913283\nThreshold = 2    -> 0.5487949406953986\nThreshold = 4    -> 0.1887218755408671\nThreshold = 6    -> 0.31127812445913283\n\n\n\n\n\nprint\n(\n\"Information gain ratio with y > 2:\"\n,\n\n      \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \n*\nsplits\n[\n\"2\"\n])\n \n/\n \nentropy\n(\n2\n,\n \n1\n,\n \n1\n,\n \n2\n,\n \n2\n))\n\n\n\n\n\n\nInformation gain ratio with y > 2: 0.24390886253128827\n\n\n\n\n\n\n\nAnd the tree is growing\n\n\n\n\ntree\n \n=\n \nDigraph\n()\n\n\n\ntree\n.\nedge\n(\n\"x > 4?\n\\n\n[15, 15]\"\n,\n \n\"blue\n\\n\n[11, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 4?\n\\n\n[15, 15]\"\n,\n \n\"x > 8?\n\\n\n[4, 15]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"x > 8?\n\\n\n[4, 15]\"\n,\n \n\"y > 2?\n\\n\n[4, 4]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 8?\n\\n\n[4, 15]\"\n,\n \n\"orange\n\\n\n[0, 11]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"y > 2?\n\\n\n[4, 4]\"\n,\n \n\"blue\n\\n\n[3, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"y > 2?\n\\n\n[4, 4]\"\n,\n \n\"[1, 4]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n\n\n\n\n\n\n\n\nBranch y > 2\n\u00b6\n\n\nplt\n.\nxlim\n([\n5.5\n,\n \n8.5\n])\n\n\nplt\n.\nylim\n([\n3.5\n,\n \n8.5\n])\n\n\n\nplt\n.\nscatter\n(\n*\nblue_train\n.\nT\n)\n\n\nplt\n.\nscatter\n(\n*\norange_train\n.\nT\n);\n\n\n\n\n\n\n\n\nNb\n \n=\n \n1\n\n\nNo\n \n=\n \n4\n\n\n\nprint\n(\n\"Information gain ratio with x > 6:\"\n,\n\n      \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \n0\n,\n \n3\n)\n \n/\n \nentropy\n(\n3\n,\n \n2\n))\n\n\n\n\n\n\nInformation gain ratio with x > 6: 0.33155970728682876\n\n\n\n\n\nprint\n(\n\"Information gain ratio with y > 4:\"\n,\n\n      \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \n0\n,\n \n1\n)\n \n/\n \nentropy\n(\n1\n,\n \n2\n,\n \n2\n))\n\n\n\nprint\n(\n\"Information gain ratio with y > 6:\"\n,\n\n      \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \n1\n,\n \n2\n)\n \n/\n \nentropy\n(\n1\n,\n \n2\n,\n \n2\n))\n\n\n\n\n\n\nInformation gain ratio with y > 4: 0.047903442721748145\nInformation gain ratio with y > 6: 0.11232501392736344\n\n\n\n\n\nThe final tree\n\u00b6\n\n\ntree\n \n=\n \nDigraph\n()\n\n\n\ntree\n.\nedge\n(\n\"x > 4?\n\\n\n[15, 15]\"\n,\n \n\"blue\n\\n\n[11, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 4?\n\\n\n[15, 15]\"\n,\n \n\"x > 8?\n\\n\n[4, 15]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"x > 8?\n\\n\n[4, 15]\"\n,\n \n\"y > 2?\n\\n\n[4, 4]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 8?\n\\n\n[4, 15]\"\n,\n \n\"orange\n\\n\n[0, 11]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"y > 2?\n\\n\n[4, 4]\"\n,\n \n\"blue\n\\n\n[3, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"y > 2?\n\\n\n[4, 4]\"\n,\n \n\"x > 6?\n\\n\n[1, 4]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"x > 6?\n\\n\n[1, 4]\"\n,\n \n\"orange\n\\n\n[0, 3]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 6?\n\\n\n[1, 4]\"\n,\n \n\"y > 6?\n\\n\n[1, 1]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"y > 6?\n\\n\n[1, 1]\"\n,\n \n\"blue\n\\n\n[1, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"y > 6?\n\\n\n[1, 1]\"\n,\n \n\"orange\n\\n\n[0, 1]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n\n\n\n\n\n\n\n\n\n\n\n\nIt is likely that this tree is overfitted\n\n\n\n\n\n\nWe will proceed with pruning as it was explained\n\n\n\n\n\n\nBut first lets implement decision rules to measure accuracy\n\n\n\n\n\n\ndef\n \ntree_nominal\n(\nx\n,\n \ny\n):\n\n  \n\"\"\"Implementation of above tree\"\"\"\n\n  \nif\n \nx\n \n<=\n \n4\n:\n\n    \nreturn\n \n\"blue\"\n\n  \nelif\n \nx\n \n>\n \n8\n:\n\n    \nreturn\n \n\"orange\"\n\n  \nelif\n \ny\n \n<=\n \n2\n:\n\n    \nreturn\n \n\"blue\"\n\n  \nelif\n \nx\n \n<=\n \n6\n:\n\n    \nreturn\n \n\"orange\"\n\n  \nelse\n:\n\n    \nreturn\n \n\"orange\"\n \nif\n \ny\n \n>\n \n6\n \nelse\n \n\"blue\"\n\n\n\n\n\n\nSanity check\n\u00b6\n\n\n\n\nIf the tree is built \ncorrectly\n we expect 100% accuracy on training set\n\n\n\n\nfor\n \nx\n,\n \ny\n \nin\n \nblue_train\n:\n\n  \nprint\n(\ntree_nominal\n(\nx\n,\n \ny\n),\n \nend\n=\n' '\n)\n\n\n\n\n\n\nblue blue blue blue blue blue blue blue blue blue blue blue blue blue blue\n\n\n\n\n\nfor\n \nx\n,\n \ny\n \nin\n \norange_train\n:\n\n  \nprint\n(\ntree_nominal\n(\nx\n,\n \ny\n),\n \nend\n=\n' '\n)\n \n\n\n\n\n\norange orange orange orange orange orange orange orange orange orange orange orange orange orange orange\n\n\n\n\n\nAccuracy before pruning\n\u00b6\n\n\ndef\n \naccuracy\n(\nsamples\n,\n \ntree\n):\n\n  \n\"\"\"Just print the result of classification\"\"\"\n\n  \nfor\n \nx\n,\n \ny\n \nin\n \nsamples\n:\n\n    \nprint\n(\n\"({}, {}) -> {}\"\n.\nformat\n(\nx\n,\n \ny\n,\n \ntree\n(\nx\n,\n \ny\n)))\n\n\n\n\n\n\naccuracy\n(\nblue_valid\n,\n \ntree_nominal\n)\n\n\n\n\n\n\n(0, 6) -> blue\n(2, 4) -> blue\n(4, 0) -> blue\n(4, 8) -> blue\n(8, 2) -> blue\n\n\n\n\n\naccuracy\n(\norange_valid\n,\n \ntree\n)\n\n\n\n\n\n\n(8, 4) -> blue\n(10, 4) -> orange\n(12, 0) -> orange\n(12, 8) -> orange\n(14, 6) -> orange\n\n\n\n\n\nPruning I\n\u00b6\n\n\n\n\n\n\nWe want to prune last decision node \ny > 6\ny > 6\n\n\n\n\n\n\nIn general, majority decides about the leaf node class\n\n\n\n\n\n\nAs it is a tie here, lets check both\n\n\n\n\n\n\ndef\n \ntree_prune01a\n(\nx\n,\n \ny\n):\n\n  \n\"\"\"Implementation of above tree\"\"\"\n\n  \nif\n \nx\n \n<=\n \n4\n:\n\n    \nreturn\n \n\"blue\"\n\n  \nelif\n \nx\n \n>\n \n8\n:\n\n    \nreturn\n \n\"orange\"\n\n  \nelif\n \ny\n \n<=\n \n2\n:\n\n    \nreturn\n \n\"blue\"\n\n  \nelif\n \nx\n \n<=\n \n6\n:\n\n    \nreturn\n \n\"orange\"\n\n  \nelse\n:\n\n    \nreturn\n \n\"blue\"\n\n\n\ndef\n \ntree_prune01b\n(\nx\n,\n \ny\n):\n\n  \n\"\"\"Implementation of above tree\"\"\"\n\n  \nif\n \nx\n \n<=\n \n4\n:\n\n    \nreturn\n \n\"blue\"\n\n  \nelif\n \nx\n \n>\n \n8\n:\n\n    \nreturn\n \n\"orange\"\n\n  \nelif\n \ny\n \n<=\n \n2\n:\n\n    \nreturn\n \n\"blue\"\n\n  \nelse\n:\n\n    \nreturn\n \n\"orange\"\n\n\n\n\n\n\naccuracy\n(\nblue_valid\n,\n \ntree_prune01a\n)\n\n\n\n\n\n\n(0, 6) -> blue\n(2, 4) -> blue\n(4, 0) -> blue\n(4, 8) -> blue\n(8, 2) -> blue\n\n\n\n\n\naccuracy\n(\norange_valid\n,\n \ntree_prune01a\n)\n\n\n\n\n\n\n(8, 4) -> blue\n(10, 4) -> orange\n(12, 0) -> orange\n(12, 8) -> orange\n(14, 6) -> orange\n\n\n\n\n\n\n\n\n\nPruning does not change the accuracy\n\n\n\n\n\n\nWe always use Occam's razor and \nprune01a\n is preferred over nominal tree\n\n\n\n\n\n\nBut lets see how \nprune01b\n works\n\n\n\n\n\n\naccuracy\n(\nblue_valid\n,\n \ntree_prune01b\n)\n\n\n\n\n\n\n(0, 6) -> blue\n(2, 4) -> blue\n(4, 0) -> blue\n(4, 8) -> blue\n(8, 2) -> blue\n\n\n\n\n\naccuracy\n(\norange_valid\n,\n \ntree_prune01b\n)\n\n\n\n\n\n\n(8, 4) -> orange\n(10, 4) -> orange\n(12, 0) -> orange\n(12, 8) -> orange\n(14, 6) -> orange\n\n\n\n\n\n\n\n\n\nIn this case we even get the increase of the accuracy\n\n\n\n\n\n\nWe decide to prune a tree by replacing \ny > 6\ny > 6\n decision node with \"orange\" leaf node\n\n\n\n\n\n\nWhich automatically removes \nx > 6\nx > 6\n decision node\n\n\n\n\n\n\ntree\n \n=\n \nDigraph\n()\n\n\n\ntree\n.\nedge\n(\n\"x > 4?\n\\n\n[15, 15]\"\n,\n \n\"blue\n\\n\n[11, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 4?\n\\n\n[15, 15]\"\n,\n \n\"x > 8?\n\\n\n[4, 15]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"x > 8?\n\\n\n[4, 15]\"\n,\n \n\"y > 2?\n\\n\n[4, 4]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 8?\n\\n\n[4, 15]\"\n,\n \n\"orange\n\\n\n[0, 11]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"y > 2?\n\\n\n[4, 4]\"\n,\n \n\"blue\n\\n\n[3, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"y > 2?\n\\n\n[4, 4]\"\n,\n \n\"orange\n\\n\n[1, 4]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n\n\n\n\n\n\n\n\nPruning II\n\u00b6\n\n\n\n\n\n\nNow, lets see the accuracy after removing \ny > 2\ny > 2\n node\n\n\n\n\n\n\nIt is once again a tie, so lets check both scenarios\n\n\n\n\n\n\ndef\n \ntree_prune02a\n(\nx\n,\n \ny\n):\n\n  \n\"\"\"Implementation of above tree\"\"\"\n\n  \nif\n \nx\n \n<=\n \n4\n:\n\n    \nreturn\n \n\"blue\"\n\n  \nelse\n:\n\n    \nreturn\n \n\"orange\"\n\n\n\ndef\n \ntree_prune02b\n(\nx\n,\n \ny\n):\n\n  \n\"\"\"Implementation of above tree\"\"\"\n\n  \nif\n \nx\n \n<=\n \n4\n:\n\n    \nreturn\n \n\"blue\"\n\n  \nelif\n \nx\n \n>\n \n8\n:\n\n    \nreturn\n \n\"orange\"\n\n  \nelse\n:\n\n    \nreturn\n \n\"blue\"\n\n\n\n\n\n\naccuracy\n(\nblue_valid\n,\n \ntree_prune02a\n)\n\n\n\n\n\n\n(0, 6) -> blue\n(2, 4) -> blue\n(4, 0) -> blue\n(4, 8) -> blue\n(8, 2) -> orange\n\n\n\n\n\naccuracy\n(\norange_valid\n,\n \ntree_prune02a\n)\n\n\n\n\n\n\n(8, 4) -> orange\n(10, 4) -> orange\n(12, 0) -> orange\n(12, 8) -> orange\n(14, 6) -> orange\n\n\n\n\n\naccuracy\n(\nblue_valid\n,\n \ntree_prune02b\n)\n\n\n\n\n\n\n(0, 6) -> blue\n(2, 4) -> blue\n(4, 0) -> blue\n(4, 8) -> blue\n(8, 2) -> blue\n\n\n\n\n\naccuracy\n(\norange_valid\n,\n \ntree_prune02b\n)\n\n\n\n\n\n\n(8, 4) -> blue\n(10, 4) -> orange\n(12, 0) -> orange\n(12, 8) -> orange\n(14, 6) -> orange\n\n\n\n\n\n\n\n\n\nIn both cases the error increased\n\n\n\n\n\n\nWe stop pruning and leave the tree as it is in \nprune01b\n version \n\n\n\n\n\n\nSummary\n\u00b6\n\n\n\n\n\n\nC4.5 algorithm gives the full and clear prescription for building decision trees\n\n\n\n\n\n\nIt may look as a long procedure, but it is only because I wanted to show everything step by step and avoid \n\"after a few trivial steps...\"\n\n\n\n\n\n\nID3/C4.5/C5.0 are based on information theory\n\n\n\n\n\n\nThere is alternative procedure based on \ngini impurity\n, which is used by CART\n\n\n\n\n\n\nCART\n\u00b6\n\n\n\n\n\n\nCART stands for Classification and Regression Tree\n\n\n\n\n\n\nIt was created independently from ID3 (more or less at the same time)\n\n\n\n\n\n\nThe main differences:\n\n\n\n\n\n\nit creates binary trees (each decision node has two branches)\n\n\n\n\n\n\nit uses gini impurity instead of information gain\n\n\n\n\n\n\nit supports numerical target variables (regression)\n\n\n\n\n\n\n\n\n\n\nGini impurity\n\u00b6\n\n\n\n\n\n\nLet \nT = \\{T_1, T_2, ..., T_n\\}\nT = \\{T_1, T_2, ..., T_n\\}\n be the set of \nn\nn\n classes\n\n\n\n\n\n\nand \nP = \\{p_1, p_2, ..., p_n\\}\nP = \\{p_1, p_2, ..., p_n\\}\n be the probability distribution\n\n\n\n\n\n\nwhere \np_i\np_i\n is the probability that a sample belongs to class \nT_i\nT_i\n\n\n\n\n\n\nand \n1 - p_i\n1 - p_i\n is the probability that it belongs to another class\n\n\n\n\n\n\nGini impurity is defines as \nI(P) = \\sum\\limits_{i=1}^n p_i\\cdot (1 - p_i) = \\sum\\limits_{i=1}^n p_i - \\sum\\limits_{i=1}^n p_i^2 = 1 - \\sum\\limits_{i=1}^n p_i^2\nI(P) = \\sum\\limits_{i=1}^n p_i\\cdot (1 - p_i) = \\sum\\limits_{i=1}^n p_i - \\sum\\limits_{i=1}^n p_i^2 = 1 - \\sum\\limits_{i=1}^n p_i^2\n\n\n\n\n\n\nAs before (for entropy), lets consider two case scenario with \nP = (p, 1 - p)\nP = (p, 1 - p)\n, so gini impurity is given by \nI = 1 - p^2 - (1 - p)^2 = -2p(p - 1)\nI = 1 - p^2 - (1 - p)^2 = -2p(p - 1)\n\n\n\n\n\n\np\n \n=\n \nnp\n.\narange\n(\n0.01\n,\n \n1.0\n,\n \n0.01\n)\n\n\n\nplt\n.\nxlabel\n(\n\"p\"\n)\n\n\nplt\n.\nylabel\n(\n\"surprise factor\"\n)\n\n\n\nplt\n.\nplot\n(\np\n,\n \n-\np\n \n*\n \nnp\n.\nlog2\n(\np\n)\n \n-\n \n(\n1\n \n-\n \np\n)\n \n*\n \nnp\n.\nlog2\n(\n1\n \n-\n \np\n),\n \nlabel\n=\n\"Entropy\"\n);\n\n\nplt\n.\nplot\n(\np\n,\n \n-\n2\n*\np\n*\n(\np\n \n-\n \n1\n),\n \nlabel\n=\n\"Gini impurity\"\n)\n\n\n\nplt\n.\nlegend\n();\n\n\n\n\n\n\n\n\nPlay Golf\n\u00b6\n\n\n\n\nLets consider once again Play Golf dataset\n\n\n\n\nimport\n \npandas\n \nas\n \npd\n\n\n\n# first row = headers\n\n\nsrc\n \n=\n \n\"http://chem-eng.utoronto.ca/~datamining/dmc/datasets/weather_nominal.csv\"\n\n\n\ngolf_data\n \n=\n \npd\n.\nread_csv\n(\nsrc\n)\n\n\n\n\n\n\ngolf_data\n\n\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nOutlook\n\n      \nTemperature\n\n      \nHumidity\n\n      \nWindy\n\n      \nPlay golf\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \nRainy\n\n      \nHot\n\n      \nHigh\n\n      \nFalse\n\n      \nNo\n\n    \n\n    \n\n      \n1\n\n      \nRainy\n\n      \nHot\n\n      \nHigh\n\n      \nTrue\n\n      \nNo\n\n    \n\n    \n\n      \n2\n\n      \nOvercast\n\n      \nHot\n\n      \nHigh\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n3\n\n      \nSunny\n\n      \nMild\n\n      \nHigh\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n4\n\n      \nSunny\n\n      \nCool\n\n      \nNormal\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n5\n\n      \nSunny\n\n      \nCool\n\n      \nNormal\n\n      \nTrue\n\n      \nNo\n\n    \n\n    \n\n      \n6\n\n      \nOvercast\n\n      \nCool\n\n      \nNormal\n\n      \nTrue\n\n      \nYes\n\n    \n\n    \n\n      \n7\n\n      \nRainy\n\n      \nMild\n\n      \nHigh\n\n      \nFalse\n\n      \nNo\n\n    \n\n    \n\n      \n8\n\n      \nRainy\n\n      \nCool\n\n      \nNormal\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n9\n\n      \nSunny\n\n      \nMild\n\n      \nNormal\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n10\n\n      \nRainy\n\n      \nMild\n\n      \nNormal\n\n      \nTrue\n\n      \nYes\n\n    \n\n    \n\n      \n11\n\n      \nOvercast\n\n      \nMild\n\n      \nHigh\n\n      \nTrue\n\n      \nYes\n\n    \n\n    \n\n      \n12\n\n      \nOvercast\n\n      \nHot\n\n      \nNormal\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n13\n\n      \nSunny\n\n      \nMild\n\n      \nHigh\n\n      \nTrue\n\n      \nNo\n\n    \n\n  \n\n\n\n\n\n\n\nGini impurity\n\u00b6\n\n\n\n\n\n\nWe treat all values as they are continues\n\n\n\n\n\n\nAnd consider all possible split\n\n\n\n\n\n\nEvery split leads to two subsets \nS_1\nS_1\n and \nS_2\nS_2\n\n\n\n\n\n\nAnd gini impurity for a set \nS\nS\n for given split is given by: \nI(S) = \\frac{|S_1|}{|S|}\\cdot I(S_1) + \\frac{|S_2|}{|S|}\\cdot I(S_2)\nI(S) = \\frac{|S_1|}{|S|}\\cdot I(S_1) + \\frac{|S_2|}{|S|}\\cdot I(S_2)\n\n\n\n\n\n\ndef\n \ngini\n(\n*\ndistribution\n):\n\n  \n\"\"\"Calculate gini impurity for given ditribution of samples\"\"\"\n\n  \nsum2\n \n=\n \nsum\n(\ndistribution\n)\n**\n2\n  \n# normalization factor\n\n\n  \nreturn\n \n1\n \n-\n \nsum\n([\np\n**\n2\n \nfor\n \np\n \nin\n \ndistribution\n])\n/\nsum2\n\n\n\n\n\n\ndef\n \ngini_split\n(\ns1\n,\n \ns2\n,\n \ng1\n,\n \ng2\n):\n\n  \n\"\"\"Calcualte impurity for given split\n\n\n\n  s1 -- the size of S1 subset\n\n\n  s1 -- the size of S2 subset\n\n\n  g1 -- I(S1)\n\n\n  g2 -- I(S2)\n\n\n  \"\"\"\n\n  \ns\n \n=\n \ns1\n \n+\n \ns2\n  \n# the total set size\n\n\n  \nreturn\n \ns1\n/\ns\n \n*\n \ng1\n \n+\n \ns2\n/\ns\n \n*\n \ng2\n\n\n\n\n\n\n            | Play golf |\n            =============\n            | yes | no  |\n      -------------------\n      | yes |  2  |  3  | 5\nrainy | no  |  7  |  2  | 9\n      -------------------\n               9     5\n\n\n\n\n\ngini_split\n(\n5\n,\n \n9\n,\n \ngini\n(\n2\n,\n \n3\n),\n \ngini\n(\n7\n,\n \n2\n))\n\n\n\n\n\n\n0.3936507936507937\n\n\n\n\n\n            | Play golf |\n            =============\n            | yes | no  |\n      -------------------\n      | yes |  3  |  2  | 5\nsunny | no  |  6  |  3  | 9\n      -------------------\n               9     5\n\n\n\n\n\ngini_split\n(\n5\n,\n \n9\n,\n \ngini\n(\n3\n,\n \n2\n),\n \ngini\n(\n6\n,\n \n3\n))\n\n\n\n\n\n\n0.45714285714285713\n\n\n\n\n\n               | Play golf |\n               =============\n               | yes | no  |\n         -------------------\n         | yes |  4  |  0  | 4\novercast | no  |  5  |  5  | 10\n         -------------------\n                  9     5\n\n\n\n\n\ngini_split\n(\n4\n,\n \n10\n,\n \ngini\n(\n4\n,\n \n0\n),\n \ngini\n(\n5\n,\n \n5\n))\n\n\n\n\n\n\n0.35714285714285715\n\n\n\n\n\n\n\n\n\nFrom \nOutlook\n feature the best choice is \nOvercast\n as it minimizes impurity\n\n\n\n\n\n\nHowever, we would have to check other features and choose the best predictor from all possibilities\n\n\n\n\n\n\nWe have one step by step example done though\n\n\n\n\n\n\nSo lets use some tool\n\n\n\n\n\n\nScikit learn\n\u00b6\n\n\n\n\n\n\nOne step by step example is behind us, so now lets use some tool\n\n\n\n\n\n\nCART is implemented in \nscikit-learn\n\n\n\n\n\n\nHowever, their implementation takes only numerical values\n\n\n\n\n\n\nSo we will use \nLabelDecoder\n to convert all values to numbers\n\n\n\n\n\n\nfrom\n \nsklearn.preprocessing\n \nimport\n \nLabelEncoder\n\n\n\n# pandas.DataFrame.apply applies a function to given axis (0 by default)\n\n\n# LabelEncoder encodes class labels with values between 0 and n-1\n\n\ngolf_data_num\n \n=\n \ngolf_data\n.\napply\n(\nLabelEncoder\n()\n.\nfit_transform\n)\n\n\n\n\n\n\ngolf_data_num\n\n\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nOutlook\n\n      \nTemperature\n\n      \nHumidity\n\n      \nWindy\n\n      \nPlay golf\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n1\n\n      \n1\n\n      \n0\n\n      \n0\n\n      \n0\n\n    \n\n    \n\n      \n1\n\n      \n1\n\n      \n1\n\n      \n0\n\n      \n1\n\n      \n0\n\n    \n\n    \n\n      \n2\n\n      \n0\n\n      \n1\n\n      \n0\n\n      \n0\n\n      \n1\n\n    \n\n    \n\n      \n3\n\n      \n2\n\n      \n2\n\n      \n0\n\n      \n0\n\n      \n1\n\n    \n\n    \n\n      \n4\n\n      \n2\n\n      \n0\n\n      \n1\n\n      \n0\n\n      \n1\n\n    \n\n    \n\n      \n5\n\n      \n2\n\n      \n0\n\n      \n1\n\n      \n1\n\n      \n0\n\n    \n\n    \n\n      \n6\n\n      \n0\n\n      \n0\n\n      \n1\n\n      \n1\n\n      \n1\n\n    \n\n    \n\n      \n7\n\n      \n1\n\n      \n2\n\n      \n0\n\n      \n0\n\n      \n0\n\n    \n\n    \n\n      \n8\n\n      \n1\n\n      \n0\n\n      \n1\n\n      \n0\n\n      \n1\n\n    \n\n    \n\n      \n9\n\n      \n2\n\n      \n2\n\n      \n1\n\n      \n0\n\n      \n1\n\n    \n\n    \n\n      \n10\n\n      \n1\n\n      \n2\n\n      \n1\n\n      \n1\n\n      \n1\n\n    \n\n    \n\n      \n11\n\n      \n0\n\n      \n2\n\n      \n0\n\n      \n1\n\n      \n1\n\n    \n\n    \n\n      \n12\n\n      \n0\n\n      \n1\n\n      \n1\n\n      \n0\n\n      \n1\n\n    \n\n    \n\n      \n13\n\n      \n2\n\n      \n2\n\n      \n0\n\n      \n1\n\n      \n0\n\n    \n\n  \n\n\n\n\n\n\n\n\n\nNow, lets splits our dataset to features and labels\n\n\n\n\n# DataFrame.iloc makes an access thourgh indices\n\n\n# we want all rows and first 4 columns for features\n\n\n# and the last column for labels\n\n\ndata\n \n=\n \nnp\n.\narray\n(\ngolf_data_num\n.\niloc\n[:,\n \n:\n4\n])\n\n\ntarget\n \n=\n \nnp\n.\narray\n(\ngolf_data_num\n.\niloc\n[:,\n \n4\n])\n\n\n\n\n\n\n\n\nOnce data is prepared, creating a tree is as easy as 2 + 2 -1\n\n\n\n\nfrom\n \nsklearn\n \nimport\n \ntree\n\n\n\ngolf_tree\n \n=\n \ntree\n.\nDecisionTreeClassifier\n()\n\n\n\ngolf_tree\n.\nfit\n(\ndata\n,\n \ntarget\n);\n\n\n\n\n\n\n\n\nsklearn.tree\n supports drawing a tree using \ngraphviz\n\n\n\n\nimport\n \ngraphviz\n\n\n\n# dot is a graph description language\n\n\ndot\n \n=\n \ntree\n.\nexport_graphviz\n(\ngolf_tree\n,\n \nout_file\n=\nNone\n,\n \n                           \nfeature_names\n=\ngolf_data\n.\ncolumns\n.\nvalues\n[:\n4\n],\n  \n                           \nclass_names\n=\n[\n\"no\"\n,\n \n\"yes\"\n],\n  \n                           \nfilled\n=\nTrue\n,\n \nrounded\n=\nTrue\n,\n  \n                           \nspecial_characters\n=\nTrue\n)\n \n\n\n# we create a graph from dot source using graphviz.Source\n\n\ngraph\n \n=\n \ngraphviz\n.\nSource\n(\ndot\n)\n \n\ngraph\n\n\n\n\n\n\n\n\n\n\nPlease note, that in the case of a real problem we would want to have a validation set and perform a pruning (\nscikit-learn\n does not support it though)\n\n\n\n\nRegression\n\u00b6\n\n\n\n\n\n\nThe difference now is that targets are numerical values (instead of categorical), e.g. in golf data - number of hours played instead of \"yes / no\"\n\n\n\n\n\n\nFeatures may be either discrete or continuous\n\n\n\n\n\n\nThe idea is the same though - we want to create a binary tree and minimize the error on in each leaf\n\n\n\n\n\n\nHowever, having continuous values as targets we can not simply use entropy or gini\n\n\n\n\n\n\nWe need to use different measurement - variance \nV(X) = \\frac{1}{n}\\sum\\limits_{i=1}^n (x_i - \\bar x)^2\nV(X) = \\frac{1}{n}\\sum\\limits_{i=1}^n (x_i - \\bar x)^2\n\n\n\n\n\n\nwhere \nX = \\{x_1, ..., x_n\\}\nX = \\{x_1, ..., x_n\\}\n and \n\\bar x\n\\bar x\n is the average value\n\n\n\n\n\n\nNote, that here \nx_i\nx_i\n are equally likely\n\n\n\n\n\n\nSimple example\n\u00b6\n\n\n\n\n\n\nBefore we learn how to grow a regression tree, lets take a look how it works on a simple example\n\n\n\n\n\n\nLets consider data distributed according to \nx^2\nx^2\n (with some noise, obviously)\n\n\n\n\n\n\nIt means with have continuous features (\nx\nx\n) and targets (\ny\ny\n)\n\n\n\n\n\n\nWe will split by hand the domain in \n0.3\n0.3\n and \n0.6\n0.6\n\n\n\n\n\n\nX\n \n=\n \nnp\n.\nrandom\n.\nsample\n(\n50\n)\n\n\nY\n \n=\n \nnp\n.\narray\n([\nx\n**\n2\n \n+\n \nnp\n.\nrandom\n.\nnormal\n(\n0\n,\n \n0.05\n)\n \nfor\n \nx\n \nin\n \nX\n])\n\n\n\nplt\n.\nxlabel\n(\n\"x\"\n)\n\n\nplt\n.\nylabel\n(\n\"y\"\n)\n\n\n\nplt\n.\nscatter\n(\nX\n,\n \nY\n,\n \ncolor\n=\n'b'\n)\n\n\nplt\n.\nplot\n([\n0.3\n,\n \n0.3\n],\n \n[\n-\n0.2\n,\n \n1.2\n],\n \n'g--'\n)\n\n\nplt\n.\nplot\n([\n0.6\n,\n \n0.6\n],\n \n[\n-\n0.2\n,\n \n1.2\n],\n \n'g--'\n);\n\n\n\n\n\n\n\n\n\n\nThe corresponding tree would look like this\n\n\n\n\ntree\n \n=\n \nDigraph\n()\n\n\n\ntree\n.\nedge\n(\n\"x < 0.3?\"\n,\n \n\"?\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x < 0.3?\"\n,\n \n\"x < 0.6?\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"x < 0.6?\"\n,\n \n\"? \"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x < 0.6?\"\n,\n \n\"?  \"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n\n\n\n\n\n\n\n\n\n\nFor each split lets find a value \n\\bar y\n\\bar y\n\n\n\n\ndef\n \navg\n(\nX\n,\n \nY\n,\n \nx_min\n,\n \nx_max\n):\n\n  \n\"\"\"Return the average value in (x_min, x_max) range\"\"\"\n\n  \nn\n \n=\n \n0\n    \n# number of samples in given split \n\n  \navg\n \n=\n \n0\n  \n# average value\n\n\n  \nfor\n \nx\n,\n \ny\n \nin\n \nzip\n(\nX\n,\n \nY\n):\n\n    \nif\n \nx\n \n>=\n \nx_min\n \nand\n \nx\n \n<\n \nx_max\n:\n\n      \nn\n \n+=\n \n1\n\n      \navg\n \n+=\n \ny\n\n\n  \nreturn\n \navg\n \n/\n \nn\n\n\n\n\n\n\nplt\n.\nscatter\n(\nX\n,\n \nY\n,\n \ncolor\n=\n'b'\n)\n\n\n\nplt\n.\nplot\n([\n0.3\n,\n \n0.3\n],\n \n[\n-\n0.2\n,\n \n1.2\n],\n \n'g--'\n)\n\n\nplt\n.\nplot\n([\n0.6\n,\n \n0.6\n],\n \n[\n-\n0.2\n,\n \n1.2\n],\n \n'g--'\n)\n\n\n\ny\n \n=\n \navg\n(\nX\n,\n \nY\n,\n \n0\n,\n \n0.3\n)\n\n\nplt\n.\nplot\n([\n0.0\n,\n \n0.3\n],\n \n[\ny\n,\n \ny\n],\n \n'r'\n)\n\n\n\ny\n \n=\n \navg\n(\nX\n,\n \nY\n,\n \n0.3\n,\n \n0.6\n)\n\n\nplt\n.\nplot\n([\n0.3\n,\n \n0.6\n],\n \n[\ny\n,\n \ny\n],\n \n'r'\n)\n\n\n\ny\n \n=\n \navg\n(\nX\n,\n \nY\n,\n \n0.6\n,\n \n1\n)\n\n\nplt\n.\nplot\n([\n0.6\n,\n \n1.0\n],\n \n[\ny\n,\n \ny\n],\n \n'r'\n);\n\n\n\n\n\n\n\n\n\n\nAlternatively, one could do linear regression for split\n\n\n\n\nGrowing a tree\n\u00b6\n\n\n\n\n\n\nThe idea is similar as for numerical values in classification problems\n\n\n\n\n\n\nFor each feature we check all possible splits and calculate variance\n\n\n\n\n\n\nWe choose a binary split which minimzes variance\n\n\n\n\n\n\nfrom\n \nsklearn.tree\n \nimport\n \nDecisionTreeRegressor\n\n\n\n# create a decision tree regressor\n\n\nfit\n \n=\n \nDecisionTreeRegressor\n()\n\n\n\n# and grow it (note that X must be reshaped)\n\n\nfit\n.\nfit\n(\nnp\n.\nreshape\n(\nX\n,\n \n(\n-\n1\n,\n \n1\n)),\n \nY\n);\n\n\n\n\n\n\n# prepare test sample with \"newaxis\" trick\n\n\nX_test\n \n=\n \nnp\n.\narange\n(\n0.0\n,\n \n1.0\n,\n \n0.01\n)[:,\n \nnp\n.\nnewaxis\n]\n\n\nY_test\n \n=\n \nfit\n.\npredict\n(\nX_test\n)\n\n\n\n\n\n\nplt\n.\nscatter\n(\nX\n,\n \nY\n,\n \ncolor\n=\n'b'\n)\n\n\nplt\n.\nplot\n(\nX_test\n,\n \nY_test\n);\n\n\n\n\n\n\n\n\n\n\n\n\nAnd this is a perfect example of \noverfitting\n\n\n\n\n\n\nEach point was \nclassified\n as a separate target\n\n\n\n\n\n\nBeacause without any stopping criterion the tree is growing until there is a single point in a leaf\n\n\n\n\n\n\nThere are several strategies to pre-prune a tree:\n\n\n\n\n\n\ndefine a max depth of a tree\n\n\n\n\n\n\ndefine a minimum number of samples in a leaf\n\n\n\n\n\n\ndefine a minimum impurity\n\n\n\n\n\n\ndefine a minimum impurity decrease\n\n\n\n\n\n\n\n\n\n\nWhatever method is chosen you get a hyperparameter\n\n\n\n\n\n\nAnd we already know how to find an optimal hyperparameter: cross-validation\n\n\n\n\n\n\nTree: cross-validation\n\u00b6\n\n\n\n\n\n\nTo make it easier to check all possible methods lets create a simple class to do that for us\n\n\n\n\n\n\nIt takes training data and hyperparameter name (as named in \nscikit-learn\n)\n\n\n\n\n\n\nIt can change hyperparameter\n\n\n\n\n\n\nIt can perform a cross-validation for a set of hyperparameter values\n\n\n\n\n\n\nIt can make accuracy and best fit plots\n\n\n\n\n\n\nfrom\n \nsklearn.model_selection\n \nimport\n \ncross_val_score\n\n\nfrom\n \nsklearn.tree\n \nimport\n \nDecisionTreeRegressor\n\n\n\nclass\n \nTreeCV\n:\n\n  \n\"\"\"Perform a cross-validation for chosen hyperparameter\"\"\"\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \nX\n,\n \nY\n,\n \nhp\n=\n\"max_depth\"\n):\n\n    \n\"\"\"Save training data\"\"\"\n\n    \nself\n.\nX\n \n=\n \nX\n    \n# features\n\n    \nself\n.\nY\n \n=\n \nY\n    \n# targets\n\n    \nself\n.\nhp\n \n=\n \nhp\n  \n# hyperparameter\n\n\n\n  \ndef\n \nset_method\n(\nself\n,\n \nhp\n):\n\n    \n\"\"\"Set hyperparameter to use\"\"\"\n\n    \nself\n.\nhp\n \n=\n \nhp\n\n\n\n  \ndef\n \ncross_me\n(\nself\n,\n \n*\nhp_vals\n):\n\n    \n\"\"\"Perform cross validation for given hyperparameter values\"\"\"\n\n    \nself\n.\nscores\n \n=\n \n[]\n  \n# the accuracy table\n\n    \nself\n.\nbest\n \n=\n \nNone\n  \n# the best fit\n\n\n    \nbest_score\n \n=\n \n0\n\n\n    \nfor\n \nhp\n \nin\n \nhp_vals\n:\n\n      \n# create a tree with given hyperparameter cut\n\n      \nfit\n \n=\n \nDecisionTreeRegressor\n(\n**\n{\nself\n.\nhp\n:\n \nhp\n})\n\n\n      \n# calculate a cross validation scores and a mean value\n\n      \nscore\n \n=\n \ncross_val_score\n(\nfit\n,\n \nnp\n.\nreshape\n(\nX\n,\n \n(\n-\n1\n,\n \n1\n)),\n \nY\n)\n.\nmean\n()\n\n\n      \n# update best fit if necessary\n\n      \nif\n \nscore\n \n>\n \nbest_score\n:\n\n        \nself\n.\nbest\n \n=\n \nfit\n\n        \nbest_score\n \n=\n \nscore\n\n\n      \nself\n.\nscores\n.\nappend\n([\nhp\n,\n \nscore\n])\n\n\n    \n# train the best fit\n\n    \nself\n.\nbest\n.\nfit\n(\nnp\n.\nreshape\n(\nX\n,\n \n(\n-\n1\n,\n \n1\n)),\n \nY\n)\n\n\n\n  \ndef\n \nplot\n(\nself\n):\n\n    \n\"\"\"Plot accuracy as a function of hyperparameter values and best fit\"\"\"\n\n    \nplt\n.\nfigure\n(\nfigsize\n=\n(\n15\n,\n \n5\n))\n\n\n    \nplt\n.\nsubplot\n(\n1\n,\n \n2\n,\n \n1\n)\n\n\n    \nplt\n.\nxlabel\n(\nself\n.\nhp\n)\n\n    \nplt\n.\nylabel\n(\n\"accuracy\"\n)\n\n\n    \nplt\n.\nplot\n(\n*\nzip\n(\n*\nself\n.\nscores\n))\n\n\n    \nplt\n.\nsubplot\n(\n1\n,\n \n2\n,\n \n2\n)\n\n\n    \nX_test\n \n=\n \nnp\n.\narange\n(\n0.0\n,\n \n1.0\n,\n \n0.01\n)[:,\n \nnp\n.\nnewaxis\n]\n\n    \nY_test\n \n=\n \nself\n.\nbest\n.\npredict\n(\nX_test\n)\n\n\n    \nplt\n.\nscatter\n(\nself\n.\nX\n,\n \nself\n.\nY\n,\n \ncolor\n=\n'b'\n,\n \nmarker\n=\n'.'\n,\n \nlabel\n=\n\"Training data\"\n)\n\n    \nplt\n.\nplot\n(\nX_test\n,\n \nX_test\n \n*\n \nX_test\n,\n \n'g'\n,\n \nlabel\n=\n\"True distribution\"\n)\n    \n    \nplt\n.\nplot\n(\nX_test\n,\n \nY_test\n,\n \n'r'\n,\n \nlabel\n=\n\"Decision tree\"\n)\n\n\n    \nplt\n.\nlegend\n()\n\n\n\n\n\n\nTraning dataset\n\u00b6\n\n\nX\n \n=\n \nnp\n.\nrandom\n.\nsample\n(\n200\n)\n\n\nY\n \n=\n \nnp\n.\narray\n([\nx\n**\n2\n \n+\n \nnp\n.\nrandom\n.\nnormal\n(\n0\n,\n \n0.05\n)\n \nfor\n \nx\n \nin\n \nX\n])\n\n\n\n\n\n\nmax_depth\n\u00b6\n\n\ntree_handler\n \n=\n \nTreeCV\n(\nX\n,\n \nY\n)\n\n\ntree_handler\n.\ncross_me\n(\n*\nrange\n(\n1\n,\n \n10\n))\n\n\ntree_handler\n.\nplot\n()\n\n\n\n\n\n\n\n\nmin_samples_leaf\n\u00b6\n\n\ntree_handler\n.\nset_method\n(\n\"min_samples_leaf\"\n)\n\n\ntree_handler\n.\ncross_me\n(\n*\nrange\n(\n1\n,\n \n10\n))\n\n\ntree_handler\n.\nplot\n()\n\n\n\n\n\n\n\n\nmin_impurity_split\n\u00b6\n\n\n# min_impurity_split is depracated so lets disable warnings\n\n\nimport\n \nwarnings\n\n\nwarnings\n.\nfilterwarnings\n(\n\"ignore\"\n,\n \ncategory\n=\nDeprecationWarning\n)\n\n\n\n\n\n\ntree_handler\n.\nset_method\n(\n\"min_impurity_split\"\n)\n\n\ntree_handler\n.\ncross_me\n(\n*\nnp\n.\narange\n(\n0.0\n,\n \n5e-3\n,\n \n1e-4\n))\n\n\ntree_handler\n.\nplot\n()\n\n\n\n\n\n\n\n\nmin_impurity_decrease\n\u00b6\n\n\ntree_handler\n.\nset_method\n(\n\"min_impurity_decrease\"\n)\n\n\ntree_handler\n.\ncross_me\n(\n*\nnp\n.\narange\n(\n0.0\n,\n \n5e-4\n,\n \n1e-5\n))\n\n\ntree_handler\n.\nplot\n()\n\n\n\n\n\n\n\n\nBias-Variance trade-off\n\u00b6\n\n\n        +================+================+\n       /\\                |                /\\\n      /  \\               |               /  \\\n     /    \\              |              /    \\\n    /      \\             |             /      \\\n   /        \\            |            /        \\\n  / variance \\           |           /   bias   \\\n  ^^^^^^^^^^^^           |           ^^^^^^^^^^^^\n                         |\n                         |\n                         |\noverfitting   <----------+--------->   underfitting\n\n\n\n\n\n\n\n\n\nBias is an error coming from wrong model assumptions, which do not allow an algorithm to learn all patterns from training data.\n\n\n\n\n\n\nVariance is an error coming from sensivity to features specific for training data.\n\n\n\n\n\n\nHigh bias leads to underfitting and high variance to overfitting.\n\n\n\n\n\n\nTotal error also depends on irreducible error (\nnoise\n that can not be reduced by algorithm)\n\n\n\n\n\n\nUltmiate goal is to minimize the total error\n\n\n\n\n\n\n# fake bias, variance and noise\n\n\ncomplexity\n \n=\n \nnp\n.\narange\n(\n1\n,\n \n2\n,\n \n0.1\n)\n\n\nvariance\n \n=\n \nnp\n.\npower\n(\ncomplexity\n,\n \n5\n)\n\n\nbias2\n \n=\n \nvariance\n[::\n-\n1\n]\n\n\nirreducible\n \n=\n \n[\n10\n*\nnp\n.\nrandom\n.\nnormal\n(\nabs\n(\nx\n \n-\n \n1.5\n),\n \n0.01\n)\n \nfor\n \nx\n \nin\n \ncomplexity\n]\n\n\n\n# total error = variance + bias^2 + irreducible\n\n\ntotal\n \n=\n \nvariance\n \n+\n \nbias2\n \n+\n \nirreducible\n\n\n\nplt\n.\nxticks\n([])\n\n\nplt\n.\nyticks\n([])\n\n\n\nplt\n.\nxlabel\n(\n\"Algorithm complexity\"\n)\n\n\nplt\n.\nylabel\n(\n\"Error\"\n)\n\n\n\nplt\n.\nplot\n(\ncomplexity\n,\n \nvariance\n,\n \n'C0o-'\n,\n \nlabel\n=\n'Variance'\n)\n\n\nplt\n.\nplot\n(\ncomplexity\n,\n \nbias2\n,\n \n'C1o-'\n,\n \nlabel\n=\n\"Bias^2\"\n)\n\n\nplt\n.\nplot\n(\ncomplexity\n,\n \ntotal\n,\n \n'C2o-'\n,\n \nlabel\n=\n\"Total = Bias^2 + Variance + Irreducible error\"\n)\n\n\n\nplt\n.\nplot\n([\n1.5\n,\n \n1.5\n],\n \n[\n0\n,\n \n25\n],\n \n'C3--'\n)\n\n\n\nplt\n.\ntext\n(\n1.0\n,\n \n7\n,\n \n\"$\\longleftarrow$ better chance of generalizing\"\n,\n \ncolor\n=\n'C0'\n)\n\n\nplt\n.\ntext\n(\n1.6\n,\n \n7\n,\n \n\"better chance of approximating $\\longrightarrow$\"\n,\n \ncolor\n=\n'C1'\n)\n\n\n\nplt\n.\nlegend\n();\n\n\n\n\n\n\n\n\n\n\n\n\nDecision trees are sensitive to splits - small changes in training data may change a tree structure\n\n\n\n\n\n\ndeep trees tend to have high variance and low bias\n\n\n\n\n\n\nshallow trees tend to have low variance and high bias\n\n\n\n\n\n\n\n\n\n\nQuick math\n\u00b6\n\n\nBasic\n\u00b6\n\n\n\n\n\n\nThe general goal of regression is to find how some dependent variable (target, \ny\ny\n) is changing when independent variable (feature, \nx\nx\n) varies\n\n\n\n\n\n\nLets assume there is some \ntrue\n relationship describing this dependence \ny = f(x)\ny = f(x)\n\n\n\n\n\n\nWe want to find \nf(x)\nf(x)\n from observations of \n(x, y)\n(x, y)\n pairs\n\n\n\n\n\n\nAlthough, in real life we get some noisy observation, so \ny = f(x) + \\epsilon\ny = f(x) + \\epsilon\n\n\n\n\n\n\nAs we do not know function \nf(x)\nf(x)\n we want to approximate it with some other function \ng(x)\ng(x)\n (estimator)\n\n\n\n\n\n\nIn general, \ng(x)\ng(x)\n is a parametrized model which can take many possible functional form\n\n\n\n\ne.g. \ng(x) = a\\cdot x^2 + b\\cdot x + c\ng(x) = a\\cdot x^2 + b\\cdot x + c\n can take different coefficients (based on a training dataset)\n\n\n\n\n\n\n\n\nBias and variance\n\u00b6\n\n\n\n\n\n\nLets imagine there are \nN\nN\n possible training datasets \n\\{D_1, D_2, ..., D_N\\}\n\\{D_1, D_2, ..., D_N\\}\n\n\n\n\n\n\nFor a given dataset one gets an estimators \ng^{(D)}(x)\ng^{(D)}(x)\n\n\n\n\n\n\nLets denote the expected estimator by \n\\mathbf{E}_{D}\\left[g^{(D)}(x)\\right] \\equiv \\bar g(x)\n\\mathbf{E}_{D}\\left[g^{(D)}(x)\\right] \\equiv \\bar g(x)\n\n\n\n\n\n\nIf \nN\nN\n is large we can approximate it by an average over all datasets (law of large numbers) \n\\bar g(x) \\approx \\frac{1}{N}\\sum\\limits_{i=1}^N g^{(D_i)}(x)\n\\bar g(x) \\approx \\frac{1}{N}\\sum\\limits_{i=1}^N g^{(D_i)}(x)\n\n\n\n\n\n\nThe \nvariance\n of an estimator tells us how far particular predictions are from the mean value \nvar = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\bar g(x)\\right)^2\\right]\nvar = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\bar g(x)\\right)^2\\right]\n\n\n\n\n\n\nThus, if the training does not depend on the choice of a dataset the variance is low\n\n\n\n\n\n\nThe \nbias\n of an estimator tells us how far the mean value is from the true value \nbias = \\bar g(x) - f(x)\nbias = \\bar g(x) - f(x)\n\n\n\n\n\n\nThus, if the model decribes data accurately the bias is low\n\n\n\n\n\n\nPlease note the hidden assumption that all possible values of \nx\nx\n are equally likely\n\n\n\n\n\n\nGoodness of a model\n\u00b6\n\n\n\n\n\n\nThe common practice to determine the goodness of a model fit is to calculate mean squared error\n\n\n\n\n\n\nThe mean squared error is, well, the mean value of error squared: \nmse = \\mathbf{E}_{x}\\left[\\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - y\\right)^2\\right]\\right]\nmse = \\mathbf{E}_{x}\\left[\\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - y\\right)^2\\right]\\right]\n\n\n\n\n\n\nLets consider MSE for a particlar point \nx\nx\n, \ny = f(x) + \\epsilon\ny = f(x) + \\epsilon\n, so \nmse = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - y\\right)^2\\right] = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x)\\right)^2\\right]- 2\\cdot\\mathbf{E}_{D}\\left[g^{(D)}(x)\\cdot y\\right] + \\mathbf{E}_{D}\\left[y^2\\right]\nmse = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - y\\right)^2\\right] = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x)\\right)^2\\right]- 2\\cdot\\mathbf{E}_{D}\\left[g^{(D)}(x)\\cdot y\\right] + \\mathbf{E}_{D}\\left[y^2\\right]\n\n\n\n\n\n\nHere, we used the linearity of the expected value operator. Lets use another common property: \n\\mathbf{E}\\left[X^2\\right] = \\mathbf{E}\\left[\\left(X - \\mathbf{E}\\left[X\\right]\\right)^2\\right] + \\mathbf{E}\\left[X\\right]^2\n\\mathbf{E}\\left[X^2\\right] = \\mathbf{E}\\left[\\left(X - \\mathbf{E}\\left[X\\right]\\right)^2\\right] + \\mathbf{E}\\left[X\\right]^2\n \n\n\n\n\n\n\nSo the first term can be rewritten in the form \n\\mathbf{E}_{D}\\left[\\left(g^{(D)}(x)\\right)^2\\right] = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\mathbf{E}_{D}\\left[g^{(D)}(x)\\right]\\right)^2\\right] + \\mathbf{E}_{D}\\left[g^{(D)}(x)\\right]^2 = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\bar g(x)\\right)^2\\right] + \\left(\\bar g(x)\\right)^2\n\\mathbf{E}_{D}\\left[\\left(g^{(D)}(x)\\right)^2\\right] = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\mathbf{E}_{D}\\left[g^{(D)}(x)\\right]\\right)^2\\right] + \\mathbf{E}_{D}\\left[g^{(D)}(x)\\right]^2 = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\bar g(x)\\right)^2\\right] + \\left(\\bar g(x)\\right)^2\n\n\n\n\n\n\nAnd the last term \n\\mathbf{E}_{D}\\left[y^2\\right] = \\mathbf{E}_{D}\\left[\\left(y - \\mathbf{E}_{D}\\left[y\\right]\\right)^2\\right] + \\mathbf{E}_{D}\\left[y\\right]^2 = \\mathbf{E}_{D}\\left[\\left(y - f(x)\\right)^2\\right] + \\left(f(x)\\right)^2\n\\mathbf{E}_{D}\\left[y^2\\right] = \\mathbf{E}_{D}\\left[\\left(y - \\mathbf{E}_{D}\\left[y\\right]\\right)^2\\right] + \\mathbf{E}_{D}\\left[y\\right]^2 = \\mathbf{E}_{D}\\left[\\left(y - f(x)\\right)^2\\right] + \\left(f(x)\\right)^2\n\n\n\n\n\n\nHere, we used the fact that \n\\mathbf{E}_{D}\\left[y\\right] = f(x)\n\\mathbf{E}_{D}\\left[y\\right] = f(x)\n (noise would average out when averaging over \ninfinite\n number of datasets)\n\n\n\n\n\n\nFor the middle term we use the fact that for independent \nX\nX\n and \nY\nY\n: \n\\mathbf{E}\\left[XY\\right] = \\mathbf{E}\\left[X\\right]\\cdot\\mathbf{E}\\left[Y\\right]\n\\mathbf{E}\\left[XY\\right] = \\mathbf{E}\\left[X\\right]\\cdot\\mathbf{E}\\left[Y\\right]\n, so \n\\mathbf{E}_{D}\\left[g^{(D)}(x)\\cdot y\\right] = \\bar g(x)\\cdot f(x)\n\\mathbf{E}_{D}\\left[g^{(D)}(x)\\cdot y\\right] = \\bar g(x)\\cdot f(x)\n\n\n\n\n\n\nTaking all together we get \nmse = \\underbrace{\\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\bar g(x)\\right)^2\\right]}_{variance} + \\underbrace{\\left(\\bar g(x) - f(x)\\right)^2}_{bias^2} + \\underbrace{\\mathbf{E}_{D}\\left[\\left(y - f(x)\\right)^2\\right]}_{noise}\nmse = \\underbrace{\\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\bar g(x)\\right)^2\\right]}_{variance} + \\underbrace{\\left(\\bar g(x) - f(x)\\right)^2}_{bias^2} + \\underbrace{\\mathbf{E}_{D}\\left[\\left(y - f(x)\\right)^2\\right]}_{noise}\n\n\n\n\n\n\nExample\n\u00b6\n\n\n\n\n\n\nLets consider \nf(x) = \\sin(\\pi x)\nf(x) = \\sin(\\pi x)\n\n\n\n\n\n\nWith a noise given by a zero-mean Gaussian with a variance \n\\sigma^2\n\\sigma^2\n\n\n\n\n\n\nSo the observation \ny = f(x) + \\mathcal{N}(0, \\sigma^2)\ny = f(x) + \\mathcal{N}(0, \\sigma^2)\n\n\n\n\n\n\nfrom\n \nmath\n \nimport\n \nsin\n,\n \ncos\n,\n \npi\n,\n \nexp\n\n\n\ndef\n \nget_dataset\n(\nN\n=\n20\n,\n \nsigma\n=\n0.1\n):\n\n  \n\"\"\"Generate N training samples\"\"\"\n\n  \n# X is a set of random points from [-1, 1]\n\n  \nX\n \n=\n \n2\n \n*\n \nnp\n.\nrandom\n.\nsample\n(\nN\n)\n \n-\n \n1\n\n  \n# Y are corresponding target values (with noise included)\n\n  \nY\n \n=\n \nnp\n.\narray\n([\nsin\n(\npi\n*\nx\n)\n \n+\n \nnp\n.\nrandom\n.\nnormal\n(\n0\n,\n \nsigma\n)\n \nfor\n \nx\n \nin\n \nX\n])\n\n\n  \nreturn\n \nX\n,\n \nY\n\n\n\n# plot a sample\n\n\nX\n,\n \nY\n \n=\n \nget_dataset\n()\n\n\n\nx_\n \n=\n \nnp\n.\narange\n(\n-\n1\n,\n \n1\n,\n \n0.01\n)\n\n\n\nplt\n.\nscatter\n(\nX\n,\n \nY\n,\n \ncolor\n=\n'C1'\n)\n\n\nplt\n.\nplot\n(\nx_\n,\n \nnp\n.\nsin\n(\nnp\n.\npi\n \n*\n \nx_\n),\n \n'C0--'\n);\n\n\n\n\n\n\n\n\n\n\n\n\nWe do not know \nf(x)\nf(x)\n\n\n\n\n\n\nWe assume it is a polynomial\n\n\n\n\n\n\nLets consider polynomials of orders: \n1 - 9\n1 - 9\n\n\n\n\n\n\ng_1(x) = a_1\\cdot x + a_0\ng_1(x) = a_1\\cdot x + a_0\n\n\n\n\n\n\ng_2(x) = a_2\\cdot x^2 + \\cdots + a_0\ng_2(x) = a_2\\cdot x^2 + \\cdots + a_0\n\n\n\n\n\n\ng_3(x) = a_3\\cdot x^3 + \\cdots + a_0\ng_3(x) = a_3\\cdot x^3 + \\cdots + a_0\n\n\n\n\n\n\n...\n\n\n\n\n\n\n\n\n\n\nLets assume we have 100 independent dataset\n\n\n\n\n\n\nEach one has 20 points \n(x, y = \\sin(\\pi x) + \\mathcal{N}(0, \\sigma^2))\n(x, y = \\sin(\\pi x) + \\mathcal{N}(0, \\sigma^2))\n\n\n\n\n\n\n# generate 100 datasets with default settings\n\n\ndatasets\n \n=\n \n[\nget_dataset\n()\n \nfor\n \ni\n \nin\n \nrange\n(\n100\n)]\n\n\n\n# and plot them all together with true signal\n\n\nfor\n \ni\n \nin\n \nrange\n(\n100\n):\n\n  \nplt\n.\nscatter\n(\ndatasets\n[\ni\n][\n0\n],\n \ndatasets\n[\ni\n][\n1\n],\n \nmarker\n=\n'.'\n)\n\n\n\nplt\n.\nplot\n(\nx_\n,\n \nnp\n.\nsin\n(\nnp\n.\npi\n \n*\n \nx_\n),\n \n'C0--'\n);\n\n\n\n\n\n\n\n\n\n\nNow we need to fit each polynomial to each dataset separately\n\n\n\n\ndef\n \nget_fit\n(\nN\n,\n \ndata\n):\n\n  \n\"\"\"Find a fit of polynomial of order N to data = (X, Y)\"\"\"\n\n  \nreturn\n \nnp\n.\npoly1d\n(\nnp\n.\npolyfit\n(\ndata\n[\n0\n],\n \ndata\n[\n1\n],\n \nN\n))\n\n\n\n# for the whole range of possible polynomials orders\n\n\n# create a list of fits to different datasets\n\n\nfits\n \n=\n \n[[\nget_fit\n(\norder\n,\n \ndata\n)\n \nfor\n \ndata\n \nin\n \ndatasets\n]\n \nfor\n \norder\n \nin\n \nrange\n(\n1\n,\n \n10\n)]\n\n\n\n\n\n\nplt\n.\nfigure\n(\nfigsize\n=\n(\n13\n,\n \n10\n))\n\n\n\nfor\n \norder\n \nin\n \nrange\n(\n1\n,\n \n10\n):\n\n  \nplt\n.\nsubplot\n(\n3\n,\n \n3\n,\n \norder\n)\n\n  \nplt\n.\nylim\n([\n-\n1.5\n,\n1.5\n])\n\n\n  \nfor\n \ng\n \nin\n \nfits\n[\norder\n \n-\n \n1\n]:\n\n    \nplt\n.\nplot\n(\nx_\n,\n \ng\n(\nx_\n),\n \n'C1-'\n,\n \nlinewidth\n=\n0.1\n)\n\n\n  \nplt\n.\nplot\n(\nx_\n,\n \nnp\n.\nsin\n(\nnp\n.\npi\n \n*\n \nx_\n),\n \n'C0--'\n)\n\n\n  \nplt\n.\ntitle\n(\n\"Polynomial of order {}\"\n.\nformat\n(\norder\n));\n\n\n\nplt\n.\ntight_layout\n();\n\n\n\n\n\n\n\n\nTraining and test errors\n\u00b6\n\n\n\n\n\n\nIn real life is impossible (unless one creates data by hand) to calculate true variance and bias\n\n\n\n\n\n\nOne would need all possible datasets \nD\nD\n and all possible input values \nx\nx\n\n\n\n\n\n\nThus, usually one looks at training and test errors\n\n\n\n\n\n\nTraining error is measured on the data used to make a fit\n\n\n\n\n\n\nTest/validation error is measured on unseen data\n\n\n\n\n\n\n\n\n\n\n# fake error\n\n\ncomplexity\n \n=\n \nnp\n.\narange\n(\n0.1\n,\n \n2\n,\n \n0.1\n)\n\n\ntrain_error\n \n=\n \n-\nnp\n.\nlog\n(\ncomplexity\n)\n\n\ntest_error\n \n=\n \n-\nnp\n.\nlog\n(\ncomplexity\n)\n \n+\n \nnp\n.\npower\n(\ncomplexity\n,\n \n1\n)\n\n\n\nplt\n.\nxticks\n([])\n\n\nplt\n.\nyticks\n([])\n\n\n\nplt\n.\nxlabel\n(\n\"Algorithm complexity\"\n)\n\n\nplt\n.\nylabel\n(\n\"Error\"\n)\n\n\n\nplt\n.\nplot\n(\ncomplexity\n,\n \ntrain_error\n,\n \n'C0o-'\n,\n \nlabel\n=\n'Training error'\n)\n\n\nplt\n.\nplot\n(\ncomplexity\n,\n \ntest_error\n,\n \n'C1o-'\n,\n \nlabel\n=\n\"Test error\"\n)\n\n\n\nplt\n.\ntext\n(\n0.1\n,\n \n0.25\n,\n \n\"$\\longleftarrow$ high bias\"\n,\n \ncolor\n=\n'C0'\n)\n\n\nplt\n.\ntext\n(\n1.5\n,\n \n0.25\n,\n \n\"high variance $\\longrightarrow$\"\n,\n \ncolor\n=\n'C1'\n)\n\n\n\nplt\n.\nlegend\n();\n\n\n\n\n\n\n\n\n\n\n\n\nHigh training error indicates high bias (which means underfitting)\n\n\n\n\n\n\nTraining error must decrease with model complexity\n\n\n\n\n\n\nIf the training error is high:\n\n\n\n\n\n\nUse more complex model (or new model architecture)\n\n\n\n\n\n\nUse more features - maybe there is just not enough information to make a good prediction\n\n\n\n\n\n\nTrain longer (if the algorithm is an iterative optimization problem)\n\n\n\n\n\n\nDecrease regularization (next lecture)\n\n\n\n\n\n\n\n\n\n\nTest error deacreses with model complexity up to a point when algorithm is to sensitive to features seen in training data\n\n\n\n\n\n\nIf test error starts to increase it indicates high variance (which means overfitting)\n\n\n\n\n\n\nIf test error is high:\n\n\n\n\n\n\nUse more data - easy to say hard to do...\n\n\n\n\n\n\nUse less features\n\n\n\n\n\n\nIncrease regularization \n\n\n\n\n\n\nUse different model\n\n\n\n\n\n\n\n\n\n\nEnsemble learning\n\u00b6\n\n\n\n\n\n\nLets first define a \nweak learner\n as a classifier which is just slighlty better than random guessing\n\n\n\n\n\n\nThe idea behind ensemble learning is to create a \nstrong learner\n as a combination of many \nweak learners\n\n\n\n\n\n\nWe will discuss two popular ensemble methods:\n\n\n\n\n\n\nBagging (\nb\nootstrap \nagg\nregat\ning\n), e.g. random forest\n\n\n\n\n\n\nBoosting, e.g. boosted decision tress\n\n\n\n\n\n\n\n\n\n\nRandom forest\n\u00b6\n\n\n\n\n\n\nOnce we know a way to produce a tree we can create a forest\n\n\n\n\n\n\nAnd each tree contributes to a final prediction\n\n\n\n\n\n\nIt is a \nrandom\n forest, because each tree is randomly \nincomplete\n - trained only on a random subsets of samples and features (\nfeatures bagging\n)\n\n\n\n\n\n\nThe final prediction of a random forest is an avearge predictions (for regression) or a majority vote (classification)\n\n\n\n\n\n\nIntuitive / naive example\n\u00b6\n\n\n\n\n\n\nImagine you want to go to a cinema and need to choose a movie to watch\n\n\n\n\n\n\nYou can ask a friend about the recommendation\n\n\n\n\n\n\nShe/he would ask you about movies you watched in the past\n\n\n\n\n\n\nand (based on your answers) create a set of rules (a decision tree)\n\n\n\n\n\n\nto finally recommend you a movie (make a prediction)\n\n\n\n\n\n\n\n\n\n\nAlternatively, you can ask many friends for an advice\n\n\n\n\n\n\nEach friend would ask you random questions to give an answer\n\n\n\n\n\n\nAt the end, you choose a movie with most votes\n\n\n\n\n\n\n\n\n\n\nThe algorithm\n\u00b6\n\n\n\n\n\n\nLets imagine we have \nN\nN\n samples in our dataset (e.g. \nN\nN\n movies you watched)\n\n\n\n\n\n\nEach sample has \nM\nM\n features (e.g. do you like a movie? do you like the leading actor / actress or director?)\n\n\n\n\n\n\nTo create a tree take \nn\nn\n random samples from the dataset and \nat each node\n select \nm << M\nm << M\n features (\nm \\sim \\sqrt M\nm \\sim \\sqrt M\n) to find the best predictor\n\n\n\n\n\n\nRepeat the procedure for next trees until you reach desired size of a forest\n\n\n\n\n\n\n                    +------------+\n                    |            |\n                    |   Dataset  |\n         +----------+            +----------+\n         |          | N features |          |\n         |          |            |          |\n         v          +------------+          v                    Hyperparameters:\n\n+-----------------+                 +-----------------+\n|                 |                 |                 |\n| Random subset 1 |      . . .      | Random subset T |\n|                 |                 |                 |              - the number of trees\n|   N features    |                 |   N features    |              - the size of subsets\n|                 |                 |                 |\n+--------+--------+                 +--------+--------+\n         |                                   |\n         v                                   v\n\n+-----------------+                 +-----------------+\n|                 |                 |                 |\n|     Tree 1      |      . . .      |     Tree T      |\n|                 |                 |                 |\n+--------+--------+                 +-----------------+\n         |\n         |          +-------------+          +--------+\n         |          |             |          |        |\n         +--------> | M1 features +--------> | Node 1 |              - the number of random features\n         |          |             |          |        |              - the size of a single tree\n         |          +-------------+          +--------+\n         |\n         |          +-------------+          +--------+\n         |          |             |          |        |\n         +--------> | M2 features +--------> | Node 2 |\n         |          |             |          |        |\n         |          +-------------+          +--------+\n\n                           .\n                           .\n                           .\n\n\n\n\n\nBoosted trees\n\u00b6\n\n\n\n\n\n\nThe idea is similar to bagging\n\n\n\n\n\n\nThe key differences are:\n\n\n\n\n\n\nData is reweighted every time a \nweak learner\n is added (so future learners focus on misclassified samples)\n\n\n\n\n\n\nThe final prediction is weighted average (better classifiers have higher weights)\n\n\n\n\n\n\n\n\n\n\n                            Bagging (parallel)\n\n       +--------------------+                +----------------+\n       |                    |                |                |\n+----> |      Dataset       +--------------> | Weak learner 1 |\n|      |                    |                |                |\n|      +--------------------+                +----------------+\n|\n|\n|      +--------------------+                +----------------+\n|      |                    |                |                |\n+----> |      Dataset       +------------->  | Weak learner 2 |\n|      |                    |                |                |\n|      +--------------------+       .        +----------------+\n|                                   .\n|                                   .\n|      +--------------------+                +----------------+\n|      |                    |                |                |\n+----> |      Dataset       +------------->  | Weak learner N |\n       |                    |                |                |\n       +--------------------+                +----------------+\n\n\n\n\n\n\\hspace{140pt}\\text{output} = \\frac{1}{N}\\sum \\text{output}_i\n\\hspace{140pt}\\text{output} = \\frac{1}{N}\\sum \\text{output}_i\n \n\n\n\n\n\n                          Boosting (sequential)\n\n      +--------------------+                +----------------+\n      |                    |                |                |\n      |      Dataset       +--------------> | Weak learner 1 |\n      |                    |                |                |\n      +--------------------+                +--------+-------+\n                                                     |\n                +------------------------------------+\n                |\n                v\n\n      +--------------------+                +----------------+\n      |                    |                |                |\n      | Reweighted dataset +------------->  | Weak learner 2 |\n      |                    |                |                |\n      +--------------------+                +--------+-------+\n                                                     |\n                +------------     . . .      --------+\n                |\n                v\n\n      +--------------------+                +----------------+\n      |                    |                |                |\n      | Reweighted dataset +------------->  | Weak learner N |\n      |                    |                |                |\n      +--------------------+                +----------------+\n\n\n\n\n\n\\hspace{130pt}\\text{output} = \\sum w_i\\cdot \\text{output}_i\n\\hspace{130pt}\\text{output} = \\sum w_i\\cdot \\text{output}_i\n \n\n\nAdaBoost\n\u00b6\n\n\n\n\n\n\nAdaBoost is on of the most famous algorithms in machine learning\n\n\n\n\n\n\nY. Freund and R. Schapire got a G\u00f6del Prize for this\n\n\n\n\n\n\nLets consider \nN\nN\n labled training examples \n(x_1, y_1), \\cdots, (x_N, y_N)\n(x_1, y_1), \\cdots, (x_N, y_N)\n, where \nx_i \\in X\nx_i \\in X\n and \ny_i = \\left\\{-1, 1\\right\\}\ny_i = \\left\\{-1, 1\\right\\}\n\n\n\n\n\n\nThe initial distribution is initizlized with \nD_1(i) = \\frac{1}{N}\nD_1(i) = \\frac{1}{N}\n, where \ni = 1, \\cdots, N\ni = 1, \\cdots, N\n (so every sample is equaly likely)\n\n\n\n\n\n\nFor \nt = 1, \\cdots, T\nt = 1, \\cdots, T\n:\n\n\n\n\n\n\ntrain a weak learner using \nD_t\nD_t\n, \nh_t: X \\rightarrow \\left\\{-1, 1\\right\\}\nh_t: X \\rightarrow \\left\\{-1, 1\\right\\}\n\n\n\n\n\n\nchoose the one which minimizes the weighted error \n\\epsilon_t = \\sum\\limits_{i=1}^{N}D_t(i)\\delta\\left(h_t(x_i)\\neq y_i\\right)\n\\epsilon_t = \\sum\\limits_{i=1}^{N}D_t(i)\\delta\\left(h_t(x_i)\\neq y_i\\right)\n\n\n\n\n\n\ncalculate \n\\alpha_t\n\\alpha_t\n \n\\alpha_t = \\frac{1}{2}\\ln\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)\n\\alpha_t = \\frac{1}{2}\\ln\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)\n\n\n\n\n\n\nFor \ni = 1, \\cdots, N\ni = 1, \\cdots, N\n update weights according to \nD_{t+1} = \\frac{D_t(i)}{Z_t}\\begin{cases}e^{-\\alpha_t} & h_t(x_i) = y_i \\\\ e^{\\alpha_t} & h_t(x_i) \\neq y_i \\end{cases} = \\frac{D_t(i)}{Z_t}e^{-\\alpha_ty_ih_t(x_i)}\nD_{t+1} = \\frac{D_t(i)}{Z_t}\\begin{cases}e^{-\\alpha_t} & h_t(x_i) = y_i \\\\ e^{\\alpha_t} & h_t(x_i) \\neq y_i \\end{cases} = \\frac{D_t(i)}{Z_t}e^{-\\alpha_ty_ih_t(x_i)}\n\n\n\n\n\n\nZ_t\nZ_t\n is a normilization factor so \nD_{t+1}\nD_{t+1}\n is a distribution \nZ_t = \\sum\\limits_{i=1}^ND_t(i)e^{-\\alpha_ty_ih_t(x_i)}\nZ_t = \\sum\\limits_{i=1}^ND_t(i)e^{-\\alpha_ty_ih_t(x_i)}\n\n\n\n\n\n\n\n\n\n\nThe final hyptohesis \nH\nH\n is computes the sign of a weighted combination of weak hypotheses \nH(x) = \\text{sign}\\left(\\sum\\limits_{t=1}^T\\alpha_th_t(x)\\right)\nH(x) = \\text{sign}\\left(\\sum\\limits_{t=1}^T\\alpha_th_t(x)\\right)\n \n\n\n\n\n\n\nOut-of-bag error\n\u00b6\n\n\n\n\n\n\nOut-of-bag (OOB) error may be used for machine learning models using bootstrap aggregation (like random forest and boosted trees) instead of cross-validation\n\n\n\n\n\n\nBagging involves random sampling with replacement\n\n\n\n\n\n\nSome samples are not used in the training process (out-of-bag samples) and therefore can be used to calculate test error\n\n\n\n\n\n\nOOB error is the average error over all training samples calculated using predictions from weak classifiers which do not contain particular sample in their bootstrap samples\n\n\n\n\n\n\nExample\n\u00b6\n\n\n\n\n\n\nLets consider some fake data generated with \nmake_blobs\n from \nscikit-learn\n\n\n\n\n\n\nand then apply decision trees with different maximum depths\n\n\n\n\n\n\nand random forests with different maximum depths\n\n\n\n\n\n\nDataset\n\u00b6\n\n\n\n\n\n\nsklearn.datasets.make_blobs\n allows to generate random Gaussian blobs\n\n\n\n\n\n\nWe generate 8 blobs with fixed random generator (just to make sure we get the same set every time)\n\n\n\n\n\n\nfrom\n \nsklearn.datasets\n \nimport\n \nmake_blobs\n\n\n\n# generate 5 blobs with fixed random generator\n\n\nX\n,\n \nY\n \n=\n \nmake_blobs\n(\nn_samples\n=\n500\n,\n \ncenters\n=\n8\n,\n \nrandom_state\n=\n300\n)\n\n\n\nplt\n.\nscatter\n(\n*\nX\n.\nT\n,\n \nc\n=\nY\n,\n \nmarker\n=\n'.'\n,\n \ncmap\n=\n'Dark2'\n);\n\n\n\n\n\n\n\n\nTrain and visualize\n\u00b6\n\n\n\n\n\n\nTo make our life easier we create a function to\n\n\n\n\n\n\nplot training data on existing axes or new one (if not provided)\n\n\n\n\n\n\ntrain given classifier on given dataset\n\n\n\n\n\n\ncreate countours representing predictions of the classifier\n\n\n\n\n\n\n\n\n\n\ndef\n \ntrain_and_look\n(\nclassifier\n,\n \nX\n,\n \nY\n,\n \nax\n=\nNone\n,\n \ntitle\n=\n''\n,\n \ncmap\n=\n'Dark2'\n):\n\n  \n\"\"\"Train classifier on (X,Y). Plot data and prediction.\"\"\"\n\n  \n# create new axis if not provided\n\n  \nax\n \n=\n \nax\n \nor\n \nplt\n.\ngca\n();\n\n\n  \nax\n.\nset_title\n(\ntitle\n)\n\n\n  \n# plot training data\n\n  \nax\n.\nscatter\n(\n*\nX\n.\nT\n,\n \nc\n=\nY\n,\n \nmarker\n=\n'.'\n,\n \ncmap\n=\ncmap\n)\n\n\n  \n# train a cliassifier\n\n  \nclassifier\n.\nfit\n(\nX\n,\n \nY\n)\n\n\n  \n# create a grid of testing points\n\n  \nx_\n,\n \ny_\n \n=\n \nnp\n.\nmeshgrid\n(\nnp\n.\nlinspace\n(\n*\nax\n.\nget_xlim\n(),\n \nnum\n=\n200\n),\n\n                       \nnp\n.\nlinspace\n(\n*\nax\n.\nget_ylim\n(),\n \nnum\n=\n200\n))\n\n\n  \n# convert to an array of 2D points\n\n  \ntest_data\n \n=\n \nnp\n.\nvstack\n([\nx_\n.\nravel\n(),\n \ny_\n.\nravel\n()])\n.\nT\n\n\n  \n# make a prediction and reshape to grid structure \n\n  \nz_\n \n=\n \nclassifier\n.\npredict\n(\ntest_data\n)\n.\nreshape\n(\nx_\n.\nshape\n)\n\n\n  \n# arange z bins so class labels are in the middle\n\n  \nz_levels\n \n=\n \nnp\n.\narange\n(\nlen\n(\nnp\n.\nunique\n(\nY\n))\n \n+\n \n1\n)\n \n-\n \n0.5\n\n\n  \n# plot contours corresponding to classifier prediction\n\n  \nax\n.\ncontourf\n(\nx_\n,\n \ny_\n,\n \nz_\n,\n \nalpha\n=\n0.25\n,\n \ncmap\n=\ncmap\n,\n \nlevels\n=\nz_levels\n)\n\n\n\n\n\n\n\n\nLet check how it works on a decision tree classifier with default \nsklearn\n setting\n\n\n\n\nfrom\n \nsklearn.tree\n \nimport\n \nDecisionTreeClassifier\n \nas\n \nDT\n\n\n\ntrain_and_look\n(\nDT\n(),\n \nX\n,\n \nY\n)\n\n\n\n\n\n\n\n\nDecision tree\n\u00b6\n\n\n\n\nWe consider decision trees with fixed maximum depths from 1 to 9\n\n\n\n\n# create a figure with 9 axes 3x3\n\n\nfig\n,\n \nax\n \n=\n \nplt\n.\nsubplots\n(\n3\n,\n \n3\n,\n \nfigsize\n=\n(\n15\n,\n15\n))\n\n\n\n# train and look at decision trees with different max depth\n\n\nfor\n \nmax_depth\n \nin\n \nrange\n(\n0\n,\n \n9\n):\n\n  \ntrain_and_look\n(\nDT\n(\nmax_depth\n=\nmax_depth\n \n+\n \n1\n),\n \nX\n,\n \nY\n,\n\n                 \nax\n=\nax\n[\nmax_depth\n \n//\n \n3\n][\nmax_depth\n \n%\n \n3\n],\n\n                 \ntitle\n=\n\"Max depth = {}\"\n.\nformat\n(\nmax_depth\n \n+\n \n1\n))\n\n\n\n\n\n\n\n\n\n\nmax_depth\n <= 3 - undefitting\n\n\nmax_depth\n <= 6 - quite good\n\n\nmax_depth\n  > 6 - overfitting\n\n\n\n\nRandom forest\n\u00b6\n\n\n\n\nLets do the same with random forests (100 trees in each forest)\n\n\n\n\nfrom\n \nsklearn.ensemble\n \nimport\n \nRandomForestClassifier\n \nas\n \nRF\n\n\n\n# create a figure with 9 axes 3x3\n\n\nfig\n,\n \nax\n \n=\n \nplt\n.\nsubplots\n(\n3\n,\n \n3\n,\n \nfigsize\n=\n(\n15\n,\n15\n))\n\n\n\n# train and look at decision trees with different max depth\n\n\nfor\n \nmax_depth\n \nin\n \nrange\n(\n0\n,\n \n9\n):\n\n  \ntrain_and_look\n(\nRF\n(\nn_estimators\n=\n100\n,\n \nmax_depth\n=\nmax_depth\n \n+\n \n1\n),\n \nX\n,\n \nY\n,\n\n                 \nax\n=\nax\n[\nmax_depth\n \n//\n \n3\n][\nmax_depth\n \n%\n \n3\n],\n\n                 \ntitle\n=\n\"Max depth = {}\"\n.\nformat\n(\nmax_depth\n \n+\n \n1\n))\n\n\n\n\n\n\n\n\n\n\n\n\nThe combination of shallow trees (weak learners) does a good job\n\n\n\n\n\n\nOverfitting is somehow prevented \n\n\n\n\n\n\nSummary\n\u00b6\n\n\n\n\n\n\nThe most important lesson today: \nbias-variance trade-off\n\n\n\n\n\n\nFor the lecture easy examples are chosen so they can be visualize\n\n\n\n\n\n\nIn real life problems, it is hard / impossible to determine using \"bye eye\" method if the model is underfitted or overfitted\n\n\n\n\n\n\nNote, that actually you should never used this method even if you think \"your eye\" is right - you would be surprised how it is not\n\n\n\n\n\n\nOne needs a way to measure the goodnes of a model - usually mean squared error\n\n\n\n\n\n\nIn practice, most people use cross-calidation technique \n\n\n\n\n\n\n\n\n\n\nThe biggest advantages of decision trees algoritms is that they are east to interpret (it is easy to explain even to non-experts how they work, which is not the case with e.g. deep neural networks)\n\n\n\n\n\n\nUsually, decision trees are used as weak learners in ensemble learning\n\n\n\n\n\n\nThe most famous boosting algorithm is AdaBoost, because it is a good one and the first one. Although, there are many other boosting methods on market right now with XGBoost being one of the most popular one\n\n\n\n\n\n\nAs for today, deep learning has better publicity, but boosted trees are still one of the most common algorithms used in \nKaggle competitions\n\n\n\n\n\n\nBoosted trees are also popular among physicist and used by them as an alternative to neural networks for experimental elementary particle physics in event reconstruction procedures",
            "title": "Decision Trees"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#decision-trees",
            "text": "",
            "title": "Decision Trees"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#graphviz",
            "text": "",
            "title": "Graphviz"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#installing-graphviz",
            "text": "! apt   install   - y   graphviz  ! pip   install   graphviz",
            "title": "Installing graphviz"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#a-tree-example",
            "text": "from   graphviz   import   Digraph  styles   =   { \n     'top' :   { 'shape' :   'ellipse' ,   'style' :   'filled' ,   'color' :   'lightblue' }, \n     'no' :    { 'shape' :   'circle' ,   'style' :   'filled' ,   'color' :   'red' }, \n     'yes' :   { 'shape' :   'circle' ,   'style' :   'filled' ,   'color' :   'lightgreen' }, \n     'qst' :   { 'shape' :   'rect' }  }  example_tree   =   Digraph ()  example_tree . node ( 'top' ,   'Should I attend the ML lecture?' ,   styles [ 'top' ])  example_tree . node ( 'q1' ,   'Do I fulfill requirements?' ,   styles [ 'qst' ])  example_tree . node ( 'q2' ,   'Do I like CS?' ,   styles [ 'qst' ])  example_tree . node ( 'no1' ,   'No ' ,   styles [ 'no' ])  example_tree . node ( 'q3' ,   'Is the lecture early in the morning?' ,   styles [ 'qst' ])  example_tree . node ( 'no2' ,   'No ' ,   styles [ 'no' ])  example_tree . node ( 'no3' ,   'No ' ,   styles [ 'no' ])  example_tree . node ( 'yes' ,   'Yes' ,   styles [ 'yes' ])  example_tree . edge ( 'top' ,   'q1' )  example_tree . edge ( 'q1' ,   'q2' ,   'Yes' )  example_tree . edge ( 'q1' ,   'no1' ,   'No' )  example_tree . edge ( 'q2' ,   'q3' ,   'Yes' )  example_tree . edge ( 'q2' ,   'no2' ,   'No' )  example_tree . edge ( 'q3' ,   'no3' ,   'Yes' )  example_tree . edge ( 'q3' ,   'yes' ,   'No' )   example_tree",
            "title": "A tree example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#introduction",
            "text": "",
            "title": "Introduction"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#decision-trees_1",
            "text": "Supervised learning algorithm - training dataset with known labels    Eager learning - final model does not need training data to make prediction (all parameters are evaluated during learning step)    It can do both classification and regression    A decision tree is built from:   decision nodes  - correspond to features (attributes)  leaf nodes  - correspond to class labels     The  root  of a tree is (should be) the best predictor (feature)",
            "title": "Decision trees"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#example",
            "text": "import   matplotlib.pyplot   as   plt  import   numpy   as   np  # just to overwrite default colab style  plt . style . use ( 'default' )  plt . style . use ( 'seaborn-talk' )   # first define some points representing two classes  grid   =   np . mgrid [ 0 : 10 : 2 ,   0 : 10 : 2 ]  set01   =   np . vstack ([ grid [ 0 ] . ravel (),   grid [ 1 ] . ravel ()]) . T  set01   =   np . delete ( set01 ,   [ 17 ,   18 ,   19 ,   22 ,   24 ],   axis = 0 )  grid   =   np . mgrid [ 6 : 16 : 2 ,   0 : 10 : 2 ]  set02   =   np . vstack ([ grid [ 0 ] . ravel (),   grid [ 1 ] . ravel ()]) . T  set02   =   np . delete ( set02 ,   [ 0 ,   1 ,   5 ,   6 ,   8 ],   axis = 0 )  plt . scatter ( * set01 . T )  plt . scatter ( * set02 . T )  plt . text ( 15 ,   4 ,   \"There are two attributes: x and y \\n\\n \" \n                 \"    * each decision node splits dataset based on one of the attributes \\n\\n \" \n                 \"    * each leaf node defines a class label\" );    plt . scatter ( * set01 . T )  plt . scatter ( * set02 . T )  plt . plot ([ 5 ,   5 ],   [ 0 ,   8 ],   'r' )  plt . plot ([ 0 ,   14 ],   [ 3 ,   3 ],   'g' )  plt . text ( 15 ,   3 ,   \"We start with [20, 20] (blue, orange) \\n\\n \" \n                 \"Red line splits dataset in [15, 0] (left) and [5, 20] (right) \\n\\n \" \n                 \"Green line split dataset in [10, 6] (bottom) and [10, 14] (top) \\n\\n \" \n                 \"Red line is a winner and should be the root of our tree\" );    tree   =   Digraph ()  tree . edge ( \"x > 5? \\n [20, 20]\" ,   \"blue \\n [15, 0]\" ,   \"No\" )  tree . edge ( \"x > 5? \\n [20, 20]\" ,   \"y > 3? \\n [5, 20]\" ,   \"Yes\" )  tree . edge ( \"y > 3? \\n [5, 20]\" ,   \"x > 9? \\n [4, 6]\" ,   \"No\" )  tree . edge ( \"y > 3? \\n [5, 20]\" ,   \"almost orange \\n [1, 14]\" ,   \"Yes\" )  tree . edge ( \"x > 9? \\n [4, 6]\" ,   \"blue \\n [4, 0]\" ,   \"No\" )  tree . edge ( \"x > 9? \\n [4, 6]\" ,   \"orange \\n [0, 6]\" ,   \"Yes\" )  tree . edge ( \"almost orange \\n [1, 14]\" ,   \"Should we continue? \\n Or would it be overfitting?\" )  tree      It is important to start with good predictor    Our choice of the root classifies 37.5% of points  in the first step    Note, that we could also start with  x > 9?    However, if we started with  y > 3  we would never classify a point in the first step - does it mean that it is worse choice?      tree   =   Digraph ()  tree . edge ( \"y > 3? \\n [20, 20]\" ,   \"x > 9? \\n [10, 6]\" ,   \"No\" )  tree . edge ( \"y > 3? \\n [20, 20]\" ,   \"x > 5? \\n [10, 14]\" ,   \"Yes\" )  tree . edge ( \"x > 9? \\n [10, 6]\" ,   \"blue \\n [10, 0]\" ,   \"No\" )  tree . edge ( \"x > 9? \\n [10, 6]\" ,   \"orange \\n [0, 6]\" ,   \"Yes\" )  tree . edge ( \"x > 5? \\n [10, 14]\" ,   \"blue \\n [9, 0]\" ,   \"No\" )  tree . edge ( \"x > 5? \\n [10, 14]\" ,   \"almost orange \\n [1, 14]\" ,   \"Yes\" )  tree      In this case we never have to make more than 2 checks    There are two open questions to answer:    How to automate the procees of chosing nodes?    How deep should we go?",
            "title": "Example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#id3-and-c45-algorithms",
            "text": "We start with algorithms based on information theory    ID3 (Iterative Dichotomiser 3)    C4.5 - extension of ID3 (why C4.5? C stands for programming language and 4.5 for version?)    C5.0/See5 - improved C4.5 (commercial; single-threaded Linux version is available under GPL though)      The idea is to find nodes which maximize information gain",
            "title": "ID3 and C4.5 algorithms"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#information-gain",
            "text": "",
            "title": "Information gain"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#self-information",
            "text": "Let  X = (x_1, x_2, ..., x_n) X = (x_1, x_2, ..., x_n)  be our  information source  (feature), e.g. weather condition:  x_1 x_1  = sunny,  x_2 x_2  = overcast,  x_3 x_3  = rainy    And let  P = (p_1, p_2, ..., p_n) P = (p_1, p_2, ..., p_n)  be corresponding probrability distribution (or more precisely - probability mass function)    We want some measure of information  I I  provided by an event. It should satisfy the following properties:    I I  depends only on the probability of  x_i x_i , thus  I \\equiv I(p_i) I \\equiv I(p_i)    I I  is continuous and deacreasing function of  p_i p_i    I I  is non-negative and  I(1) = 0 I(1) = 0    if  p_i = p_{i, 1} \\cdot p_{i, 2} p_i = p_{i, 1} \\cdot p_{i, 2}  (independent events) then  I(p_i) = I(p_{i, 1}) + I(p_{i, 2}) I(p_i) = I(p_{i, 1}) + I(p_{i, 2})      Logarithmic function satisfies all above condition, so we define self-information as:  I(p) = -\\log(p) I(p) = -\\log(p)    The most common log base is  2  and then information is in  shannons (Sh) , also known as  bits    In the case of  natural logarithm  the unit is  nat  (natural unit of information)    In the case of base  10  the unit is  hartley (Hart) , also known as  dit      x   =   np . arange ( 0.01 ,   1.01 ,   0.01 )  plt . xlabel ( \"p\" )  plt . ylabel ( \"I(p)\" )  plt . plot ( x ,   - np . log2 ( x ),   label = \"bit\" )  plt . plot ( x ,   - np . log ( x ),   label = \"nat\" )  plt . plot ( x ,   - np . log10 ( x ),   label = \"dit\" )  plt . legend ();      Lets X = (head, tail) with P = (0.5, 0.5)   We get 1 Sh of information     Lets X = (sunny, overcast, rainy) with P = (0.25, 0.75, 0.25)    If it is overcast, we get 0.415 Sh of information    Otherwise, we get 2 Sh of information      If an event is more likely we learn less",
            "title": "Self-information"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#information-entropy",
            "text": "Also called Shannon entropy (after the father of intromation theory)    Usually information entropy is denoted as  H H    H H  is defined as the weighted average of the self-information of all possible outcomes  H(X) = \\sum\\limits_{i=1}^N p_i \\cdot I(p_i) = -\\sum\\limits_{i=1}^N p_i\\cdot\\log(p_i) H(X) = \\sum\\limits_{i=1}^N p_i \\cdot I(p_i) = -\\sum\\limits_{i=1}^N p_i\\cdot\\log(p_i)    Lets consider two case scenario with  P = (p, 1 - p) P = (p, 1 - p) , so entropy is given by  H = -p \\log(p) - (1 - p) \\log(1 - p) H = -p \\log(p) - (1 - p) \\log(1 - p)    p   =   np . arange ( 0.01 ,   1.0 ,   0.01 )  plt . xlabel ( \"p\" )  plt . ylabel ( \"H\" )  plt . annotate ( 'we are surprised' ,   xy = ( 0.5 ,   1 ),   xytext = ( 0.5 ,   0.75 ), \n              arrowprops = dict ( facecolor = 'black' ,   shrink = 0.1 ))  plt . annotate ( 'we are not that surprised' ,   xy = ( 1 ,   0.1 ),   xytext = ( 0.5 ,   0.25 ), \n              arrowprops = dict ( facecolor = 'black' ,   shrink = 0.1 ))  plt . plot ( p ,   - p   *   np . log2 ( p )   -   ( 1   -   p )   *   np . log2 ( 1   -   p ));     Lets consider three case scenario with  P = (p, q, 1 - p - q) P = (p, q, 1 - p - q) , so entropy is given by  H = -p \\log(p) - q\\log(q) - (1 - p - q) \\log(1 - p - q) H = -p \\log(p) - q\\log(q) - (1 - p - q) \\log(1 - p - q)   from   mpl_toolkits   import   mplot3d  # grid of p, q probabilities  p ,   q   =   np . meshgrid ( np . arange ( 0.01 ,   1.0 ,   0.01 ),   np . arange ( 0.01 ,   1.0 ,   0.01 ))  # remove (set to 0) points which do not fulfill P <= 1  idx   =   p   +   q   >   1  p [ idx ]   =   0  q [ idx ]   =   0  # calculate entropy (disable warnings - we are aware of log(0))  np . warnings . filterwarnings ( 'ignore' )  h   =   - p   *   np . log2 ( p )   -   q   *   np . log2 ( q )   -   ( 1   -   p   -   q )   *   np . log2 ( 1   -   p   -   q )  # make a plot  plt . axes ( projection = '3d' ) . plot_surface ( p ,   q ,   h );",
            "title": "Information entropy"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#information-gain_1",
            "text": "Let  T T  be the set of training samples with  n n  possible outcomes, thus  T = \\{T_1, T_2, ..., T_n\\} T = \\{T_1, T_2, ..., T_n\\}    The entropy is given by  H(T) = -\\sum\\limits_{i=1}^N p_i\\cdot\\log(p_i) = -\\sum\\limits_{i=1}^N \\frac{|T_i|}{|T|}\\cdot\\log(\\frac{|T_i|}{|T|}) H(T) = -\\sum\\limits_{i=1}^N p_i\\cdot\\log(p_i) = -\\sum\\limits_{i=1}^N \\frac{|T_i|}{|T|}\\cdot\\log(\\frac{|T_i|}{|T|})    We can also calulate the entropy after  T T  was partitioned in  T_i T_i  with respect to some feature  X X   H(T, X) = \\sum\\limits_{i=1}^N p_i\\cdot H(T_i) H(T, X) = \\sum\\limits_{i=1}^N p_i\\cdot H(T_i)    And the information gain is defined as  G(X) = H(T) - H(T, X) G(X) = H(T) - H(T, X)",
            "title": "Information gain"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#example_1",
            "text": "Lets calculate some example step by step    Lets consider a fake dataset    two classes: C01, C02    three features: X1, X2, X3         X1  ||  A  |  A  |  A  |  B  |  B  |  C  |  C  |  C  |  C  |\n---------------------------------------------------------------\n   X2  ||  0  |  0  |  1  |  1  |  0  |  1  |  1  |  1  |  0  |\n---------------------------------------------------------------\n   X3  || RED | GRN | GRN | BLU | RED | GRN | BLU | RED | GRN |\n===============================================================\n Class || C01 | C01 | C02 | C02 | C02 | C02 | C01 | C01 | C02 |  from   math   import   log  def   entropy ( * probs ): \n   \"\"\"Calculate information entropy\"\"\" \n   try : \n     total   =   sum ( probs ) \n     return   sum ([ - p   /   total   *   log ( p   /   total ,   2 )   for   p   in   probs ]) \n   except : \n     return   0  print ( entropy ( 4 ,   5 ),   entropy ( 2 ,   1 ),   entropy ( 2 ,   2 ))   0.9910760598382222 0.9182958340544896 1.0    The  root  entropy   We have 9 samples: 4 belong to class C01 and 5 to C02  H(T) = -\\frac{4}{9}\\log(\\frac{4}{9}) - \\frac{5}{9}\\log(\\frac{5}{9}) = 0.99 H(T) = -\\frac{4}{9}\\log(\\frac{4}{9}) - \\frac{5}{9}\\log(\\frac{5}{9}) = 0.99     Now lets consider feature X1, which splits data into subsets  T_1 T_1 ,  T_2 T_2 , and  T_3 T_3  (with X1 value A, B, and C, respectively)    Within  T_1 T_1  there are 3 samples: 2 from C01 and 1 from C02  H(T_1) = -\\frac{2}{3}\\log(\\frac{2}{3}) - \\frac{1}{3}\\log(\\frac{1}{3}) = 0.92 H(T_1) = -\\frac{2}{3}\\log(\\frac{2}{3}) - \\frac{1}{3}\\log(\\frac{1}{3}) = 0.92    Within  T_2 T_2  there are 2 samples: 0 from C01 and 2 from C02  H(T_2) = -\\frac{2}{2}\\log(\\frac{2}{2}) - \\frac{0}{2}\\log(\\frac{0}{2}) = 0.00 H(T_2) = -\\frac{2}{2}\\log(\\frac{2}{2}) - \\frac{0}{2}\\log(\\frac{0}{2}) = 0.00    Within  T_3 T_3  there are 4 samples: 2 from C01 and 2 from C02  H(T_3) = -\\frac{2}{4}\\log(\\frac{2}{4}) - \\frac{2}{4}\\log(\\frac{2}{4}) = 1.00 H(T_3) = -\\frac{2}{4}\\log(\\frac{2}{4}) - \\frac{2}{4}\\log(\\frac{2}{4}) = 1.00    The resulting entropy is  H(T, X1) = \\frac{3}{9}\\cdot H(T_1) + \\frac{2}{9}\\cdot H(T_2) + \\frac{4}{9}\\cdot H(T_3) = 0.75 H(T, X1) = \\frac{3}{9}\\cdot H(T_1) + \\frac{2}{9}\\cdot H(T_2) + \\frac{4}{9}\\cdot H(T_3) = 0.75    Thus, infromation gain if the set is split according to X1  G(X1) = H(T) - H(T, X1) = 0.99 - 0.75 = 0.24 \\mbox{ Sh } G(X1) = H(T) - H(T, X1) = 0.99 - 0.75 = 0.24 \\mbox{ Sh }",
            "title": "Example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#id3-algorithm",
            "text": "For every attribute (feature) calculate the entropy    Split the training set using the one for which information gain is maximum    Continue recursively on subsets using remaining features",
            "title": "ID3 algorithm"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#play-golf-dataset",
            "text": "Popular dataset to explain decision trees    4 features:    outlook :  rainy, overcast, sunny    temperature :  cool, mild, hot    humidity :  normal, high    windy :  false, true      Possible outcomes (play golf?):    false    true      import   pandas   as   pd  # first row = headers  src   =   \"http://chem-eng.utoronto.ca/~datamining/dmc/datasets/weather_nominal.csv\"  golf_data   =   pd . read_csv ( src )   golf_data    \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       Outlook \n       Temperature \n       Humidity \n       Windy \n       Play golf \n     \n   \n   \n     \n       0 \n       Rainy \n       Hot \n       High \n       False \n       No \n     \n     \n       1 \n       Rainy \n       Hot \n       High \n       True \n       No \n     \n     \n       2 \n       Overcast \n       Hot \n       High \n       False \n       Yes \n     \n     \n       3 \n       Sunny \n       Mild \n       High \n       False \n       Yes \n     \n     \n       4 \n       Sunny \n       Cool \n       Normal \n       False \n       Yes \n     \n     \n       5 \n       Sunny \n       Cool \n       Normal \n       True \n       No \n     \n     \n       6 \n       Overcast \n       Cool \n       Normal \n       True \n       Yes \n     \n     \n       7 \n       Rainy \n       Mild \n       High \n       False \n       No \n     \n     \n       8 \n       Rainy \n       Cool \n       Normal \n       False \n       Yes \n     \n     \n       9 \n       Sunny \n       Mild \n       Normal \n       False \n       Yes \n     \n     \n       10 \n       Rainy \n       Mild \n       Normal \n       True \n       Yes \n     \n     \n       11 \n       Overcast \n       Mild \n       High \n       True \n       Yes \n     \n     \n       12 \n       Overcast \n       Hot \n       Normal \n       False \n       Yes \n     \n     \n       13 \n       Sunny \n       Mild \n       High \n       True \n       No",
            "title": "Play Golf dataset"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#play-golf-entropy",
            "text": "entropy ( 9 ,   5 )   0.9402859586706309  | Play golf |\n=============\n| yes | no  |  -> H(T) = 0.94\n-------------\n|  9  |  5  |",
            "title": "Play golf entropy"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#play-golf-vs-outlook",
            "text": "| Play golf |\n                   =============\n                   | yes | no  |\n        ------------------------\n        | sunny    |  3  |  2  |  5\noutlook | overcast |  4  |  0  |  4\n        | rainy    |  2  |  3  |  5\n        ------------------------\n                      9     5  entropy ( 3 ,   2 ),   0 ,   entropy ( 2 ,   3 )   (0.9709505944546686, 0, 0.9709505944546686)   \n\\begin{eqnarray}\n   H(\\mbox{sunny}) & = & 0.97 \\\\\n   H(\\mbox{rainy}) & = & 0.97 \\\\\nH(\\mbox{overcast}) & = & 0\n\\end{eqnarray}     \n\\begin{eqnarray}\nH(T, \\mbox{outlook}) & = & P(\\mbox{sunny})\\cdot H(\\mbox{sunny}) + P(\\mbox{overcast})\\cdot H(\\mbox{overcast}) + P(\\mbox{rainy})\\cdot H(\\mbox{rainy}) \\\\\n                     & = & \\frac{5}{14}\\cdot 0.97 + \\frac{4}{14} \\cdot 0 + \\frac{5}{14}\\cdot 0.97 = 0.69\n\\end{eqnarray}     \n\\begin{eqnarray}\nG(\\mbox{outlook}) & = & H(T) - H(T, \\mbox{outlook}) = 0.94 - 0.69 = 0.25\n\\end{eqnarray}",
            "title": "Play golf vs outlook"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#results-for-all-features",
            "text": "| Play golf |                         | Play golf |\n                    =============                         =============\n                    | yes | no  |                         | yes | no  |\n         ------------------------                 --------------------\n         | sunny    |  3  |  2  |                 | hot   |  2  |  2  |\n outlook | overcast |  4  |  0  |     temperature | mild  |  4  |  2  |\n         | rainy    |  2  |  3  |                 | cool  |  3  |  1  |\n         ------------------------                 --------------------\n            Info. gain = 0.25                       Info gain = 0.03\n\n\n                    | Play golf |                         | Play golf |\n                    =============                         =============\n                    | yes | no  |                         | yes | no  |\n         ------------------------                 --------------------\n         | high     |  3  |  4  |                 | false |  6  |  2  |\nhumidity | normal   |  6  |  1  |           windy | true  |  3  |  3  |\n         ------------------------                 --------------------\n            Info. gain = 0.15                       Info gain = 0.05",
            "title": "Results for all features"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#root-of-the-tree",
            "text": "Start building a tree with the feature with the largest information gain:  outlook    A branch with  entropy 0  is a leaf node:  overcast    Other branches must be spliited using other features     tree   =   Digraph ()  tree . edge ( \"outlook\" ,   \"sunny\" )  tree . edge ( \"outlook\" ,   \"overcast\" )  tree . edge ( \"outlook\" ,   \"rainy\" )  tree . edge ( \"overcast\" ,   \"yes\" )  tree",
            "title": "Root of the tree"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#next-branch",
            "text": "golf_data . loc [ golf_data [ 'Outlook' ]   ==   \"Sunny\" ]    \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       Outlook \n       Temperature \n       Humidity \n       Windy \n       Play golf \n     \n   \n   \n     \n       3 \n       Sunny \n       Mild \n       High \n       False \n       Yes \n     \n     \n       4 \n       Sunny \n       Cool \n       Normal \n       False \n       Yes \n     \n     \n       5 \n       Sunny \n       Cool \n       Normal \n       True \n       No \n     \n     \n       9 \n       Sunny \n       Mild \n       Normal \n       False \n       Yes \n     \n     \n       13 \n       Sunny \n       Mild \n       High \n       True \n       No \n     \n        In general, one should calculate information gain for each feature for this subset    In this case it is clear that we can take  windy      tree . edge ( \"sunny\" ,   \"windy\" )  tree . edge ( \"windy\" ,   \"false\" )  tree . edge ( \"windy\" ,   \"true\" )  tree . edge ( \"false\" ,   \"yes\" )  tree . edge ( \"true\" ,   \"no\" )  tree",
            "title": "Next branch"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#last-branch",
            "text": "golf_data . loc [ golf_data [ 'Outlook' ]   ==   \"Rainy\" ]    \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       Outlook \n       Temperature \n       Humidity \n       Windy \n       Play golf \n     \n   \n   \n     \n       0 \n       Rainy \n       Hot \n       High \n       False \n       No \n     \n     \n       1 \n       Rainy \n       Hot \n       High \n       True \n       No \n     \n     \n       7 \n       Rainy \n       Mild \n       High \n       False \n       No \n     \n     \n       8 \n       Rainy \n       Cool \n       Normal \n       False \n       Yes \n     \n     \n       10 \n       Rainy \n       Mild \n       Normal \n       True \n       Yes \n     \n      tree . edge ( \"rainy\" ,   \"humidity\" )  tree . edge ( \"humidity\" ,   \"high\" )  tree . edge ( \"humidity\" ,   \"normal\" )  tree . edge ( \"normal\" ,   \"yes\" )  tree . edge ( \"high\" ,   \"no \" )  tree",
            "title": "Last branch"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#summary",
            "text": "We got the final tree for Play Golf dataset using ID3 algorithm    We do not even use temperature attribute (for which information gain was 0.03)    The main problem is that the algorithm may overfit easily (tree does not stop growing until the whole training set is classified)    Imagine some crazy guys went playing on a  rainy ,  windy  day with  high humidity , beacaue it was still  hot    With this extra data point we would have to create more branches    Is one unique data sample worth to extend the whole tree?      And there is more disadvantages:    It handles only discrete attributes    There is a strong bias for features with many possible outcomes    And finally, it does not handle missing values",
            "title": "Summary"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#c45-algorithm",
            "text": "C4.5 introduces some improvements to ID3:    continuous values using threshold    tree pruning to avoid overfitting    normalized information gain    missing values",
            "title": "C4.5 algorithm"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#information-gain-ratio",
            "text": "To avoid a bias in favor of features with a lot of different values C4.5 uses information gain ratio instead of information gain    Lets define intrinsic value  V V  of an attribute  X X  as  V(X) = -\\sum\\limits_{i=1}^N \\frac{|T_i|}{|T|}\\cdot\\log(\\frac{|T_i|}{|T|}) V(X) = -\\sum\\limits_{i=1}^N \\frac{|T_i|}{|T|}\\cdot\\log(\\frac{|T_i|}{|T|})    where  T_i T_i  are samples corresponding to  i i -th possible value of  X X  feature    Information gain ratio  R(X) R(X)  is defined as  R(X) = \\frac{G(X)}{V(X)} R(X) = \\frac{G(X)}{V(X)}",
            "title": "Information gain ratio"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#example_2",
            "text": "Lets consider a fake data set    The goal is to determine if someone plays or not video games    We have three features:    name - mostly unique    sex - 50% females and 50% males     age - just old or young      Looking at data we can say that    most young people play video games, why old people don't    sex does not matter    names are almost distinct           name   ||  John  |  Mark  |  Anne  |  Adam  |  John  |  Alex  |  Alex  |  Xena  |  Tina  |  Lucy  |\n--------------------------------------------------------------------------------------------------------\n     sex    ||   M    |   M    |   F    |   M    |   M    |   F    |   M    |   F    |   F    |   F    |\n--------------------------------------------------------------------------------------------------------\n     age    ||  old   | young  |  old   | young  | young  | young  |  old   |  old   | young  | young  |\n========================================================================================================\n play games ||   N    |   Y    |   Y    |   Y    |   Y    |   N    |   N    |   N    |   Y    |   Y    |   Information gain for  name   h   =   entropy ( 4 ,   6 )    # dataset entropy H(T)  # one John plays and the other one doesn't  # in other cases entropy = 0  g_name   =   h   -   2 / 10   *   entropy ( 1 ,   1 )  print ( g_name )   0.7709505944546686   Information gain for  sex   # 5 men - 3 play  # 5 women - 3 play  g_sex   =   h   -   5 / 10   *   entropy ( 2 ,   3 )   -   5 / 10   *   entropy ( 2 ,   3 )  print ( g_sex )   0.0   Information gain for  age   # 4 old people - 1 plays  # 6 young people - 5 play  g_age   =   h   -   4 / 10   *   entropy ( 1 ,   3 )   -   6 / 10   *   entropy ( 1 ,   5 )  print ( g_age )   0.256425891682003    In ID3 a feature with entropy = 0 is always a winner   Imagine having all distinct values (e.g. credit card numbers)     In this case we would choose  name  as the best predictor    Creating a tree with 8 branches (from 10 samples)    Training data would be perfectly classify    But it is unlikely that the algorithm would be able to generalize for unseen data      Lets calculate information gain ratio and see how it changes the choice of the best feature    Information gain ratio for  name    # 2x John, 2x Alex, 6x unique name   g_name   /   entropy ( 2 ,   2 ,   * [ 1 ] * 6 )   0.26384995435159336   Information gain ratio for  sex   # 5 males and 5 females - zero stays zero though  g_sex   /   entropy ( 5 ,   5 )   0.0   Information gain ratio for  age   # 4x old and 6x young  g_age   /   entropy ( 4 ,   6 )   0.26409777505314147    Based on information gain ratio we choose  age  as the best predictor    Because the denominator in a ratio penalizes features with many values    print ( \"Two possible values: \\n \" )  for   i   in   range ( 0 ,   11 ): \n   print ( \" \\t ({}, {}) split -> entropy = {}\" . format ( i ,   10 - i ,   entropy ( i ,   10 - i )))  print ( \" \\n 10 possible values:\" ,   entropy ( * [ 1 ] * 10 ))   Two possible values:\n\n    (0, 10) split -> entropy = 0\n    (1, 9) split -> entropy = 0.4689955935892812\n    (2, 8) split -> entropy = 0.7219280948873623\n    (3, 7) split -> entropy = 0.8812908992306927\n    (4, 6) split -> entropy = 0.9709505944546686\n    (5, 5) split -> entropy = 1.0\n    (6, 4) split -> entropy = 0.9709505944546686\n    (7, 3) split -> entropy = 0.8812908992306927\n    (8, 2) split -> entropy = 0.7219280948873623\n    (9, 1) split -> entropy = 0.4689955935892812\n    (10, 0) split -> entropy = 0\n\n10 possible values: 3.321928094887362   This datset was handcrafted to make a point, but I hope the message is still clear",
            "title": "Example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#continuous-values",
            "text": "Attributes with continuous values must be first discretize    The best way is to find an optimal threshold which splits the set    The optimal threshold is the one which maximize the infromation gain",
            "title": "Continuous values"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#example_3",
            "text": "Lets consider the same example as before    But this time age has numerical values         name   ||  John  |  Mark  |  Anne  |  Adam  |  John  |  Alex  |  Alex  |  Xena  |  Tina  |  Lucy  |\n--------------------------------------------------------------------------------------------------------\n     sex    ||   M    |   M    |   F    |   M    |   M    |   F    |   M    |   F    |   F    |   F    |\n--------------------------------------------------------------------------------------------------------\n     age    ||   50   |   18   |   65   |   24   |   31   |   18   |   50   |   50   |   24   |   31   |\n========================================================================================================\n play games ||   N    |   Y    |   Y    |   Y    |   Y    |   N    |   N    |   N    |   Y    |   Y    |   The possible thesholds are therefore  \\{18, 24, 31, 50\\} \\{18, 24, 31, 50\\}   # calculate entropy for all possible thresholds  e18   =   2 / 10   *   entropy ( 1 ,   1 )   +   8 / 10   *   entropy ( 3 ,   5 )  e24   =   4 / 10   *   entropy ( 1 ,   3 )   +   6 / 10   *   entropy ( 3 ,   3 )  e31   =   6 / 10   *   entropy ( 1 ,   5 )   +   4 / 10   *   entropy ( 3 ,   1 )  e50   =   9 / 10   *   entropy ( 4 ,   5 )   +   1 / 10   *   entropy ( 0 ,   1 )  print ( \"With threshold = {}, entropy = {}\" . format ( 18 ,   e18 ))  print ( \"With threshold = {}, entropy = {}\" . format ( 24 ,   e24 ))  print ( \"With threshold = {}, entropy = {}\" . format ( 31 ,   e31 ))  print ( \"With threshold = {}, entropy = {}\" . format ( 50 ,   e50 ))   With threshold = 18, entropy = 0.963547202339972\nWith threshold = 24, entropy = 0.9245112497836532\nWith threshold = 31, entropy = 0.7145247027726656\nWith threshold = 50, entropy = 0.8919684538544    The best test is  if age > 31   it splits the dataset to 6 samples (with 5 players) and 4 samples (with 3 non-players)     Please note, that the best threshold may change once a node is created",
            "title": "Example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#unknown-parameters",
            "text": "In the case some samples are incomplete one needs to correct the information gain    The information gain is calculated as before for samples with known attributes    But then it is normalized with respect to the probability that the given attribute has known values    Lets define the factor  F F  as the ratio of the number of samples with known value for a given feature to the number of all samples in a dataset    Then information gain is defines as  G(X) = F\\cdot (H(T) - H(T, X)) G(X) = F\\cdot (H(T) - H(T, X))    Please note, that  F = 1 F = 1  if all values are known    Otherwise, information gain is scaled accordingly",
            "title": "Unknown parameters"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#pruning",
            "text": "The algorithm creates as many nodes as needed to classify all test samples    It may lead to overfitting and the resulting tree would fail to classify correctly unseen samples    To avoid this one can prune a tree    pre-pruning (early stopping)    stop building a tree before leaves with few samples are produced    how to decide when it is good time to stop? e.g. using cross-validation on validation set (stop if the error does not increase significantly)    underfitting if stop to early      post-pruning    let a tree grow completely    then go from bottom to top and try to replace a node with a leaf    if there is improvement in accuracy - cut a tree    if the accuracy stays the same - cut a tree (Occam's razor)    otherwise leave a node",
            "title": "Pruning"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#first-example-step-by-step",
            "text": "Lets consider the problem from the beginning of the lecture    Our dataset has 20 blue points and 20 orange points    Each point has two features (both are numerical)    We expect overfitting if pruning is not applied    We will calculate everything step by step (it is boring, but demonstrates how the algorithm works)    # first define some points representing two classes  grid   =   np . mgrid [ 0 : 10 : 2 ,   0 : 10 : 2 ]  set01   =   np . vstack ([ grid [ 0 ] . ravel (),   grid [ 1 ] . ravel ()]) . T  set01   =   np . delete ( set01 ,   [ 17 ,   18 ,   19 ,   22 ,   24 ],   axis = 0 )  grid   =   np . mgrid [ 6 : 16 : 2 ,   0 : 10 : 2 ]  set02   =   np . vstack ([ grid [ 0 ] . ravel (),   grid [ 1 ] . ravel ()]) . T  set02   =   np . delete ( set02 ,   [ 0 ,   1 ,   5 ,   6 ,   8 ],   axis = 0 )  plt . scatter ( * set01 . T )  plt . scatter ( * set02 . T );",
            "title": "First example - step by step"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#validation-set",
            "text": "We will use 10 points from the dataset for validation    This time selected manually to perform by hand calculations    On the plot below X denotes validation samples    # split dataset to training and validation set  # note, we should splt them randomly  # but here we do this by hand  valid_idx   =   [ 3 ,   7 ,   10 ,   14 ,   18 ]  blue_valid   =   set01 [ valid_idx ]  blue_train   =   np . delete ( set01 ,   valid_idx ,   axis = 0 )  orange_valid   =   set02 [ valid_idx ]  orange_train   =   np . delete ( set02 ,   valid_idx ,   axis = 0 )  # circles - training set  # x - validation set  plt . scatter ( * blue_train . T )  plt . scatter ( * blue_valid . T ,   color = 'C0' ,   marker = 'x' )  plt . scatter ( * orange_train . T )  plt . scatter ( * orange_valid . T ,   color = 'C1' ,   marker = 'x' );",
            "title": "Validation set"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#thresholds-finder",
            "text": "When building a tree we need to calculate information gain for every threshold in current subset    Every subset  S S  has  N_b N_b  blue samples and  N_o N_o  orange samples    After split into accoring to some threshold we get two subsets    n_b n_b  of blue points and  n_o n_o  of orange points ( S_1 S_1 )    N_b - n_b N_b - n_b  of blue points and  N_o - n_o N_o - n_o  of orange points ( S_2 S_2 )      def   info_gain ( Nb ,   No ,   nb ,   no ): \n   \"\"\"Calculate information gain for given split\"\"\" \n   h   =   entropy ( Nb ,   No )   # H(S) \n   total   =   Nb   +   No       # total number of samples \n   subtotal   =   nb   +   no    # number of samples in subset \n\n   return   h   -   subtotal   /   total   *   entropy ( nb ,   no )  \\\n            -   ( total   -   subtotal )   /   total   *   entropy ( Nb   -   nb ,   No   -   no )",
            "title": "Thresholds finder"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#feature-x",
            "text": "We need to calculate information gain ratio for the best threshold (the one that maximize information gain)    Possible thresholds  \\{0, 2, 4, 6, 8, 10, 12\\} \\{0, 2, 4, 6, 8, 10, 12\\}    Nb   =   15  No   =   15  splits   =   { \"0\" :   ( 4 ,   0 ),   \"2 \" :   ( 8 ,   0 ),   \"4\" :   ( 11 ,   0 ),   \"6\" :   ( 13 ,   3 ), \n           \"8\" :   ( 15 ,   4 ),   \"10\" :   ( 15 ,   8 ),   \"12\" :   ( 15 ,   11 )}  for   threshold ,   ( no ,   nb )   in   splits . items (): \n   print ( \"Threshold = {} \\t  -> {}\" . format ( threshold ,   info_gain ( Nb ,   No ,   no ,   nb )))   Threshold = 0    -> 0.14818913558232172\nThreshold = 2    -> 0.33824492595034883\nThreshold = 4    -> 0.5297578726233217\nThreshold = 6    -> 0.3525728312615027\nThreshold = 8    -> 0.5297578726233217\nThreshold = 10   -> 0.28538113149388267\nThreshold = 12   -> 0.14818913558232172    We got the same cuts as predicted at the beginning of the lecture:  x > 4 x > 4  or  x > 8 x > 8    Lets choose  x > 4 x > 4  and calculate information gain ratio    # 4 samples with x = 0, 4 samples with x = 2 etc  info_gain ( Nb ,   No ,   * splits [ \"4\" ])   /   entropy ( 4 ,   4 ,   3 ,   5 ,   3 ,   4 ,   3 ,   4 )   0.1779055922617179",
            "title": "Feature X"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#feature-y",
            "text": "Repeat the procedure    This time possible thresholds =  \\{0, 2, 4, 6\\} \\{0, 2, 4, 6\\}    Nb   =   15  No   =   15  splits   =   { \"0\" :   ( 4 ,   2 ),   \"2\" :   ( 8 ,   5 ),   \"4\" :   ( 10 ,   8 ),   \"6\" :   ( 13 ,   11 )}  for   threshold ,   ( no ,   nb )   in   splits . items (): \n   print ( \"Threshold = {} \\t  -> {}\" . format ( threshold ,   info_gain ( Nb ,   No ,   no ,   nb )))   Threshold = 0    -> 0.02035297064032593\nThreshold = 2    -> 0.029594041354123246\nThreshold = 4    -> 0.013406861436605633\nThreshold = 6    -> 0.0203529706403259    The best cut is  y > 2 y > 2  (as predicted before)    Lets calculate information gain ratio    info_gain ( Nb ,   No ,   * splits [ \"2\" ])   /   entropy ( 6 ,   7 ,   5 ,   6 ,   6 )   0.01278981522839263",
            "title": "Feature Y"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#the-root",
            "text": "At the beginning we discussed the choice of  y y  as a root predictor    ID3 and C4.5 are greedy algorithms and select optimal solution at given stage    We can start to build the tree with the first best predictor    tree   =   Digraph ()  tree . edge ( \"x > 4? \\n [15, 15]\" ,   \"blue \\n [11, 0]\" ,   \"No\" )  tree . edge ( \"x > 4? \\n [15, 15]\" ,   \"[4, 15]\" ,   \"Yes\" )  tree",
            "title": "The root"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#branch-x-4",
            "text": "Now we have to repeat the procedure for the branch  [4, 15] [4, 15]    Lets take a look what points are left    plt . xlim ([ 5.5 ,   14.5 ])  plt . scatter ( * blue_train . T )  plt . scatter ( * orange_train . T );     Check  x x  maximum information gain ratio   Nb   =   4  No   =   15  splits   =   { \"6\" :   ( 2 ,   3 ),   \"8\" :   ( 4 ,   4 ),   \"10\" :   ( 4 ,   8 ),   \"12\" :   ( 4 ,   11 )}  for   threshold ,   ( no ,   nb )   in   splits . items (): \n   print ( \"Threshold = {} \\t  -> {}\" . format ( threshold ,   info_gain ( Nb ,   No ,   no ,   nb )))   Threshold = 6    -> 0.051004839414443226\nThreshold = 8    -> 0.32143493796317624\nThreshold = 10   -> 0.16251125329718286\nThreshold = 12   -> 0.08198172064120202  print ( \"Information gain ratio with x > 8:\" , \n       info_gain ( Nb ,   No ,   * splits [ \"8\" ])   /   entropy ( 5 ,   3 ,   4 ,   3 ,   4 ))   Information gain ratio with x > 8: 0.14010311259651076   Check  y y  maximum information gain ratio   Nb   =   4  No   =   15  splits   =   { \"0\" :   ( 2 ,   2 ),   \"2\" :   ( 3 ,   5 ),   \"4\" :   ( 3 ,   6 ),   \"6\" :   ( 4 ,   9 )}  for   threshold ,   ( no ,   nb )   in   splits . items (): \n   print ( \"Threshold = {} \\t  -> {}\" . format ( threshold ,   info_gain ( Nb ,   No ,   no ,   nb )))   Threshold = 0    -> 0.08471690647404045\nThreshold = 2    -> 0.08617499693494635\nThreshold = 4    -> 0.06066554625879636\nThreshold = 6    -> 0.13320381570773476  print ( \"Information gain ratio with y > 6:\" , \n       info_gain ( Nb ,   No ,   * splits [ \"6\" ])   /   entropy ( 4 ,   4 ,   3 ,   4 ,   4 ))   Information gain ratio with y > 6: 0.05757775370755489    Once again  x x  is a winner    And we have a new node    tree   =   Digraph ()  tree . edge ( \"x > 4? \\n [15, 15]\" ,   \"blue \\n [11, 0]\" ,   \"No\" )  tree . edge ( \"x > 4? \\n [15, 15]\" ,   \"x > 8? \\n [4, 15]\" ,   \"Yes\" )  tree . edge ( \"x > 8? \\n [4, 15]\" ,   \"[4, 4]\" ,   \"No\" )  tree . edge ( \"x > 8? \\n [4, 15]\" ,   \"orange \\n [0, 11]\" ,   \"Yes\" )  tree",
            "title": "Branch x &gt; 4"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#branch-x-8",
            "text": "We will continue until the tree is fully grown   plt . xlim ([ 5.5 ,   8.5 ])  plt . scatter ( * blue_train . T )  plt . scatter ( * orange_train . T );     Again, the best cut may be pretty obvious, but lets check the math  We have one possible cut in  x x   Nb   =   4  No   =   4  print ( \"Information gain ratio with x > 6:\" , \n       info_gain ( Nb ,   No ,   2 ,   3 )   /   entropy ( 5 ,   3 ))   Information gain ratio with x > 6: 0.05112447853477686   And usual threshold candidates in  y y   splits   =   { \"0\" :   ( 2 ,   0 ),   \"2\" :   ( 3 ,   0 ),   \"4\" :   ( 3 ,   1 ),   \"6\" :   ( 4 ,   2 )}  for   threshold ,   ( no ,   nb )   in   splits . items (): \n   print ( \"Threshold = {} \\t  -> {}\" . format ( threshold ,   info_gain ( Nb ,   No ,   no ,   nb )))   Threshold = 0    -> 0.31127812445913283\nThreshold = 2    -> 0.5487949406953986\nThreshold = 4    -> 0.1887218755408671\nThreshold = 6    -> 0.31127812445913283  print ( \"Information gain ratio with y > 2:\" , \n       info_gain ( Nb ,   No ,   * splits [ \"2\" ])   /   entropy ( 2 ,   1 ,   1 ,   2 ,   2 ))   Information gain ratio with y > 2: 0.24390886253128827   And the tree is growing   tree   =   Digraph ()  tree . edge ( \"x > 4? \\n [15, 15]\" ,   \"blue \\n [11, 0]\" ,   \"No\" )  tree . edge ( \"x > 4? \\n [15, 15]\" ,   \"x > 8? \\n [4, 15]\" ,   \"Yes\" )  tree . edge ( \"x > 8? \\n [4, 15]\" ,   \"y > 2? \\n [4, 4]\" ,   \"No\" )  tree . edge ( \"x > 8? \\n [4, 15]\" ,   \"orange \\n [0, 11]\" ,   \"Yes\" )  tree . edge ( \"y > 2? \\n [4, 4]\" ,   \"blue \\n [3, 0]\" ,   \"No\" )  tree . edge ( \"y > 2? \\n [4, 4]\" ,   \"[1, 4]\" ,   \"Yes\" )  tree",
            "title": "Branch x&lt;= 8"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#branch-y-2",
            "text": "plt . xlim ([ 5.5 ,   8.5 ])  plt . ylim ([ 3.5 ,   8.5 ])  plt . scatter ( * blue_train . T )  plt . scatter ( * orange_train . T );    Nb   =   1  No   =   4  print ( \"Information gain ratio with x > 6:\" , \n       info_gain ( Nb ,   No ,   0 ,   3 )   /   entropy ( 3 ,   2 ))   Information gain ratio with x > 6: 0.33155970728682876  print ( \"Information gain ratio with y > 4:\" , \n       info_gain ( Nb ,   No ,   0 ,   1 )   /   entropy ( 1 ,   2 ,   2 ))  print ( \"Information gain ratio with y > 6:\" , \n       info_gain ( Nb ,   No ,   1 ,   2 )   /   entropy ( 1 ,   2 ,   2 ))   Information gain ratio with y > 4: 0.047903442721748145\nInformation gain ratio with y > 6: 0.11232501392736344",
            "title": "Branch y &gt; 2"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#the-final-tree",
            "text": "tree   =   Digraph ()  tree . edge ( \"x > 4? \\n [15, 15]\" ,   \"blue \\n [11, 0]\" ,   \"No\" )  tree . edge ( \"x > 4? \\n [15, 15]\" ,   \"x > 8? \\n [4, 15]\" ,   \"Yes\" )  tree . edge ( \"x > 8? \\n [4, 15]\" ,   \"y > 2? \\n [4, 4]\" ,   \"No\" )  tree . edge ( \"x > 8? \\n [4, 15]\" ,   \"orange \\n [0, 11]\" ,   \"Yes\" )  tree . edge ( \"y > 2? \\n [4, 4]\" ,   \"blue \\n [3, 0]\" ,   \"No\" )  tree . edge ( \"y > 2? \\n [4, 4]\" ,   \"x > 6? \\n [1, 4]\" ,   \"Yes\" )  tree . edge ( \"x > 6? \\n [1, 4]\" ,   \"orange \\n [0, 3]\" ,   \"No\" )  tree . edge ( \"x > 6? \\n [1, 4]\" ,   \"y > 6? \\n [1, 1]\" ,   \"Yes\" )  tree . edge ( \"y > 6? \\n [1, 1]\" ,   \"blue \\n [1, 0]\" ,   \"No\" )  tree . edge ( \"y > 6? \\n [1, 1]\" ,   \"orange \\n [0, 1]\" ,   \"Yes\" )  tree      It is likely that this tree is overfitted    We will proceed with pruning as it was explained    But first lets implement decision rules to measure accuracy    def   tree_nominal ( x ,   y ): \n   \"\"\"Implementation of above tree\"\"\" \n   if   x   <=   4 : \n     return   \"blue\" \n   elif   x   >   8 : \n     return   \"orange\" \n   elif   y   <=   2 : \n     return   \"blue\" \n   elif   x   <=   6 : \n     return   \"orange\" \n   else : \n     return   \"orange\"   if   y   >   6   else   \"blue\"",
            "title": "The final tree"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#sanity-check",
            "text": "If the tree is built  correctly  we expect 100% accuracy on training set   for   x ,   y   in   blue_train : \n   print ( tree_nominal ( x ,   y ),   end = ' ' )   blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue  for   x ,   y   in   orange_train : \n   print ( tree_nominal ( x ,   y ),   end = ' ' )    orange orange orange orange orange orange orange orange orange orange orange orange orange orange orange",
            "title": "Sanity check"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#accuracy-before-pruning",
            "text": "def   accuracy ( samples ,   tree ): \n   \"\"\"Just print the result of classification\"\"\" \n   for   x ,   y   in   samples : \n     print ( \"({}, {}) -> {}\" . format ( x ,   y ,   tree ( x ,   y )))   accuracy ( blue_valid ,   tree_nominal )   (0, 6) -> blue\n(2, 4) -> blue\n(4, 0) -> blue\n(4, 8) -> blue\n(8, 2) -> blue  accuracy ( orange_valid ,   tree )   (8, 4) -> blue\n(10, 4) -> orange\n(12, 0) -> orange\n(12, 8) -> orange\n(14, 6) -> orange",
            "title": "Accuracy before pruning"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#pruning-i",
            "text": "We want to prune last decision node  y > 6 y > 6    In general, majority decides about the leaf node class    As it is a tie here, lets check both    def   tree_prune01a ( x ,   y ): \n   \"\"\"Implementation of above tree\"\"\" \n   if   x   <=   4 : \n     return   \"blue\" \n   elif   x   >   8 : \n     return   \"orange\" \n   elif   y   <=   2 : \n     return   \"blue\" \n   elif   x   <=   6 : \n     return   \"orange\" \n   else : \n     return   \"blue\"  def   tree_prune01b ( x ,   y ): \n   \"\"\"Implementation of above tree\"\"\" \n   if   x   <=   4 : \n     return   \"blue\" \n   elif   x   >   8 : \n     return   \"orange\" \n   elif   y   <=   2 : \n     return   \"blue\" \n   else : \n     return   \"orange\"   accuracy ( blue_valid ,   tree_prune01a )   (0, 6) -> blue\n(2, 4) -> blue\n(4, 0) -> blue\n(4, 8) -> blue\n(8, 2) -> blue  accuracy ( orange_valid ,   tree_prune01a )   (8, 4) -> blue\n(10, 4) -> orange\n(12, 0) -> orange\n(12, 8) -> orange\n(14, 6) -> orange    Pruning does not change the accuracy    We always use Occam's razor and  prune01a  is preferred over nominal tree    But lets see how  prune01b  works    accuracy ( blue_valid ,   tree_prune01b )   (0, 6) -> blue\n(2, 4) -> blue\n(4, 0) -> blue\n(4, 8) -> blue\n(8, 2) -> blue  accuracy ( orange_valid ,   tree_prune01b )   (8, 4) -> orange\n(10, 4) -> orange\n(12, 0) -> orange\n(12, 8) -> orange\n(14, 6) -> orange    In this case we even get the increase of the accuracy    We decide to prune a tree by replacing  y > 6 y > 6  decision node with \"orange\" leaf node    Which automatically removes  x > 6 x > 6  decision node    tree   =   Digraph ()  tree . edge ( \"x > 4? \\n [15, 15]\" ,   \"blue \\n [11, 0]\" ,   \"No\" )  tree . edge ( \"x > 4? \\n [15, 15]\" ,   \"x > 8? \\n [4, 15]\" ,   \"Yes\" )  tree . edge ( \"x > 8? \\n [4, 15]\" ,   \"y > 2? \\n [4, 4]\" ,   \"No\" )  tree . edge ( \"x > 8? \\n [4, 15]\" ,   \"orange \\n [0, 11]\" ,   \"Yes\" )  tree . edge ( \"y > 2? \\n [4, 4]\" ,   \"blue \\n [3, 0]\" ,   \"No\" )  tree . edge ( \"y > 2? \\n [4, 4]\" ,   \"orange \\n [1, 4]\" ,   \"Yes\" )  tree",
            "title": "Pruning I"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#pruning-ii",
            "text": "Now, lets see the accuracy after removing  y > 2 y > 2  node    It is once again a tie, so lets check both scenarios    def   tree_prune02a ( x ,   y ): \n   \"\"\"Implementation of above tree\"\"\" \n   if   x   <=   4 : \n     return   \"blue\" \n   else : \n     return   \"orange\"  def   tree_prune02b ( x ,   y ): \n   \"\"\"Implementation of above tree\"\"\" \n   if   x   <=   4 : \n     return   \"blue\" \n   elif   x   >   8 : \n     return   \"orange\" \n   else : \n     return   \"blue\"   accuracy ( blue_valid ,   tree_prune02a )   (0, 6) -> blue\n(2, 4) -> blue\n(4, 0) -> blue\n(4, 8) -> blue\n(8, 2) -> orange  accuracy ( orange_valid ,   tree_prune02a )   (8, 4) -> orange\n(10, 4) -> orange\n(12, 0) -> orange\n(12, 8) -> orange\n(14, 6) -> orange  accuracy ( blue_valid ,   tree_prune02b )   (0, 6) -> blue\n(2, 4) -> blue\n(4, 0) -> blue\n(4, 8) -> blue\n(8, 2) -> blue  accuracy ( orange_valid ,   tree_prune02b )   (8, 4) -> blue\n(10, 4) -> orange\n(12, 0) -> orange\n(12, 8) -> orange\n(14, 6) -> orange    In both cases the error increased    We stop pruning and leave the tree as it is in  prune01b  version",
            "title": "Pruning II"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#summary_1",
            "text": "C4.5 algorithm gives the full and clear prescription for building decision trees    It may look as a long procedure, but it is only because I wanted to show everything step by step and avoid  \"after a few trivial steps...\"    ID3/C4.5/C5.0 are based on information theory    There is alternative procedure based on  gini impurity , which is used by CART",
            "title": "Summary"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#cart",
            "text": "CART stands for Classification and Regression Tree    It was created independently from ID3 (more or less at the same time)    The main differences:    it creates binary trees (each decision node has two branches)    it uses gini impurity instead of information gain    it supports numerical target variables (regression)",
            "title": "CART"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#gini-impurity",
            "text": "Let  T = \\{T_1, T_2, ..., T_n\\} T = \\{T_1, T_2, ..., T_n\\}  be the set of  n n  classes    and  P = \\{p_1, p_2, ..., p_n\\} P = \\{p_1, p_2, ..., p_n\\}  be the probability distribution    where  p_i p_i  is the probability that a sample belongs to class  T_i T_i    and  1 - p_i 1 - p_i  is the probability that it belongs to another class    Gini impurity is defines as  I(P) = \\sum\\limits_{i=1}^n p_i\\cdot (1 - p_i) = \\sum\\limits_{i=1}^n p_i - \\sum\\limits_{i=1}^n p_i^2 = 1 - \\sum\\limits_{i=1}^n p_i^2 I(P) = \\sum\\limits_{i=1}^n p_i\\cdot (1 - p_i) = \\sum\\limits_{i=1}^n p_i - \\sum\\limits_{i=1}^n p_i^2 = 1 - \\sum\\limits_{i=1}^n p_i^2    As before (for entropy), lets consider two case scenario with  P = (p, 1 - p) P = (p, 1 - p) , so gini impurity is given by  I = 1 - p^2 - (1 - p)^2 = -2p(p - 1) I = 1 - p^2 - (1 - p)^2 = -2p(p - 1)    p   =   np . arange ( 0.01 ,   1.0 ,   0.01 )  plt . xlabel ( \"p\" )  plt . ylabel ( \"surprise factor\" )  plt . plot ( p ,   - p   *   np . log2 ( p )   -   ( 1   -   p )   *   np . log2 ( 1   -   p ),   label = \"Entropy\" );  plt . plot ( p ,   - 2 * p * ( p   -   1 ),   label = \"Gini impurity\" )  plt . legend ();",
            "title": "Gini impurity"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#play-golf",
            "text": "Lets consider once again Play Golf dataset   import   pandas   as   pd  # first row = headers  src   =   \"http://chem-eng.utoronto.ca/~datamining/dmc/datasets/weather_nominal.csv\"  golf_data   =   pd . read_csv ( src )   golf_data    \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       Outlook \n       Temperature \n       Humidity \n       Windy \n       Play golf \n     \n   \n   \n     \n       0 \n       Rainy \n       Hot \n       High \n       False \n       No \n     \n     \n       1 \n       Rainy \n       Hot \n       High \n       True \n       No \n     \n     \n       2 \n       Overcast \n       Hot \n       High \n       False \n       Yes \n     \n     \n       3 \n       Sunny \n       Mild \n       High \n       False \n       Yes \n     \n     \n       4 \n       Sunny \n       Cool \n       Normal \n       False \n       Yes \n     \n     \n       5 \n       Sunny \n       Cool \n       Normal \n       True \n       No \n     \n     \n       6 \n       Overcast \n       Cool \n       Normal \n       True \n       Yes \n     \n     \n       7 \n       Rainy \n       Mild \n       High \n       False \n       No \n     \n     \n       8 \n       Rainy \n       Cool \n       Normal \n       False \n       Yes \n     \n     \n       9 \n       Sunny \n       Mild \n       Normal \n       False \n       Yes \n     \n     \n       10 \n       Rainy \n       Mild \n       Normal \n       True \n       Yes \n     \n     \n       11 \n       Overcast \n       Mild \n       High \n       True \n       Yes \n     \n     \n       12 \n       Overcast \n       Hot \n       Normal \n       False \n       Yes \n     \n     \n       13 \n       Sunny \n       Mild \n       High \n       True \n       No",
            "title": "Play Golf"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#gini-impurity_1",
            "text": "We treat all values as they are continues    And consider all possible split    Every split leads to two subsets  S_1 S_1  and  S_2 S_2    And gini impurity for a set  S S  for given split is given by:  I(S) = \\frac{|S_1|}{|S|}\\cdot I(S_1) + \\frac{|S_2|}{|S|}\\cdot I(S_2) I(S) = \\frac{|S_1|}{|S|}\\cdot I(S_1) + \\frac{|S_2|}{|S|}\\cdot I(S_2)    def   gini ( * distribution ): \n   \"\"\"Calculate gini impurity for given ditribution of samples\"\"\" \n   sum2   =   sum ( distribution ) ** 2    # normalization factor \n\n   return   1   -   sum ([ p ** 2   for   p   in   distribution ]) / sum2   def   gini_split ( s1 ,   s2 ,   g1 ,   g2 ): \n   \"\"\"Calcualte impurity for given split    s1 -- the size of S1 subset    s1 -- the size of S2 subset    g1 -- I(S1)    g2 -- I(S2)    \"\"\" \n   s   =   s1   +   s2    # the total set size \n\n   return   s1 / s   *   g1   +   s2 / s   *   g2               | Play golf |\n            =============\n            | yes | no  |\n      -------------------\n      | yes |  2  |  3  | 5\nrainy | no  |  7  |  2  | 9\n      -------------------\n               9     5  gini_split ( 5 ,   9 ,   gini ( 2 ,   3 ),   gini ( 7 ,   2 ))   0.3936507936507937              | Play golf |\n            =============\n            | yes | no  |\n      -------------------\n      | yes |  3  |  2  | 5\nsunny | no  |  6  |  3  | 9\n      -------------------\n               9     5  gini_split ( 5 ,   9 ,   gini ( 3 ,   2 ),   gini ( 6 ,   3 ))   0.45714285714285713                 | Play golf |\n               =============\n               | yes | no  |\n         -------------------\n         | yes |  4  |  0  | 4\novercast | no  |  5  |  5  | 10\n         -------------------\n                  9     5  gini_split ( 4 ,   10 ,   gini ( 4 ,   0 ),   gini ( 5 ,   5 ))   0.35714285714285715    From  Outlook  feature the best choice is  Overcast  as it minimizes impurity    However, we would have to check other features and choose the best predictor from all possibilities    We have one step by step example done though    So lets use some tool",
            "title": "Gini impurity"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#scikit-learn",
            "text": "One step by step example is behind us, so now lets use some tool    CART is implemented in  scikit-learn    However, their implementation takes only numerical values    So we will use  LabelDecoder  to convert all values to numbers    from   sklearn.preprocessing   import   LabelEncoder  # pandas.DataFrame.apply applies a function to given axis (0 by default)  # LabelEncoder encodes class labels with values between 0 and n-1  golf_data_num   =   golf_data . apply ( LabelEncoder () . fit_transform )   golf_data_num    \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       Outlook \n       Temperature \n       Humidity \n       Windy \n       Play golf \n     \n   \n   \n     \n       0 \n       1 \n       1 \n       0 \n       0 \n       0 \n     \n     \n       1 \n       1 \n       1 \n       0 \n       1 \n       0 \n     \n     \n       2 \n       0 \n       1 \n       0 \n       0 \n       1 \n     \n     \n       3 \n       2 \n       2 \n       0 \n       0 \n       1 \n     \n     \n       4 \n       2 \n       0 \n       1 \n       0 \n       1 \n     \n     \n       5 \n       2 \n       0 \n       1 \n       1 \n       0 \n     \n     \n       6 \n       0 \n       0 \n       1 \n       1 \n       1 \n     \n     \n       7 \n       1 \n       2 \n       0 \n       0 \n       0 \n     \n     \n       8 \n       1 \n       0 \n       1 \n       0 \n       1 \n     \n     \n       9 \n       2 \n       2 \n       1 \n       0 \n       1 \n     \n     \n       10 \n       1 \n       2 \n       1 \n       1 \n       1 \n     \n     \n       11 \n       0 \n       2 \n       0 \n       1 \n       1 \n     \n     \n       12 \n       0 \n       1 \n       1 \n       0 \n       1 \n     \n     \n       13 \n       2 \n       2 \n       0 \n       1 \n       0 \n     \n       Now, lets splits our dataset to features and labels   # DataFrame.iloc makes an access thourgh indices  # we want all rows and first 4 columns for features  # and the last column for labels  data   =   np . array ( golf_data_num . iloc [:,   : 4 ])  target   =   np . array ( golf_data_num . iloc [:,   4 ])    Once data is prepared, creating a tree is as easy as 2 + 2 -1   from   sklearn   import   tree  golf_tree   =   tree . DecisionTreeClassifier ()  golf_tree . fit ( data ,   target );    sklearn.tree  supports drawing a tree using  graphviz   import   graphviz  # dot is a graph description language  dot   =   tree . export_graphviz ( golf_tree ,   out_file = None ,  \n                            feature_names = golf_data . columns . values [: 4 ],   \n                            class_names = [ \"no\" ,   \"yes\" ],   \n                            filled = True ,   rounded = True ,   \n                            special_characters = True )   # we create a graph from dot source using graphviz.Source  graph   =   graphviz . Source ( dot )   graph     Please note, that in the case of a real problem we would want to have a validation set and perform a pruning ( scikit-learn  does not support it though)",
            "title": "Scikit learn"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#regression",
            "text": "The difference now is that targets are numerical values (instead of categorical), e.g. in golf data - number of hours played instead of \"yes / no\"    Features may be either discrete or continuous    The idea is the same though - we want to create a binary tree and minimize the error on in each leaf    However, having continuous values as targets we can not simply use entropy or gini    We need to use different measurement - variance  V(X) = \\frac{1}{n}\\sum\\limits_{i=1}^n (x_i - \\bar x)^2 V(X) = \\frac{1}{n}\\sum\\limits_{i=1}^n (x_i - \\bar x)^2    where  X = \\{x_1, ..., x_n\\} X = \\{x_1, ..., x_n\\}  and  \\bar x \\bar x  is the average value    Note, that here  x_i x_i  are equally likely",
            "title": "Regression"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#simple-example",
            "text": "Before we learn how to grow a regression tree, lets take a look how it works on a simple example    Lets consider data distributed according to  x^2 x^2  (with some noise, obviously)    It means with have continuous features ( x x ) and targets ( y y )    We will split by hand the domain in  0.3 0.3  and  0.6 0.6    X   =   np . random . sample ( 50 )  Y   =   np . array ([ x ** 2   +   np . random . normal ( 0 ,   0.05 )   for   x   in   X ])  plt . xlabel ( \"x\" )  plt . ylabel ( \"y\" )  plt . scatter ( X ,   Y ,   color = 'b' )  plt . plot ([ 0.3 ,   0.3 ],   [ - 0.2 ,   1.2 ],   'g--' )  plt . plot ([ 0.6 ,   0.6 ],   [ - 0.2 ,   1.2 ],   'g--' );     The corresponding tree would look like this   tree   =   Digraph ()  tree . edge ( \"x < 0.3?\" ,   \"?\" ,   \"No\" )  tree . edge ( \"x < 0.3?\" ,   \"x < 0.6?\" ,   \"Yes\" )  tree . edge ( \"x < 0.6?\" ,   \"? \" ,   \"No\" )  tree . edge ( \"x < 0.6?\" ,   \"?  \" ,   \"Yes\" )  tree     For each split lets find a value  \\bar y \\bar y   def   avg ( X ,   Y ,   x_min ,   x_max ): \n   \"\"\"Return the average value in (x_min, x_max) range\"\"\" \n   n   =   0      # number of samples in given split  \n   avg   =   0    # average value \n\n   for   x ,   y   in   zip ( X ,   Y ): \n     if   x   >=   x_min   and   x   <   x_max : \n       n   +=   1 \n       avg   +=   y \n\n   return   avg   /   n   plt . scatter ( X ,   Y ,   color = 'b' )  plt . plot ([ 0.3 ,   0.3 ],   [ - 0.2 ,   1.2 ],   'g--' )  plt . plot ([ 0.6 ,   0.6 ],   [ - 0.2 ,   1.2 ],   'g--' )  y   =   avg ( X ,   Y ,   0 ,   0.3 )  plt . plot ([ 0.0 ,   0.3 ],   [ y ,   y ],   'r' )  y   =   avg ( X ,   Y ,   0.3 ,   0.6 )  plt . plot ([ 0.3 ,   0.6 ],   [ y ,   y ],   'r' )  y   =   avg ( X ,   Y ,   0.6 ,   1 )  plt . plot ([ 0.6 ,   1.0 ],   [ y ,   y ],   'r' );     Alternatively, one could do linear regression for split",
            "title": "Simple example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#growing-a-tree",
            "text": "The idea is similar as for numerical values in classification problems    For each feature we check all possible splits and calculate variance    We choose a binary split which minimzes variance    from   sklearn.tree   import   DecisionTreeRegressor  # create a decision tree regressor  fit   =   DecisionTreeRegressor ()  # and grow it (note that X must be reshaped)  fit . fit ( np . reshape ( X ,   ( - 1 ,   1 )),   Y );   # prepare test sample with \"newaxis\" trick  X_test   =   np . arange ( 0.0 ,   1.0 ,   0.01 )[:,   np . newaxis ]  Y_test   =   fit . predict ( X_test )   plt . scatter ( X ,   Y ,   color = 'b' )  plt . plot ( X_test ,   Y_test );      And this is a perfect example of  overfitting    Each point was  classified  as a separate target    Beacause without any stopping criterion the tree is growing until there is a single point in a leaf    There are several strategies to pre-prune a tree:    define a max depth of a tree    define a minimum number of samples in a leaf    define a minimum impurity    define a minimum impurity decrease      Whatever method is chosen you get a hyperparameter    And we already know how to find an optimal hyperparameter: cross-validation",
            "title": "Growing a tree"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#tree-cross-validation",
            "text": "To make it easier to check all possible methods lets create a simple class to do that for us    It takes training data and hyperparameter name (as named in  scikit-learn )    It can change hyperparameter    It can perform a cross-validation for a set of hyperparameter values    It can make accuracy and best fit plots    from   sklearn.model_selection   import   cross_val_score  from   sklearn.tree   import   DecisionTreeRegressor  class   TreeCV : \n   \"\"\"Perform a cross-validation for chosen hyperparameter\"\"\" \n\n   def   __init__ ( self ,   X ,   Y ,   hp = \"max_depth\" ): \n     \"\"\"Save training data\"\"\" \n     self . X   =   X      # features \n     self . Y   =   Y      # targets \n     self . hp   =   hp    # hyperparameter \n\n\n   def   set_method ( self ,   hp ): \n     \"\"\"Set hyperparameter to use\"\"\" \n     self . hp   =   hp \n\n\n   def   cross_me ( self ,   * hp_vals ): \n     \"\"\"Perform cross validation for given hyperparameter values\"\"\" \n     self . scores   =   []    # the accuracy table \n     self . best   =   None    # the best fit \n\n     best_score   =   0 \n\n     for   hp   in   hp_vals : \n       # create a tree with given hyperparameter cut \n       fit   =   DecisionTreeRegressor ( ** { self . hp :   hp }) \n\n       # calculate a cross validation scores and a mean value \n       score   =   cross_val_score ( fit ,   np . reshape ( X ,   ( - 1 ,   1 )),   Y ) . mean () \n\n       # update best fit if necessary \n       if   score   >   best_score : \n         self . best   =   fit \n         best_score   =   score \n\n       self . scores . append ([ hp ,   score ]) \n\n     # train the best fit \n     self . best . fit ( np . reshape ( X ,   ( - 1 ,   1 )),   Y ) \n\n\n   def   plot ( self ): \n     \"\"\"Plot accuracy as a function of hyperparameter values and best fit\"\"\" \n     plt . figure ( figsize = ( 15 ,   5 )) \n\n     plt . subplot ( 1 ,   2 ,   1 ) \n\n     plt . xlabel ( self . hp ) \n     plt . ylabel ( \"accuracy\" ) \n\n     plt . plot ( * zip ( * self . scores )) \n\n     plt . subplot ( 1 ,   2 ,   2 ) \n\n     X_test   =   np . arange ( 0.0 ,   1.0 ,   0.01 )[:,   np . newaxis ] \n     Y_test   =   self . best . predict ( X_test ) \n\n     plt . scatter ( self . X ,   self . Y ,   color = 'b' ,   marker = '.' ,   label = \"Training data\" ) \n     plt . plot ( X_test ,   X_test   *   X_test ,   'g' ,   label = \"True distribution\" )     \n     plt . plot ( X_test ,   Y_test ,   'r' ,   label = \"Decision tree\" ) \n\n     plt . legend ()",
            "title": "Tree: cross-validation"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#traning-dataset",
            "text": "X   =   np . random . sample ( 200 )  Y   =   np . array ([ x ** 2   +   np . random . normal ( 0 ,   0.05 )   for   x   in   X ])",
            "title": "Traning dataset"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#max_depth",
            "text": "tree_handler   =   TreeCV ( X ,   Y )  tree_handler . cross_me ( * range ( 1 ,   10 ))  tree_handler . plot ()",
            "title": "max_depth"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#min_samples_leaf",
            "text": "tree_handler . set_method ( \"min_samples_leaf\" )  tree_handler . cross_me ( * range ( 1 ,   10 ))  tree_handler . plot ()",
            "title": "min_samples_leaf"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#min_impurity_split",
            "text": "# min_impurity_split is depracated so lets disable warnings  import   warnings  warnings . filterwarnings ( \"ignore\" ,   category = DeprecationWarning )   tree_handler . set_method ( \"min_impurity_split\" )  tree_handler . cross_me ( * np . arange ( 0.0 ,   5e-3 ,   1e-4 ))  tree_handler . plot ()",
            "title": "min_impurity_split"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#min_impurity_decrease",
            "text": "tree_handler . set_method ( \"min_impurity_decrease\" )  tree_handler . cross_me ( * np . arange ( 0.0 ,   5e-4 ,   1e-5 ))  tree_handler . plot ()",
            "title": "min_impurity_decrease"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#bias-variance-trade-off",
            "text": "+================+================+\n       /\\                |                /\\\n      /  \\               |               /  \\\n     /    \\              |              /    \\\n    /      \\             |             /      \\\n   /        \\            |            /        \\\n  / variance \\           |           /   bias   \\\n  ^^^^^^^^^^^^           |           ^^^^^^^^^^^^\n                         |\n                         |\n                         |\noverfitting   <----------+--------->   underfitting    Bias is an error coming from wrong model assumptions, which do not allow an algorithm to learn all patterns from training data.    Variance is an error coming from sensivity to features specific for training data.    High bias leads to underfitting and high variance to overfitting.    Total error also depends on irreducible error ( noise  that can not be reduced by algorithm)    Ultmiate goal is to minimize the total error    # fake bias, variance and noise  complexity   =   np . arange ( 1 ,   2 ,   0.1 )  variance   =   np . power ( complexity ,   5 )  bias2   =   variance [:: - 1 ]  irreducible   =   [ 10 * np . random . normal ( abs ( x   -   1.5 ),   0.01 )   for   x   in   complexity ]  # total error = variance + bias^2 + irreducible  total   =   variance   +   bias2   +   irreducible  plt . xticks ([])  plt . yticks ([])  plt . xlabel ( \"Algorithm complexity\" )  plt . ylabel ( \"Error\" )  plt . plot ( complexity ,   variance ,   'C0o-' ,   label = 'Variance' )  plt . plot ( complexity ,   bias2 ,   'C1o-' ,   label = \"Bias^2\" )  plt . plot ( complexity ,   total ,   'C2o-' ,   label = \"Total = Bias^2 + Variance + Irreducible error\" )  plt . plot ([ 1.5 ,   1.5 ],   [ 0 ,   25 ],   'C3--' )  plt . text ( 1.0 ,   7 ,   \"$\\longleftarrow$ better chance of generalizing\" ,   color = 'C0' )  plt . text ( 1.6 ,   7 ,   \"better chance of approximating $\\longrightarrow$\" ,   color = 'C1' )  plt . legend ();      Decision trees are sensitive to splits - small changes in training data may change a tree structure    deep trees tend to have high variance and low bias    shallow trees tend to have low variance and high bias",
            "title": "Bias-Variance trade-off"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#quick-math",
            "text": "",
            "title": "Quick math"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#basic",
            "text": "The general goal of regression is to find how some dependent variable (target,  y y ) is changing when independent variable (feature,  x x ) varies    Lets assume there is some  true  relationship describing this dependence  y = f(x) y = f(x)    We want to find  f(x) f(x)  from observations of  (x, y) (x, y)  pairs    Although, in real life we get some noisy observation, so  y = f(x) + \\epsilon y = f(x) + \\epsilon    As we do not know function  f(x) f(x)  we want to approximate it with some other function  g(x) g(x)  (estimator)    In general,  g(x) g(x)  is a parametrized model which can take many possible functional form   e.g.  g(x) = a\\cdot x^2 + b\\cdot x + c g(x) = a\\cdot x^2 + b\\cdot x + c  can take different coefficients (based on a training dataset)",
            "title": "Basic"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#bias-and-variance",
            "text": "Lets imagine there are  N N  possible training datasets  \\{D_1, D_2, ..., D_N\\} \\{D_1, D_2, ..., D_N\\}    For a given dataset one gets an estimators  g^{(D)}(x) g^{(D)}(x)    Lets denote the expected estimator by  \\mathbf{E}_{D}\\left[g^{(D)}(x)\\right] \\equiv \\bar g(x) \\mathbf{E}_{D}\\left[g^{(D)}(x)\\right] \\equiv \\bar g(x)    If  N N  is large we can approximate it by an average over all datasets (law of large numbers)  \\bar g(x) \\approx \\frac{1}{N}\\sum\\limits_{i=1}^N g^{(D_i)}(x) \\bar g(x) \\approx \\frac{1}{N}\\sum\\limits_{i=1}^N g^{(D_i)}(x)    The  variance  of an estimator tells us how far particular predictions are from the mean value  var = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\bar g(x)\\right)^2\\right] var = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\bar g(x)\\right)^2\\right]    Thus, if the training does not depend on the choice of a dataset the variance is low    The  bias  of an estimator tells us how far the mean value is from the true value  bias = \\bar g(x) - f(x) bias = \\bar g(x) - f(x)    Thus, if the model decribes data accurately the bias is low    Please note the hidden assumption that all possible values of  x x  are equally likely",
            "title": "Bias and variance"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#goodness-of-a-model",
            "text": "The common practice to determine the goodness of a model fit is to calculate mean squared error    The mean squared error is, well, the mean value of error squared:  mse = \\mathbf{E}_{x}\\left[\\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - y\\right)^2\\right]\\right] mse = \\mathbf{E}_{x}\\left[\\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - y\\right)^2\\right]\\right]    Lets consider MSE for a particlar point  x x ,  y = f(x) + \\epsilon y = f(x) + \\epsilon , so  mse = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - y\\right)^2\\right] = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x)\\right)^2\\right]- 2\\cdot\\mathbf{E}_{D}\\left[g^{(D)}(x)\\cdot y\\right] + \\mathbf{E}_{D}\\left[y^2\\right] mse = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - y\\right)^2\\right] = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x)\\right)^2\\right]- 2\\cdot\\mathbf{E}_{D}\\left[g^{(D)}(x)\\cdot y\\right] + \\mathbf{E}_{D}\\left[y^2\\right]    Here, we used the linearity of the expected value operator. Lets use another common property:  \\mathbf{E}\\left[X^2\\right] = \\mathbf{E}\\left[\\left(X - \\mathbf{E}\\left[X\\right]\\right)^2\\right] + \\mathbf{E}\\left[X\\right]^2 \\mathbf{E}\\left[X^2\\right] = \\mathbf{E}\\left[\\left(X - \\mathbf{E}\\left[X\\right]\\right)^2\\right] + \\mathbf{E}\\left[X\\right]^2      So the first term can be rewritten in the form  \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x)\\right)^2\\right] = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\mathbf{E}_{D}\\left[g^{(D)}(x)\\right]\\right)^2\\right] + \\mathbf{E}_{D}\\left[g^{(D)}(x)\\right]^2 = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\bar g(x)\\right)^2\\right] + \\left(\\bar g(x)\\right)^2 \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x)\\right)^2\\right] = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\mathbf{E}_{D}\\left[g^{(D)}(x)\\right]\\right)^2\\right] + \\mathbf{E}_{D}\\left[g^{(D)}(x)\\right]^2 = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\bar g(x)\\right)^2\\right] + \\left(\\bar g(x)\\right)^2    And the last term  \\mathbf{E}_{D}\\left[y^2\\right] = \\mathbf{E}_{D}\\left[\\left(y - \\mathbf{E}_{D}\\left[y\\right]\\right)^2\\right] + \\mathbf{E}_{D}\\left[y\\right]^2 = \\mathbf{E}_{D}\\left[\\left(y - f(x)\\right)^2\\right] + \\left(f(x)\\right)^2 \\mathbf{E}_{D}\\left[y^2\\right] = \\mathbf{E}_{D}\\left[\\left(y - \\mathbf{E}_{D}\\left[y\\right]\\right)^2\\right] + \\mathbf{E}_{D}\\left[y\\right]^2 = \\mathbf{E}_{D}\\left[\\left(y - f(x)\\right)^2\\right] + \\left(f(x)\\right)^2    Here, we used the fact that  \\mathbf{E}_{D}\\left[y\\right] = f(x) \\mathbf{E}_{D}\\left[y\\right] = f(x)  (noise would average out when averaging over  infinite  number of datasets)    For the middle term we use the fact that for independent  X X  and  Y Y :  \\mathbf{E}\\left[XY\\right] = \\mathbf{E}\\left[X\\right]\\cdot\\mathbf{E}\\left[Y\\right] \\mathbf{E}\\left[XY\\right] = \\mathbf{E}\\left[X\\right]\\cdot\\mathbf{E}\\left[Y\\right] , so  \\mathbf{E}_{D}\\left[g^{(D)}(x)\\cdot y\\right] = \\bar g(x)\\cdot f(x) \\mathbf{E}_{D}\\left[g^{(D)}(x)\\cdot y\\right] = \\bar g(x)\\cdot f(x)    Taking all together we get  mse = \\underbrace{\\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\bar g(x)\\right)^2\\right]}_{variance} + \\underbrace{\\left(\\bar g(x) - f(x)\\right)^2}_{bias^2} + \\underbrace{\\mathbf{E}_{D}\\left[\\left(y - f(x)\\right)^2\\right]}_{noise} mse = \\underbrace{\\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\bar g(x)\\right)^2\\right]}_{variance} + \\underbrace{\\left(\\bar g(x) - f(x)\\right)^2}_{bias^2} + \\underbrace{\\mathbf{E}_{D}\\left[\\left(y - f(x)\\right)^2\\right]}_{noise}",
            "title": "Goodness of a model"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#example_4",
            "text": "Lets consider  f(x) = \\sin(\\pi x) f(x) = \\sin(\\pi x)    With a noise given by a zero-mean Gaussian with a variance  \\sigma^2 \\sigma^2    So the observation  y = f(x) + \\mathcal{N}(0, \\sigma^2) y = f(x) + \\mathcal{N}(0, \\sigma^2)    from   math   import   sin ,   cos ,   pi ,   exp  def   get_dataset ( N = 20 ,   sigma = 0.1 ): \n   \"\"\"Generate N training samples\"\"\" \n   # X is a set of random points from [-1, 1] \n   X   =   2   *   np . random . sample ( N )   -   1 \n   # Y are corresponding target values (with noise included) \n   Y   =   np . array ([ sin ( pi * x )   +   np . random . normal ( 0 ,   sigma )   for   x   in   X ]) \n\n   return   X ,   Y  # plot a sample  X ,   Y   =   get_dataset ()  x_   =   np . arange ( - 1 ,   1 ,   0.01 )  plt . scatter ( X ,   Y ,   color = 'C1' )  plt . plot ( x_ ,   np . sin ( np . pi   *   x_ ),   'C0--' );      We do not know  f(x) f(x)    We assume it is a polynomial    Lets consider polynomials of orders:  1 - 9 1 - 9    g_1(x) = a_1\\cdot x + a_0 g_1(x) = a_1\\cdot x + a_0    g_2(x) = a_2\\cdot x^2 + \\cdots + a_0 g_2(x) = a_2\\cdot x^2 + \\cdots + a_0    g_3(x) = a_3\\cdot x^3 + \\cdots + a_0 g_3(x) = a_3\\cdot x^3 + \\cdots + a_0    ...      Lets assume we have 100 independent dataset    Each one has 20 points  (x, y = \\sin(\\pi x) + \\mathcal{N}(0, \\sigma^2)) (x, y = \\sin(\\pi x) + \\mathcal{N}(0, \\sigma^2))    # generate 100 datasets with default settings  datasets   =   [ get_dataset ()   for   i   in   range ( 100 )]  # and plot them all together with true signal  for   i   in   range ( 100 ): \n   plt . scatter ( datasets [ i ][ 0 ],   datasets [ i ][ 1 ],   marker = '.' )  plt . plot ( x_ ,   np . sin ( np . pi   *   x_ ),   'C0--' );     Now we need to fit each polynomial to each dataset separately   def   get_fit ( N ,   data ): \n   \"\"\"Find a fit of polynomial of order N to data = (X, Y)\"\"\" \n   return   np . poly1d ( np . polyfit ( data [ 0 ],   data [ 1 ],   N ))  # for the whole range of possible polynomials orders  # create a list of fits to different datasets  fits   =   [[ get_fit ( order ,   data )   for   data   in   datasets ]   for   order   in   range ( 1 ,   10 )]   plt . figure ( figsize = ( 13 ,   10 ))  for   order   in   range ( 1 ,   10 ): \n   plt . subplot ( 3 ,   3 ,   order ) \n   plt . ylim ([ - 1.5 , 1.5 ]) \n\n   for   g   in   fits [ order   -   1 ]: \n     plt . plot ( x_ ,   g ( x_ ),   'C1-' ,   linewidth = 0.1 ) \n\n   plt . plot ( x_ ,   np . sin ( np . pi   *   x_ ),   'C0--' ) \n\n   plt . title ( \"Polynomial of order {}\" . format ( order ));  plt . tight_layout ();",
            "title": "Example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#training-and-test-errors",
            "text": "In real life is impossible (unless one creates data by hand) to calculate true variance and bias    One would need all possible datasets  D D  and all possible input values  x x    Thus, usually one looks at training and test errors    Training error is measured on the data used to make a fit    Test/validation error is measured on unseen data      # fake error  complexity   =   np . arange ( 0.1 ,   2 ,   0.1 )  train_error   =   - np . log ( complexity )  test_error   =   - np . log ( complexity )   +   np . power ( complexity ,   1 )  plt . xticks ([])  plt . yticks ([])  plt . xlabel ( \"Algorithm complexity\" )  plt . ylabel ( \"Error\" )  plt . plot ( complexity ,   train_error ,   'C0o-' ,   label = 'Training error' )  plt . plot ( complexity ,   test_error ,   'C1o-' ,   label = \"Test error\" )  plt . text ( 0.1 ,   0.25 ,   \"$\\longleftarrow$ high bias\" ,   color = 'C0' )  plt . text ( 1.5 ,   0.25 ,   \"high variance $\\longrightarrow$\" ,   color = 'C1' )  plt . legend ();      High training error indicates high bias (which means underfitting)    Training error must decrease with model complexity    If the training error is high:    Use more complex model (or new model architecture)    Use more features - maybe there is just not enough information to make a good prediction    Train longer (if the algorithm is an iterative optimization problem)    Decrease regularization (next lecture)      Test error deacreses with model complexity up to a point when algorithm is to sensitive to features seen in training data    If test error starts to increase it indicates high variance (which means overfitting)    If test error is high:    Use more data - easy to say hard to do...    Use less features    Increase regularization     Use different model",
            "title": "Training and test errors"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#ensemble-learning",
            "text": "Lets first define a  weak learner  as a classifier which is just slighlty better than random guessing    The idea behind ensemble learning is to create a  strong learner  as a combination of many  weak learners    We will discuss two popular ensemble methods:    Bagging ( b ootstrap  agg regat ing ), e.g. random forest    Boosting, e.g. boosted decision tress",
            "title": "Ensemble learning"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#random-forest",
            "text": "Once we know a way to produce a tree we can create a forest    And each tree contributes to a final prediction    It is a  random  forest, because each tree is randomly  incomplete  - trained only on a random subsets of samples and features ( features bagging )    The final prediction of a random forest is an avearge predictions (for regression) or a majority vote (classification)",
            "title": "Random forest"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#intuitive-naive-example",
            "text": "Imagine you want to go to a cinema and need to choose a movie to watch    You can ask a friend about the recommendation    She/he would ask you about movies you watched in the past    and (based on your answers) create a set of rules (a decision tree)    to finally recommend you a movie (make a prediction)      Alternatively, you can ask many friends for an advice    Each friend would ask you random questions to give an answer    At the end, you choose a movie with most votes",
            "title": "Intuitive / naive example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#the-algorithm",
            "text": "Lets imagine we have  N N  samples in our dataset (e.g.  N N  movies you watched)    Each sample has  M M  features (e.g. do you like a movie? do you like the leading actor / actress or director?)    To create a tree take  n n  random samples from the dataset and  at each node  select  m << M m << M  features ( m \\sim \\sqrt M m \\sim \\sqrt M ) to find the best predictor    Repeat the procedure for next trees until you reach desired size of a forest                        +------------+\n                    |            |\n                    |   Dataset  |\n         +----------+            +----------+\n         |          | N features |          |\n         |          |            |          |\n         v          +------------+          v                    Hyperparameters:\n\n+-----------------+                 +-----------------+\n|                 |                 |                 |\n| Random subset 1 |      . . .      | Random subset T |\n|                 |                 |                 |              - the number of trees\n|   N features    |                 |   N features    |              - the size of subsets\n|                 |                 |                 |\n+--------+--------+                 +--------+--------+\n         |                                   |\n         v                                   v\n\n+-----------------+                 +-----------------+\n|                 |                 |                 |\n|     Tree 1      |      . . .      |     Tree T      |\n|                 |                 |                 |\n+--------+--------+                 +-----------------+\n         |\n         |          +-------------+          +--------+\n         |          |             |          |        |\n         +--------> | M1 features +--------> | Node 1 |              - the number of random features\n         |          |             |          |        |              - the size of a single tree\n         |          +-------------+          +--------+\n         |\n         |          +-------------+          +--------+\n         |          |             |          |        |\n         +--------> | M2 features +--------> | Node 2 |\n         |          |             |          |        |\n         |          +-------------+          +--------+\n\n                           .\n                           .\n                           .",
            "title": "The algorithm"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#boosted-trees",
            "text": "The idea is similar to bagging    The key differences are:    Data is reweighted every time a  weak learner  is added (so future learners focus on misclassified samples)    The final prediction is weighted average (better classifiers have higher weights)                                  Bagging (parallel)\n\n       +--------------------+                +----------------+\n       |                    |                |                |\n+----> |      Dataset       +--------------> | Weak learner 1 |\n|      |                    |                |                |\n|      +--------------------+                +----------------+\n|\n|\n|      +--------------------+                +----------------+\n|      |                    |                |                |\n+----> |      Dataset       +------------->  | Weak learner 2 |\n|      |                    |                |                |\n|      +--------------------+       .        +----------------+\n|                                   .\n|                                   .\n|      +--------------------+                +----------------+\n|      |                    |                |                |\n+----> |      Dataset       +------------->  | Weak learner N |\n       |                    |                |                |\n       +--------------------+                +----------------+  \\hspace{140pt}\\text{output} = \\frac{1}{N}\\sum \\text{output}_i \\hspace{140pt}\\text{output} = \\frac{1}{N}\\sum \\text{output}_i                               Boosting (sequential)\n\n      +--------------------+                +----------------+\n      |                    |                |                |\n      |      Dataset       +--------------> | Weak learner 1 |\n      |                    |                |                |\n      +--------------------+                +--------+-------+\n                                                     |\n                +------------------------------------+\n                |\n                v\n\n      +--------------------+                +----------------+\n      |                    |                |                |\n      | Reweighted dataset +------------->  | Weak learner 2 |\n      |                    |                |                |\n      +--------------------+                +--------+-------+\n                                                     |\n                +------------     . . .      --------+\n                |\n                v\n\n      +--------------------+                +----------------+\n      |                    |                |                |\n      | Reweighted dataset +------------->  | Weak learner N |\n      |                    |                |                |\n      +--------------------+                +----------------+  \\hspace{130pt}\\text{output} = \\sum w_i\\cdot \\text{output}_i \\hspace{130pt}\\text{output} = \\sum w_i\\cdot \\text{output}_i",
            "title": "Boosted trees"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#adaboost",
            "text": "AdaBoost is on of the most famous algorithms in machine learning    Y. Freund and R. Schapire got a G\u00f6del Prize for this    Lets consider  N N  labled training examples  (x_1, y_1), \\cdots, (x_N, y_N) (x_1, y_1), \\cdots, (x_N, y_N) , where  x_i \\in X x_i \\in X  and  y_i = \\left\\{-1, 1\\right\\} y_i = \\left\\{-1, 1\\right\\}    The initial distribution is initizlized with  D_1(i) = \\frac{1}{N} D_1(i) = \\frac{1}{N} , where  i = 1, \\cdots, N i = 1, \\cdots, N  (so every sample is equaly likely)    For  t = 1, \\cdots, T t = 1, \\cdots, T :    train a weak learner using  D_t D_t ,  h_t: X \\rightarrow \\left\\{-1, 1\\right\\} h_t: X \\rightarrow \\left\\{-1, 1\\right\\}    choose the one which minimizes the weighted error  \\epsilon_t = \\sum\\limits_{i=1}^{N}D_t(i)\\delta\\left(h_t(x_i)\\neq y_i\\right) \\epsilon_t = \\sum\\limits_{i=1}^{N}D_t(i)\\delta\\left(h_t(x_i)\\neq y_i\\right)    calculate  \\alpha_t \\alpha_t   \\alpha_t = \\frac{1}{2}\\ln\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right) \\alpha_t = \\frac{1}{2}\\ln\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)    For  i = 1, \\cdots, N i = 1, \\cdots, N  update weights according to  D_{t+1} = \\frac{D_t(i)}{Z_t}\\begin{cases}e^{-\\alpha_t} & h_t(x_i) = y_i \\\\ e^{\\alpha_t} & h_t(x_i) \\neq y_i \\end{cases} = \\frac{D_t(i)}{Z_t}e^{-\\alpha_ty_ih_t(x_i)} D_{t+1} = \\frac{D_t(i)}{Z_t}\\begin{cases}e^{-\\alpha_t} & h_t(x_i) = y_i \\\\ e^{\\alpha_t} & h_t(x_i) \\neq y_i \\end{cases} = \\frac{D_t(i)}{Z_t}e^{-\\alpha_ty_ih_t(x_i)}    Z_t Z_t  is a normilization factor so  D_{t+1} D_{t+1}  is a distribution  Z_t = \\sum\\limits_{i=1}^ND_t(i)e^{-\\alpha_ty_ih_t(x_i)} Z_t = \\sum\\limits_{i=1}^ND_t(i)e^{-\\alpha_ty_ih_t(x_i)}      The final hyptohesis  H H  is computes the sign of a weighted combination of weak hypotheses  H(x) = \\text{sign}\\left(\\sum\\limits_{t=1}^T\\alpha_th_t(x)\\right) H(x) = \\text{sign}\\left(\\sum\\limits_{t=1}^T\\alpha_th_t(x)\\right)",
            "title": "AdaBoost"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#out-of-bag-error",
            "text": "Out-of-bag (OOB) error may be used for machine learning models using bootstrap aggregation (like random forest and boosted trees) instead of cross-validation    Bagging involves random sampling with replacement    Some samples are not used in the training process (out-of-bag samples) and therefore can be used to calculate test error    OOB error is the average error over all training samples calculated using predictions from weak classifiers which do not contain particular sample in their bootstrap samples",
            "title": "Out-of-bag error"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#example_5",
            "text": "Lets consider some fake data generated with  make_blobs  from  scikit-learn    and then apply decision trees with different maximum depths    and random forests with different maximum depths",
            "title": "Example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#dataset",
            "text": "sklearn.datasets.make_blobs  allows to generate random Gaussian blobs    We generate 8 blobs with fixed random generator (just to make sure we get the same set every time)    from   sklearn.datasets   import   make_blobs  # generate 5 blobs with fixed random generator  X ,   Y   =   make_blobs ( n_samples = 500 ,   centers = 8 ,   random_state = 300 )  plt . scatter ( * X . T ,   c = Y ,   marker = '.' ,   cmap = 'Dark2' );",
            "title": "Dataset"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#train-and-visualize",
            "text": "To make our life easier we create a function to    plot training data on existing axes or new one (if not provided)    train given classifier on given dataset    create countours representing predictions of the classifier      def   train_and_look ( classifier ,   X ,   Y ,   ax = None ,   title = '' ,   cmap = 'Dark2' ): \n   \"\"\"Train classifier on (X,Y). Plot data and prediction.\"\"\" \n   # create new axis if not provided \n   ax   =   ax   or   plt . gca (); \n\n   ax . set_title ( title ) \n\n   # plot training data \n   ax . scatter ( * X . T ,   c = Y ,   marker = '.' ,   cmap = cmap ) \n\n   # train a cliassifier \n   classifier . fit ( X ,   Y ) \n\n   # create a grid of testing points \n   x_ ,   y_   =   np . meshgrid ( np . linspace ( * ax . get_xlim (),   num = 200 ), \n                        np . linspace ( * ax . get_ylim (),   num = 200 )) \n\n   # convert to an array of 2D points \n   test_data   =   np . vstack ([ x_ . ravel (),   y_ . ravel ()]) . T \n\n   # make a prediction and reshape to grid structure  \n   z_   =   classifier . predict ( test_data ) . reshape ( x_ . shape ) \n\n   # arange z bins so class labels are in the middle \n   z_levels   =   np . arange ( len ( np . unique ( Y ))   +   1 )   -   0.5 \n\n   # plot contours corresponding to classifier prediction \n   ax . contourf ( x_ ,   y_ ,   z_ ,   alpha = 0.25 ,   cmap = cmap ,   levels = z_levels )    Let check how it works on a decision tree classifier with default  sklearn  setting   from   sklearn.tree   import   DecisionTreeClassifier   as   DT  train_and_look ( DT (),   X ,   Y )",
            "title": "Train and visualize"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#decision-tree",
            "text": "We consider decision trees with fixed maximum depths from 1 to 9   # create a figure with 9 axes 3x3  fig ,   ax   =   plt . subplots ( 3 ,   3 ,   figsize = ( 15 , 15 ))  # train and look at decision trees with different max depth  for   max_depth   in   range ( 0 ,   9 ): \n   train_and_look ( DT ( max_depth = max_depth   +   1 ),   X ,   Y , \n                  ax = ax [ max_depth   //   3 ][ max_depth   %   3 ], \n                  title = \"Max depth = {}\" . format ( max_depth   +   1 ))     max_depth  <= 3 - undefitting  max_depth  <= 6 - quite good  max_depth   > 6 - overfitting",
            "title": "Decision tree"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#random-forest_1",
            "text": "Lets do the same with random forests (100 trees in each forest)   from   sklearn.ensemble   import   RandomForestClassifier   as   RF  # create a figure with 9 axes 3x3  fig ,   ax   =   plt . subplots ( 3 ,   3 ,   figsize = ( 15 , 15 ))  # train and look at decision trees with different max depth  for   max_depth   in   range ( 0 ,   9 ): \n   train_and_look ( RF ( n_estimators = 100 ,   max_depth = max_depth   +   1 ),   X ,   Y , \n                  ax = ax [ max_depth   //   3 ][ max_depth   %   3 ], \n                  title = \"Max depth = {}\" . format ( max_depth   +   1 ))      The combination of shallow trees (weak learners) does a good job    Overfitting is somehow prevented",
            "title": "Random forest"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#summary_2",
            "text": "The most important lesson today:  bias-variance trade-off    For the lecture easy examples are chosen so they can be visualize    In real life problems, it is hard / impossible to determine using \"bye eye\" method if the model is underfitted or overfitted    Note, that actually you should never used this method even if you think \"your eye\" is right - you would be surprised how it is not    One needs a way to measure the goodnes of a model - usually mean squared error    In practice, most people use cross-calidation technique       The biggest advantages of decision trees algoritms is that they are east to interpret (it is easy to explain even to non-experts how they work, which is not the case with e.g. deep neural networks)    Usually, decision trees are used as weak learners in ensemble learning    The most famous boosting algorithm is AdaBoost, because it is a good one and the first one. Although, there are many other boosting methods on market right now with XGBoost being one of the most popular one    As for today, deep learning has better publicity, but boosted trees are still one of the most common algorithms used in  Kaggle competitions    Boosted trees are also popular among physicist and used by them as an alternative to neural networks for experimental elementary particle physics in event reconstruction procedures",
            "title": "Summary"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/",
            "text": "Support Vector Machine\n\u00b6\n\n\n\n\n\n\nSupport vector machine (SVM) is a binary linear classifier\n\n\n\n\n\n\nThere are tricks to make SVM able to solve non-linear problems\n\n\n\n\n\n\nThere are extensions which allows using SVM to multiclass classification or regression\n\n\n\n\n\n\nSVM is a supervised learning algorithm\n\n\n\n\n\n\nThere are extensions which allows using SVM for (unsupervised) clustering\n\n\n\n\n\n\nLinear SVM\n\u00b6\n\n\n\n\n\n\nLets consider a training dataset of \nN\nN\n samples \n(\\vec x_1, y_1), \\cdots, (\\vec x_N, y_N)\n(\\vec x_1, y_1), \\cdots, (\\vec x_N, y_N)\n\n\n\n\n\n\n\\vec x_i\n\\vec x_i\n is \nD\nD\n-dimensional vector representing features\n\n\n\n\n\n\ny_i\ny_i\n is a class label, for convenience \ny_i = \\left\\{-1, 1\\right\\}\ny_i = \\left\\{-1, 1\\right\\}\n\n\n\n\n\n\nAt this point we assume that classes are linearly separable\n\n\n\n\n\n\nFor visualization purpose lets use \nD = 2\nD = 2\n\n\n\n\n\n\n# our standard imports: matplotlib and numpy\n\n\nimport\n \nmatplotlib.pyplot\n \nas\n \nplt\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\n\n# just to overwrite default colab style\n\n\nplt\n.\nstyle\n.\nuse\n(\n'default'\n)\n\n\nplt\n.\nstyle\n.\nuse\n(\n'seaborn-talk'\n)\n\n\n\n\n\n\n# class I\n\n\nX01\n \n=\n \n[(\n2\n,\n \n9\n),\n \n(\n7\n,\n \n19\n),\n \n(\n1\n,\n \n10\n),\n \n(\n3\n,\n \n19\n),\n \n(\n4\n,\n \n16\n),\n \n(\n5\n,\n \n18\n)]\n\n\n\n# class II\n\n\nX02\n \n=\n \n[(\n4\n,\n \n3\n),\n \n(\n6\n,\n \n7\n),\n \n(\n1\n,\n \n-\n10\n),\n \n(\n3\n,\n \n-\n1\n),\n \n(\n9\n,\n \n5\n),\n \n(\n5\n,\n \n-\n7\n)]\n\n\n\nplt\n.\nxlim\n([\n0\n,\n \n10\n])\n\n\n\nplt\n.\nscatter\n(\n*\n(\nzip\n(\n*\nX01\n)))\n\n\nplt\n.\nscatter\n(\n*\n(\nzip\n(\n*\nX02\n)));\n\n\n\n\n\n\n\n\n\n\n\n\nIn general we want to find a hyperplane which separates two classes\n\n\n\n\n\n\nIn the example above is just a line\n\n\n\n\n\n\nplt\n.\nscatter\n(\n*\n(\nzip\n(\n*\nX01\n)))\n\n\nplt\n.\nscatter\n(\n*\n(\nzip\n(\n*\nX02\n)))\n\n\n\nplt\n.\nplot\n([\n0\n,\n \n10\n],\n \n[\n0\n,\n \n15\n],\n \n'C2-'\n);\n\n\n\n\n\n\n\n\n\n\n\n\nOnce a hyperplane is found, the classification is straightforward\n\n\n\n\n\n\nIn this case, everything above a line is classified as a blue point and below as an orange point\n\n\n\n\n\n\nHowever, there is infinitely many possible lines / hyperplanes\n\n\n\n\n\n\nplt\n.\nscatter\n(\n*\n(\nzip\n(\n*\nX01\n)))\n\n\nplt\n.\nscatter\n(\n*\n(\nzip\n(\n*\nX02\n)))\n\n\n\nplt\n.\nscatter\n(\n5\n,\n \n9\n,\n \nc\n=\n'C3'\n)\n  \n# test point\n\n\n\nplt\n.\nplot\n([\n0\n,\n \n10\n],\n \n[\n2\n,\n \n22\n],\n \n'C4-'\n);\n\n\nplt\n.\nplot\n([\n0\n,\n \n10\n],\n \n[\n0\n,\n \n15\n],\n \n'C2-'\n);\n\n\n\n\n\n\n\n\n\n\n\n\nOn the plot above two possible lines which correctly classify all training data are drawn\n\n\n\n\n\n\nThe red point represents a test sample\n\n\n\n\n\n\nUsing \"by eye\" method one could say it is rather orange than blue\n\n\n\n\n\n\nThe final predictions depends on the choice of a line though\n\n\n\n\n\n\nThus, one need a criterion to choose the best line / hyperplane\n\n\n\n\n\n\nSVM chooses the hyperplane which is maximally far away from any training point\n\n\n\n\n\n\nplt\n.\nplot\n([\n0\n,\n \n10\n],\n \n[\n0\n,\n \n20\n],\n \n'C2-'\n,\n \nzorder\n=\n0\n)\n\n\nplt\n.\nplot\n([\n0\n,\n \n10\n],\n \n[\n5\n,\n \n25\n],\n \n'C2--'\n,\n \nzorder\n=\n0\n)\n\n\nplt\n.\nplot\n([\n0\n,\n \n10\n],\n \n[\n-\n5\n,\n \n15\n],\n \n'C2--'\n,\n \nzorder\n=\n0\n)\n\n\n\nplt\n.\nscatter\n(\n5\n,\n \n9\n,\n \nc\n=\n'C3'\n)\n  \n# test point\n\n\n\nplt\n.\nannotate\n(\n''\n,\n \n(\n0\n,\n5\n),\n \n(\n0.55\n,\n \n-\n4\n),\n \narrowprops\n=\ndict\n(\narrowstyle\n=\n'<->'\n))\n\n\nplt\n.\ntext\n(\n0.5\n,\n \n-\n1.5\n,\n \n'$m$'\n)\n\n\nplt\n.\ntext\n(\n0.3\n,\n \n2.5\n,\n \n'$m$'\n)\n\n\n\nplt\n.\nannotate\n(\n''\n,\n \n(\n3.05\n,\n18.1\n),\n \n(\n3.8\n,\n \n7.75\n),\n \narrowprops\n=\ndict\n(\narrowstyle\n=\n'<->'\n))\n\n\nplt\n.\ntext\n(\n2.9\n,\n \n14\n,\n \n'$m_i$'\n)\n\n\n\nplt\n.\nscatter\n(\n*\n(\nzip\n(\n*\nX01\n)),\n \nzorder\n=\n1\n)\n\n\nplt\n.\nscatter\n(\n*\n(\nzip\n(\n*\nX02\n)),\n \nzorder\n=\n1\n)\n\n\n\nsv\n \n=\n \nX01\n[:\n2\n]\n \n+\n \nX02\n[:\n2\n]\n\n\n\nplt\n.\nscatter\n(\n*\n(\nzip\n(\n*\nsv\n)),\n \nzorder\n=\n1\n,\n \nfacecolors\n=\n'none'\n,\n \nedgecolors\n=\n'r'\n,\n \ns\n=\n500\n);\n\n\n\n\n\n\n\n\n\n\n\n\nThe goal is to maximize the \nmargin\n \n2m\n2m\n\n\n\n\n\n\nPlease note, that the margin is given by the distance of closest data point to the hyperplane\n\n\n\n\n\n\nWhich means, that the classifier depends only on a small subset of training data, which are called \nsupport vectors\n\n\n\n\n\n\nHard margin\n\u00b6\n\n\n\n\n\n\nAny hyperplane can be defined by an intercept term \nb\nb\n and a normal vector \n\\vec w\n\\vec w\n (which is usually called \nweights\n in machine learning)\n\n\n\n\n\n\nAll the points \n\\vec x\n\\vec x\n on the hyperplane satisfy \n\\vec w \\cdot \\vec x + b = 0\n\\vec w \\cdot \\vec x + b = 0\n\n\n\n\n\n\nIf \n\\vec w \\cdot \\vec x + b~{> \\choose <}~0\n\\vec w \\cdot \\vec x + b~{> \\choose <}~0\n the point is on the \n\\text{one} \\choose \\text{other}\n\\text{one} \\choose \\text{other}\n side\n\n\n\n\n\n\nWe can easily defined the linear classifier \nf(\\vec x) = \\text{sign}\\left(\\vec w \\cdot \\vec x  + b\\right)\nf(\\vec x) = \\text{sign}\\left(\\vec w \\cdot \\vec x  + b\\right)\n\n\n\n\n\n\nThat is why we wanted class labels to be \n-1\n-1\n or \n1\n1\n (instead more \nstandard\n \n0\n0\n and \n1\n1\n)\n\n\n\n\n\n\nAll we need is to find weights\n\n\n\n\n\n\nFunctional margin\n\u00b6\n\n\n\n\n\n\nLets define functional margin of hyperplane \n(\\vec w, b)\n(\\vec w, b)\n w.r.t. a training sample \n(\\vec x_i, y_i)\n(\\vec x_i, y_i)\n as \n\\hat m_i = y_i\\cdot \\left(\\vec w \\cdot \\vec x_i + b\\right)\n\\hat m_i = y_i\\cdot \\left(\\vec w \\cdot \\vec x_i + b\\right)\n\n\n\n\n\n\nIt is only positive if the training sample label has the same sign as prediction\n\n\n\n\n\n\nThe magnitude tells us somewhat about the confidence of a prediction\n\n\n\n\n\n\nBut please note, that we can choose arbitrary factor \nk\nk\n and \n\\text{sign}\\left(\\vec w \\cdot \\vec x  + b\\right) =  \\text{sign}\\left(k\\cdot\\vec w \\cdot \\vec x  + k\\cdot b\\right)\n\\text{sign}\\left(\\vec w \\cdot \\vec x  + b\\right) =  \\text{sign}\\left(k\\cdot\\vec w \\cdot \\vec x  + k\\cdot b\\right)\n\n\n\n\n\n\nFunction margin w.r.t. the whole training dataset is the smallest one \n\\hat m = \\min\\limits_{i = 1, \\cdots, N}\\hat m_i\n\\hat m = \\min\\limits_{i = 1, \\cdots, N}\\hat m_i\n\n\n\n\n\n\nGeometric margin\n\u00b6\n\n\n\n\n\n\nLets \nm_i\nm_i\n be a distance from a training point \n\\vec x_i\n\\vec x_i\n to a decision boundary / hyperplane \n(\\vec w, b)\n(\\vec w, b)\n\n\n\n\n\n\nLets \n\\vec x_h\n\\vec x_h\n be a point on a hyperplane closest to \n\\vec x_i\n\\vec x_i\n\n\n\n\n\n\nShortest distance must be peprendicular to a hyperplane (so parallel to \n\\vec w\n\\vec w\n)\n\n\n\n\n\n\nWe can express \n\\vec x_h\n\\vec x_h\n by \n\\vec x_h = \\vec x_i - y_i\\cdot m_i\\cdot \\frac{\\vec w}{|\\vec w|}\n\\vec x_h = \\vec x_i - y_i\\cdot m_i\\cdot \\frac{\\vec w}{|\\vec w|}\n\n\n\n\n\n\nBecause \n\\vec x_h\n\\vec x_h\n lies on a hyperplane it must fulfill \n\\vec w \\cdot \\vec x_h + b = 0\n\\vec w \\cdot \\vec x_h + b = 0\n, thus \n\\vec w\\left(\\vec x_i - y_i\\cdot m_i\\cdot \\frac{\\vec w}{|\\vec w|}\\right) + b = 0\n\\vec w\\left(\\vec x_i - y_i\\cdot m_i\\cdot \\frac{\\vec w}{|\\vec w|}\\right) + b = 0\n\n\n\n\n\n\nPretty straightforward to solve for \nm_i\nm_i\n \nm_i = y_i\\frac{\\vec w\\cdot \\vec x_i + b}{|\\vec w|} = \\frac{\\hat m_i}{|\\vec w|}\nm_i = y_i\\frac{\\vec w\\cdot \\vec x_i + b}{|\\vec w|} = \\frac{\\hat m_i}{|\\vec w|}\n\n\n\n\n\n\nAs before geometric margin w.r.t. the whole training dataset is the smallest one \nm = \\min\\limits_{i = 1, \\cdots, N} m_i = \\frac{\\hat m}{|\\vec w|}\nm = \\min\\limits_{i = 1, \\cdots, N} m_i = \\frac{\\hat m}{|\\vec w|}\n\n\n\n\n\n\nPlease note, that \nm\nm\n does not change when weights are scaled\n\n\n\n\n\n\n\\hat m = m\n\\hat m = m\n for \n|\\vec w| = 1\n|\\vec w| = 1\n \n\n\n\n\n\n\nThe optimal margin\n\u00b6\n\n\n\n\n\n\nWe want to find the maximum geometric margin \nm\nm\n\n\n\n\n\n\nBut with a constraint that for every sample outside the margin \n\\left.\\begin{array}{cc}\\text{maximize}_{w, b, m} & m\\\\\\text{subject to} & m_i \\geq m\\end{array}\\right.\n\\left.\\begin{array}{cc}\\text{maximize}_{w, b, m} & m\\\\\\text{subject to} & m_i \\geq m\\end{array}\\right.\n\n\n\n\n\n\nAs we discussed, we can choose any normalization of normal vector \n|\\vec w|\n|\\vec w|\n as it does not change the value of \nm_i = y_i\\frac{\\vec w\\cdot \\vec x_i + b}{|\\vec w|}\nm_i = y_i\\frac{\\vec w\\cdot \\vec x_i + b}{|\\vec w|}\n\n\n\n\n\n\nLets choose \n|\\vec w| = \\frac{1}{m}\n|\\vec w| = \\frac{1}{m}\n, so \n\\left.\\begin{array}{cc}\\text{maximize}_{w, b} & \\frac{1}{|\\vec w|}\\\\\\text{subject to} & |\\vec w|\\cdot m_i \\geq 1\\end{array}\\right.\n\\left.\\begin{array}{cc}\\text{maximize}_{w, b} & \\frac{1}{|\\vec w|}\\\\\\text{subject to} & |\\vec w|\\cdot m_i \\geq 1\\end{array}\\right.\n\n\n\n\n\n\nSince maximizing \n|\\vec w|^{-1}\n|\\vec w|^{-1}\n means the same as minimizing \n|\\vec w|\n|\\vec w|\n, we can finally write \n\\left.\\begin{array}{cc}\\text{minimize}_{w,b} & \\frac{1}{2}|\\vec w|^2\\\\\\text{subject to} & \\hat m_i \\geq 1\\end{array}\\right.\n\\left.\\begin{array}{cc}\\text{minimize}_{w,b} & \\frac{1}{2}|\\vec w|^2\\\\\\text{subject to} & \\hat m_i \\geq 1\\end{array}\\right.\n\n\n\n\n\n\nPlease note that we choose \n|\\vec w|^2\n|\\vec w|^2\n over \n|\\vec w|\n|\\vec w|\n to avoid square root, and \n\\frac{1}{2}\n\\frac{1}{2}\n is for math convenience\n\n\n\n\n\n\nNow, all we need is to optimize quadratic function over variables subject to linear constraints (quadratic programming)\n\n\n\n\n\n\nLagrange multipliers\n\u00b6\n\n\n\n\n\n\nLagrange multipliers is the common method used to solve contrained optimization problems\n\n\n\n\n\n\nLets consider the following problem (with \nequality constraint\n): \n\\left.\\begin{array}{cc}\\text{minimize} & f(\\vec x)\\\\\\text{subject to} & g(\\vec x) = 0\\end{array}\\right.\n\\left.\\begin{array}{cc}\\text{minimize} & f(\\vec x)\\\\\\text{subject to} & g(\\vec x) = 0\\end{array}\\right.\n\n\n\n\n\n\nPlease note, that one could also consider \ng(\\vec x) = c\ng(\\vec x) = c\n, where \nc\nc\n is constant (then \ng'(\\vec x) = g(\\vec x) - c = 0\ng'(\\vec x) = g(\\vec x) - c = 0\n)\n\n\n\n\n\n\nLagrange multiplier theorem says that at constrained optimum (if exists) \n\\nabla f\n\\nabla f\n will be parallel to \n\\nabla g\n\\nabla g\n, so \n\\nabla f(\\vec x) = \\lambda\\nabla g(\\vec x)\n\\nabla f(\\vec x) = \\lambda\\nabla g(\\vec x)\n\n\n\n\n\n\n\\lambda\n\\lambda\n - a Lagrange multiplier\n\n\n\n\n\n\nHow it works\n\u00b6\n\n\n\n\nLets consider the following problem: \n\\left.\\begin{array}{cc}\\text{minimize} & f(x, y) = x^2 + y^2\\\\\\text{subject to} & g(x, y) = x\\cdot y - 1 = 0\\end{array}\\right.\n\\left.\\begin{array}{cc}\\text{minimize} & f(x, y) = x^2 + y^2\\\\\\text{subject to} & g(x, y) = x\\cdot y - 1 = 0\\end{array}\\right.\n\n\n\n\ndef\n \nflc\n(\nc\n,\n \nn\n=\n100\n):\n\n  \n\"\"\"Level curves of objective functions\"\"\"\n\n  \nreturn\n \nnp\n.\narray\n([(\nc\n \n*\n \nnp\n.\ncos\n(\nang\n),\n \nc\n \n*\n \nnp\n.\nsin\n(\nang\n))\n\n                   \nfor\n \nang\n \nin\n \nnp\n.\nlinspace\n(\n0\n,\n \n2\n*\nnp\n.\npi\n,\n \nn\n)])\n\n\n\ndef\n \ng\n(\nn\n=\n100\n):\n\n  \n\"\"\"Constraint\"\"\"\n\n  \nreturn\n \nnp\n.\narray\n([(\nx\n,\n \n1.\n/\nx\n)\n \nfor\n \nx\n \nin\n \nnp\n.\nlinspace\n(\n0.1\n,\n \n2.5\n,\n \nn\n)])\n\n\n\n##### PLOT SETTINGS #####\n\n\n\nplt\n.\nfigure\n(\nfigsize\n=\n(\n8\n,\n \n8\n))\n\n\nplt\n.\nxlim\n([\n-\n2.5\n,\n \n2.5\n])\n\n\nplt\n.\nylim\n([\n-\n2.5\n,\n \n2.5\n])\n\n\nplt\n.\ngrid\n(\ncolor\n=\n'0.5'\n,\n \nlinestyle\n=\n'--'\n,\n \nlinewidth\n=\n0.5\n)\n\n\n\n##### LEVEL CURVES OF f(x, y) #####\n\n\n\nfor\n \nc\n \nin\n \n(\n0.2\n,\n \n0.6\n,\n \n1\n,\n \n1.8\n,\n \n2.2\n):\n\n  \nplt\n.\nplot\n(\n*\nflc\n(\nc\n)\n.\nT\n,\n \ncolor\n=\n'C0'\n)\n\n\n\nplt\n.\nplot\n(\n*\nflc\n(\nnp\n.\nsqrt\n(\n2\n))\n.\nT\n,\n \ncolor\n=\n'C0'\n,\n \nlabel\n=\n'$f(x, y) = c$'\n)\n\n\n\n##### CONSTRAINTS #####\n\n\n\nplt\n.\nplot\n(\n*\ng\n()\n.\nT\n,\n \ncolor\n=\n'C1'\n,\n \nlabel\n=\n'$g(x, y) = 0$'\n)\n\n\nplt\n.\nplot\n(\n*-\ng\n()\n.\nT\n,\n \ncolor\n=\n'C1'\n)\n\n\n\n##### INTERSECTIONS #####\n\n\n\nplt\n.\nscatter\n(\n1\n,\n \n1\n,\n \nc\n=\n'C2'\n,\n \nzorder\n=\n4\n)\n\n\nplt\n.\nscatter\n(\nnp\n.\nsqrt\n(\n0.345\n),\n \nnp\n.\nsqrt\n(\n1\n/\n0.345\n),\n \nc\n=\n'C3'\n,\n \nzorder\n=\n4\n)\n\n\n\nplt\n.\nlegend\n();\n\n\n\n\n\n\n\n\n\n\n\n\nBlue lines represent the level curves of the objective function (\nf(x, y) = const\nf(x, y) = const\n)\n\n\n\n\n\n\nOrange lines represent the constraints (\ng(x, y) = 0\ng(x, y) = 0\n)\n\n\n\n\n\n\nLets first consider the case that blue and orange curves are not tangent (red point)\n\n\n\n\n\n\ngoing along the constraint one direction would result in the decrease of the objective function\n\n\n\n\n\n\nwhile going in another direction would result in the increase of the objective function\n\n\n\n\n\n\nthus, this point can not be an optimum\n\n\n\n\n\n\n\n\n\n\nThe only possibility is that a constrained optimum is where curves are tanget (it still may not be the case, but at least it could be)\n\n\n\n\n\n\nLagrangian\n\u00b6\n\n\n\n\n\n\nIn general, there may be many constraints:  \n\\left.\\begin{array}{cc}\\text{minimize} & f(\\vec x)\\\\\\text{subject to} & g_i(\\vec x) = 0, \\hspace{5pt} i = 0,\\cdots, M\\end{array}\\right.\n\\left.\\begin{array}{cc}\\text{minimize} & f(\\vec x)\\\\\\text{subject to} & g_i(\\vec x) = 0, \\hspace{5pt} i = 0,\\cdots, M\\end{array}\\right.\n\n\n\n\n\n\nThen, there are \nM\nM\n Lagrange multipliers \n\\lambda_i\n\\lambda_i\n\n\n\n\n\n\nIt is convenient to define Lagrangian: \n\\mathcal{L}(\\vec x, \\lambda) = f(\\vec x) + \\sum\\limits_{i=1}^M\\lambda_ig_i(\\vec x)\n\\mathcal{L}(\\vec x, \\lambda) = f(\\vec x) + \\sum\\limits_{i=1}^M\\lambda_ig_i(\\vec x)\n\n\n\n\n\n\nand solve: \n\\nabla_{x_1,\\cdots, x_n, \\lambda_1,\\cdots, \\lambda_M} \\mathcal{L}(\\vec x, \\lambda) = 0\n\\nabla_{x_1,\\cdots, x_n, \\lambda_1,\\cdots, \\lambda_M} \\mathcal{L}(\\vec x, \\lambda) = 0\n\n\n\n\n\n\nwhich is equivalent to:  \n\\nabla f(\\vec x) = \\sum\\limits_{i=1}^M\\lambda_i\\nabla g_i(\\vec x)\n\\nabla f(\\vec x) = \\sum\\limits_{i=1}^M\\lambda_i\\nabla g_i(\\vec x)\ng_1(\\vec x) = \\cdots = g_M(\\vec x) = 0\ng_1(\\vec x) = \\cdots = g_M(\\vec x) = 0\n\n\n\n\n\n\nThe optimum (if exists) is always a saddle point of the Lagrangian\n\n\n\n\n\n\nOn the one hand, we want to minimize the Lagrangian over \n\\vec x\n\\vec x\n\n\n\n\n\n\nAnd on the other hand, we want to maximize the Lagrangian over \n\\lambda_i\n\\lambda_i\n\n\n\n\n\n\n\n\n\n\nExample\n\u00b6\n\n\n\n\n\n\nLets consider the objective function \nf(x) = x^2\nf(x) = x^2\n with a constraint \ng(x) = x - 1 = 0\ng(x) = x - 1 = 0\n\n\n\n\n\n\nThe Lagrangian is given by: \n\\mathcal{L}(x, \\lambda) = x^2 + \\lambda\\cdot (x - 1)\n\\mathcal{L}(x, \\lambda) = x^2 + \\lambda\\cdot (x - 1)\n\\frac{\\partial\\mathcal{L}}{\\partial x} = 2x + \\lambda = 0\n\\frac{\\partial\\mathcal{L}}{\\partial x} = 2x + \\lambda = 0\n\\frac{\\partial\\mathcal{L}}{\\partial\\lambda} = x - 1 = 0\n\\frac{\\partial\\mathcal{L}}{\\partial\\lambda} = x - 1 = 0\n\n\n\n\n\n\nIt has a saddle point at \nx = 1\nx = 1\n and \np = -2\np = -2\n\n\n\n\n\n\nfrom\n \nmpl_toolkits.mplot3d\n \nimport\n \nAxes3D\n\n\nfrom\n \nmatplotlib\n \nimport\n \ncm\n\n\n\nax\n \n=\n \nplt\n.\nfigure\n()\n.\nadd_subplot\n(\n111\n,\n \nprojection\n=\nAxes3D\n.\nname\n)\n\n\n\nX\n,\n \nY\n \n=\n \nnp\n.\nmeshgrid\n(\nnp\n.\nlinspace\n(\n-\n1\n,\n \n3\n,\n \n50\n),\n \nnp\n.\nlinspace\n(\n-\n5\n,\n \n1\n,\n \n50\n))\n\n\nL\n \n=\n \nX\n**\n2\n \n+\n \nY\n \n*\n \n(\nX\n \n-\n \n1\n)\n\n\n\nax\n.\nplot_surface\n(\nX\n,\n \nY\n,\n \nL\n,\n \ncmap\n=\ncm\n.\nhsv\n)\n\n\n\nax\n.\nview_init\n(\nelev\n=\n45\n,\n \nazim\n=\n120\n)\n\n\n\nax\n.\nset_xlabel\n(\n'$x$'\n,\n \nlabelpad\n=\n20\n)\n\n\nax\n.\nset_ylabel\n(\n'$\\lambda$'\n,\n \nlabelpad\n=\n20\n)\n\n\nax\n.\nset_zlabel\n(\n'$\\mathcal{L}$'\n,\n \nlabelpad\n=\n10\n);\n\n\n\n\n\n\n\n\nLagrange duality\n\u00b6\n\n\n\n\n\n\nLets consider the optimization problem with inequality constraints: \n\\left.\\begin{array}{cc}\\text{minimize} & f(\\vec x)\\\\\\text{subject to} & g_i(\\vec x) \\leq 0, \\hspace{5pt} i = 0,\\cdots, M\\\\\\text{} & h_i(\\vec x) = 0, \\hspace{5pt} i = 0,\\cdots, N\\end{array}\\right.\n\\left.\\begin{array}{cc}\\text{minimize} & f(\\vec x)\\\\\\text{subject to} & g_i(\\vec x) \\leq 0, \\hspace{5pt} i = 0,\\cdots, M\\\\\\text{} & h_i(\\vec x) = 0, \\hspace{5pt} i = 0,\\cdots, N\\end{array}\\right.\n\n\n\n\n\n\nWe will call it the \nprimal\n optimization problem and define generalized Lagrangian: \n\\mathcal{L}(\\vec x, \\mu, \\lambda) = f(\\vec x) + \\sum\\limits_{i=1}^M\\mu_ig_i(\\vec x) + \\sum\\limits_{i=1}^N\\lambda_ih_i(\\vec x)\n\\mathcal{L}(\\vec x, \\mu, \\lambda) = f(\\vec x) + \\sum\\limits_{i=1}^M\\mu_ig_i(\\vec x) + \\sum\\limits_{i=1}^N\\lambda_ih_i(\\vec x)\n\n\n\n\n\n\nwith the additional restriction that \n\\mu_i \\geq 0\n\\mu_i \\geq 0\n\n\n\n\n\n\nif \ni\ni\n-th constraint is fulfilled, making \n\\mu_i\n\\mu_i\n more positive decrease the Lagrangian\n\n\n\n\n\n\nmaking \n\\mu_i\n\\mu_i\n negative is not allowed\n\n\n\n\n\n\n\n\n\n\nPlease note, that in the case of constraints \ng_i \\geq 0\ng_i \\geq 0\n, the restriction is \n\\mu_i \\leq 0\n\\mu_i \\leq 0\n\n\n\n\n\n\nThe task can be expressed in terms of minimizing the following function: \n\\theta_P(\\vec x) = \\left\\{\\begin{array}{cc}f(\\vec x) & \\text{if x satisfies primal constraints} \\\\ \\infty & \\text{otherwise}\\end{array}\\right. = \\max\\limits_{\\mu, \\lambda; \\mu_i \\geq 0}\\mathcal{L}(\\vec x, \\mu, \\lambda)\n\\theta_P(\\vec x) = \\left\\{\\begin{array}{cc}f(\\vec x) & \\text{if x satisfies primal constraints} \\\\ \\infty & \\text{otherwise}\\end{array}\\right. = \\max\\limits_{\\mu, \\lambda; \\mu_i \\geq 0}\\mathcal{L}(\\vec x, \\mu, \\lambda)\n\n\n\n\n\n\nThus, we can write the final task in the form: \n\\min\\limits_{\\vec x}\\theta_P(\\vec x) = \\min\\limits_{\\vec x}\\max\\limits_{\\mu, \\lambda; \\mu_i \\geq 0}\\mathcal{L}(\\vec x, \\mu, \\lambda)\n\\min\\limits_{\\vec x}\\theta_P(\\vec x) = \\min\\limits_{\\vec x}\\max\\limits_{\\mu, \\lambda; \\mu_i \\geq 0}\\mathcal{L}(\\vec x, \\mu, \\lambda)\n\n\n\n\n\n\nNow, lets consider the following \"reversed\" task: \n\\max\\limits_{\\mu, \\lambda; \\mu_i \\geq 0}\\min\\limits_{\\vec x}\\mathcal{L}(\\vec x, \\mu, \\lambda) = \\max\\limits_{\\mu, \\lambda; \\mu_i \\geq 0}\\theta_D(\\mu, \\lambda)\n\\max\\limits_{\\mu, \\lambda; \\mu_i \\geq 0}\\min\\limits_{\\vec x}\\mathcal{L}(\\vec x, \\mu, \\lambda) = \\max\\limits_{\\mu, \\lambda; \\mu_i \\geq 0}\\theta_D(\\mu, \\lambda)\n\n\n\n\n\n\n\\theta_D(\\mu, \\lambda)\n\\theta_D(\\mu, \\lambda)\n is known as the \ndual function\n and the maximizing it is called the \ndual\n problem\n\n\n\n\n\n\n\\theta_D(\\mu, \\lambda)\n\\theta_D(\\mu, \\lambda)\n is a concave function (becasue Lagrangian is affine, i.e. linear in multipliers) - maximing \n\\theta_D\n\\theta_D\n is a convex optimisation problem\n\n\n\n\n\n\nHow it is related to the primal problem: \n\\mathcal{L}(\\vec x, \\mu, \\lambda) \\leq \\theta_P(\\vec x)\n\\mathcal{L}(\\vec x, \\mu, \\lambda) \\leq \\theta_P(\\vec x)\n \n \n\\min\\limits_{\\vec x}\\mathcal{L}(\\vec x, \\mu, \\lambda) = \\theta_D(\\mu, \\lambda) \\leq \\min\\limits_{\\vec x}\\theta_P(\\vec x)\\equiv p^*\n\\min\\limits_{\\vec x}\\mathcal{L}(\\vec x, \\mu, \\lambda) = \\theta_D(\\mu, \\lambda) \\leq \\min\\limits_{\\vec x}\\theta_P(\\vec x)\\equiv p^*\n \n \nd^* \\equiv \\max\\limits_{\\mu, \\lambda; \\mu_i \\geq 0}\\theta_D(\\mu, \\lambda) \\leq p^*\nd^* \\equiv \\max\\limits_{\\mu, \\lambda; \\mu_i \\geq 0}\\theta_D(\\mu, \\lambda) \\leq p^*\n\n\n\n\n\n\nThus, the optimum of the dual problem \nd^*\nd^*\n is a lower bound for the optimum of the primal problem \np^*\np^*\n (known as weak duality)\n\n\n\n\n\n\nThe difference \np^* - d^*\np^* - d^*\n is known as the duality gap\n\n\n\n\n\n\nIf the gap is zero (\np^* = d^*\np^* = d^*\n) we have \nstrong duality\n\n\n\n\n\n\nIf the optimization problem is convex and (strictly) feasible (i.e. there is \n\\vec x\n\\vec x\n which satisfies all constraints) strong duality holds, so there must exist \n\\vec x^*\n\\vec x^*\n, \n\\mu^*\n\\mu^*\n, \n\\lambda^*\n\\lambda^*\n, so that \n\\vec x^*\n\\vec x^*\n is the solution of the primal problem and \n\\mu^*\n\\mu^*\n and \n\\lambda^*\n\\lambda^*\n are solutions of the dual problem, and \np^* = d^* = \\mathcal{L}(\\vec x^*, \\mu^*, \\lambda^*)\np^* = d^* = \\mathcal{L}(\\vec x^*, \\mu^*, \\lambda^*)\n\n\n\n\n\n\nKKT conditions\n\u00b6\n\n\n\n\n\n\nMoreover, \n\\vec x^*\n\\vec x^*\n, \n\\mu^*\n\\mu^*\n, \n\\lambda^*\n\\lambda^*\n satisfy the \nKarush-Kuhn-Tucker (KKT)\n conditions\n\n\n\n\n\n\nStationarity\n \n\\nabla_{\\vec x} \\mathcal{L}(\\vec x^*, \\mu^*, \\lambda^*) = 0\n\\nabla_{\\vec x} \\mathcal{L}(\\vec x^*, \\mu^*, \\lambda^*) = 0\n\n\n\n\n\n\nPrimal feasibility\n \ng_i(\\vec x^*) \\leq 0, \\hspace{10pt} h_i(\\vec x^*) = 0\ng_i(\\vec x^*) \\leq 0, \\hspace{10pt} h_i(\\vec x^*) = 0\n\n\n\n\n\n\nDual feasibility\n \n\\mu_i^* \\geq 0\n\\mu_i^* \\geq 0\n\n\n\n\n\n\nComplementary slackness (or dual complementarity)\n \n\\mu_i^*\\cdot g_i(\\vec x^*) = 0\n\\mu_i^*\\cdot g_i(\\vec x^*) = 0\n\n\n\n\n\n\nWhy the last one - for strong duality: \nf(\\vec x^*) = \\theta_D(\\mu^*, \\lambda^*) = \\min\\limits_{\\vec x}\\mathcal{L}(\\vec x, \\mu^*, \\lambda^*) \\leq f(\\vec x^*) + \\sum\\limits_{i=1}^M\\mu_i^*\\cdot g_i(\\vec x^*) + \\sum\\limits_{i=1}^N\\lambda_i^*\\cdot h_i(\\vec x ^*) \\leq f(\\vec x^*)\nf(\\vec x^*) = \\theta_D(\\mu^*, \\lambda^*) = \\min\\limits_{\\vec x}\\mathcal{L}(\\vec x, \\mu^*, \\lambda^*) \\leq f(\\vec x^*) + \\sum\\limits_{i=1}^M\\mu_i^*\\cdot g_i(\\vec x^*) + \\sum\\limits_{i=1}^N\\lambda_i^*\\cdot h_i(\\vec x ^*) \\leq f(\\vec x^*)\n\n\n\n\n\n\nThe last inequality holds beacause \n\\sum\\limits_{i=1}^M\\mu_i^*\\cdot g_i(\\vec x^*) \\leq 0\n\\sum\\limits_{i=1}^M\\mu_i^*\\cdot g_i(\\vec x^*) \\leq 0\n\n\n\n\n\n\nSince \nf(\\vec x^*)\nf(\\vec x^*)\n must be equal to \nf(\\vec x^*)\nf(\\vec x^*)\n these inequalities are in fact equalities, so \n\\sum\\limits_{i=1}^M\\mu_i^*\\cdot g_i(\\vec x^*) = 0\n\\sum\\limits_{i=1}^M\\mu_i^*\\cdot g_i(\\vec x^*) = 0\n\n\n\n\n\n\nBecause \n\\mu_i \\geq 0\n\\mu_i \\geq 0\n and \ng_i \\leq 0\ng_i \\leq 0\n we get the complementary slackness condition: \n\\mu_i^*\\cdot g_i(\\vec x^*) = 0\n\\mu_i^*\\cdot g_i(\\vec x^*) = 0\n\n\n\n\n\n\nIf \n\\vec x^*\n\\vec x^*\n, \n\\mu^*\n\\mu^*\n, \n\\lambda^*\n\\lambda^*\n satisfy KKT conditions, then they are primal and dual solutions\n\n\n\n\n\n\nOptimal margin\n\u00b6\n\n\n\n\n\n\nLets back to our original optimization problem: \n\\left.\\begin{array}{cc}\\text{minimize}_{w,b} & \\frac{1}{2}|\\vec w|^2\\\\\\text{subject to} & y_i\\cdot \\left(\\vec w \\cdot \\vec x_i + b\\right) \\geq 1\\end{array}\\right.\n\\left.\\begin{array}{cc}\\text{minimize}_{w,b} & \\frac{1}{2}|\\vec w|^2\\\\\\text{subject to} & y_i\\cdot \\left(\\vec w \\cdot \\vec x_i + b\\right) \\geq 1\\end{array}\\right.\n\n\n\n\n\n\nLets rewrite the constraints in the form: \ng_i(\\vec w) = - y_i\\cdot \\left(\\vec w \\cdot \\vec x_i + b\\right) + 1 \\leq 0\ng_i(\\vec w) = - y_i\\cdot \\left(\\vec w \\cdot \\vec x_i + b\\right) + 1 \\leq 0\n\n\n\n\n\n\nPlease note, that because of the complementary slackness condition, \n\\mu_i > 0\n\\mu_i > 0\n only for the training examples (\n\\vec x_i\n\\vec x_i\n) that have functional margin equal to one (\nsupport vectors\n)\n\n\n\n\n\n\nLets write the Langrangian for this problem: \n\\mathcal{L}(\\vec w, b, \\mu) = \\frac{1}{2}|\\vec w|^2 - \\sum\\limits_{i=1}^M\\mu_i\\cdot \\left[y_i\\cdot\\left(\\vec w \\cdot \\vec x_i + b\\right) - 1\\right]\n\\mathcal{L}(\\vec w, b, \\mu) = \\frac{1}{2}|\\vec w|^2 - \\sum\\limits_{i=1}^M\\mu_i\\cdot \\left[y_i\\cdot\\left(\\vec w \\cdot \\vec x_i + b\\right) - 1\\right]\n\n\n\n\n\n\nDual problem\n\u00b6\n\n\n\n\n\n\nTo find the dual form of the problem we need to minimize the Lagrangian over \n\\vec w\n\\vec w\n and \nb\nb\n: \n\\nabla_{w}\\mathcal{L}(\\vec w, b, \\mu) = \\vec w - \\sum\\limits_{i=1}^M\\mu_iy_i\\vec x_i = 0 \\Rightarrow \\vec w = \\sum\\limits_{i=1}^M\\mu_iy_i\\vec x_i\n\\nabla_{w}\\mathcal{L}(\\vec w, b, \\mu) = \\vec w - \\sum\\limits_{i=1}^M\\mu_iy_i\\vec x_i = 0 \\Rightarrow \\vec w = \\sum\\limits_{i=1}^M\\mu_iy_i\\vec x_i\n\n\n\n\n\n\nAnd for the intercept term: \n\\frac{\\partial}{\\partial b}\\mathcal{L}(\\vec w, b, \\mu) = \\sum\\limits_{i=1}^M\\mu_iy_i = 0\n\\frac{\\partial}{\\partial b}\\mathcal{L}(\\vec w, b, \\mu) = \\sum\\limits_{i=1}^M\\mu_iy_i = 0\n\n\n\n\n\n\nIt is straightforawrd to show, that using two above equations one can obtain:  \n\\nabla_{w}\\mathcal{L}(\\vec w, b, \\mu) = \\sum\\limits_{i=1}^M\\mu_i - \\frac{1}{2}|\\vec w|^2 =  \\sum\\limits_{i=1}^M\\mu_i - \\frac{1}{2}\\sum\\limits_{i,j=1}^My_iy_j\\mu_i\\mu_j\\cdot\\left(\\vec x_i\\cdot \\vec x_j\\right)\n\\nabla_{w}\\mathcal{L}(\\vec w, b, \\mu) = \\sum\\limits_{i=1}^M\\mu_i - \\frac{1}{2}|\\vec w|^2 =  \\sum\\limits_{i=1}^M\\mu_i - \\frac{1}{2}\\sum\\limits_{i,j=1}^My_iy_j\\mu_i\\mu_j\\cdot\\left(\\vec x_i\\cdot \\vec x_j\\right)\n\n\n\n\n\n\nBy examing the dual form the optimization problem is expressed in terms of the \ninner product of input feature vectors\n:  \n\\left.\\begin{array}{cc}\\text{maximize}_{\\mu} & \\sum\\limits_{i=1}^M\\mu_i - \\frac{1}{2}\\sum\\limits_{i,j=1}^My_iy_j\\mu_i\\mu_j\\cdot\\left<\\vec x_i, \\vec x_j\\right>\\\\\\text{subject to} & \\mu_i \\geq 0 \\\\ & \\sum\\limits_{i=1}^M\\mu_iy_i = 0\\end{array}\\right.\n\\left.\\begin{array}{cc}\\text{maximize}_{\\mu} & \\sum\\limits_{i=1}^M\\mu_i - \\frac{1}{2}\\sum\\limits_{i,j=1}^My_iy_j\\mu_i\\mu_j\\cdot\\left<\\vec x_i, \\vec x_j\\right>\\\\\\text{subject to} & \\mu_i \\geq 0 \\\\ & \\sum\\limits_{i=1}^M\\mu_iy_i = 0\\end{array}\\right.\n\n\n\n\n\n\nNon-linear SVM\n\u00b6\n\n\n\n\n\n\nSVM can be applied to non-linear problems using the \nkernel trick\n\n\n\n\n\n\nHowever, before we go there, lets consider a simple non-linear problem\n\n\n\n\n\n\nExample\n\u00b6\n\n\n\n\n\n\nLets consider two classes of 2D points:\n\n\n\n\n\n\ninside a circle\n\n\n\n\n\n\noutside a circle\n\n\n\n\n\n\n\n\n\n\ndef\n \ngenerate_circle_data\n(\nR1\n=\n0\n,\n \nR2\n=\n1\n,\n \nN\n=\n500\n):\n\n  \n\"\"\"Generate N points in a circle for radius range (R1, R2)\"\"\"\n\n  \nr\n \n=\n \nlambda\n:\n \nR1\n \n+\n \nnp\n.\nrandom\n.\nrandom\n()\n \n*\n \n(\nR2\n \n-\n \nR1\n)\n\n\n  \nreturn\n \nnp\n.\narray\n([(\nr\n()\n \n*\n \nnp\n.\ncos\n(\nang\n),\n \nr\n()\n \n*\n \nnp\n.\nsin\n(\nang\n))\n\n                   \nfor\n \nang\n \nin\n \nnp\n.\nlinspace\n(\n0\n,\n \n2\n*\nnp\n.\npi\n,\n \nN\n)])\n\n\n\nC01\n \n=\n \ngenerate_circle_data\n()\n\n\nC02\n \n=\n \ngenerate_circle_data\n(\n1\n,\n \n2\n)\n\n\n\nplt\n.\nscatter\n(\n*\nC01\n.\nT\n,\n \nmarker\n=\n'.'\n)\n\n\nplt\n.\nscatter\n(\n*\nC02\n.\nT\n,\n \nmarker\n=\n'.'\n);\n\n\n\n\n\n\n\n\n\n\n\n\nThere is no way to find a line which separates these classes\n\n\n\n\n\n\nBut would it be possible to add another dimension and find a plane?\n\n\n\n\n\n\nLets consider a \nfeature mapping\n \n\\phi\n\\phi\n which maps \noriginal attributes\n: \n\\phi(x, y) = \\left[\\begin{array}{c} x \\\\ y \\\\ x^2 + y^2\\end{array}\\right]\n\\phi(x, y) = \\left[\\begin{array}{c} x \\\\ y \\\\ x^2 + y^2\\end{array}\\right]\n\n\n\n\n\n\nfrom\n \nmpl_toolkits.mplot3d\n \nimport\n \nAxes3D\n\n\nfrom\n \nmatplotlib\n \nimport\n \ncm\n\n\n\nax\n \n=\n \nplt\n.\nfigure\n()\n.\nadd_subplot\n(\n111\n,\n \nprojection\n=\nAxes3D\n.\nname\n)\n\n\n\nZ01\n \n=\n \nnp\n.\narray\n([\nx\n**\n2\n \n+\n \ny\n**\n2\n \nfor\n \nx\n,\n \ny\n \nin\n \nC01\n])\n\n\nZ02\n \n=\n \nnp\n.\narray\n([\nx\n**\n2\n \n+\n \ny\n**\n2\n \nfor\n \nx\n,\n \ny\n \nin\n \nC02\n])\n\n\n\nax\n.\nscatter\n(\n*\nC01\n.\nT\n,\n \nZ01\n,\n \ncmap\n=\ncm\n.\nhsv\n)\n\n\nax\n.\nscatter\n(\n*\nC02\n.\nT\n,\n \nZ02\n,\n \ncmap\n=\ncm\n.\nhsv\n)\n\n\n\nax\n.\nview_init\n(\nelev\n=\n15\n,\n \nazim\n=\n60\n)\n\n\n\n\n\n\n\n\n\n\n\n\nThe \nnew\n dataset is a linear problem and can be \neasily\n solved with SVM\n\n\n\n\n\n\nAll we need is to replace (in the dual formulation): \n\\left<\\vec x_i, \\vec x_j\\right> \\rightarrow \\left<\\phi(\\vec x_i), \\phi(\\vec x_j)\\right> = \\phi^T(\\vec x_i)\\phi(\\vec x_j)\n\\left<\\vec x_i, \\vec x_j\\right> \\rightarrow \\left<\\phi(\\vec x_i), \\phi(\\vec x_j)\\right> = \\phi^T(\\vec x_i)\\phi(\\vec x_j)\n\n\n\n\n\n\nKernel trick\n\u00b6\n\n\n\n\n\n\nLets define the \nkernel\n (for given feature mapping \n\\phi\n\\phi\n) as \nK(\\vec x_i, \\vec x_j) = \\phi^T(\\vec x_i)\\phi(\\vec x_j)\nK(\\vec x_i, \\vec x_j) = \\phi^T(\\vec x_i)\\phi(\\vec x_j)\n\n\n\n\n\n\nThus, the optimization problem we are trying to solve now is: \n\\left.\\begin{array}{cc}\\text{maximize}_{\\mu} & \\sum\\limits_{i=1}^M\\mu_i - \\frac{1}{2}\\sum\\limits_{i,j=1}^My_iy_j\\mu_i\\mu_j\\cdot K(\\vec x_i, \\vec x_j)\\\\\\text{subject to} & \\mu_i \\geq 0 \\\\ & \\sum\\limits_{i=1}^M\\mu_iy_i = 0\\end{array}\\right.\n\\left.\\begin{array}{cc}\\text{maximize}_{\\mu} & \\sum\\limits_{i=1}^M\\mu_i - \\frac{1}{2}\\sum\\limits_{i,j=1}^My_iy_j\\mu_i\\mu_j\\cdot K(\\vec x_i, \\vec x_j)\\\\\\text{subject to} & \\mu_i \\geq 0 \\\\ & \\sum\\limits_{i=1}^M\\mu_iy_i = 0\\end{array}\\right.\n\n\n\n\n\n\nThe trick is now, that we do not have to calculate (or even know) the mapping \n\\phi\n\\phi\n, which could be in general very expensive\n\n\n\n\n\n\nThe classifier was deinfed as: \n\\hat y = \\text{sign}\\left(\\vec w \\cdot \\vec x  + b\\right)\n\\hat y = \\text{sign}\\left(\\vec w \\cdot \\vec x  + b\\right)\n\n\n\n\n\n\nand its dual form is: \n\\hat y = \\text{sign}\\left(\\sum\\limits_{i=1}^M \\mu_iy_i\\left<\\vec x_i, \\vec x\\right> + b\\right)\n\\hat y = \\text{sign}\\left(\\sum\\limits_{i=1}^M \\mu_iy_i\\left<\\vec x_i, \\vec x\\right> + b\\right)\n\n\n\n\n\n\nand can be now rewritten in the form: \n\\hat y = \\text{sign}\\left(\\sum\\limits_{i=1}^M \\mu_iy_iK(\\vec x_i, \\vec x) + b\\right)\n\\hat y = \\text{sign}\\left(\\sum\\limits_{i=1}^M \\mu_iy_iK(\\vec x_i, \\vec x) + b\\right)\n\n\n\n\n\n\nExample\n\u00b6\n\n\n\n\n\n\nLets consider the feature mapping \n\\phi\n\\phi\n (in 3D): \n\\phi(\\vec x) = \\left[\\begin{array}{c} x_1x_1 \\\\ x_1x_2 \\\\ x_1x_3 \\\\ x_2x_1 \\\\ x_2x_2 \\\\ x_2x_3 \\\\ x_3x_1 \\\\ x_3x_2 \\\\ x_3x_3\\end{array}\\right]\n\\phi(\\vec x) = \\left[\\begin{array}{c} x_1x_1 \\\\ x_1x_2 \\\\ x_1x_3 \\\\ x_2x_1 \\\\ x_2x_2 \\\\ x_2x_3 \\\\ x_3x_1 \\\\ x_3x_2 \\\\ x_3x_3\\end{array}\\right]\n\n\n\n\n\n\nCalculating \n\\phi(\\vec x)\n\\phi(\\vec x)\n, where \n\\vec x \\in \\mathcal{R}^N\n\\vec x \\in \\mathcal{R}^N\n, requires \nO(n^2)\nO(n^2)\n time\n\n\n\n\n\n\nThe related kernel is given by: \nK(\\vec x_i, \\vec x_j) = \\left<x_i, \\vec x_j\\right>^2\nK(\\vec x_i, \\vec x_j) = \\left<x_i, \\vec x_j\\right>^2\n\n\n\n\n\n\nrequires only \nO(n)\nO(n)\n time\n\n\n\n\n\n\nMercer theorem\n\u00b6\n\n\n\n\n\n\nHaving some function \nK\nK\n, how can we tell if it is a valid kernel?\n\n\n\n\n\n\nFor a valid kernel, there must exist a feature mapping \n\\phi\n\\phi\n, so that \nK(\\vec x_i, \\vec x_j) = \\phi^T(\\vec x_i)\\phi(\\vec x_j)\nK(\\vec x_i, \\vec x_j) = \\phi^T(\\vec x_i)\\phi(\\vec x_j)\n\n\n\n\n\n\nLets define the \nkernel matrix\n \nK_{ij} = K(\\vec x_i, \\vec x_j)\nK_{ij} = K(\\vec x_i, \\vec x_j)\n\n\n\n\n\n\nIf \nK\nK\n is a valid kernel the corresponding matrix must be symmetric: \nK_{ij} = \\phi^T(\\vec x_i)\\phi(\\vec x_j) = \\phi^T(\\vec x_j)\\phi(\\vec x_i) = K_{ji}\nK_{ij} = \\phi^T(\\vec x_i)\\phi(\\vec x_j) = \\phi^T(\\vec x_j)\\phi(\\vec x_i) = K_{ji}\n\n\n\n\n\n\nMoreover, \nK\nK\n must be positive semi-definite (\nK \\geq 0\nK \\geq 0\n): \nz^TKz = \\sum\\limits_i\\sum\\limits_jz_iK_{ij}z_j = \\sum\\limits_i\\sum\\limits_jz_i\\phi^T(\\vec x_i)\\phi(\\vec x_j)z_j = \\sum\\limits_i\\sum\\limits_j\\sum\\limits_kz_i\\phi_k(\\vec x_i)\\phi_k(\\vec x_j)z_j = \\sum\\limits_k\\left(\\sum\\limits_iz_i\\phi_k(\\vec x_i)\\right)^2 \\geq 0\nz^TKz = \\sum\\limits_i\\sum\\limits_jz_iK_{ij}z_j = \\sum\\limits_i\\sum\\limits_jz_i\\phi^T(\\vec x_i)\\phi(\\vec x_j)z_j = \\sum\\limits_i\\sum\\limits_j\\sum\\limits_kz_i\\phi_k(\\vec x_i)\\phi_k(\\vec x_j)z_j = \\sum\\limits_k\\left(\\sum\\limits_iz_i\\phi_k(\\vec x_i)\\right)^2 \\geq 0\n\n\n\n\n\n\nMercer theorem\n\n\n\n\n\n\nLet $K: \\mathcal{R}^n\\times \\mathcal{R}^N \\rightarrow \\mathcal{R}$ be given. Then, for $K$ to be a valid kernel, it is necessary and **sufficient** that for any $\\left\\{x_1, \\cdots x_m\\right\\}$, the corresponding kernel matrix is symmetric positive semi-definite.\n\n\n\n\n\nKernel examples\n\u00b6\n\n\n\n\n\n\nGaussian kernel\n \nK(\\vec x, \\vec y) = \\exp\\left(-\\frac{||\\vec x - \\vec y||}{2\\sigma^2}\\right)\nK(\\vec x, \\vec y) = \\exp\\left(-\\frac{||\\vec x - \\vec y||}{2\\sigma^2}\\right)\n\n\n\n\n\n\nPolynomial kernel\n \nK(\\vec x, \\vec y) = \\left(\\vec x \\cdot \\vec y + c\\right)^d\nK(\\vec x, \\vec y) = \\left(\\vec x \\cdot \\vec y + c\\right)^d\n\n\n\n\n\n\nSigmoid kernel\n \nK(\\vec x, \\vec y) = \\tanh\\left(a\\vec x \\cdot \\vec y + c\\right)\nK(\\vec x, \\vec y) = \\tanh\\left(a\\vec x \\cdot \\vec y + c\\right)\n\n\n\n\n\n\nSoft margin\n\u00b6\n\n\n\n\n\n\nSo far, everything was considered with the assumption that data is (linearly) separable\n\n\n\n\n\n\nMapping to a high-dimensional feature space may make data separable, but it can not guarantee that\n\n\n\n\n\n\nAlso, due to some noise in data, some outliers may lead to overfitting\n\n\n\n\n\n\nLets consider the example below\n\n\n\n\n\n\nplt\n.\nplot\n([\n0\n,\n \n10\n],\n \n[\n0\n,\n \n20\n],\n \n'C2-'\n,\n \nzorder\n=\n0\n)\n\n\nplt\n.\nplot\n([\n0\n,\n \n10\n],\n \n[\n4\n,\n \n24\n],\n \n'C3-'\n,\n \nzorder\n=\n0\n)\n\n\n\nplt\n.\nscatter\n(\n3\n,\n \n9\n,\n \nc\n=\n'C1'\n,\n \nmarker\n=\n','\n)\n\n\n\nplt\n.\nscatter\n(\n*\n(\nzip\n(\n*\nX01\n)),\n \nzorder\n=\n1\n)\n\n\nplt\n.\nscatter\n(\n*\n(\nzip\n(\n*\nX02\n)),\n \nzorder\n=\n1\n);\n\n\n\n\n\n\n\n\n\n\n\n\nThe green line represents the optimal decision boundary if the orange square point is not included\n\n\n\n\n\n\nThe red line represents the case when the outlier is considered\n\n\n\n\n\n\nIncluding the extra point drastically changes the result\n\n\n\n\n\n\nThe idea behind the \nsoft margin\n is to allow some points to lie of the \nwrong\n side of a decision boundary  \n\n\n\n\n\n\nplt\n.\nplot\n([\n0\n,\n \n10\n],\n \n[\n0\n,\n \n20\n],\n \n'C2-'\n,\n \nzorder\n=\n0\n)\n\n\nplt\n.\nplot\n([\n0\n,\n \n10\n],\n \n[\n5\n,\n \n25\n],\n \n'C2--'\n,\n \nzorder\n=\n0\n)\n\n\nplt\n.\nplot\n([\n0\n,\n \n10\n],\n \n[\n-\n5\n,\n \n15\n],\n \n'C2--'\n,\n \nzorder\n=\n0\n)\n\n\n\nplt\n.\nscatter\n(\n3\n,\n \n9\n,\n \nc\n=\n'C1'\n,\n \nmarker\n=\n','\n)\n\n\nplt\n.\nannotate\n(\n''\n,\n \n(\n3.05\n,\n8.7\n),\n \n(\n3.5\n,\n \n1.75\n),\n \narrowprops\n=\ndict\n(\narrowstyle\n=\n'<->'\n))\n\n\nplt\n.\ntext\n(\n3.5\n,\n \n4.5\n,\n \n'$\n\\\\\nxi_i$'\n)\n\n\n\nplt\n.\nannotate\n(\n''\n,\n \n(\n0\n,\n5\n),\n \n(\n0.55\n,\n \n-\n4\n),\n \narrowprops\n=\ndict\n(\narrowstyle\n=\n'<->'\n))\n\n\nplt\n.\ntext\n(\n0.5\n,\n \n-\n1.5\n,\n \n'$m$'\n)\n\n\nplt\n.\ntext\n(\n0.3\n,\n \n2.5\n,\n \n'$m$'\n)\n\n\n\nplt\n.\nannotate\n(\n''\n,\n \n(\n3.05\n,\n18.1\n),\n \n(\n3.8\n,\n \n7.75\n),\n \narrowprops\n=\ndict\n(\narrowstyle\n=\n'<->'\n))\n\n\nplt\n.\ntext\n(\n2.9\n,\n \n14\n,\n \n'$m_i$'\n)\n\n\n\nplt\n.\nscatter\n(\n*\n(\nzip\n(\n*\nX01\n)),\n \nzorder\n=\n1\n)\n\n\nplt\n.\nscatter\n(\n*\n(\nzip\n(\n*\nX02\n)),\n \nzorder\n=\n1\n)\n\n\n\nsv\n \n=\n \nX01\n[:\n2\n]\n \n+\n \nX02\n[:\n2\n]\n\n\n\nplt\n.\nscatter\n(\n*\n(\nzip\n(\n*\nsv\n)),\n \nzorder\n=\n1\n,\n \nfacecolors\n=\n'none'\n,\n \nedgecolors\n=\n'r'\n,\n \ns\n=\n500\n);\n\n\n\n\n\n\n\n\nRegularization\n\u00b6\n\n\n\n\n\n\nRegularization is a common technique to prevent overfitting (more about that next week)\n\n\n\n\n\n\nLets reformulate the original optimization problem: \n\\left.\\begin{array}{cc}\\text{minimize}_{w,b} & \\frac{1}{2}|\\vec w|^2 + C\\sum\n\\limits_i\\xi_i\\\\\\text{subject to} & \\hat y_i\\cdot\\left(\\vec w \\cdot \\vec x + b\\right) \\geq 1 - \\xi_i \\\\ & \\xi_i \\geq 0\\end{array}\\right.\n\\left.\\begin{array}{cc}\\text{minimize}_{w,b} & \\frac{1}{2}|\\vec w|^2 + C\\sum\n\\limits_i\\xi_i\\\\\\text{subject to} & \\hat y_i\\cdot\\left(\\vec w \\cdot \\vec x + b\\right) \\geq 1 - \\xi_i \\\\ & \\xi_i \\geq 0\\end{array}\\right.\n\n\n\n\n\n\nWe now allow to have a functional margin less than 1, but if a training point has functional margin \n1 - \\xi_i\n1 - \\xi_i\n we would pay a cost of the objective function being increased by \nC\\xi_i\nC\\xi_i\n\n\n\n\n\n\nThus, \nC\nC\n controls how much we penalize \"bad\" points (with \nC = \\infty\nC = \\infty\n we are back to the original perfectly separate case)\n\n\n\n\n\n\nLets write the Lagrangian for this optimization problem: \n\\mathcal{L}(\\vec w, b, \\xi, \\mu, r) = \\frac{1}{2}|\\vec w|^2 + C\\sum\n\\limits_{i=1}^M \\xi_i - \\sum\\limits_{i=1}^M\\mu_i\\cdot \\left[y_i\\cdot\\left(\\vec w \\cdot \\vec x_i + b\\right) - 1 + \\xi_i\\right] - \\sum\\limits_{i=1}^Mr_i\\xi_i\n\\mathcal{L}(\\vec w, b, \\xi, \\mu, r) = \\frac{1}{2}|\\vec w|^2 + C\\sum\n\\limits_{i=1}^M \\xi_i - \\sum\\limits_{i=1}^M\\mu_i\\cdot \\left[y_i\\cdot\\left(\\vec w \\cdot \\vec x_i + b\\right) - 1 + \\xi_i\\right] - \\sum\\limits_{i=1}^Mr_i\\xi_i\n\n\n\n\n\n\nAfter the same procedure as before we get similar dual problem: \n\\left.\\begin{array}{cc}\\text{maximize}_{\\mu} & \\sum\\limits_{i=1}^M\\mu_i - \\frac{1}{2}\\sum\\limits_{i,j=1}^My_iy_j\\mu_i\\mu_j\\cdot\\left<\\vec x_i, \\vec x_j\\right>\\\\\\text{subject to} & C \\geq \\mu_i \\geq 0 \\\\ & \\sum\\limits_{i=1}^M\\mu_iy_i = 0\\end{array}\\right.\n\\left.\\begin{array}{cc}\\text{maximize}_{\\mu} & \\sum\\limits_{i=1}^M\\mu_i - \\frac{1}{2}\\sum\\limits_{i,j=1}^My_iy_j\\mu_i\\mu_j\\cdot\\left<\\vec x_i, \\vec x_j\\right>\\\\\\text{subject to} & C \\geq \\mu_i \\geq 0 \\\\ & \\sum\\limits_{i=1}^M\\mu_iy_i = 0\\end{array}\\right.\n\n\n\n\n\n\nThe only difference is the extra constraint on \n\\mu_i\n\\mu_i\n, which comes from \n\\frac{\\partial\\mathcal{L}}{\\partial\\xi_i} = C - \\mu_i - r_i = 0\n\\frac{\\partial\\mathcal{L}}{\\partial\\xi_i} = C - \\mu_i - r_i = 0\n\n\n\n\n\n\nLets consider also complementary slackness conditions: \n\\mu_i\\cdot\\left[y_i\\cdot\\left(\\vec w \\cdot \\vec x_i + b\\right) - 1 + \\xi_i\\right] = 0\n\\mu_i\\cdot\\left[y_i\\cdot\\left(\\vec w \\cdot \\vec x_i + b\\right) - 1 + \\xi_i\\right] = 0\n \n \nr_i\\xi_i = 0\nr_i\\xi_i = 0\n\n\n\n\n\n\nIn the case \n\\mu_i = 0\n\\mu_i = 0\n, \nr_i = C > 0 \\Rightarrow \\xi_i = 0\nr_i = C > 0 \\Rightarrow \\xi_i = 0\n: \ny_i\\cdot\\left(\\vec w \\cdot \\vec x + b\\right) \\geq 1\ny_i\\cdot\\left(\\vec w \\cdot \\vec x + b\\right) \\geq 1\n\n\n\n\n\n\nIn the case \n\\mu_i = C\n\\mu_i = C\n, \nr_i = 0 \\Rightarrow \\xi_i \\geq 0\nr_i = 0 \\Rightarrow \\xi_i \\geq 0\n: \ny_i\\cdot\\left(\\vec w \\cdot \\vec x + b\\right) \\leq 1\ny_i\\cdot\\left(\\vec w \\cdot \\vec x + b\\right) \\leq 1\n\n\n\n\n\n\nIn the case \n0 < \\mu_i < C\n0 < \\mu_i < C\n, \nr_i = C - \\mu_i > 0 \\Rightarrow \\xi_i = 0\nr_i = C - \\mu_i > 0 \\Rightarrow \\xi_i = 0\n: \ny_i\\cdot\\left(\\vec w \\cdot \\vec x + b\\right) = 1\ny_i\\cdot\\left(\\vec w \\cdot \\vec x + b\\right) = 1\n\n\n\n\n\n\nSMO algorithm\n\u00b6\n\n\n\n\n\n\nWe need to solve:  \n\\left.\\begin{array}{cc}\\text{maximize}_{\\mu} & \\sum\\limits_{i=1}^M\\mu_i - \\frac{1}{2}\\sum\\limits_{i,j=1}^My_iy_j\\mu_i\\mu_jk_{ij} \\equiv \\mathcal{L}_D(\\mu)\\\\\\text{subject to} & C \\geq \\mu_i \\geq 0 \\\\ & \\sum\\limits_{i=1}^M\\mu_iy_i = 0\\end{array}\\right.\n\\left.\\begin{array}{cc}\\text{maximize}_{\\mu} & \\sum\\limits_{i=1}^M\\mu_i - \\frac{1}{2}\\sum\\limits_{i,j=1}^My_iy_j\\mu_i\\mu_jk_{ij} \\equiv \\mathcal{L}_D(\\mu)\\\\\\text{subject to} & C \\geq \\mu_i \\geq 0 \\\\ & \\sum\\limits_{i=1}^M\\mu_iy_i = 0\\end{array}\\right.\n\n\n\n\n\n\nwhere (for convenience) \nk_{ij} = K(\\vec x_i, \\vec x_j)\nk_{ij} = K(\\vec x_i, \\vec x_j)\n\n\n\n\n\n\nSequential minimal optimization (SMO) algorithm solves the smallest possible optimization problem at a time\n\n\n\n\n\n\nBy updating two Lagrange multipliers \n\\mu_i\n\\mu_i\n in a step\n\n\n\n\n\n\nOne can not update only one because of the constraint \n\\sum\\limits_i\\mu_iy_i = 0\n\\sum\\limits_i\\mu_iy_i = 0\n\n\n\n\n\n\nQuick math\n\u00b6\n\n\n\n\n\n\nWithout loss of generality, lets consider optimizing \n\\mu_1\n\\mu_1\n and \n\\mu_2\n\\mu_2\n from an old set of feasible solution: \n\\mu_1^{old}, \\mu_2^{old}, \\mu_3 \\cdots \\mu_M\n\\mu_1^{old}, \\mu_2^{old}, \\mu_3 \\cdots \\mu_M\n\n\n\n\n\n\nBecause of the constraint, \ny_1\\mu_1 + y_2\\mu_2 = y_1\\mu_1^{old} + y_2\\mu_2^{old}\ny_1\\mu_1 + y_2\\mu_2 = y_1\\mu_1^{old} + y_2\\mu_2^{old}\n\n\n\n\n\n\nWhich can be rewritten as: \n\\mu_1 = \\gamma - s\\mu_2\n\\mu_1 = \\gamma - s\\mu_2\n\n\n\n\n\n\nwhere \ns = y_1y_2\ns = y_1y_2\n and \n\\gamma = \\mu_1 + s\\mu_2 = \\mu_1^{old} + s\\mu_2^{old}\n\\gamma = \\mu_1 + s\\mu_2 = \\mu_1^{old} + s\\mu_2^{old}\n\n\n\n\n\n\nThus, the optimization is on a line as shown below (two possibilities based on the sign of \ns\ns\n):\n\n\n\n\n\n\nplt\n.\nxticks\n([],\n \n[])\n\n\nplt\n.\nyticks\n([],\n \n[])\n\n\nplt\n.\nxlim\n([\n0\n,\n1\n])\n\n\nplt\n.\nylim\n([\n0\n,\n1\n])\n\n\n\nplt\n.\ntext\n(\n1.05\n,\n \n0.5\n,\n \n\"$\\mu_1 = C$\"\n)\n\n\nplt\n.\ntext\n(\n-\n0.1\n,\n \n0.5\n,\n \n\"$\\mu_1 = 0$\"\n)\n\n\nplt\n.\ntext\n(\n0.5\n,\n \n1.05\n,\n \n\"$\\mu_2 = C$\"\n)\n\n\nplt\n.\ntext\n(\n0.5\n,\n \n-\n0.05\n,\n \n\"$\\mu_2 = 0$\"\n)\n\n\n\nplt\n.\nplot\n((\n0.6\n,\n1\n),\n \n(\n0\n,\n0.75\n),\n \nlabel\n=\n\"$y_1 \n\\\\\nneq y_2 \\Rightarrow \\mu_1 - \\mu_2 = \\gamma$\"\n)\n\n\nplt\n.\nplot\n((\n0.4\n,\n0\n),\n \n(\n0\n,\n0.75\n),\n  \nlabel\n=\n\"$y_1 = y_2 \\Rightarrow \\mu_1 + \\mu_2 = \\gamma$\"\n)\n\n\n\nplt\n.\nlegend\n();\n\n\n\n\n\n\n\n\n\n\nFirst lets fixed all \n\\mu_i\n\\mu_i\n for \ni \\neq 1,2\ni \\neq 1,2\n and write the objective funtion in the following term:\n\n\n\n\n$\\begin{eqnarray}\n\\mathcal{L}_D & = & \\mu_1 + \\mu_2 + const \\\\\n              & - & \\frac{1}{2}\\left(k_{11}\\mu_1^2 + k_{22}\\mu_2^2 + 2sk_{12}\\mu_1\\mu_2\\right) \\\\\n              & - & y_1\\mu_1\\sum\\limits_{i=3}^My_i\\mu_ik_{1i} - y_2\\mu_2\\sum\\limits_{i=3}^My_2\\mu_2k_{2i} \\\\\n              & + & const\n\\end{eqnarray}$\n\n\n\n\n\n\n\nWhere \nconst\nconst\n does not depend on either \n\\mu_1\n\\mu_1\n nor \n\\mu_2\n\\mu_2\n\n\n\n\n\n\nPlease note, that when formulating the dual problem we got: \n\\nabla_{w}\\mathcal{L}(\\vec w, b, \\mu) = \\vec w - \\sum\\limits_{i=1}^M\\mu_iy_i\\vec x_i = 0 \\Rightarrow \\vec w = \\sum\\limits_{i=1}^M\\mu_iy_i\\vec x_i\n\\nabla_{w}\\mathcal{L}(\\vec w, b, \\mu) = \\vec w - \\sum\\limits_{i=1}^M\\mu_iy_i\\vec x_i = 0 \\Rightarrow \\vec w = \\sum\\limits_{i=1}^M\\mu_iy_i\\vec x_i\n\n\n\n\n\n\nFor convenience, lets introduce:\n\n\n\n\n\n\n$\\begin{eqnarray}\nv _j & = & \\sum\\limits_{i=3}y_i\\mu_ik_{ij} \\\\\n     & = & \\sum\\limits_{i=1}y_i\\mu_i^{old}k_{ij} + b^{old} - y_1\\mu_1^{old}k_{1j} - y_2\\mu_2^{old}k_{2j} - b^{old} \\\\\n     & = & z_j^{old} - b^{old} -  y_1\\mu_1^{old}k_{1j} - y_2\\mu_2^{old}k_{2j}\n\\end{eqnarray}$\n\n\n\n\n\n\n\n\n\nwhere \nz_j^{old} = \\vec x_j \\cdot \\vec w + b^{old}\nz_j^{old} = \\vec x_j \\cdot \\vec w + b^{old}\n is the output of \n\\vec x_j\n\\vec x_j\n under old parameters.\n\n\n\n\n\n\nLets get back to the objective function:\n\n\n\n\n\n\n$\\begin{eqnarray}\n\\mathcal{L}_D & = & \\mu_1 + \\mu_2 + const \\\\\n              & - & \\frac{1}{2}\\left(k_{11}\\mu_1^2 + k_{22}\\mu_2^2 + 2sk_{12}\\mu_1\\mu_2\\right) \\\\\n              & - & y_1\\mu_1v_1 - y_2\\mu_2v_2 + const\n\\end{eqnarray}$\n\n\n\n\n\n\n\nWe can remove \n\\mu_1\n\\mu_1\n from the equation using \n\\mu_1 = \\gamma - s \\mu_2\n\\mu_1 = \\gamma - s \\mu_2\n\n\n\n\n\n\nAfter long (but relatively straightforward) algebra we get: \n\\mathcal{L}_D = \\frac{1}{2}\\eta\\mu_2^2 + \\left[y_2\\cdot\\left(E_1^{old} - E_2^{old}\\right) - \\eta\\mu_2^{old}\\right]\\mu_2 + const\n\\mathcal{L}_D = \\frac{1}{2}\\eta\\mu_2^2 + \\left[y_2\\cdot\\left(E_1^{old} - E_2^{old}\\right) - \\eta\\mu_2^{old}\\right]\\mu_2 + const\n\n\n\n\n\n\nwhere\n\n\n\n\n\n\n$\\begin{eqnarray}\n\\eta & = & 2k_{12} - k_{11} - k_{22} \\\\\nE_i^{old} & = & z_i^{old} - y_i\n\\end{eqnarray}$\n\n\n\n\n\nTo get a maximum we need first derivative to be zero and the second one to be negative\n\n\n\n\n$\\begin{eqnarray}\n\\frac{d\\mathcal{L}_D}{d\\mu_2} & = & \\eta\\mu_2 + \\left[y_2\\cdot\\left(E_1^{old} - E_2^{old}\\right) - \\eta\\mu_2^{old}\\right] = 0 \\\\\n\\frac{d^2\\mathcal{L}_D}{d\\mu_2^2} & = & \\eta\n\\end{eqnarray}$\n\n\n\n\n\n\n\nPlease note, that \n\\eta \\leq 0\n\\eta \\leq 0\n from the definition, but one must be careful when \n\\eta = 0\n\\eta = 0\n\n\n\n\n\n\nIf \n\\eta < 0\n\\eta < 0\n we get the \nunconstrained\n maximum point: \n\\mu_2^{new} = \\mu_2^{old} + \\frac{y_2\\left(E_2^{old} - E_1^{old}\\right)}{\\eta}\n\\mu_2^{new} = \\mu_2^{old} + \\frac{y_2\\left(E_2^{old} - E_1^{old}\\right)}{\\eta}\n\n\n\n\n\n\nHowever, because of the constraint \n0 \\leq \\mu_i \\leq C\n0 \\leq \\mu_i \\leq C\n, we have to clip \n\\mu_2^{new}\n\\mu_2^{new}\n\n\n\n\n\n\nThe minimum and maximum values depends on \ns\ns\n (see the plot above)\n\n\n\n\n\n\nIf \ns = 1\ns = 1\n, then \n\\mu_1 + \\mu_2 = \\gamma\n\\mu_1 + \\mu_2 = \\gamma\n:\n\n\n\n\n\n\nIf \n\\gamma > C\n\\gamma > C\n, then \n\\max\\mu_2 = C\n\\max\\mu_2 = C\n and \n\\min\\mu_2 = \\gamma - C\n\\min\\mu_2 = \\gamma - C\n\n\n\n\n\n\nIf \n\\gamma < C\n\\gamma < C\n, then \n\\max\\mu_2 = \\gamma\n\\max\\mu_2 = \\gamma\n and \n\\min\\mu_2 = 0\n\\min\\mu_2 = 0\n\n\n\n\n\n\n\n\n\n\nIf \ns = -1\ns = -1\n, then \n\\mu_1 - \\mu_2 = \\gamma\n\\mu_1 - \\mu_2 = \\gamma\n:\n\n\n\n\n\n\nIf \n\\gamma > C\n\\gamma > C\n, then \n\\max\\mu_2 = C - \\gamma\n\\max\\mu_2 = C - \\gamma\n and \n\\min\\mu_2 = 0\n\\min\\mu_2 = 0\n\n\n\n\n\n\nIf \n\\gamma < C\n\\gamma < C\n, then \n\\max\\mu_2 = C\n\\max\\mu_2 = C\n and \n\\min\\mu_2 = -\\gamma\n\\min\\mu_2 = -\\gamma\n\n\n\n\n\n\n\n\n\n\nLet the minimum feasible value of \n\\mu_2\n\\mu_2\n be \nL\nL\n, and maximum be \nH\nH\n: \n\\mu_2^{new} \\rightarrow \\left\\{\\begin{array}{ll}H, & \\mu_2^{new} > H \\\\ \\mu_2^{new}, & L \\leq \\mu_2^{new} \\leq H \\\\ L & \\mu_2^{new} < L\\end{array}\\right.\n\\mu_2^{new} \\rightarrow \\left\\{\\begin{array}{ll}H, & \\mu_2^{new} > H \\\\ \\mu_2^{new}, & L \\leq \\mu_2^{new} \\leq H \\\\ L & \\mu_2^{new} < L\\end{array}\\right.\n \n\n\n\n\n\n\nMath summary\n\u00b6\n\n\n\n\n\n\nGiven \n\\mu_1\n\\mu_1\n, \n\\mu_2\n\\mu_2\n and the corresponding \ny_1\ny_1\n, \ny_2\ny_2\n, \nk_{11}\nk_{11}\n, \nk_{12}\nk_{12}\n, \nk_{22}\nk_{22}\n, \n\\Delta E = E_2^{old} - E_1^{old}\n\\Delta E = E_2^{old} - E_1^{old}\n\n\n\n\n\n\nCalculate \n\\eta = 2k_{12} - k_{11} - k_{22}\n\\eta = 2k_{12} - k_{11} - k_{22}\n\n\n\n\n\n\nIf \n\\eta > 0\n\\eta > 0\n: calculate \n\\Delta\\alpha_2 = y_2\\Delta E / \\eta\n\\Delta\\alpha_2 = y_2\\Delta E / \\eta\n, clip the solution to the feasible region, and calculate \n\\Delta\\mu_1 = -s\\Delta\\mu_2\n\\Delta\\mu_1 = -s\\Delta\\mu_2\n\n\n\n\n\n\nIf \n\\eta = 0\n\\eta = 0\n: evaluate the objective function at the two endpoints and choose \n\\mu_2\n\\mu_2\n which correponds to the larger value of the objective function\n\n\n\n\n\n\nUpdating after a successful optimization step\n\u00b6\n\n\n\n\n\n\nLets define the predition error on \n(\\vec x, y)\n(\\vec x, y)\n: \nE(\\vec x, y) = \\sum\\limits_{i=1}^My_i\\mu_iK(\\vec x_i, \\vec x) + b - y\nE(\\vec x, y) = \\sum\\limits_{i=1}^My_i\\mu_iK(\\vec x_i, \\vec x) + b - y\n\n\n\n\n\n\nThe change in error after an update: \n\\Delta E(\\vec x, y) = E^{new}(\\vec x, y) - E^{old}(\\vec x, y) =  y_1\\Delta\\mu_1 K(\\vec x_1, \\vec x) + y_2\\Delta\\mu_2 K(\\vec x_2, \\vec x) + \\Delta b\n\\Delta E(\\vec x, y) = E^{new}(\\vec x, y) - E^{old}(\\vec x, y) =  y_1\\Delta\\mu_1 K(\\vec x_1, \\vec x) + y_2\\Delta\\mu_2 K(\\vec x_2, \\vec x) + \\Delta b\n\n\n\n\n\n\nIf \n\\mu_1\n\\mu_1\n is not at the bounds it forces \nE^{new}(\\vec x_1, y_1) \\equiv E_1 = 0\nE^{new}(\\vec x_1, y_1) \\equiv E_1 = 0\n, thus \n\\Delta b_1 = -E_1 - y_1\\Delta\\mu_1 K(\\vec x_1, \\vec x_1) - y_2\\Delta\\mu_2 K(\\vec x_2, \\vec x_1)\n\\Delta b_1 = -E_1 - y_1\\Delta\\mu_1 K(\\vec x_1, \\vec x_1) - y_2\\Delta\\mu_2 K(\\vec x_2, \\vec x_1)\n\n\n\n\n\n\nAlternatively, if \n\\mu_2\n\\mu_2\n is not at the bounds it forces \nE^{new}(\\vec x_2, y_2) \\equiv E_2 = 0\nE^{new}(\\vec x_2, y_2) \\equiv E_2 = 0\n, thus \n\\Delta b_2 = -E_2 - y_1\\Delta\\mu_1 K(\\vec x_1, \\vec x_2) - y_2\\Delta\\mu_2 K(\\vec x_2, \\vec x_2)\n\\Delta b_2 = -E_2 - y_1\\Delta\\mu_1 K(\\vec x_1, \\vec x_2) - y_2\\Delta\\mu_2 K(\\vec x_2, \\vec x_2)\n\n\n\n\n\n\nIf both \n\\mu_1\n\\mu_1\n and \n\\mu_2\n\\mu_2\n take \n0\n0\n or \nC\nC\n, SMO algorithm calculates new \nb\nb\n for both and takes average (any \nb\nb\n between new \nb_1\nb_1\n and \nb_2\nb_2\n satisfies KKT conditions)\n\n\n\n\n\n\nUpdating weights is straightforward: \n\\Delta \\vec w = y_1\\Delta\\mu_1\\vec x_1 + y_2\\Delta\\mu_2\\vec x_2\n\\Delta \\vec w = y_1\\Delta\\mu_1\\vec x_1 + y_2\\Delta\\mu_2\\vec x_2\n\n\n\n\n\n\nChoosing \n\\mu\n\\mu\n to optimize\n\u00b6\n\n\n\n\n\n\nThe outer loop selects the first \n\\mu_i\n\\mu_i\n and the inner loop selects the second one \n\\mu_j\n\\mu_j\n that minimized \n|E_j - E_i|\n|E_j - E_i|\n\n\n\n\n\n\nThe outer loop first goes through all examples selecting the ones violating KKT conditions\n\n\n\n\n\n\nThen it iterates over all examples whose Lagrange multipliers are neither \n0\n0\n nor \nC\nC\n (the non-bound examples)\n\n\n\n\n\n\nIt repeats passes through the non-bound examples until all satisify KKT condition within some error \n\\epsilon\n\\epsilon\n\n\n\n\n\n\nThe it goes back and loop over all training set\n\n\n\n\n\n\nSMO repeats the procedure until all examples satisfy KKT conditions within \n\\epsilon\n\\epsilon\n\n\n\n\n\n\nTo avoid bullying CPU SMO keeps a cashed error value for every non-bound example\n\n\n\n\n\n\nExamples\n\u00b6\n\n\nInitial problem\n\u00b6\n\n\n\n\nLets first consider the simple example given at the beginning of the lecture\n\n\n\n\n# class I\n\n\nX01\n \n=\n \n[(\n2\n,\n \n9\n),\n \n(\n7\n,\n \n19\n),\n \n(\n1\n,\n \n10\n),\n \n(\n3\n,\n \n19\n),\n \n(\n4\n,\n \n16\n),\n \n(\n5\n,\n \n18\n)]\n\n\n\n# class II\n\n\nX02\n \n=\n \n[(\n4\n,\n \n3\n),\n \n(\n6\n,\n \n7\n),\n \n(\n1\n,\n \n-\n10\n),\n \n(\n3\n,\n \n-\n1\n),\n \n(\n9\n,\n \n5\n),\n \n(\n5\n,\n \n-\n7\n)]\n\n\n\nplt\n.\nxlim\n([\n0\n,\n \n10\n])\n\n\n\nplt\n.\nscatter\n(\n*\n(\nzip\n(\n*\nX01\n)))\n\n\nplt\n.\nscatter\n(\n*\n(\nzip\n(\n*\nX02\n)));\n\n\n\n\n\n\n\n\n\n\nAs we know it is linear problem we will use the linear kernel\n\n\n\n\nfrom\n \nsklearn\n \nimport\n \nsvm\n\n\n\n# create a classifier\n\n\nclf\n \n=\n \nsvm\n.\nSVC\n(\nkernel\n=\n\"linear\"\n)\n\n\n\n# train classifier - assign -1 label for X01 and 1 for X02\n\n\nclf\n.\nfit\n(\nX01\n \n+\n \nX02\n,\n \n[\n-\n1\n]\n*\nlen\n(\nX01\n)\n \n+\n \n[\n1\n]\n*\nlen\n(\nX02\n))\n\n\n\n\n\n\nSVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n  max_iter=-1, probability=False, random_state=None, shrinking=True,\n  tol=0.001, verbose=False)\n\n\n\n\n\n\n\nLets visualize the result\n\n\n\n\nsv\n \n=\n \nclf\n.\nsupport_vectors_\n  \n# support vectors\n\n\nw\n \n=\n \nclf\n.\ncoef_\n[\n0\n]\n           \n# weights\n\n\nb\n \n=\n \nclf\n.\nintercept_\n         \n# intercept\n\n\n\n# w[0] * x + w[1] * y + b = 0\n\n\nf\n \n=\n \nlambda\n \nx\n:\n \n-\n(\nb\n \n+\n \nw\n[\n0\n]\n*\nx\n)\n \n/\n \nw\n[\n1\n]\n\n\n\n# plot training data\n\n\nplt\n.\nscatter\n(\n*\n(\nzip\n(\n*\nX01\n)))\n\n\nplt\n.\nscatter\n(\n*\n(\nzip\n(\n*\nX02\n)))\n\n\n\n# plt decision boundary\n\n\nplt\n.\nplot\n([\n1\n,\n9\n],\n \n[\nf\n(\n1\n),\n \nf\n(\n9\n)])\n\n\n\n# mark support vectors\n\n\nplt\n.\nscatter\n(\n*\n(\nzip\n(\n*\nsv\n)),\n \nzorder\n=\n1\n,\n \nfacecolors\n=\n'none'\n,\n \nedgecolors\n=\n'r'\n,\n \ns\n=\n500\n);\n\n\n\n\n\n\n\n\nCircle\n\u00b6\n\n\n\n\n\n\nFor the simple non-linear problem lets consider once again points inside / outside a circle\n\n\n\n\n\n\nBut this time let them also overlap a little bit die to some noise in data\n\n\n\n\n\n\ndef\n \ngenerate_circle_data\n(\nR1\n=\n0\n,\n \nR2\n=\n1\n,\n \nN\n=\n500\n):\n\n  \n\"\"\"Generate N points in a circle for radius range (R1, R2)\"\"\"\n\n  \nr\n \n=\n \nlambda\n:\n \nR1\n \n+\n \nnp\n.\nrandom\n.\nrandom\n()\n \n*\n \n(\nR2\n \n-\n \nR1\n)\n \n+\n \nnp\n.\nrandom\n.\nnormal\n(\n0\n,\n \n0.2\n)\n\n\n  \nreturn\n \nnp\n.\narray\n([(\nr\n()\n \n*\n \nnp\n.\ncos\n(\nang\n),\n \nr\n()\n \n*\n \nnp\n.\nsin\n(\nang\n))\n\n                   \nfor\n \nang\n \nin\n \nnp\n.\nlinspace\n(\n0\n,\n \n2\n*\nnp\n.\npi\n,\n \nN\n)])\n\n\n\nC01\n \n=\n \ngenerate_circle_data\n()\n\n\nC02\n \n=\n \ngenerate_circle_data\n(\n1\n,\n \n2\n)\n\n\n\nplt\n.\nscatter\n(\n*\nC01\n.\nT\n,\n \nmarker\n=\n'.'\n)\n\n\nplt\n.\nscatter\n(\n*\nC02\n.\nT\n,\n \nmarker\n=\n'.'\n);\n\n\n\n\n\n\n\n\n\n\n\n\nWe will consider 4 different kernels:\n\n\n\n\n\n\nlinear\n\n\n\n\n\n\npolynomial of degree 3\n\n\n\n\n\n\npolynomial of degree 10\n\n\n\n\n\n\nGaussian radial basis function (RBF)\n\n\n\n\n\n\n\n\n\n\n# create classifier with different kernels\n\n\nclf_linear\n \n=\n \nsvm\n.\nSVC\n(\nkernel\n=\n\"linear\"\n)\n\n\nclf_rbf\n \n=\n \nsvm\n.\nSVC\n(\nkernel\n=\n\"rbf\"\n)\n\n\nclf_poly3\n \n=\n \nsvm\n.\nSVC\n(\nkernel\n=\n\"poly\"\n,\n \ndegree\n=\n3\n)\n\n\nclf_poly10\n \n=\n \nsvm\n.\nSVC\n(\nkernel\n=\n\"poly\"\n,\n \ndegree\n=\n10\n)\n\n\n\ntitles\n \n=\n \n(\n\"Linear\"\n,\n \n\"RBF\"\n,\n \n\"Polynomial, degree = 3\"\n,\n \n\"Plynomial, degree = 10\"\n)\n\n\n\n# create a mesh to plot in\n\n\nxx\n,\n \nyy\n \n=\n \nnp\n.\nmeshgrid\n(\nnp\n.\narange\n(\n-\n3\n,\n \n3\n,\n \n0.01\n),\n \nnp\n.\narange\n(\n-\n3\n,\n \n3\n,\n \n0.01\n))\n\n\n\n# loop over classifiers\n\n\nfor\n \ni\n,\n \nclf\n \nin\n \nenumerate\n((\nclf_linear\n,\n \nclf_rbf\n,\n \nclf_poly3\n,\n \nclf_poly10\n)):\n\n  \n# train classifier - assign -1 label for C01 and 1 for C02\n\n  \nclf\n.\nfit\n(\nnp\n.\nconcatenate\n((\nC01\n,\n \nC02\n),\n \naxis\n=\n0\n),\n \n[\n-\n1\n]\n*\nlen\n(\nC01\n)\n \n+\n \n[\n1\n]\n*\nlen\n(\nC02\n))\n\n\n  \n# visualize results\n\n  \nplt\n.\nsubplot\n(\n2\n,\n \n2\n,\n \ni\n \n+\n \n1\n)\n\n\n  \n# decision boundary\n\n  \nZ\n \n=\n \nclf\n.\npredict\n(\nnp\n.\nc_\n[\nxx\n.\nravel\n(),\n \nyy\n.\nravel\n()])\n\n  \nZ\n \n=\n \nZ\n.\nreshape\n(\nxx\n.\nshape\n)\n\n  \nplt\n.\ncontourf\n(\nxx\n,\n \nyy\n,\n \nZ\n,\n \ncmap\n=\nplt\n.\ncm\n.\nPaired\n)\n\n\n  \n# training data\n\n  \nplt\n.\nscatter\n(\n*\nC01\n.\nT\n,\n \nmarker\n=\n'.'\n)\n\n  \nplt\n.\nscatter\n(\n*\nC02\n.\nT\n,\n \nmarker\n=\n'.'\n)\n\n\n  \nplt\n.\ntitle\n(\ntitles\n[\ni\n])\n\n\n\nplt\n.\ntight_layout\n()\n\n\n\n\n\n\n\n\nMulticlass classification\n\u00b6\n\n\n\n\n\n\nThere are two popular methods used to apply binary classificators to multiclass problem\n\n\n\n\n\n\none-vs-rest\n (ovr) or \none-vs-all\n (ova)\n\n\n\n\n\n\nhaving \nK\nK\n classes with labels \ny_i = \\left\\{1, \\cdots, K\\right\\}\ny_i = \\left\\{1, \\cdots, K\\right\\}\n\n\n\n\n\n\ntrain \nK\nK\n classifiers assuming \nsvm_i = 1\nsvm_i = 1\n and \nsvm_{j\\neq i} = -1\nsvm_{j\\neq i} = -1\n\n\n\n\n\n\nthe prediction of a label for an unseen sample is based on a classifier which corresponds to the highest confidence score\n\n\n\n\n\n\n\n\n\n\none-vs-one\n (ovo)\n\n\n\n\n\n\nhaving \nK\nK\n classes with labels \ny_i = \\left\\{1, \\cdots, K\\right\\}\ny_i = \\left\\{1, \\cdots, K\\right\\}\n\n\n\n\n\n\ntrain \nK(K-1)/2\nK(K-1)/2\n classifiers for each possible pair of classes\n\n\n\n\n\n\nthe prediction is based on the voting scheme\n\n\n\n\n\n\n\n\n\n\nExample: blobs\n\u00b6\n\n\n\n\nLets use blobs form the previous lecture\n\n\n\n\nfrom\n \nsklearn.datasets\n \nimport\n \nmake_blobs\n\n\n\n# generate 5 blobs with fixed random generator\n\n\nX\n,\n \nY\n \n=\n \nmake_blobs\n(\nn_samples\n=\n500\n,\n \ncenters\n=\n8\n,\n \nrandom_state\n=\n300\n)\n\n\n\nplt\n.\nscatter\n(\n*\nX\n.\nT\n,\n \nc\n=\nY\n,\n \nmarker\n=\n'.'\n,\n \ncmap\n=\n'Dark2'\n);\n\n\n\n\n\n\n\n\n\n\nWe can use the same function as last time to train and visualize\n\n\n\n\ndef\n \ntrain_and_look\n(\nclassifier\n,\n \nX\n,\n \nY\n,\n \nax\n=\nNone\n,\n \ntitle\n=\n''\n,\n \ncmap\n=\n'Dark2'\n):\n\n  \n\"\"\"Train classifier on (X,Y). Plot data and prediction.\"\"\"\n\n  \n# create new axis if not provided\n\n  \nax\n \n=\n \nax\n \nor\n \nplt\n.\ngca\n();\n\n\n  \nax\n.\nset_title\n(\ntitle\n)\n\n\n  \n# plot training data\n\n  \nax\n.\nscatter\n(\n*\nX\n.\nT\n,\n \nc\n=\nY\n,\n \nmarker\n=\n'.'\n,\n \ncmap\n=\ncmap\n)\n\n\n  \n# train a cliassifier\n\n  \nclassifier\n.\nfit\n(\nX\n,\n \nY\n)\n\n\n  \n# create a grid of testing points\n\n  \nx_\n,\n \ny_\n \n=\n \nnp\n.\nmeshgrid\n(\nnp\n.\nlinspace\n(\n*\nax\n.\nget_xlim\n(),\n \nnum\n=\n200\n),\n\n                       \nnp\n.\nlinspace\n(\n*\nax\n.\nget_ylim\n(),\n \nnum\n=\n200\n))\n\n\n  \n# convert to an array of 2D points\n\n  \ntest_data\n \n=\n \nnp\n.\nvstack\n([\nx_\n.\nravel\n(),\n \ny_\n.\nravel\n()])\n.\nT\n\n\n  \n# make a prediction and reshape to grid structure \n\n  \nz_\n \n=\n \nclassifier\n.\npredict\n(\ntest_data\n)\n.\nreshape\n(\nx_\n.\nshape\n)\n\n\n  \n# arange z bins so class labels are in the middle\n\n  \nz_levels\n \n=\n \nnp\n.\narange\n(\nlen\n(\nnp\n.\nunique\n(\nY\n))\n \n+\n \n1\n)\n \n-\n \n0.5\n\n\n  \n# plot contours corresponding to classifier prediction\n\n  \nax\n.\ncontourf\n(\nx_\n,\n \ny_\n,\n \nz_\n,\n \nalpha\n=\n0.25\n,\n \ncmap\n=\ncmap\n,\n \nlevels\n=\nz_levels\n)\n\n\n\n\n\n\nfig\n,\n \nax\n \n=\n \nplt\n.\nsubplots\n(\n1\n,\n \n3\n,\n \nfigsize\n=\n(\n15\n,\n5\n))\n\n\n\ntitle\n \n=\n \n(\n\"Linear\"\n,\n \n\"RBF\"\n,\n \n\"Polynomial\"\n)\n\n\n\nsettings\n \n=\n \n({\n\"kernel\"\n:\n \n\"linear\"\n},\n \n{\n\"kernel\"\n:\n \n\"rbf\"\n},\n\n            \n{\n\"kernel\"\n:\n \n\"poly\"\n,\n \n\"degree\"\n:\n \n5\n})\n\n\n\n# train and look at SVM with different kernels\n\n\nfor\n \ni\n \nin\n \nrange\n(\n0\n,\n \n3\n):\n\n  \ntrain_and_look\n(\nsvm\n.\nSVC\n(\n**\nsettings\n[\ni\n]),\n \nX\n,\n \nY\n,\n\n                 \nax\n=\nax\n[\ni\n],\n \ntitle\n=\ntitle\n[\ni\n])\n\n\n\n\n\n\n\n\nSVM regression\n\u00b6\n\n\n\n\n\n\nLets consider the problem of approximating the set of data \n\\left\\{(\\vec x_1, y_1), \\cdots, (\\vec x_N, y_N)\\right\\}\n\\left\\{(\\vec x_1, y_1), \\cdots, (\\vec x_N, y_N)\\right\\}\n, where \n\\vec x_i \\in \\mathcal{R}^n\n\\vec x_i \\in \\mathcal{R}^n\n and \ny_i \\in \\mathcal{R}\ny_i \\in \\mathcal{R}\n with a linear function: \nf(\\vec x) = \\left<\\vec w, \\vec x\\right> + b\nf(\\vec x) = \\left<\\vec w, \\vec x\\right> + b\n\n\n\n\n\n\nIn epsilon-insensitive SVM (\n\\varepsilon\n\\varepsilon\n-SVM) the goal is to find \nf(x)\nf(x)\n that deviates from \ny_i\ny_i\n by a value not greater than \n\\varepsilon\n\\varepsilon\n for each training point, and at the same time is as flat as possible.\n\n\n\n\n\n\nAnd once again we end up with a optimization problem: \n\\left.\\begin{array}{cc}\\text{minimize}_{w,b} & \\frac{1}{2}|\\vec w|^2 \\\\\\text{subject to} & y_i - \\left<\\vec w, \\vec x_i\\right> - b \\leq \\varepsilon\\\\ & \\left<\\vec w, \\vec x_i\\right> + b - y_i \\leq \\varepsilon\\end{array}\\right.\n\\left.\\begin{array}{cc}\\text{minimize}_{w,b} & \\frac{1}{2}|\\vec w|^2 \\\\\\text{subject to} & y_i - \\left<\\vec w, \\vec x_i\\right> - b \\leq \\varepsilon\\\\ & \\left<\\vec w, \\vec x_i\\right> + b - y_i \\leq \\varepsilon\\end{array}\\right.\n\n\n\n\n\n\nIt is enough if such \nf\nf\n actually exists\n\n\n\n\n\n\nHowever, analogously to soft margin, one can allow for some errors by introducing slack variables \n\\xi_i\n\\xi_i\n and \n\\xi^*_i\n\\xi^*_i\n, and reformulate the optimization problem: \n\\left.\\begin{array}{cc}\\text{minimize}_{w,b} & \\frac{1}{2}|\\vec w|^2 + C\\sum\\limits_{i=1}^N(\\xi_i + \\xi^*_i) \\\\\\text{subject to} & y_i - \\left<\\vec w, \\vec x_i\\right> - b \\leq \\varepsilon + \\xi_i\\\\ & \\left<\\vec w, \\vec x_i\\right> + b - y_i \\leq \\varepsilon + \\xi^*_i \\\\ & \\xi_i, \\xi^*_i \\geq 0\\end{array}\\right.\n\\left.\\begin{array}{cc}\\text{minimize}_{w,b} & \\frac{1}{2}|\\vec w|^2 + C\\sum\\limits_{i=1}^N(\\xi_i + \\xi^*_i) \\\\\\text{subject to} & y_i - \\left<\\vec w, \\vec x_i\\right> - b \\leq \\varepsilon + \\xi_i\\\\ & \\left<\\vec w, \\vec x_i\\right> + b - y_i \\leq \\varepsilon + \\xi^*_i \\\\ & \\xi_i, \\xi^*_i \\geq 0\\end{array}\\right.\n\n\n\n\n\n\nThe corresponding dual problem is given by: \n\\left.\\begin{array}{cc}\\text{minimize}_{\\mu} & -\\frac{1}{2}\\sum\\limits_{i, j = 1}^N(\\mu_i - \\mu^*_i)(\\mu_j - \\mu^*_j)\\left<\\vec x_i, \\vec x_j\\right> - \\varepsilon\\sum\\limits_{i=1}^N(\\mu_i + \\mu^*_i) + \\sum\\limits_{i=1}^Ny_i(\\mu_i - \\mu^*_i)\\\\\\text{subject to} & \\sum\\limits_{i=1}^N(\\mu_i - \\mu^*_i) = 0 \\\\ & \\mu_i, \\mu^*_i\\in\\left[0, C\\right]\\end{array}\\right.\n\\left.\\begin{array}{cc}\\text{minimize}_{\\mu} & -\\frac{1}{2}\\sum\\limits_{i, j = 1}^N(\\mu_i - \\mu^*_i)(\\mu_j - \\mu^*_j)\\left<\\vec x_i, \\vec x_j\\right> - \\varepsilon\\sum\\limits_{i=1}^N(\\mu_i + \\mu^*_i) + \\sum\\limits_{i=1}^Ny_i(\\mu_i - \\mu^*_i)\\\\\\text{subject to} & \\sum\\limits_{i=1}^N(\\mu_i - \\mu^*_i) = 0 \\\\ & \\mu_i, \\mu^*_i\\in\\left[0, C\\right]\\end{array}\\right.\n\n\n\n\n\n\nAnd the solution is given by: \n\\vec w = \\sum\\limits_{i=1}^N(\\mu_i - \\mu^*_i)\\vec x_i\n\\vec w = \\sum\\limits_{i=1}^N(\\mu_i - \\mu^*_i)\\vec x_i\n, thus: \nf(\\vec x) = \\sum\\limits_{i=1}^N(\\mu_i - \\mu^*_i)\\left<\\vec x_i, \\vec x\\right> + b\nf(\\vec x) = \\sum\\limits_{i=1}^N(\\mu_i - \\mu^*_i)\\left<\\vec x_i, \\vec x\\right> + b\n\n\n\n\n\n\nOr for the kernelized version: \nf(\\vec x) = \\sum\\limits_{i=1}^N(\\mu_i - \\mu^*_i)K(\\vec x_i, \\vec x)+ b\nf(\\vec x) = \\sum\\limits_{i=1}^N(\\mu_i - \\mu^*_i)K(\\vec x_i, \\vec x)+ b\n\n\n\n\n\n\nThe intercept \nb\nb\n can be calculate for any support vector as \nf(\\vec x_{SV}) = y_{SV}\nf(\\vec x_{SV}) = y_{SV}\n\n\n\n\n\n\nExample - Boston Housing dataset\n\u00b6\n\n\nfrom\n \nsklearn.datasets\n \nimport\n \nload_boston\n\n\n\nboston\n \n=\n \nload_boston\n()\n\n\n\nprint\n(\nboston\n.\nDESCR\n)\n\n\n\n\n\n\nBoston House Prices dataset\n===========================\n\nNotes\n------\nData Set Characteristics:\n\n    :Number of Instances: 506\n\n    :Number of Attributes: 13 numeric/categorical predictive\n\n    :Median Value (attribute 14) is usually the target\n\n    :Attribute Information (in order):\n        - CRIM     per capita crime rate by town\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n        - INDUS    proportion of non-retail business acres per town\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n        - NOX      nitric oxides concentration (parts per 10 million)\n        - RM       average number of rooms per dwelling\n        - AGE      proportion of owner-occupied units built prior to 1940\n        - DIS      weighted distances to five Boston employment centres\n        - RAD      index of accessibility to radial highways\n        - TAX      full-value property-tax rate per $10,000\n        - PTRATIO  pupil-teacher ratio by town\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n        - LSTAT    % lower status of the population\n        - MEDV     Median value of owner-occupied homes in $1000's\n\n    :Missing Attribute Values: None\n\n    :Creator: Harrison, D. and Rubinfeld, D.L.\n\nThis is a copy of UCI ML housing dataset.\nhttp://archive.ics.uci.edu/ml/datasets/Housing\n\n\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\nprices and the demand for clean air', J. Environ. Economics & Management,\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\npages 244-261 of the latter.\n\nThe Boston house-price data has been used in many machine learning papers that address regression\nproblems.\n\n**References**\n\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n\n\n\n\n\nVisualize dataset\n\u00b6\n\n\nfor\n \ni\n,\n \nfeature\n \nin\n \nenumerate\n(\nboston\n.\ndata\n.\nT\n):\n\n  \nplt\n.\nsubplot\n(\n3\n,\n \n5\n,\n \ni\n+\n1\n)\n\n  \nplt\n.\nscatter\n(\nfeature\n,\n \nboston\n.\ntarget\n,\n \nmarker\n=\n'.'\n)\n\n  \nplt\n.\ntitle\n(\nboston\n.\nfeature_names\n[\ni\n])\n\n\n\nplt\n.\ntight_layout\n()\n\n\n\n\n\n\n\n\nimport\n \npandas\n \nas\n \npd\n\n\n\nboston_pd\n \n=\n \npd\n.\nDataFrame\n(\nboston\n.\ndata\n)\n     \n# load features\n\n\nboston_pd\n.\ncolumns\n \n=\n \nboston\n.\nfeature_names\n  \n# add features names\n\n\nboston_pd\n[\n'PRICE'\n]\n \n=\n \nboston\n.\ntarget\n        \n# add a column with price\n\n\n\nboston_pd\n.\nhead\n()\n\n\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nCRIM\n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \nPRICE\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n0.00632\n\n      \n18.0\n\n      \n2.31\n\n      \n0.0\n\n      \n0.538\n\n      \n6.575\n\n      \n65.2\n\n      \n4.0900\n\n      \n1.0\n\n      \n296.0\n\n      \n15.3\n\n      \n396.90\n\n      \n4.98\n\n      \n24.0\n\n    \n\n    \n\n      \n1\n\n      \n0.02731\n\n      \n0.0\n\n      \n7.07\n\n      \n0.0\n\n      \n0.469\n\n      \n6.421\n\n      \n78.9\n\n      \n4.9671\n\n      \n2.0\n\n      \n242.0\n\n      \n17.8\n\n      \n396.90\n\n      \n9.14\n\n      \n21.6\n\n    \n\n    \n\n      \n2\n\n      \n0.02729\n\n      \n0.0\n\n      \n7.07\n\n      \n0.0\n\n      \n0.469\n\n      \n7.185\n\n      \n61.1\n\n      \n4.9671\n\n      \n2.0\n\n      \n242.0\n\n      \n17.8\n\n      \n392.83\n\n      \n4.03\n\n      \n34.7\n\n    \n\n    \n\n      \n3\n\n      \n0.03237\n\n      \n0.0\n\n      \n2.18\n\n      \n0.0\n\n      \n0.458\n\n      \n6.998\n\n      \n45.8\n\n      \n6.0622\n\n      \n3.0\n\n      \n222.0\n\n      \n18.7\n\n      \n394.63\n\n      \n2.94\n\n      \n33.4\n\n    \n\n    \n\n      \n4\n\n      \n0.06905\n\n      \n0.0\n\n      \n2.18\n\n      \n0.0\n\n      \n0.458\n\n      \n7.147\n\n      \n54.2\n\n      \n6.0622\n\n      \n3.0\n\n      \n222.0\n\n      \n18.7\n\n      \n396.90\n\n      \n5.33\n\n      \n36.2\n\n    \n\n  \n\n\n\n\n\n\n\nTest SVR\n\u00b6\n\n\n\n\nBelow there is a result for the linear kernel - feel free to play with kernels and/or parameter \nC\nC\n on your own\n\n\n\n\nfrom\n \nsklearn.svm\n \nimport\n \nSVR\n\n\n\nregressor\n \n=\n \nSVR\n(\nkernel\n=\n\"linear\"\n)\n\n\nregressor\n.\nfit\n(\nboston\n.\ndata\n,\n \nboston\n.\ntarget\n)\n\n\n\nprediction\n \n=\n \nregressor\n.\npredict\n(\nboston\n.\ndata\n)\n\n\n\nplt\n.\nxlabel\n(\n\"True price\"\n)\n\n\nplt\n.\nylabel\n(\n\"Predicted price\"\n)\n\n\n\nplt\n.\nscatter\n(\nboston\n.\ntarget\n,\n \nprediction\n,\n \nmarker\n=\n'.'\n);\n\n\n\n\n\n\n\n\nSummary\n\u00b6\n\n\n\n\n\n\nSupport vector machine is a powerful model for both classification and regression\n\n\n\n\n\n\nIt guarantees finding the optimal hyperplane / decision boundary (if exists)\n\n\n\n\n\n\nIt is effective with high dimensional data\n\n\n\n\n\n\nThe dual formulation of the optimization problem allows easily to introduce kernels and deal with non-linear data\n\n\n\n\n\n\nIt has just few hyperparameters: kernel + kernel's parameters and \nC\nC\n\n\n\n\n\n\nInference is fast as it depends only on a subset of training samples (support vectors) - although training may be slow for large datasets",
            "title": "Support Vector Machine"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#support-vector-machine",
            "text": "Support vector machine (SVM) is a binary linear classifier    There are tricks to make SVM able to solve non-linear problems    There are extensions which allows using SVM to multiclass classification or regression    SVM is a supervised learning algorithm    There are extensions which allows using SVM for (unsupervised) clustering",
            "title": "Support Vector Machine"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#linear-svm",
            "text": "Lets consider a training dataset of  N N  samples  (\\vec x_1, y_1), \\cdots, (\\vec x_N, y_N) (\\vec x_1, y_1), \\cdots, (\\vec x_N, y_N)    \\vec x_i \\vec x_i  is  D D -dimensional vector representing features    y_i y_i  is a class label, for convenience  y_i = \\left\\{-1, 1\\right\\} y_i = \\left\\{-1, 1\\right\\}    At this point we assume that classes are linearly separable    For visualization purpose lets use  D = 2 D = 2    # our standard imports: matplotlib and numpy  import   matplotlib.pyplot   as   plt  import   numpy   as   np  # just to overwrite default colab style  plt . style . use ( 'default' )  plt . style . use ( 'seaborn-talk' )   # class I  X01   =   [( 2 ,   9 ),   ( 7 ,   19 ),   ( 1 ,   10 ),   ( 3 ,   19 ),   ( 4 ,   16 ),   ( 5 ,   18 )]  # class II  X02   =   [( 4 ,   3 ),   ( 6 ,   7 ),   ( 1 ,   - 10 ),   ( 3 ,   - 1 ),   ( 9 ,   5 ),   ( 5 ,   - 7 )]  plt . xlim ([ 0 ,   10 ])  plt . scatter ( * ( zip ( * X01 )))  plt . scatter ( * ( zip ( * X02 )));      In general we want to find a hyperplane which separates two classes    In the example above is just a line    plt . scatter ( * ( zip ( * X01 )))  plt . scatter ( * ( zip ( * X02 )))  plt . plot ([ 0 ,   10 ],   [ 0 ,   15 ],   'C2-' );      Once a hyperplane is found, the classification is straightforward    In this case, everything above a line is classified as a blue point and below as an orange point    However, there is infinitely many possible lines / hyperplanes    plt . scatter ( * ( zip ( * X01 )))  plt . scatter ( * ( zip ( * X02 )))  plt . scatter ( 5 ,   9 ,   c = 'C3' )    # test point  plt . plot ([ 0 ,   10 ],   [ 2 ,   22 ],   'C4-' );  plt . plot ([ 0 ,   10 ],   [ 0 ,   15 ],   'C2-' );      On the plot above two possible lines which correctly classify all training data are drawn    The red point represents a test sample    Using \"by eye\" method one could say it is rather orange than blue    The final predictions depends on the choice of a line though    Thus, one need a criterion to choose the best line / hyperplane    SVM chooses the hyperplane which is maximally far away from any training point    plt . plot ([ 0 ,   10 ],   [ 0 ,   20 ],   'C2-' ,   zorder = 0 )  plt . plot ([ 0 ,   10 ],   [ 5 ,   25 ],   'C2--' ,   zorder = 0 )  plt . plot ([ 0 ,   10 ],   [ - 5 ,   15 ],   'C2--' ,   zorder = 0 )  plt . scatter ( 5 ,   9 ,   c = 'C3' )    # test point  plt . annotate ( '' ,   ( 0 , 5 ),   ( 0.55 ,   - 4 ),   arrowprops = dict ( arrowstyle = '<->' ))  plt . text ( 0.5 ,   - 1.5 ,   '$m$' )  plt . text ( 0.3 ,   2.5 ,   '$m$' )  plt . annotate ( '' ,   ( 3.05 , 18.1 ),   ( 3.8 ,   7.75 ),   arrowprops = dict ( arrowstyle = '<->' ))  plt . text ( 2.9 ,   14 ,   '$m_i$' )  plt . scatter ( * ( zip ( * X01 )),   zorder = 1 )  plt . scatter ( * ( zip ( * X02 )),   zorder = 1 )  sv   =   X01 [: 2 ]   +   X02 [: 2 ]  plt . scatter ( * ( zip ( * sv )),   zorder = 1 ,   facecolors = 'none' ,   edgecolors = 'r' ,   s = 500 );      The goal is to maximize the  margin   2m 2m    Please note, that the margin is given by the distance of closest data point to the hyperplane    Which means, that the classifier depends only on a small subset of training data, which are called  support vectors",
            "title": "Linear SVM"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#hard-margin",
            "text": "Any hyperplane can be defined by an intercept term  b b  and a normal vector  \\vec w \\vec w  (which is usually called  weights  in machine learning)    All the points  \\vec x \\vec x  on the hyperplane satisfy  \\vec w \\cdot \\vec x + b = 0 \\vec w \\cdot \\vec x + b = 0    If  \\vec w \\cdot \\vec x + b~{> \\choose <}~0 \\vec w \\cdot \\vec x + b~{> \\choose <}~0  the point is on the  \\text{one} \\choose \\text{other} \\text{one} \\choose \\text{other}  side    We can easily defined the linear classifier  f(\\vec x) = \\text{sign}\\left(\\vec w \\cdot \\vec x  + b\\right) f(\\vec x) = \\text{sign}\\left(\\vec w \\cdot \\vec x  + b\\right)    That is why we wanted class labels to be  -1 -1  or  1 1  (instead more  standard   0 0  and  1 1 )    All we need is to find weights",
            "title": "Hard margin"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#functional-margin",
            "text": "Lets define functional margin of hyperplane  (\\vec w, b) (\\vec w, b)  w.r.t. a training sample  (\\vec x_i, y_i) (\\vec x_i, y_i)  as  \\hat m_i = y_i\\cdot \\left(\\vec w \\cdot \\vec x_i + b\\right) \\hat m_i = y_i\\cdot \\left(\\vec w \\cdot \\vec x_i + b\\right)    It is only positive if the training sample label has the same sign as prediction    The magnitude tells us somewhat about the confidence of a prediction    But please note, that we can choose arbitrary factor  k k  and  \\text{sign}\\left(\\vec w \\cdot \\vec x  + b\\right) =  \\text{sign}\\left(k\\cdot\\vec w \\cdot \\vec x  + k\\cdot b\\right) \\text{sign}\\left(\\vec w \\cdot \\vec x  + b\\right) =  \\text{sign}\\left(k\\cdot\\vec w \\cdot \\vec x  + k\\cdot b\\right)    Function margin w.r.t. the whole training dataset is the smallest one  \\hat m = \\min\\limits_{i = 1, \\cdots, N}\\hat m_i \\hat m = \\min\\limits_{i = 1, \\cdots, N}\\hat m_i",
            "title": "Functional margin"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#geometric-margin",
            "text": "Lets  m_i m_i  be a distance from a training point  \\vec x_i \\vec x_i  to a decision boundary / hyperplane  (\\vec w, b) (\\vec w, b)    Lets  \\vec x_h \\vec x_h  be a point on a hyperplane closest to  \\vec x_i \\vec x_i    Shortest distance must be peprendicular to a hyperplane (so parallel to  \\vec w \\vec w )    We can express  \\vec x_h \\vec x_h  by  \\vec x_h = \\vec x_i - y_i\\cdot m_i\\cdot \\frac{\\vec w}{|\\vec w|} \\vec x_h = \\vec x_i - y_i\\cdot m_i\\cdot \\frac{\\vec w}{|\\vec w|}    Because  \\vec x_h \\vec x_h  lies on a hyperplane it must fulfill  \\vec w \\cdot \\vec x_h + b = 0 \\vec w \\cdot \\vec x_h + b = 0 , thus  \\vec w\\left(\\vec x_i - y_i\\cdot m_i\\cdot \\frac{\\vec w}{|\\vec w|}\\right) + b = 0 \\vec w\\left(\\vec x_i - y_i\\cdot m_i\\cdot \\frac{\\vec w}{|\\vec w|}\\right) + b = 0    Pretty straightforward to solve for  m_i m_i   m_i = y_i\\frac{\\vec w\\cdot \\vec x_i + b}{|\\vec w|} = \\frac{\\hat m_i}{|\\vec w|} m_i = y_i\\frac{\\vec w\\cdot \\vec x_i + b}{|\\vec w|} = \\frac{\\hat m_i}{|\\vec w|}    As before geometric margin w.r.t. the whole training dataset is the smallest one  m = \\min\\limits_{i = 1, \\cdots, N} m_i = \\frac{\\hat m}{|\\vec w|} m = \\min\\limits_{i = 1, \\cdots, N} m_i = \\frac{\\hat m}{|\\vec w|}    Please note, that  m m  does not change when weights are scaled    \\hat m = m \\hat m = m  for  |\\vec w| = 1 |\\vec w| = 1",
            "title": "Geometric margin"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#the-optimal-margin",
            "text": "We want to find the maximum geometric margin  m m    But with a constraint that for every sample outside the margin  \\left.\\begin{array}{cc}\\text{maximize}_{w, b, m} & m\\\\\\text{subject to} & m_i \\geq m\\end{array}\\right. \\left.\\begin{array}{cc}\\text{maximize}_{w, b, m} & m\\\\\\text{subject to} & m_i \\geq m\\end{array}\\right.    As we discussed, we can choose any normalization of normal vector  |\\vec w| |\\vec w|  as it does not change the value of  m_i = y_i\\frac{\\vec w\\cdot \\vec x_i + b}{|\\vec w|} m_i = y_i\\frac{\\vec w\\cdot \\vec x_i + b}{|\\vec w|}    Lets choose  |\\vec w| = \\frac{1}{m} |\\vec w| = \\frac{1}{m} , so  \\left.\\begin{array}{cc}\\text{maximize}_{w, b} & \\frac{1}{|\\vec w|}\\\\\\text{subject to} & |\\vec w|\\cdot m_i \\geq 1\\end{array}\\right. \\left.\\begin{array}{cc}\\text{maximize}_{w, b} & \\frac{1}{|\\vec w|}\\\\\\text{subject to} & |\\vec w|\\cdot m_i \\geq 1\\end{array}\\right.    Since maximizing  |\\vec w|^{-1} |\\vec w|^{-1}  means the same as minimizing  |\\vec w| |\\vec w| , we can finally write  \\left.\\begin{array}{cc}\\text{minimize}_{w,b} & \\frac{1}{2}|\\vec w|^2\\\\\\text{subject to} & \\hat m_i \\geq 1\\end{array}\\right. \\left.\\begin{array}{cc}\\text{minimize}_{w,b} & \\frac{1}{2}|\\vec w|^2\\\\\\text{subject to} & \\hat m_i \\geq 1\\end{array}\\right.    Please note that we choose  |\\vec w|^2 |\\vec w|^2  over  |\\vec w| |\\vec w|  to avoid square root, and  \\frac{1}{2} \\frac{1}{2}  is for math convenience    Now, all we need is to optimize quadratic function over variables subject to linear constraints (quadratic programming)",
            "title": "The optimal margin"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#lagrange-multipliers",
            "text": "Lagrange multipliers is the common method used to solve contrained optimization problems    Lets consider the following problem (with  equality constraint ):  \\left.\\begin{array}{cc}\\text{minimize} & f(\\vec x)\\\\\\text{subject to} & g(\\vec x) = 0\\end{array}\\right. \\left.\\begin{array}{cc}\\text{minimize} & f(\\vec x)\\\\\\text{subject to} & g(\\vec x) = 0\\end{array}\\right.    Please note, that one could also consider  g(\\vec x) = c g(\\vec x) = c , where  c c  is constant (then  g'(\\vec x) = g(\\vec x) - c = 0 g'(\\vec x) = g(\\vec x) - c = 0 )    Lagrange multiplier theorem says that at constrained optimum (if exists)  \\nabla f \\nabla f  will be parallel to  \\nabla g \\nabla g , so  \\nabla f(\\vec x) = \\lambda\\nabla g(\\vec x) \\nabla f(\\vec x) = \\lambda\\nabla g(\\vec x)    \\lambda \\lambda  - a Lagrange multiplier",
            "title": "Lagrange multipliers"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#how-it-works",
            "text": "Lets consider the following problem:  \\left.\\begin{array}{cc}\\text{minimize} & f(x, y) = x^2 + y^2\\\\\\text{subject to} & g(x, y) = x\\cdot y - 1 = 0\\end{array}\\right. \\left.\\begin{array}{cc}\\text{minimize} & f(x, y) = x^2 + y^2\\\\\\text{subject to} & g(x, y) = x\\cdot y - 1 = 0\\end{array}\\right.   def   flc ( c ,   n = 100 ): \n   \"\"\"Level curves of objective functions\"\"\" \n   return   np . array ([( c   *   np . cos ( ang ),   c   *   np . sin ( ang )) \n                    for   ang   in   np . linspace ( 0 ,   2 * np . pi ,   n )])  def   g ( n = 100 ): \n   \"\"\"Constraint\"\"\" \n   return   np . array ([( x ,   1. / x )   for   x   in   np . linspace ( 0.1 ,   2.5 ,   n )])  ##### PLOT SETTINGS #####  plt . figure ( figsize = ( 8 ,   8 ))  plt . xlim ([ - 2.5 ,   2.5 ])  plt . ylim ([ - 2.5 ,   2.5 ])  plt . grid ( color = '0.5' ,   linestyle = '--' ,   linewidth = 0.5 )  ##### LEVEL CURVES OF f(x, y) #####  for   c   in   ( 0.2 ,   0.6 ,   1 ,   1.8 ,   2.2 ): \n   plt . plot ( * flc ( c ) . T ,   color = 'C0' )  plt . plot ( * flc ( np . sqrt ( 2 )) . T ,   color = 'C0' ,   label = '$f(x, y) = c$' )  ##### CONSTRAINTS #####  plt . plot ( * g () . T ,   color = 'C1' ,   label = '$g(x, y) = 0$' )  plt . plot ( *- g () . T ,   color = 'C1' )  ##### INTERSECTIONS #####  plt . scatter ( 1 ,   1 ,   c = 'C2' ,   zorder = 4 )  plt . scatter ( np . sqrt ( 0.345 ),   np . sqrt ( 1 / 0.345 ),   c = 'C3' ,   zorder = 4 )  plt . legend ();      Blue lines represent the level curves of the objective function ( f(x, y) = const f(x, y) = const )    Orange lines represent the constraints ( g(x, y) = 0 g(x, y) = 0 )    Lets first consider the case that blue and orange curves are not tangent (red point)    going along the constraint one direction would result in the decrease of the objective function    while going in another direction would result in the increase of the objective function    thus, this point can not be an optimum      The only possibility is that a constrained optimum is where curves are tanget (it still may not be the case, but at least it could be)",
            "title": "How it works"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#lagrangian",
            "text": "In general, there may be many constraints:   \\left.\\begin{array}{cc}\\text{minimize} & f(\\vec x)\\\\\\text{subject to} & g_i(\\vec x) = 0, \\hspace{5pt} i = 0,\\cdots, M\\end{array}\\right. \\left.\\begin{array}{cc}\\text{minimize} & f(\\vec x)\\\\\\text{subject to} & g_i(\\vec x) = 0, \\hspace{5pt} i = 0,\\cdots, M\\end{array}\\right.    Then, there are  M M  Lagrange multipliers  \\lambda_i \\lambda_i    It is convenient to define Lagrangian:  \\mathcal{L}(\\vec x, \\lambda) = f(\\vec x) + \\sum\\limits_{i=1}^M\\lambda_ig_i(\\vec x) \\mathcal{L}(\\vec x, \\lambda) = f(\\vec x) + \\sum\\limits_{i=1}^M\\lambda_ig_i(\\vec x)    and solve:  \\nabla_{x_1,\\cdots, x_n, \\lambda_1,\\cdots, \\lambda_M} \\mathcal{L}(\\vec x, \\lambda) = 0 \\nabla_{x_1,\\cdots, x_n, \\lambda_1,\\cdots, \\lambda_M} \\mathcal{L}(\\vec x, \\lambda) = 0    which is equivalent to:   \\nabla f(\\vec x) = \\sum\\limits_{i=1}^M\\lambda_i\\nabla g_i(\\vec x) \\nabla f(\\vec x) = \\sum\\limits_{i=1}^M\\lambda_i\\nabla g_i(\\vec x) g_1(\\vec x) = \\cdots = g_M(\\vec x) = 0 g_1(\\vec x) = \\cdots = g_M(\\vec x) = 0    The optimum (if exists) is always a saddle point of the Lagrangian    On the one hand, we want to minimize the Lagrangian over  \\vec x \\vec x    And on the other hand, we want to maximize the Lagrangian over  \\lambda_i \\lambda_i",
            "title": "Lagrangian"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#example",
            "text": "Lets consider the objective function  f(x) = x^2 f(x) = x^2  with a constraint  g(x) = x - 1 = 0 g(x) = x - 1 = 0    The Lagrangian is given by:  \\mathcal{L}(x, \\lambda) = x^2 + \\lambda\\cdot (x - 1) \\mathcal{L}(x, \\lambda) = x^2 + \\lambda\\cdot (x - 1) \\frac{\\partial\\mathcal{L}}{\\partial x} = 2x + \\lambda = 0 \\frac{\\partial\\mathcal{L}}{\\partial x} = 2x + \\lambda = 0 \\frac{\\partial\\mathcal{L}}{\\partial\\lambda} = x - 1 = 0 \\frac{\\partial\\mathcal{L}}{\\partial\\lambda} = x - 1 = 0    It has a saddle point at  x = 1 x = 1  and  p = -2 p = -2    from   mpl_toolkits.mplot3d   import   Axes3D  from   matplotlib   import   cm  ax   =   plt . figure () . add_subplot ( 111 ,   projection = Axes3D . name )  X ,   Y   =   np . meshgrid ( np . linspace ( - 1 ,   3 ,   50 ),   np . linspace ( - 5 ,   1 ,   50 ))  L   =   X ** 2   +   Y   *   ( X   -   1 )  ax . plot_surface ( X ,   Y ,   L ,   cmap = cm . hsv )  ax . view_init ( elev = 45 ,   azim = 120 )  ax . set_xlabel ( '$x$' ,   labelpad = 20 )  ax . set_ylabel ( '$\\lambda$' ,   labelpad = 20 )  ax . set_zlabel ( '$\\mathcal{L}$' ,   labelpad = 10 );",
            "title": "Example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#lagrange-duality",
            "text": "Lets consider the optimization problem with inequality constraints:  \\left.\\begin{array}{cc}\\text{minimize} & f(\\vec x)\\\\\\text{subject to} & g_i(\\vec x) \\leq 0, \\hspace{5pt} i = 0,\\cdots, M\\\\\\text{} & h_i(\\vec x) = 0, \\hspace{5pt} i = 0,\\cdots, N\\end{array}\\right. \\left.\\begin{array}{cc}\\text{minimize} & f(\\vec x)\\\\\\text{subject to} & g_i(\\vec x) \\leq 0, \\hspace{5pt} i = 0,\\cdots, M\\\\\\text{} & h_i(\\vec x) = 0, \\hspace{5pt} i = 0,\\cdots, N\\end{array}\\right.    We will call it the  primal  optimization problem and define generalized Lagrangian:  \\mathcal{L}(\\vec x, \\mu, \\lambda) = f(\\vec x) + \\sum\\limits_{i=1}^M\\mu_ig_i(\\vec x) + \\sum\\limits_{i=1}^N\\lambda_ih_i(\\vec x) \\mathcal{L}(\\vec x, \\mu, \\lambda) = f(\\vec x) + \\sum\\limits_{i=1}^M\\mu_ig_i(\\vec x) + \\sum\\limits_{i=1}^N\\lambda_ih_i(\\vec x)    with the additional restriction that  \\mu_i \\geq 0 \\mu_i \\geq 0    if  i i -th constraint is fulfilled, making  \\mu_i \\mu_i  more positive decrease the Lagrangian    making  \\mu_i \\mu_i  negative is not allowed      Please note, that in the case of constraints  g_i \\geq 0 g_i \\geq 0 , the restriction is  \\mu_i \\leq 0 \\mu_i \\leq 0    The task can be expressed in terms of minimizing the following function:  \\theta_P(\\vec x) = \\left\\{\\begin{array}{cc}f(\\vec x) & \\text{if x satisfies primal constraints} \\\\ \\infty & \\text{otherwise}\\end{array}\\right. = \\max\\limits_{\\mu, \\lambda; \\mu_i \\geq 0}\\mathcal{L}(\\vec x, \\mu, \\lambda) \\theta_P(\\vec x) = \\left\\{\\begin{array}{cc}f(\\vec x) & \\text{if x satisfies primal constraints} \\\\ \\infty & \\text{otherwise}\\end{array}\\right. = \\max\\limits_{\\mu, \\lambda; \\mu_i \\geq 0}\\mathcal{L}(\\vec x, \\mu, \\lambda)    Thus, we can write the final task in the form:  \\min\\limits_{\\vec x}\\theta_P(\\vec x) = \\min\\limits_{\\vec x}\\max\\limits_{\\mu, \\lambda; \\mu_i \\geq 0}\\mathcal{L}(\\vec x, \\mu, \\lambda) \\min\\limits_{\\vec x}\\theta_P(\\vec x) = \\min\\limits_{\\vec x}\\max\\limits_{\\mu, \\lambda; \\mu_i \\geq 0}\\mathcal{L}(\\vec x, \\mu, \\lambda)    Now, lets consider the following \"reversed\" task:  \\max\\limits_{\\mu, \\lambda; \\mu_i \\geq 0}\\min\\limits_{\\vec x}\\mathcal{L}(\\vec x, \\mu, \\lambda) = \\max\\limits_{\\mu, \\lambda; \\mu_i \\geq 0}\\theta_D(\\mu, \\lambda) \\max\\limits_{\\mu, \\lambda; \\mu_i \\geq 0}\\min\\limits_{\\vec x}\\mathcal{L}(\\vec x, \\mu, \\lambda) = \\max\\limits_{\\mu, \\lambda; \\mu_i \\geq 0}\\theta_D(\\mu, \\lambda)    \\theta_D(\\mu, \\lambda) \\theta_D(\\mu, \\lambda)  is known as the  dual function  and the maximizing it is called the  dual  problem    \\theta_D(\\mu, \\lambda) \\theta_D(\\mu, \\lambda)  is a concave function (becasue Lagrangian is affine, i.e. linear in multipliers) - maximing  \\theta_D \\theta_D  is a convex optimisation problem    How it is related to the primal problem:  \\mathcal{L}(\\vec x, \\mu, \\lambda) \\leq \\theta_P(\\vec x) \\mathcal{L}(\\vec x, \\mu, \\lambda) \\leq \\theta_P(\\vec x)     \\min\\limits_{\\vec x}\\mathcal{L}(\\vec x, \\mu, \\lambda) = \\theta_D(\\mu, \\lambda) \\leq \\min\\limits_{\\vec x}\\theta_P(\\vec x)\\equiv p^* \\min\\limits_{\\vec x}\\mathcal{L}(\\vec x, \\mu, \\lambda) = \\theta_D(\\mu, \\lambda) \\leq \\min\\limits_{\\vec x}\\theta_P(\\vec x)\\equiv p^*     d^* \\equiv \\max\\limits_{\\mu, \\lambda; \\mu_i \\geq 0}\\theta_D(\\mu, \\lambda) \\leq p^* d^* \\equiv \\max\\limits_{\\mu, \\lambda; \\mu_i \\geq 0}\\theta_D(\\mu, \\lambda) \\leq p^*    Thus, the optimum of the dual problem  d^* d^*  is a lower bound for the optimum of the primal problem  p^* p^*  (known as weak duality)    The difference  p^* - d^* p^* - d^*  is known as the duality gap    If the gap is zero ( p^* = d^* p^* = d^* ) we have  strong duality    If the optimization problem is convex and (strictly) feasible (i.e. there is  \\vec x \\vec x  which satisfies all constraints) strong duality holds, so there must exist  \\vec x^* \\vec x^* ,  \\mu^* \\mu^* ,  \\lambda^* \\lambda^* , so that  \\vec x^* \\vec x^*  is the solution of the primal problem and  \\mu^* \\mu^*  and  \\lambda^* \\lambda^*  are solutions of the dual problem, and  p^* = d^* = \\mathcal{L}(\\vec x^*, \\mu^*, \\lambda^*) p^* = d^* = \\mathcal{L}(\\vec x^*, \\mu^*, \\lambda^*)",
            "title": "Lagrange duality"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#kkt-conditions",
            "text": "Moreover,  \\vec x^* \\vec x^* ,  \\mu^* \\mu^* ,  \\lambda^* \\lambda^*  satisfy the  Karush-Kuhn-Tucker (KKT)  conditions    Stationarity   \\nabla_{\\vec x} \\mathcal{L}(\\vec x^*, \\mu^*, \\lambda^*) = 0 \\nabla_{\\vec x} \\mathcal{L}(\\vec x^*, \\mu^*, \\lambda^*) = 0    Primal feasibility   g_i(\\vec x^*) \\leq 0, \\hspace{10pt} h_i(\\vec x^*) = 0 g_i(\\vec x^*) \\leq 0, \\hspace{10pt} h_i(\\vec x^*) = 0    Dual feasibility   \\mu_i^* \\geq 0 \\mu_i^* \\geq 0    Complementary slackness (or dual complementarity)   \\mu_i^*\\cdot g_i(\\vec x^*) = 0 \\mu_i^*\\cdot g_i(\\vec x^*) = 0    Why the last one - for strong duality:  f(\\vec x^*) = \\theta_D(\\mu^*, \\lambda^*) = \\min\\limits_{\\vec x}\\mathcal{L}(\\vec x, \\mu^*, \\lambda^*) \\leq f(\\vec x^*) + \\sum\\limits_{i=1}^M\\mu_i^*\\cdot g_i(\\vec x^*) + \\sum\\limits_{i=1}^N\\lambda_i^*\\cdot h_i(\\vec x ^*) \\leq f(\\vec x^*) f(\\vec x^*) = \\theta_D(\\mu^*, \\lambda^*) = \\min\\limits_{\\vec x}\\mathcal{L}(\\vec x, \\mu^*, \\lambda^*) \\leq f(\\vec x^*) + \\sum\\limits_{i=1}^M\\mu_i^*\\cdot g_i(\\vec x^*) + \\sum\\limits_{i=1}^N\\lambda_i^*\\cdot h_i(\\vec x ^*) \\leq f(\\vec x^*)    The last inequality holds beacause  \\sum\\limits_{i=1}^M\\mu_i^*\\cdot g_i(\\vec x^*) \\leq 0 \\sum\\limits_{i=1}^M\\mu_i^*\\cdot g_i(\\vec x^*) \\leq 0    Since  f(\\vec x^*) f(\\vec x^*)  must be equal to  f(\\vec x^*) f(\\vec x^*)  these inequalities are in fact equalities, so  \\sum\\limits_{i=1}^M\\mu_i^*\\cdot g_i(\\vec x^*) = 0 \\sum\\limits_{i=1}^M\\mu_i^*\\cdot g_i(\\vec x^*) = 0    Because  \\mu_i \\geq 0 \\mu_i \\geq 0  and  g_i \\leq 0 g_i \\leq 0  we get the complementary slackness condition:  \\mu_i^*\\cdot g_i(\\vec x^*) = 0 \\mu_i^*\\cdot g_i(\\vec x^*) = 0    If  \\vec x^* \\vec x^* ,  \\mu^* \\mu^* ,  \\lambda^* \\lambda^*  satisfy KKT conditions, then they are primal and dual solutions",
            "title": "KKT conditions"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#optimal-margin",
            "text": "Lets back to our original optimization problem:  \\left.\\begin{array}{cc}\\text{minimize}_{w,b} & \\frac{1}{2}|\\vec w|^2\\\\\\text{subject to} & y_i\\cdot \\left(\\vec w \\cdot \\vec x_i + b\\right) \\geq 1\\end{array}\\right. \\left.\\begin{array}{cc}\\text{minimize}_{w,b} & \\frac{1}{2}|\\vec w|^2\\\\\\text{subject to} & y_i\\cdot \\left(\\vec w \\cdot \\vec x_i + b\\right) \\geq 1\\end{array}\\right.    Lets rewrite the constraints in the form:  g_i(\\vec w) = - y_i\\cdot \\left(\\vec w \\cdot \\vec x_i + b\\right) + 1 \\leq 0 g_i(\\vec w) = - y_i\\cdot \\left(\\vec w \\cdot \\vec x_i + b\\right) + 1 \\leq 0    Please note, that because of the complementary slackness condition,  \\mu_i > 0 \\mu_i > 0  only for the training examples ( \\vec x_i \\vec x_i ) that have functional margin equal to one ( support vectors )    Lets write the Langrangian for this problem:  \\mathcal{L}(\\vec w, b, \\mu) = \\frac{1}{2}|\\vec w|^2 - \\sum\\limits_{i=1}^M\\mu_i\\cdot \\left[y_i\\cdot\\left(\\vec w \\cdot \\vec x_i + b\\right) - 1\\right] \\mathcal{L}(\\vec w, b, \\mu) = \\frac{1}{2}|\\vec w|^2 - \\sum\\limits_{i=1}^M\\mu_i\\cdot \\left[y_i\\cdot\\left(\\vec w \\cdot \\vec x_i + b\\right) - 1\\right]",
            "title": "Optimal margin"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#dual-problem",
            "text": "To find the dual form of the problem we need to minimize the Lagrangian over  \\vec w \\vec w  and  b b :  \\nabla_{w}\\mathcal{L}(\\vec w, b, \\mu) = \\vec w - \\sum\\limits_{i=1}^M\\mu_iy_i\\vec x_i = 0 \\Rightarrow \\vec w = \\sum\\limits_{i=1}^M\\mu_iy_i\\vec x_i \\nabla_{w}\\mathcal{L}(\\vec w, b, \\mu) = \\vec w - \\sum\\limits_{i=1}^M\\mu_iy_i\\vec x_i = 0 \\Rightarrow \\vec w = \\sum\\limits_{i=1}^M\\mu_iy_i\\vec x_i    And for the intercept term:  \\frac{\\partial}{\\partial b}\\mathcal{L}(\\vec w, b, \\mu) = \\sum\\limits_{i=1}^M\\mu_iy_i = 0 \\frac{\\partial}{\\partial b}\\mathcal{L}(\\vec w, b, \\mu) = \\sum\\limits_{i=1}^M\\mu_iy_i = 0    It is straightforawrd to show, that using two above equations one can obtain:   \\nabla_{w}\\mathcal{L}(\\vec w, b, \\mu) = \\sum\\limits_{i=1}^M\\mu_i - \\frac{1}{2}|\\vec w|^2 =  \\sum\\limits_{i=1}^M\\mu_i - \\frac{1}{2}\\sum\\limits_{i,j=1}^My_iy_j\\mu_i\\mu_j\\cdot\\left(\\vec x_i\\cdot \\vec x_j\\right) \\nabla_{w}\\mathcal{L}(\\vec w, b, \\mu) = \\sum\\limits_{i=1}^M\\mu_i - \\frac{1}{2}|\\vec w|^2 =  \\sum\\limits_{i=1}^M\\mu_i - \\frac{1}{2}\\sum\\limits_{i,j=1}^My_iy_j\\mu_i\\mu_j\\cdot\\left(\\vec x_i\\cdot \\vec x_j\\right)    By examing the dual form the optimization problem is expressed in terms of the  inner product of input feature vectors :   \\left.\\begin{array}{cc}\\text{maximize}_{\\mu} & \\sum\\limits_{i=1}^M\\mu_i - \\frac{1}{2}\\sum\\limits_{i,j=1}^My_iy_j\\mu_i\\mu_j\\cdot\\left<\\vec x_i, \\vec x_j\\right>\\\\\\text{subject to} & \\mu_i \\geq 0 \\\\ & \\sum\\limits_{i=1}^M\\mu_iy_i = 0\\end{array}\\right. \\left.\\begin{array}{cc}\\text{maximize}_{\\mu} & \\sum\\limits_{i=1}^M\\mu_i - \\frac{1}{2}\\sum\\limits_{i,j=1}^My_iy_j\\mu_i\\mu_j\\cdot\\left<\\vec x_i, \\vec x_j\\right>\\\\\\text{subject to} & \\mu_i \\geq 0 \\\\ & \\sum\\limits_{i=1}^M\\mu_iy_i = 0\\end{array}\\right.",
            "title": "Dual problem"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#non-linear-svm",
            "text": "SVM can be applied to non-linear problems using the  kernel trick    However, before we go there, lets consider a simple non-linear problem",
            "title": "Non-linear SVM"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#example_1",
            "text": "Lets consider two classes of 2D points:    inside a circle    outside a circle      def   generate_circle_data ( R1 = 0 ,   R2 = 1 ,   N = 500 ): \n   \"\"\"Generate N points in a circle for radius range (R1, R2)\"\"\" \n   r   =   lambda :   R1   +   np . random . random ()   *   ( R2   -   R1 ) \n\n   return   np . array ([( r ()   *   np . cos ( ang ),   r ()   *   np . sin ( ang )) \n                    for   ang   in   np . linspace ( 0 ,   2 * np . pi ,   N )])  C01   =   generate_circle_data ()  C02   =   generate_circle_data ( 1 ,   2 )  plt . scatter ( * C01 . T ,   marker = '.' )  plt . scatter ( * C02 . T ,   marker = '.' );      There is no way to find a line which separates these classes    But would it be possible to add another dimension and find a plane?    Lets consider a  feature mapping   \\phi \\phi  which maps  original attributes :  \\phi(x, y) = \\left[\\begin{array}{c} x \\\\ y \\\\ x^2 + y^2\\end{array}\\right] \\phi(x, y) = \\left[\\begin{array}{c} x \\\\ y \\\\ x^2 + y^2\\end{array}\\right]    from   mpl_toolkits.mplot3d   import   Axes3D  from   matplotlib   import   cm  ax   =   plt . figure () . add_subplot ( 111 ,   projection = Axes3D . name )  Z01   =   np . array ([ x ** 2   +   y ** 2   for   x ,   y   in   C01 ])  Z02   =   np . array ([ x ** 2   +   y ** 2   for   x ,   y   in   C02 ])  ax . scatter ( * C01 . T ,   Z01 ,   cmap = cm . hsv )  ax . scatter ( * C02 . T ,   Z02 ,   cmap = cm . hsv )  ax . view_init ( elev = 15 ,   azim = 60 )      The  new  dataset is a linear problem and can be  easily  solved with SVM    All we need is to replace (in the dual formulation):  \\left<\\vec x_i, \\vec x_j\\right> \\rightarrow \\left<\\phi(\\vec x_i), \\phi(\\vec x_j)\\right> = \\phi^T(\\vec x_i)\\phi(\\vec x_j) \\left<\\vec x_i, \\vec x_j\\right> \\rightarrow \\left<\\phi(\\vec x_i), \\phi(\\vec x_j)\\right> = \\phi^T(\\vec x_i)\\phi(\\vec x_j)",
            "title": "Example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#kernel-trick",
            "text": "Lets define the  kernel  (for given feature mapping  \\phi \\phi ) as  K(\\vec x_i, \\vec x_j) = \\phi^T(\\vec x_i)\\phi(\\vec x_j) K(\\vec x_i, \\vec x_j) = \\phi^T(\\vec x_i)\\phi(\\vec x_j)    Thus, the optimization problem we are trying to solve now is:  \\left.\\begin{array}{cc}\\text{maximize}_{\\mu} & \\sum\\limits_{i=1}^M\\mu_i - \\frac{1}{2}\\sum\\limits_{i,j=1}^My_iy_j\\mu_i\\mu_j\\cdot K(\\vec x_i, \\vec x_j)\\\\\\text{subject to} & \\mu_i \\geq 0 \\\\ & \\sum\\limits_{i=1}^M\\mu_iy_i = 0\\end{array}\\right. \\left.\\begin{array}{cc}\\text{maximize}_{\\mu} & \\sum\\limits_{i=1}^M\\mu_i - \\frac{1}{2}\\sum\\limits_{i,j=1}^My_iy_j\\mu_i\\mu_j\\cdot K(\\vec x_i, \\vec x_j)\\\\\\text{subject to} & \\mu_i \\geq 0 \\\\ & \\sum\\limits_{i=1}^M\\mu_iy_i = 0\\end{array}\\right.    The trick is now, that we do not have to calculate (or even know) the mapping  \\phi \\phi , which could be in general very expensive    The classifier was deinfed as:  \\hat y = \\text{sign}\\left(\\vec w \\cdot \\vec x  + b\\right) \\hat y = \\text{sign}\\left(\\vec w \\cdot \\vec x  + b\\right)    and its dual form is:  \\hat y = \\text{sign}\\left(\\sum\\limits_{i=1}^M \\mu_iy_i\\left<\\vec x_i, \\vec x\\right> + b\\right) \\hat y = \\text{sign}\\left(\\sum\\limits_{i=1}^M \\mu_iy_i\\left<\\vec x_i, \\vec x\\right> + b\\right)    and can be now rewritten in the form:  \\hat y = \\text{sign}\\left(\\sum\\limits_{i=1}^M \\mu_iy_iK(\\vec x_i, \\vec x) + b\\right) \\hat y = \\text{sign}\\left(\\sum\\limits_{i=1}^M \\mu_iy_iK(\\vec x_i, \\vec x) + b\\right)",
            "title": "Kernel trick"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#example_2",
            "text": "Lets consider the feature mapping  \\phi \\phi  (in 3D):  \\phi(\\vec x) = \\left[\\begin{array}{c} x_1x_1 \\\\ x_1x_2 \\\\ x_1x_3 \\\\ x_2x_1 \\\\ x_2x_2 \\\\ x_2x_3 \\\\ x_3x_1 \\\\ x_3x_2 \\\\ x_3x_3\\end{array}\\right] \\phi(\\vec x) = \\left[\\begin{array}{c} x_1x_1 \\\\ x_1x_2 \\\\ x_1x_3 \\\\ x_2x_1 \\\\ x_2x_2 \\\\ x_2x_3 \\\\ x_3x_1 \\\\ x_3x_2 \\\\ x_3x_3\\end{array}\\right]    Calculating  \\phi(\\vec x) \\phi(\\vec x) , where  \\vec x \\in \\mathcal{R}^N \\vec x \\in \\mathcal{R}^N , requires  O(n^2) O(n^2)  time    The related kernel is given by:  K(\\vec x_i, \\vec x_j) = \\left<x_i, \\vec x_j\\right>^2 K(\\vec x_i, \\vec x_j) = \\left<x_i, \\vec x_j\\right>^2    requires only  O(n) O(n)  time",
            "title": "Example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#mercer-theorem",
            "text": "Having some function  K K , how can we tell if it is a valid kernel?    For a valid kernel, there must exist a feature mapping  \\phi \\phi , so that  K(\\vec x_i, \\vec x_j) = \\phi^T(\\vec x_i)\\phi(\\vec x_j) K(\\vec x_i, \\vec x_j) = \\phi^T(\\vec x_i)\\phi(\\vec x_j)    Lets define the  kernel matrix   K_{ij} = K(\\vec x_i, \\vec x_j) K_{ij} = K(\\vec x_i, \\vec x_j)    If  K K  is a valid kernel the corresponding matrix must be symmetric:  K_{ij} = \\phi^T(\\vec x_i)\\phi(\\vec x_j) = \\phi^T(\\vec x_j)\\phi(\\vec x_i) = K_{ji} K_{ij} = \\phi^T(\\vec x_i)\\phi(\\vec x_j) = \\phi^T(\\vec x_j)\\phi(\\vec x_i) = K_{ji}    Moreover,  K K  must be positive semi-definite ( K \\geq 0 K \\geq 0 ):  z^TKz = \\sum\\limits_i\\sum\\limits_jz_iK_{ij}z_j = \\sum\\limits_i\\sum\\limits_jz_i\\phi^T(\\vec x_i)\\phi(\\vec x_j)z_j = \\sum\\limits_i\\sum\\limits_j\\sum\\limits_kz_i\\phi_k(\\vec x_i)\\phi_k(\\vec x_j)z_j = \\sum\\limits_k\\left(\\sum\\limits_iz_i\\phi_k(\\vec x_i)\\right)^2 \\geq 0 z^TKz = \\sum\\limits_i\\sum\\limits_jz_iK_{ij}z_j = \\sum\\limits_i\\sum\\limits_jz_i\\phi^T(\\vec x_i)\\phi(\\vec x_j)z_j = \\sum\\limits_i\\sum\\limits_j\\sum\\limits_kz_i\\phi_k(\\vec x_i)\\phi_k(\\vec x_j)z_j = \\sum\\limits_k\\left(\\sum\\limits_iz_i\\phi_k(\\vec x_i)\\right)^2 \\geq 0    Mercer theorem    Let $K: \\mathcal{R}^n\\times \\mathcal{R}^N \\rightarrow \\mathcal{R}$ be given. Then, for $K$ to be a valid kernel, it is necessary and **sufficient** that for any $\\left\\{x_1, \\cdots x_m\\right\\}$, the corresponding kernel matrix is symmetric positive semi-definite.",
            "title": "Mercer theorem"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#kernel-examples",
            "text": "Gaussian kernel   K(\\vec x, \\vec y) = \\exp\\left(-\\frac{||\\vec x - \\vec y||}{2\\sigma^2}\\right) K(\\vec x, \\vec y) = \\exp\\left(-\\frac{||\\vec x - \\vec y||}{2\\sigma^2}\\right)    Polynomial kernel   K(\\vec x, \\vec y) = \\left(\\vec x \\cdot \\vec y + c\\right)^d K(\\vec x, \\vec y) = \\left(\\vec x \\cdot \\vec y + c\\right)^d    Sigmoid kernel   K(\\vec x, \\vec y) = \\tanh\\left(a\\vec x \\cdot \\vec y + c\\right) K(\\vec x, \\vec y) = \\tanh\\left(a\\vec x \\cdot \\vec y + c\\right)",
            "title": "Kernel examples"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#soft-margin",
            "text": "So far, everything was considered with the assumption that data is (linearly) separable    Mapping to a high-dimensional feature space may make data separable, but it can not guarantee that    Also, due to some noise in data, some outliers may lead to overfitting    Lets consider the example below    plt . plot ([ 0 ,   10 ],   [ 0 ,   20 ],   'C2-' ,   zorder = 0 )  plt . plot ([ 0 ,   10 ],   [ 4 ,   24 ],   'C3-' ,   zorder = 0 )  plt . scatter ( 3 ,   9 ,   c = 'C1' ,   marker = ',' )  plt . scatter ( * ( zip ( * X01 )),   zorder = 1 )  plt . scatter ( * ( zip ( * X02 )),   zorder = 1 );      The green line represents the optimal decision boundary if the orange square point is not included    The red line represents the case when the outlier is considered    Including the extra point drastically changes the result    The idea behind the  soft margin  is to allow some points to lie of the  wrong  side of a decision boundary      plt . plot ([ 0 ,   10 ],   [ 0 ,   20 ],   'C2-' ,   zorder = 0 )  plt . plot ([ 0 ,   10 ],   [ 5 ,   25 ],   'C2--' ,   zorder = 0 )  plt . plot ([ 0 ,   10 ],   [ - 5 ,   15 ],   'C2--' ,   zorder = 0 )  plt . scatter ( 3 ,   9 ,   c = 'C1' ,   marker = ',' )  plt . annotate ( '' ,   ( 3.05 , 8.7 ),   ( 3.5 ,   1.75 ),   arrowprops = dict ( arrowstyle = '<->' ))  plt . text ( 3.5 ,   4.5 ,   '$ \\\\ xi_i$' )  plt . annotate ( '' ,   ( 0 , 5 ),   ( 0.55 ,   - 4 ),   arrowprops = dict ( arrowstyle = '<->' ))  plt . text ( 0.5 ,   - 1.5 ,   '$m$' )  plt . text ( 0.3 ,   2.5 ,   '$m$' )  plt . annotate ( '' ,   ( 3.05 , 18.1 ),   ( 3.8 ,   7.75 ),   arrowprops = dict ( arrowstyle = '<->' ))  plt . text ( 2.9 ,   14 ,   '$m_i$' )  plt . scatter ( * ( zip ( * X01 )),   zorder = 1 )  plt . scatter ( * ( zip ( * X02 )),   zorder = 1 )  sv   =   X01 [: 2 ]   +   X02 [: 2 ]  plt . scatter ( * ( zip ( * sv )),   zorder = 1 ,   facecolors = 'none' ,   edgecolors = 'r' ,   s = 500 );",
            "title": "Soft margin"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#regularization",
            "text": "Regularization is a common technique to prevent overfitting (more about that next week)    Lets reformulate the original optimization problem:  \\left.\\begin{array}{cc}\\text{minimize}_{w,b} & \\frac{1}{2}|\\vec w|^2 + C\\sum\n\\limits_i\\xi_i\\\\\\text{subject to} & \\hat y_i\\cdot\\left(\\vec w \\cdot \\vec x + b\\right) \\geq 1 - \\xi_i \\\\ & \\xi_i \\geq 0\\end{array}\\right. \\left.\\begin{array}{cc}\\text{minimize}_{w,b} & \\frac{1}{2}|\\vec w|^2 + C\\sum\n\\limits_i\\xi_i\\\\\\text{subject to} & \\hat y_i\\cdot\\left(\\vec w \\cdot \\vec x + b\\right) \\geq 1 - \\xi_i \\\\ & \\xi_i \\geq 0\\end{array}\\right.    We now allow to have a functional margin less than 1, but if a training point has functional margin  1 - \\xi_i 1 - \\xi_i  we would pay a cost of the objective function being increased by  C\\xi_i C\\xi_i    Thus,  C C  controls how much we penalize \"bad\" points (with  C = \\infty C = \\infty  we are back to the original perfectly separate case)    Lets write the Lagrangian for this optimization problem:  \\mathcal{L}(\\vec w, b, \\xi, \\mu, r) = \\frac{1}{2}|\\vec w|^2 + C\\sum\n\\limits_{i=1}^M \\xi_i - \\sum\\limits_{i=1}^M\\mu_i\\cdot \\left[y_i\\cdot\\left(\\vec w \\cdot \\vec x_i + b\\right) - 1 + \\xi_i\\right] - \\sum\\limits_{i=1}^Mr_i\\xi_i \\mathcal{L}(\\vec w, b, \\xi, \\mu, r) = \\frac{1}{2}|\\vec w|^2 + C\\sum\n\\limits_{i=1}^M \\xi_i - \\sum\\limits_{i=1}^M\\mu_i\\cdot \\left[y_i\\cdot\\left(\\vec w \\cdot \\vec x_i + b\\right) - 1 + \\xi_i\\right] - \\sum\\limits_{i=1}^Mr_i\\xi_i    After the same procedure as before we get similar dual problem:  \\left.\\begin{array}{cc}\\text{maximize}_{\\mu} & \\sum\\limits_{i=1}^M\\mu_i - \\frac{1}{2}\\sum\\limits_{i,j=1}^My_iy_j\\mu_i\\mu_j\\cdot\\left<\\vec x_i, \\vec x_j\\right>\\\\\\text{subject to} & C \\geq \\mu_i \\geq 0 \\\\ & \\sum\\limits_{i=1}^M\\mu_iy_i = 0\\end{array}\\right. \\left.\\begin{array}{cc}\\text{maximize}_{\\mu} & \\sum\\limits_{i=1}^M\\mu_i - \\frac{1}{2}\\sum\\limits_{i,j=1}^My_iy_j\\mu_i\\mu_j\\cdot\\left<\\vec x_i, \\vec x_j\\right>\\\\\\text{subject to} & C \\geq \\mu_i \\geq 0 \\\\ & \\sum\\limits_{i=1}^M\\mu_iy_i = 0\\end{array}\\right.    The only difference is the extra constraint on  \\mu_i \\mu_i , which comes from  \\frac{\\partial\\mathcal{L}}{\\partial\\xi_i} = C - \\mu_i - r_i = 0 \\frac{\\partial\\mathcal{L}}{\\partial\\xi_i} = C - \\mu_i - r_i = 0    Lets consider also complementary slackness conditions:  \\mu_i\\cdot\\left[y_i\\cdot\\left(\\vec w \\cdot \\vec x_i + b\\right) - 1 + \\xi_i\\right] = 0 \\mu_i\\cdot\\left[y_i\\cdot\\left(\\vec w \\cdot \\vec x_i + b\\right) - 1 + \\xi_i\\right] = 0     r_i\\xi_i = 0 r_i\\xi_i = 0    In the case  \\mu_i = 0 \\mu_i = 0 ,  r_i = C > 0 \\Rightarrow \\xi_i = 0 r_i = C > 0 \\Rightarrow \\xi_i = 0 :  y_i\\cdot\\left(\\vec w \\cdot \\vec x + b\\right) \\geq 1 y_i\\cdot\\left(\\vec w \\cdot \\vec x + b\\right) \\geq 1    In the case  \\mu_i = C \\mu_i = C ,  r_i = 0 \\Rightarrow \\xi_i \\geq 0 r_i = 0 \\Rightarrow \\xi_i \\geq 0 :  y_i\\cdot\\left(\\vec w \\cdot \\vec x + b\\right) \\leq 1 y_i\\cdot\\left(\\vec w \\cdot \\vec x + b\\right) \\leq 1    In the case  0 < \\mu_i < C 0 < \\mu_i < C ,  r_i = C - \\mu_i > 0 \\Rightarrow \\xi_i = 0 r_i = C - \\mu_i > 0 \\Rightarrow \\xi_i = 0 :  y_i\\cdot\\left(\\vec w \\cdot \\vec x + b\\right) = 1 y_i\\cdot\\left(\\vec w \\cdot \\vec x + b\\right) = 1",
            "title": "Regularization"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#smo-algorithm",
            "text": "We need to solve:   \\left.\\begin{array}{cc}\\text{maximize}_{\\mu} & \\sum\\limits_{i=1}^M\\mu_i - \\frac{1}{2}\\sum\\limits_{i,j=1}^My_iy_j\\mu_i\\mu_jk_{ij} \\equiv \\mathcal{L}_D(\\mu)\\\\\\text{subject to} & C \\geq \\mu_i \\geq 0 \\\\ & \\sum\\limits_{i=1}^M\\mu_iy_i = 0\\end{array}\\right. \\left.\\begin{array}{cc}\\text{maximize}_{\\mu} & \\sum\\limits_{i=1}^M\\mu_i - \\frac{1}{2}\\sum\\limits_{i,j=1}^My_iy_j\\mu_i\\mu_jk_{ij} \\equiv \\mathcal{L}_D(\\mu)\\\\\\text{subject to} & C \\geq \\mu_i \\geq 0 \\\\ & \\sum\\limits_{i=1}^M\\mu_iy_i = 0\\end{array}\\right.    where (for convenience)  k_{ij} = K(\\vec x_i, \\vec x_j) k_{ij} = K(\\vec x_i, \\vec x_j)    Sequential minimal optimization (SMO) algorithm solves the smallest possible optimization problem at a time    By updating two Lagrange multipliers  \\mu_i \\mu_i  in a step    One can not update only one because of the constraint  \\sum\\limits_i\\mu_iy_i = 0 \\sum\\limits_i\\mu_iy_i = 0",
            "title": "SMO algorithm"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#quick-math",
            "text": "Without loss of generality, lets consider optimizing  \\mu_1 \\mu_1  and  \\mu_2 \\mu_2  from an old set of feasible solution:  \\mu_1^{old}, \\mu_2^{old}, \\mu_3 \\cdots \\mu_M \\mu_1^{old}, \\mu_2^{old}, \\mu_3 \\cdots \\mu_M    Because of the constraint,  y_1\\mu_1 + y_2\\mu_2 = y_1\\mu_1^{old} + y_2\\mu_2^{old} y_1\\mu_1 + y_2\\mu_2 = y_1\\mu_1^{old} + y_2\\mu_2^{old}    Which can be rewritten as:  \\mu_1 = \\gamma - s\\mu_2 \\mu_1 = \\gamma - s\\mu_2    where  s = y_1y_2 s = y_1y_2  and  \\gamma = \\mu_1 + s\\mu_2 = \\mu_1^{old} + s\\mu_2^{old} \\gamma = \\mu_1 + s\\mu_2 = \\mu_1^{old} + s\\mu_2^{old}    Thus, the optimization is on a line as shown below (two possibilities based on the sign of  s s ):    plt . xticks ([],   [])  plt . yticks ([],   [])  plt . xlim ([ 0 , 1 ])  plt . ylim ([ 0 , 1 ])  plt . text ( 1.05 ,   0.5 ,   \"$\\mu_1 = C$\" )  plt . text ( - 0.1 ,   0.5 ,   \"$\\mu_1 = 0$\" )  plt . text ( 0.5 ,   1.05 ,   \"$\\mu_2 = C$\" )  plt . text ( 0.5 ,   - 0.05 ,   \"$\\mu_2 = 0$\" )  plt . plot (( 0.6 , 1 ),   ( 0 , 0.75 ),   label = \"$y_1  \\\\ neq y_2 \\Rightarrow \\mu_1 - \\mu_2 = \\gamma$\" )  plt . plot (( 0.4 , 0 ),   ( 0 , 0.75 ),    label = \"$y_1 = y_2 \\Rightarrow \\mu_1 + \\mu_2 = \\gamma$\" )  plt . legend ();     First lets fixed all  \\mu_i \\mu_i  for  i \\neq 1,2 i \\neq 1,2  and write the objective funtion in the following term:   $\\begin{eqnarray}\n\\mathcal{L}_D & = & \\mu_1 + \\mu_2 + const \\\\\n              & - & \\frac{1}{2}\\left(k_{11}\\mu_1^2 + k_{22}\\mu_2^2 + 2sk_{12}\\mu_1\\mu_2\\right) \\\\\n              & - & y_1\\mu_1\\sum\\limits_{i=3}^My_i\\mu_ik_{1i} - y_2\\mu_2\\sum\\limits_{i=3}^My_2\\mu_2k_{2i} \\\\\n              & + & const\n\\end{eqnarray}$    Where  const const  does not depend on either  \\mu_1 \\mu_1  nor  \\mu_2 \\mu_2    Please note, that when formulating the dual problem we got:  \\nabla_{w}\\mathcal{L}(\\vec w, b, \\mu) = \\vec w - \\sum\\limits_{i=1}^M\\mu_iy_i\\vec x_i = 0 \\Rightarrow \\vec w = \\sum\\limits_{i=1}^M\\mu_iy_i\\vec x_i \\nabla_{w}\\mathcal{L}(\\vec w, b, \\mu) = \\vec w - \\sum\\limits_{i=1}^M\\mu_iy_i\\vec x_i = 0 \\Rightarrow \\vec w = \\sum\\limits_{i=1}^M\\mu_iy_i\\vec x_i    For convenience, lets introduce:    $\\begin{eqnarray}\nv _j & = & \\sum\\limits_{i=3}y_i\\mu_ik_{ij} \\\\\n     & = & \\sum\\limits_{i=1}y_i\\mu_i^{old}k_{ij} + b^{old} - y_1\\mu_1^{old}k_{1j} - y_2\\mu_2^{old}k_{2j} - b^{old} \\\\\n     & = & z_j^{old} - b^{old} -  y_1\\mu_1^{old}k_{1j} - y_2\\mu_2^{old}k_{2j}\n\\end{eqnarray}$     where  z_j^{old} = \\vec x_j \\cdot \\vec w + b^{old} z_j^{old} = \\vec x_j \\cdot \\vec w + b^{old}  is the output of  \\vec x_j \\vec x_j  under old parameters.    Lets get back to the objective function:    $\\begin{eqnarray}\n\\mathcal{L}_D & = & \\mu_1 + \\mu_2 + const \\\\\n              & - & \\frac{1}{2}\\left(k_{11}\\mu_1^2 + k_{22}\\mu_2^2 + 2sk_{12}\\mu_1\\mu_2\\right) \\\\\n              & - & y_1\\mu_1v_1 - y_2\\mu_2v_2 + const\n\\end{eqnarray}$    We can remove  \\mu_1 \\mu_1  from the equation using  \\mu_1 = \\gamma - s \\mu_2 \\mu_1 = \\gamma - s \\mu_2    After long (but relatively straightforward) algebra we get:  \\mathcal{L}_D = \\frac{1}{2}\\eta\\mu_2^2 + \\left[y_2\\cdot\\left(E_1^{old} - E_2^{old}\\right) - \\eta\\mu_2^{old}\\right]\\mu_2 + const \\mathcal{L}_D = \\frac{1}{2}\\eta\\mu_2^2 + \\left[y_2\\cdot\\left(E_1^{old} - E_2^{old}\\right) - \\eta\\mu_2^{old}\\right]\\mu_2 + const    where    $\\begin{eqnarray}\n\\eta & = & 2k_{12} - k_{11} - k_{22} \\\\\nE_i^{old} & = & z_i^{old} - y_i\n\\end{eqnarray}$   To get a maximum we need first derivative to be zero and the second one to be negative   $\\begin{eqnarray}\n\\frac{d\\mathcal{L}_D}{d\\mu_2} & = & \\eta\\mu_2 + \\left[y_2\\cdot\\left(E_1^{old} - E_2^{old}\\right) - \\eta\\mu_2^{old}\\right] = 0 \\\\\n\\frac{d^2\\mathcal{L}_D}{d\\mu_2^2} & = & \\eta\n\\end{eqnarray}$    Please note, that  \\eta \\leq 0 \\eta \\leq 0  from the definition, but one must be careful when  \\eta = 0 \\eta = 0    If  \\eta < 0 \\eta < 0  we get the  unconstrained  maximum point:  \\mu_2^{new} = \\mu_2^{old} + \\frac{y_2\\left(E_2^{old} - E_1^{old}\\right)}{\\eta} \\mu_2^{new} = \\mu_2^{old} + \\frac{y_2\\left(E_2^{old} - E_1^{old}\\right)}{\\eta}    However, because of the constraint  0 \\leq \\mu_i \\leq C 0 \\leq \\mu_i \\leq C , we have to clip  \\mu_2^{new} \\mu_2^{new}    The minimum and maximum values depends on  s s  (see the plot above)    If  s = 1 s = 1 , then  \\mu_1 + \\mu_2 = \\gamma \\mu_1 + \\mu_2 = \\gamma :    If  \\gamma > C \\gamma > C , then  \\max\\mu_2 = C \\max\\mu_2 = C  and  \\min\\mu_2 = \\gamma - C \\min\\mu_2 = \\gamma - C    If  \\gamma < C \\gamma < C , then  \\max\\mu_2 = \\gamma \\max\\mu_2 = \\gamma  and  \\min\\mu_2 = 0 \\min\\mu_2 = 0      If  s = -1 s = -1 , then  \\mu_1 - \\mu_2 = \\gamma \\mu_1 - \\mu_2 = \\gamma :    If  \\gamma > C \\gamma > C , then  \\max\\mu_2 = C - \\gamma \\max\\mu_2 = C - \\gamma  and  \\min\\mu_2 = 0 \\min\\mu_2 = 0    If  \\gamma < C \\gamma < C , then  \\max\\mu_2 = C \\max\\mu_2 = C  and  \\min\\mu_2 = -\\gamma \\min\\mu_2 = -\\gamma      Let the minimum feasible value of  \\mu_2 \\mu_2  be  L L , and maximum be  H H :  \\mu_2^{new} \\rightarrow \\left\\{\\begin{array}{ll}H, & \\mu_2^{new} > H \\\\ \\mu_2^{new}, & L \\leq \\mu_2^{new} \\leq H \\\\ L & \\mu_2^{new} < L\\end{array}\\right. \\mu_2^{new} \\rightarrow \\left\\{\\begin{array}{ll}H, & \\mu_2^{new} > H \\\\ \\mu_2^{new}, & L \\leq \\mu_2^{new} \\leq H \\\\ L & \\mu_2^{new} < L\\end{array}\\right.",
            "title": "Quick math"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#math-summary",
            "text": "Given  \\mu_1 \\mu_1 ,  \\mu_2 \\mu_2  and the corresponding  y_1 y_1 ,  y_2 y_2 ,  k_{11} k_{11} ,  k_{12} k_{12} ,  k_{22} k_{22} ,  \\Delta E = E_2^{old} - E_1^{old} \\Delta E = E_2^{old} - E_1^{old}    Calculate  \\eta = 2k_{12} - k_{11} - k_{22} \\eta = 2k_{12} - k_{11} - k_{22}    If  \\eta > 0 \\eta > 0 : calculate  \\Delta\\alpha_2 = y_2\\Delta E / \\eta \\Delta\\alpha_2 = y_2\\Delta E / \\eta , clip the solution to the feasible region, and calculate  \\Delta\\mu_1 = -s\\Delta\\mu_2 \\Delta\\mu_1 = -s\\Delta\\mu_2    If  \\eta = 0 \\eta = 0 : evaluate the objective function at the two endpoints and choose  \\mu_2 \\mu_2  which correponds to the larger value of the objective function",
            "title": "Math summary"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#updating-after-a-successful-optimization-step",
            "text": "Lets define the predition error on  (\\vec x, y) (\\vec x, y) :  E(\\vec x, y) = \\sum\\limits_{i=1}^My_i\\mu_iK(\\vec x_i, \\vec x) + b - y E(\\vec x, y) = \\sum\\limits_{i=1}^My_i\\mu_iK(\\vec x_i, \\vec x) + b - y    The change in error after an update:  \\Delta E(\\vec x, y) = E^{new}(\\vec x, y) - E^{old}(\\vec x, y) =  y_1\\Delta\\mu_1 K(\\vec x_1, \\vec x) + y_2\\Delta\\mu_2 K(\\vec x_2, \\vec x) + \\Delta b \\Delta E(\\vec x, y) = E^{new}(\\vec x, y) - E^{old}(\\vec x, y) =  y_1\\Delta\\mu_1 K(\\vec x_1, \\vec x) + y_2\\Delta\\mu_2 K(\\vec x_2, \\vec x) + \\Delta b    If  \\mu_1 \\mu_1  is not at the bounds it forces  E^{new}(\\vec x_1, y_1) \\equiv E_1 = 0 E^{new}(\\vec x_1, y_1) \\equiv E_1 = 0 , thus  \\Delta b_1 = -E_1 - y_1\\Delta\\mu_1 K(\\vec x_1, \\vec x_1) - y_2\\Delta\\mu_2 K(\\vec x_2, \\vec x_1) \\Delta b_1 = -E_1 - y_1\\Delta\\mu_1 K(\\vec x_1, \\vec x_1) - y_2\\Delta\\mu_2 K(\\vec x_2, \\vec x_1)    Alternatively, if  \\mu_2 \\mu_2  is not at the bounds it forces  E^{new}(\\vec x_2, y_2) \\equiv E_2 = 0 E^{new}(\\vec x_2, y_2) \\equiv E_2 = 0 , thus  \\Delta b_2 = -E_2 - y_1\\Delta\\mu_1 K(\\vec x_1, \\vec x_2) - y_2\\Delta\\mu_2 K(\\vec x_2, \\vec x_2) \\Delta b_2 = -E_2 - y_1\\Delta\\mu_1 K(\\vec x_1, \\vec x_2) - y_2\\Delta\\mu_2 K(\\vec x_2, \\vec x_2)    If both  \\mu_1 \\mu_1  and  \\mu_2 \\mu_2  take  0 0  or  C C , SMO algorithm calculates new  b b  for both and takes average (any  b b  between new  b_1 b_1  and  b_2 b_2  satisfies KKT conditions)    Updating weights is straightforward:  \\Delta \\vec w = y_1\\Delta\\mu_1\\vec x_1 + y_2\\Delta\\mu_2\\vec x_2 \\Delta \\vec w = y_1\\Delta\\mu_1\\vec x_1 + y_2\\Delta\\mu_2\\vec x_2",
            "title": "Updating after a successful optimization step"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#choosing-mumu-to-optimize",
            "text": "The outer loop selects the first  \\mu_i \\mu_i  and the inner loop selects the second one  \\mu_j \\mu_j  that minimized  |E_j - E_i| |E_j - E_i|    The outer loop first goes through all examples selecting the ones violating KKT conditions    Then it iterates over all examples whose Lagrange multipliers are neither  0 0  nor  C C  (the non-bound examples)    It repeats passes through the non-bound examples until all satisify KKT condition within some error  \\epsilon \\epsilon    The it goes back and loop over all training set    SMO repeats the procedure until all examples satisfy KKT conditions within  \\epsilon \\epsilon    To avoid bullying CPU SMO keeps a cashed error value for every non-bound example",
            "title": "Choosing \\mu\\mu to optimize"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#initial-problem",
            "text": "Lets first consider the simple example given at the beginning of the lecture   # class I  X01   =   [( 2 ,   9 ),   ( 7 ,   19 ),   ( 1 ,   10 ),   ( 3 ,   19 ),   ( 4 ,   16 ),   ( 5 ,   18 )]  # class II  X02   =   [( 4 ,   3 ),   ( 6 ,   7 ),   ( 1 ,   - 10 ),   ( 3 ,   - 1 ),   ( 9 ,   5 ),   ( 5 ,   - 7 )]  plt . xlim ([ 0 ,   10 ])  plt . scatter ( * ( zip ( * X01 )))  plt . scatter ( * ( zip ( * X02 )));     As we know it is linear problem we will use the linear kernel   from   sklearn   import   svm  # create a classifier  clf   =   svm . SVC ( kernel = \"linear\" )  # train classifier - assign -1 label for X01 and 1 for X02  clf . fit ( X01   +   X02 ,   [ - 1 ] * len ( X01 )   +   [ 1 ] * len ( X02 ))   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n  max_iter=-1, probability=False, random_state=None, shrinking=True,\n  tol=0.001, verbose=False)   Lets visualize the result   sv   =   clf . support_vectors_    # support vectors  w   =   clf . coef_ [ 0 ]             # weights  b   =   clf . intercept_           # intercept  # w[0] * x + w[1] * y + b = 0  f   =   lambda   x :   - ( b   +   w [ 0 ] * x )   /   w [ 1 ]  # plot training data  plt . scatter ( * ( zip ( * X01 )))  plt . scatter ( * ( zip ( * X02 )))  # plt decision boundary  plt . plot ([ 1 , 9 ],   [ f ( 1 ),   f ( 9 )])  # mark support vectors  plt . scatter ( * ( zip ( * sv )),   zorder = 1 ,   facecolors = 'none' ,   edgecolors = 'r' ,   s = 500 );",
            "title": "Initial problem"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#circle",
            "text": "For the simple non-linear problem lets consider once again points inside / outside a circle    But this time let them also overlap a little bit die to some noise in data    def   generate_circle_data ( R1 = 0 ,   R2 = 1 ,   N = 500 ): \n   \"\"\"Generate N points in a circle for radius range (R1, R2)\"\"\" \n   r   =   lambda :   R1   +   np . random . random ()   *   ( R2   -   R1 )   +   np . random . normal ( 0 ,   0.2 ) \n\n   return   np . array ([( r ()   *   np . cos ( ang ),   r ()   *   np . sin ( ang )) \n                    for   ang   in   np . linspace ( 0 ,   2 * np . pi ,   N )])  C01   =   generate_circle_data ()  C02   =   generate_circle_data ( 1 ,   2 )  plt . scatter ( * C01 . T ,   marker = '.' )  plt . scatter ( * C02 . T ,   marker = '.' );      We will consider 4 different kernels:    linear    polynomial of degree 3    polynomial of degree 10    Gaussian radial basis function (RBF)      # create classifier with different kernels  clf_linear   =   svm . SVC ( kernel = \"linear\" )  clf_rbf   =   svm . SVC ( kernel = \"rbf\" )  clf_poly3   =   svm . SVC ( kernel = \"poly\" ,   degree = 3 )  clf_poly10   =   svm . SVC ( kernel = \"poly\" ,   degree = 10 )  titles   =   ( \"Linear\" ,   \"RBF\" ,   \"Polynomial, degree = 3\" ,   \"Plynomial, degree = 10\" )  # create a mesh to plot in  xx ,   yy   =   np . meshgrid ( np . arange ( - 3 ,   3 ,   0.01 ),   np . arange ( - 3 ,   3 ,   0.01 ))  # loop over classifiers  for   i ,   clf   in   enumerate (( clf_linear ,   clf_rbf ,   clf_poly3 ,   clf_poly10 )): \n   # train classifier - assign -1 label for C01 and 1 for C02 \n   clf . fit ( np . concatenate (( C01 ,   C02 ),   axis = 0 ),   [ - 1 ] * len ( C01 )   +   [ 1 ] * len ( C02 )) \n\n   # visualize results \n   plt . subplot ( 2 ,   2 ,   i   +   1 ) \n\n   # decision boundary \n   Z   =   clf . predict ( np . c_ [ xx . ravel (),   yy . ravel ()]) \n   Z   =   Z . reshape ( xx . shape ) \n   plt . contourf ( xx ,   yy ,   Z ,   cmap = plt . cm . Paired ) \n\n   # training data \n   plt . scatter ( * C01 . T ,   marker = '.' ) \n   plt . scatter ( * C02 . T ,   marker = '.' ) \n\n   plt . title ( titles [ i ])  plt . tight_layout ()",
            "title": "Circle"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#multiclass-classification",
            "text": "There are two popular methods used to apply binary classificators to multiclass problem    one-vs-rest  (ovr) or  one-vs-all  (ova)    having  K K  classes with labels  y_i = \\left\\{1, \\cdots, K\\right\\} y_i = \\left\\{1, \\cdots, K\\right\\}    train  K K  classifiers assuming  svm_i = 1 svm_i = 1  and  svm_{j\\neq i} = -1 svm_{j\\neq i} = -1    the prediction of a label for an unseen sample is based on a classifier which corresponds to the highest confidence score      one-vs-one  (ovo)    having  K K  classes with labels  y_i = \\left\\{1, \\cdots, K\\right\\} y_i = \\left\\{1, \\cdots, K\\right\\}    train  K(K-1)/2 K(K-1)/2  classifiers for each possible pair of classes    the prediction is based on the voting scheme",
            "title": "Multiclass classification"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#example-blobs",
            "text": "Lets use blobs form the previous lecture   from   sklearn.datasets   import   make_blobs  # generate 5 blobs with fixed random generator  X ,   Y   =   make_blobs ( n_samples = 500 ,   centers = 8 ,   random_state = 300 )  plt . scatter ( * X . T ,   c = Y ,   marker = '.' ,   cmap = 'Dark2' );     We can use the same function as last time to train and visualize   def   train_and_look ( classifier ,   X ,   Y ,   ax = None ,   title = '' ,   cmap = 'Dark2' ): \n   \"\"\"Train classifier on (X,Y). Plot data and prediction.\"\"\" \n   # create new axis if not provided \n   ax   =   ax   or   plt . gca (); \n\n   ax . set_title ( title ) \n\n   # plot training data \n   ax . scatter ( * X . T ,   c = Y ,   marker = '.' ,   cmap = cmap ) \n\n   # train a cliassifier \n   classifier . fit ( X ,   Y ) \n\n   # create a grid of testing points \n   x_ ,   y_   =   np . meshgrid ( np . linspace ( * ax . get_xlim (),   num = 200 ), \n                        np . linspace ( * ax . get_ylim (),   num = 200 )) \n\n   # convert to an array of 2D points \n   test_data   =   np . vstack ([ x_ . ravel (),   y_ . ravel ()]) . T \n\n   # make a prediction and reshape to grid structure  \n   z_   =   classifier . predict ( test_data ) . reshape ( x_ . shape ) \n\n   # arange z bins so class labels are in the middle \n   z_levels   =   np . arange ( len ( np . unique ( Y ))   +   1 )   -   0.5 \n\n   # plot contours corresponding to classifier prediction \n   ax . contourf ( x_ ,   y_ ,   z_ ,   alpha = 0.25 ,   cmap = cmap ,   levels = z_levels )   fig ,   ax   =   plt . subplots ( 1 ,   3 ,   figsize = ( 15 , 5 ))  title   =   ( \"Linear\" ,   \"RBF\" ,   \"Polynomial\" )  settings   =   ({ \"kernel\" :   \"linear\" },   { \"kernel\" :   \"rbf\" }, \n             { \"kernel\" :   \"poly\" ,   \"degree\" :   5 })  # train and look at SVM with different kernels  for   i   in   range ( 0 ,   3 ): \n   train_and_look ( svm . SVC ( ** settings [ i ]),   X ,   Y , \n                  ax = ax [ i ],   title = title [ i ])",
            "title": "Example: blobs"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#svm-regression",
            "text": "Lets consider the problem of approximating the set of data  \\left\\{(\\vec x_1, y_1), \\cdots, (\\vec x_N, y_N)\\right\\} \\left\\{(\\vec x_1, y_1), \\cdots, (\\vec x_N, y_N)\\right\\} , where  \\vec x_i \\in \\mathcal{R}^n \\vec x_i \\in \\mathcal{R}^n  and  y_i \\in \\mathcal{R} y_i \\in \\mathcal{R}  with a linear function:  f(\\vec x) = \\left<\\vec w, \\vec x\\right> + b f(\\vec x) = \\left<\\vec w, \\vec x\\right> + b    In epsilon-insensitive SVM ( \\varepsilon \\varepsilon -SVM) the goal is to find  f(x) f(x)  that deviates from  y_i y_i  by a value not greater than  \\varepsilon \\varepsilon  for each training point, and at the same time is as flat as possible.    And once again we end up with a optimization problem:  \\left.\\begin{array}{cc}\\text{minimize}_{w,b} & \\frac{1}{2}|\\vec w|^2 \\\\\\text{subject to} & y_i - \\left<\\vec w, \\vec x_i\\right> - b \\leq \\varepsilon\\\\ & \\left<\\vec w, \\vec x_i\\right> + b - y_i \\leq \\varepsilon\\end{array}\\right. \\left.\\begin{array}{cc}\\text{minimize}_{w,b} & \\frac{1}{2}|\\vec w|^2 \\\\\\text{subject to} & y_i - \\left<\\vec w, \\vec x_i\\right> - b \\leq \\varepsilon\\\\ & \\left<\\vec w, \\vec x_i\\right> + b - y_i \\leq \\varepsilon\\end{array}\\right.    It is enough if such  f f  actually exists    However, analogously to soft margin, one can allow for some errors by introducing slack variables  \\xi_i \\xi_i  and  \\xi^*_i \\xi^*_i , and reformulate the optimization problem:  \\left.\\begin{array}{cc}\\text{minimize}_{w,b} & \\frac{1}{2}|\\vec w|^2 + C\\sum\\limits_{i=1}^N(\\xi_i + \\xi^*_i) \\\\\\text{subject to} & y_i - \\left<\\vec w, \\vec x_i\\right> - b \\leq \\varepsilon + \\xi_i\\\\ & \\left<\\vec w, \\vec x_i\\right> + b - y_i \\leq \\varepsilon + \\xi^*_i \\\\ & \\xi_i, \\xi^*_i \\geq 0\\end{array}\\right. \\left.\\begin{array}{cc}\\text{minimize}_{w,b} & \\frac{1}{2}|\\vec w|^2 + C\\sum\\limits_{i=1}^N(\\xi_i + \\xi^*_i) \\\\\\text{subject to} & y_i - \\left<\\vec w, \\vec x_i\\right> - b \\leq \\varepsilon + \\xi_i\\\\ & \\left<\\vec w, \\vec x_i\\right> + b - y_i \\leq \\varepsilon + \\xi^*_i \\\\ & \\xi_i, \\xi^*_i \\geq 0\\end{array}\\right.    The corresponding dual problem is given by:  \\left.\\begin{array}{cc}\\text{minimize}_{\\mu} & -\\frac{1}{2}\\sum\\limits_{i, j = 1}^N(\\mu_i - \\mu^*_i)(\\mu_j - \\mu^*_j)\\left<\\vec x_i, \\vec x_j\\right> - \\varepsilon\\sum\\limits_{i=1}^N(\\mu_i + \\mu^*_i) + \\sum\\limits_{i=1}^Ny_i(\\mu_i - \\mu^*_i)\\\\\\text{subject to} & \\sum\\limits_{i=1}^N(\\mu_i - \\mu^*_i) = 0 \\\\ & \\mu_i, \\mu^*_i\\in\\left[0, C\\right]\\end{array}\\right. \\left.\\begin{array}{cc}\\text{minimize}_{\\mu} & -\\frac{1}{2}\\sum\\limits_{i, j = 1}^N(\\mu_i - \\mu^*_i)(\\mu_j - \\mu^*_j)\\left<\\vec x_i, \\vec x_j\\right> - \\varepsilon\\sum\\limits_{i=1}^N(\\mu_i + \\mu^*_i) + \\sum\\limits_{i=1}^Ny_i(\\mu_i - \\mu^*_i)\\\\\\text{subject to} & \\sum\\limits_{i=1}^N(\\mu_i - \\mu^*_i) = 0 \\\\ & \\mu_i, \\mu^*_i\\in\\left[0, C\\right]\\end{array}\\right.    And the solution is given by:  \\vec w = \\sum\\limits_{i=1}^N(\\mu_i - \\mu^*_i)\\vec x_i \\vec w = \\sum\\limits_{i=1}^N(\\mu_i - \\mu^*_i)\\vec x_i , thus:  f(\\vec x) = \\sum\\limits_{i=1}^N(\\mu_i - \\mu^*_i)\\left<\\vec x_i, \\vec x\\right> + b f(\\vec x) = \\sum\\limits_{i=1}^N(\\mu_i - \\mu^*_i)\\left<\\vec x_i, \\vec x\\right> + b    Or for the kernelized version:  f(\\vec x) = \\sum\\limits_{i=1}^N(\\mu_i - \\mu^*_i)K(\\vec x_i, \\vec x)+ b f(\\vec x) = \\sum\\limits_{i=1}^N(\\mu_i - \\mu^*_i)K(\\vec x_i, \\vec x)+ b    The intercept  b b  can be calculate for any support vector as  f(\\vec x_{SV}) = y_{SV} f(\\vec x_{SV}) = y_{SV}",
            "title": "SVM regression"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#example-boston-housing-dataset",
            "text": "from   sklearn.datasets   import   load_boston  boston   =   load_boston ()  print ( boston . DESCR )   Boston House Prices dataset\n===========================\n\nNotes\n------\nData Set Characteristics:\n\n    :Number of Instances: 506\n\n    :Number of Attributes: 13 numeric/categorical predictive\n\n    :Median Value (attribute 14) is usually the target\n\n    :Attribute Information (in order):\n        - CRIM     per capita crime rate by town\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n        - INDUS    proportion of non-retail business acres per town\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n        - NOX      nitric oxides concentration (parts per 10 million)\n        - RM       average number of rooms per dwelling\n        - AGE      proportion of owner-occupied units built prior to 1940\n        - DIS      weighted distances to five Boston employment centres\n        - RAD      index of accessibility to radial highways\n        - TAX      full-value property-tax rate per $10,000\n        - PTRATIO  pupil-teacher ratio by town\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n        - LSTAT    % lower status of the population\n        - MEDV     Median value of owner-occupied homes in $1000's\n\n    :Missing Attribute Values: None\n\n    :Creator: Harrison, D. and Rubinfeld, D.L.\n\nThis is a copy of UCI ML housing dataset.\nhttp://archive.ics.uci.edu/ml/datasets/Housing\n\n\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\nprices and the demand for clean air', J. Environ. Economics & Management,\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\npages 244-261 of the latter.\n\nThe Boston house-price data has been used in many machine learning papers that address regression\nproblems.\n\n**References**\n\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)",
            "title": "Example - Boston Housing dataset"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#visualize-dataset",
            "text": "for   i ,   feature   in   enumerate ( boston . data . T ): \n   plt . subplot ( 3 ,   5 ,   i + 1 ) \n   plt . scatter ( feature ,   boston . target ,   marker = '.' ) \n   plt . title ( boston . feature_names [ i ])  plt . tight_layout ()    import   pandas   as   pd  boston_pd   =   pd . DataFrame ( boston . data )       # load features  boston_pd . columns   =   boston . feature_names    # add features names  boston_pd [ 'PRICE' ]   =   boston . target          # add a column with price  boston_pd . head ()    \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       CRIM \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       PRICE \n     \n   \n   \n     \n       0 \n       0.00632 \n       18.0 \n       2.31 \n       0.0 \n       0.538 \n       6.575 \n       65.2 \n       4.0900 \n       1.0 \n       296.0 \n       15.3 \n       396.90 \n       4.98 \n       24.0 \n     \n     \n       1 \n       0.02731 \n       0.0 \n       7.07 \n       0.0 \n       0.469 \n       6.421 \n       78.9 \n       4.9671 \n       2.0 \n       242.0 \n       17.8 \n       396.90 \n       9.14 \n       21.6 \n     \n     \n       2 \n       0.02729 \n       0.0 \n       7.07 \n       0.0 \n       0.469 \n       7.185 \n       61.1 \n       4.9671 \n       2.0 \n       242.0 \n       17.8 \n       392.83 \n       4.03 \n       34.7 \n     \n     \n       3 \n       0.03237 \n       0.0 \n       2.18 \n       0.0 \n       0.458 \n       6.998 \n       45.8 \n       6.0622 \n       3.0 \n       222.0 \n       18.7 \n       394.63 \n       2.94 \n       33.4 \n     \n     \n       4 \n       0.06905 \n       0.0 \n       2.18 \n       0.0 \n       0.458 \n       7.147 \n       54.2 \n       6.0622 \n       3.0 \n       222.0 \n       18.7 \n       396.90 \n       5.33 \n       36.2",
            "title": "Visualize dataset"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#test-svr",
            "text": "Below there is a result for the linear kernel - feel free to play with kernels and/or parameter  C C  on your own   from   sklearn.svm   import   SVR  regressor   =   SVR ( kernel = \"linear\" )  regressor . fit ( boston . data ,   boston . target )  prediction   =   regressor . predict ( boston . data )  plt . xlabel ( \"True price\" )  plt . ylabel ( \"Predicted price\" )  plt . scatter ( boston . target ,   prediction ,   marker = '.' );",
            "title": "Test SVR"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_03_svm/introduction_to_machine_learning_03_svm/#summary",
            "text": "Support vector machine is a powerful model for both classification and regression    It guarantees finding the optimal hyperplane / decision boundary (if exists)    It is effective with high dimensional data    The dual formulation of the optimization problem allows easily to introduce kernels and deal with non-linear data    It has just few hyperparameters: kernel + kernel's parameters and  C C    Inference is fast as it depends only on a subset of training samples (support vectors) - although training may be slow for large datasets",
            "title": "Summary"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/",
            "text": "Neural Network\n\u00b6\n\n\n\n\n\n\nArtificial neural network (in particular deep NN) is the most popular machine learning method these days\n\n\n\n\n\n\nThey are inspired by human brains (at least initially)\n\n\n\n\n\n\nArtifical neuron is a mathematical function\n\n\n\n\n\n\nNeurons are connected with each other (kind of synapses)\n\n\n\n\n\n\nUsually connections have some weights\n\n\n\n\n\n\nToday, feedforward neural networks (multilayer perceptrons) will be discussed\n\n\n\n\n\n\nHowever, before we go there, lets start with linear and logistic regression\n\n\n\n\n\n\n# our standard imports: matplotlib and numpy\n\n\nimport\n \nmatplotlib.pyplot\n \nas\n \nplt\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\n\n# just to overwrite default colab style\n\n\nplt\n.\nstyle\n.\nuse\n(\n'default'\n)\n\n\nplt\n.\nstyle\n.\nuse\n(\n'seaborn-talk'\n)\n\n\n\n\n\n\nLinear regression\n\u00b6\n\n\n\n\n\n\nLets consider dataset \n\\left\\{(\\vec x^{(i)}, y^{(i)}); i = 1,\\cdots, N\\right\\}\n\\left\\{(\\vec x^{(i)}, y^{(i)}); i = 1,\\cdots, N\\right\\}\n\n\n\n\n\n\nwhere explanatory variables (\nfeatures\n) \n\\vec x^{(i)} = (x^{(i)}_1,\\cdots, x^{(i)}_n) \\in \\mathcal{R}^n\n\\vec x^{(i)} = (x^{(i)}_1,\\cdots, x^{(i)}_n) \\in \\mathcal{R}^n\n\n\n\n\n\n\nand dependent variables (\ntargets\n) \ny^{(i)} \\in \\mathcal{R}\ny^{(i)} \\in \\mathcal{R}\n\n\n\n\n\n\nLets define the \nhypothesis\n: \nh(\\vec x) = w_0 + w_1x_1 + \\cdots w_nx_n\nh(\\vec x) = w_0 + w_1x_1 + \\cdots w_nx_n\n\n\n\n\n\n\nIn other words we claim that \ny^{(i)}\ny^{(i)}\n can be calculated from \nh(\\vec x^{(i)})\nh(\\vec x^{(i)})\n, if we know \nweights\n \nw_i\nw_i\n\n\n\n\n\n\nFor convenience lets set \nx^{(i)}_0 = 1\nx^{(i)}_0 = 1\n, so we can rewrite the hypothesis: \nh(\\vec x) = \\sum_{i=0}^nw_ix_i = \\vec w \\cdot \\vec x\nh(\\vec x) = \\sum_{i=0}^nw_ix_i = \\vec w \\cdot \\vec x\n\n\n\n\n\n\nNormal equation\n\u00b6\n\n\n\n\n\n\nOne could find weights using normal equation\n\n\n\n\n\n\nLet \nY^T = (y^{(1)}, \\cdots, y^{(N)})\nY^T = (y^{(1)}, \\cdots, y^{(N)})\n, \nX^T = \\left((\\vec x^{(1)})^T, \\cdots, (\\vec x^{(N)})^T\\right)\nX^T = \\left((\\vec x^{(1)})^T, \\cdots, (\\vec x^{(N)})^T\\right)\n and \nW^T = (w_0, \\cdots, w_n)\nW^T = (w_0, \\cdots, w_n)\n\n\n\n\n\n\nThen we can write a matrix equation: \nY = XW\nY = XW\n\n\n\n\n\n\nThe normal equation (minimizing the sum of the square differences between left and right sides): \nX^TY = X^TXW\nX^TY = X^TXW\n\n\n\n\n\n\nThus, one could find weights by calculating: \nW = (X^TX)^{-1}X^TY\nW = (X^TX)^{-1}X^TY\n\n\n\n\n\n\nDoable, but computational expensive\n\n\n\n\n\n\nGradient descent\n\u00b6\n\n\n\n\n\n\nGradient descent is an iterative algorithm for finding the minimum\n\n\n\n\n\n\nFor linear regression the \ncost function\n (or \nloss function\n) is given by mean squared error: \nL(\\vec w) = \\frac{1}{2N}\\sum\\limits_{i=1}^N\\left(h(\\vec x^{(i)}) - y^{(i)}\\right)^2\nL(\\vec w) = \\frac{1}{2N}\\sum\\limits_{i=1}^N\\left(h(\\vec x^{(i)}) - y^{(i)}\\right)^2\n\n\n\n\n\n\nIt measures the quality of given set of parameters / weights\n\n\n\n\n\n\nPlease note, that \n\\frac{1}{2}\n\\frac{1}{2}\n is added for convenience to MSE definition\n\n\n\n\n\n\nIn gradient descent method weights are updated w.r.t. the gradient of cost function: \nw_j \\rightarrow w_j - \\alpha\\frac{\\partial L(\\vec w)}{\\partial w_j} = w_j - \\frac{\\alpha}{N}\\sum\\limits_{i=1}^{N}\\left(h(\\vec x^{(i)}) - y^{(i)}\\right)x^{(i)}_j\nw_j \\rightarrow w_j - \\alpha\\frac{\\partial L(\\vec w)}{\\partial w_j} = w_j - \\frac{\\alpha}{N}\\sum\\limits_{i=1}^{N}\\left(h(\\vec x^{(i)}) - y^{(i)}\\right)x^{(i)}_j\n\n\n\n\n\n\nWhere \n\\alpha\n\\alpha\n is training rate\n\n\n\n\n\n\nExample\n\u00b6\n\n\n\n\n\n\nLets generate some fake data according to \ny = ax + b\ny = ax + b\n for given slope and intercept\n\n\n\n\n\n\nAnd add some noise to \ny\ny\n\n\n\n\n\n\n### SETTINGS ###\n\n\n\nN\n \n=\n \n100\n \n# number of samples\n\n\n\na\n \n=\n \n0.50\n \n# slope\n\n\nb\n \n=\n \n0.50\n \n# y-intercept\n\n\ns\n \n=\n \n0.25\n \n# sigma\n\n\n\n### GENERATE SAMPLES ###\n\n\n\nX\n \n=\n \n(\n10.0\n \n*\n \nnp\n.\nrandom\n.\nsample\n(\nN\n))\n                               \n# features\n\n\nY\n \n=\n \n[(\na\n \n*\n \nX\n[\ni\n]\n \n+\n \nb\n)\n \n+\n \nnp\n.\nrandom\n.\nnormal\n(\n0\n,\ns\n)\n \nfor\n \ni\n \nin\n \nrange\n(\nN\n)]\n \n# targets\n\n\n\n### PLOT SAMPLES ###\n\n\n\nplt\n.\nxlabel\n(\n'Feature'\n)\n\n\nplt\n.\nylabel\n(\n'Target'\n)\n\n\nplt\n.\nscatter\n(\nX\n,\n \nY\n,\n \nmarker\n=\n'.'\n);\n\n\n\n\n\n\n\n\n\n\nIt is time to learn about new framework\n\n\n\n\n\n\nTheano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. It can use GPUs and perform efficient symbolic differentiation.\n\n\n\n\n!\npip\n \ninstall\n \ntheano\n\n\n\n\n\n\n\n\n\n\nIn this example the hypothesis is given by: \nh(x) = \\vec w \\cdot \\vec x = ax + b\nh(x) = \\vec w \\cdot \\vec x = ax + b\n\n\n\n\n\n\nwhere \n\\vec w = (b, a)\n\\vec w = (b, a)\n and \n\\vec x = (1, x)\n\\vec x = (1, x)\n\n\n\n\n\n\nLets first create symbolic variable for:\n\n\n\n\n\n\nfeature vector \nX = (x_1, \\cdots, x_N)\nX = (x_1, \\cdots, x_N)\n\n\n\n\n\n\ntarget \nY = (y_1, \\cdots, y_N)\nY = (y_1, \\cdots, y_N)\n\n\n\n\n\n\nweights \na\na\n and \nb\nb\n (or \nw_1\nw_1\n and \nw_0\nw_0\n)\n\n\n\n\n\n\n\n\n\n\nimport\n \ntheano\n\n\nimport\n \ntheano.tensor\n \nas\n \nT\n\n\n\nx\n \n=\n \nT\n.\nvector\n(\n'x'\n)\n \n# feature vector\n\n\ny\n \n=\n \nT\n.\nvector\n(\n'y'\n)\n \n# target vector\n\n\n\n# weights initialized randomly\n\n\n#a = theano.shared(np.random.randn(), name = 'w')\n\n\n#b = theano.shared(np.random.randn(), name = 'b')\n\n\n\n# initial weights by hand for demonstration (random may be to close)\n\n\na\n \n=\n \ntheano\n.\nshared\n(\n-\n0.5\n,\n \nname\n \n=\n \n'w'\n)\n\n\nb\n \n=\n \ntheano\n.\nshared\n(\n1.0\n,\n \nname\n \n=\n \n'b'\n)\n\n\n\n\n\n\n\n\n\n\nHaving that, we can define:\n\n\n\n\n\n\nhypothesis\n\n\n\n\n\n\ncost function\n\n\n\n\n\n\ngradients\n\n\n\n\n\n\n\n\n\n\npred\n \n=\n \nT\n.\ndot\n(\nx\n,\n \na\n)\n \n+\n \nb\n                 \n# hyphothesis\n\n\ncost\n \n=\n \nT\n.\nsum\n(\nT\n.\npow\n(\npred\n \n-\n \ny\n,\n \n2\n))\n \n/\n \nN\n   \n# cost function\n\n\ngrad_a\n,\n \ngrad_b\n \n=\n \nT\n.\ngrad\n(\ncost\n,\n \n[\na\n,\n \nb\n])\n  \n# gradients\n\n\n\n\n\n\n\n\nAnd finally, we define gradient descent method (which also returns the value of the cost function)\n\n\n\n\nalpha\n \n=\n \n0.005\n \n# learning rate\n\n\n\n# at each training step we update weights:\n\n\n# w -> w - alpha * grad_w and b -> b - alpha * grad_b\n\n\ntrain\n \n=\n \ntheano\n.\nfunction\n(\ninputs\n \n=\n \n[\nx\n,\ny\n],\n\n                        \noutputs\n \n=\n \ncost\n,\n\n                        \nupdates\n \n=\n \n((\na\n,\n \na\n \n-\n \nalpha\n \n*\n \ngrad_a\n),\n\n                                   \n(\nb\n,\n \nb\n \n-\n \nalpha\n \n*\n \ngrad_b\n)))\n\n\n\n\n\n\n\n\nEach training step involves the full cycle on training data (\nepoch\n)\n\n\n\n\nn_epochs\n \n=\n \n1000\n  \n# number of training steps / epochs\n\n\ncosts\n \n=\n \n[]\n       \n# to keep track on the value of cost function on each step\n\n\nweights\n \n=\n \n[]\n     \n# to store few set of weights\n\n\n\nkeep\n \n=\n \n(\n0\n,\n \n10\n,\n \n100\n,\n \n500\n,\n \n1000\n)\n  \n# save result for some epochs passed\n\n\n\nfor\n \ni\n \nin\n \nrange\n(\nn_epochs\n \n+\n \n1\n):\n\n  \nif\n \ni\n \nin\n \nkeep\n:\n\n    \nweights\n.\nappend\n((\na\n.\nget_value\n(),\n \nb\n.\nget_value\n()))\n\n\n  \ncosts\n.\nappend\n(\ntrain\n(\nX\n,\n \nY\n))\n\n\n\n\n\n\n\n\nFinally, we can visualize the results\n\n\n\n\nplt\n.\nfigure\n(\nfigsize\n=\n(\n10\n,\n \n15\n))\n\n\nn_rows\n \n=\n \n3\n\n\nn_cols\n \n=\n \n2\n\n\n\nfor\n \ni\n,\n \n(\na_\n,\n \nb_\n)\n \nin\n \nenumerate\n(\nweights\n):\n\n  \nplt\n.\nsubplot\n(\nn_rows\n,\n \nn_cols\n,\n \ni\n+\n1\n)\n\n\n  \nplt\n.\ntitle\n(\n'Epoch \n%i\n: y = \n%.2f\n x + \n%.2f\n'\n \n%\n \n(\nkeep\n[\ni\n],\n \na_\n,\n \nb_\n))\n\n  \nplt\n.\nxlabel\n(\n'Feature'\n)\n\n  \nplt\n.\nylabel\n(\n'Target'\n)\n\n\n  \nx_\n \n=\n \nnp\n.\narange\n(\n0\n,\n \n10\n,\n \n0.1\n)\n\n\n  \nplt\n.\nplot\n(\nx_\n,\n \na_\n*\nx_\n \n+\n \nb_\n,\n \ncolor\n=\n'C1'\n)\n\n  \nplt\n.\nscatter\n(\nX\n,\n \nY\n,\n \nmarker\n=\n'.'\n)\n\n\n\n\nplt\n.\nsubplot\n(\nn_rows\n,\n \nn_cols\n,\n \nlen\n(\nweights\n)\n \n+\n \n1\n)\n\n\nplt\n.\ntitle\n(\n\"Cost function\"\n)\n\n\nplt\n.\nxlabel\n(\n\"Epoch\"\n)\n\n\nplt\n.\nylabel\n(\n\"L\"\n)\n\n\nplt\n.\nylim\n([\n0\n,\n0.2\n])\n\n\n\nplt\n.\nplot\n(\nrange\n(\nlen\n(\ncosts\n)),\n \ncosts\n)\n\n\n\nplt\n.\ntight_layout\n();\n\n\n\n\n\n\n\n\nLogistic regression\n\u00b6\n\n\n\n\n\n\nLogistic regression is used when dependent variable (target) is categorical\n\n\n\n\n\n\nLets consider dataset \n\\left\\{(\\vec x^{(i)}, y^{(i)}); i = 1,\\cdots, N\\right\\}\n\\left\\{(\\vec x^{(i)}, y^{(i)}); i = 1,\\cdots, N\\right\\}\n\n\n\n\n\n\nwhere independent variables \n\\vec x^{(i)} = (1, x^{(i)}_1,\\cdots, x^{(i)}_n)\n\\vec x^{(i)} = (1, x^{(i)}_1,\\cdots, x^{(i)}_n)\n\n\n\n\n\n\nand dependent variables (we start with binary case) \ny^{(i)} \\in \\{0, 1\\}\ny^{(i)} \\in \\{0, 1\\}\n\n\n\n\n\n\nHypothesis\n\u00b6\n\n\n\n\n\n\nThe dependent variable follows Bernoulli distribution (\n1\n1\n with probability \np\np\n, \n0\n0\n with probability \n1-p\n1-p\n)\n\n\n\n\n\n\nWe want to link the independent variable with Bernoulli distribution\n\n\n\n\n\n\nThe \nlogit function\n \ntranslates\n a linear combination \n\\vec w \\cdot \\vec x\n\\vec w \\cdot \\vec x\n (which can result in any value) into probability distribution: \nlogit(p) = \\ln(odds) = \\ln\\left(\\frac{p}{1-p}\\right) = \\vec w \\cdot \\vec x\nlogit(p) = \\ln(odds) = \\ln\\left(\\frac{p}{1-p}\\right) = \\vec w \\cdot \\vec x\n\n\n\n\n\n\np_\n \n=\n \nnp\n.\narange\n(\n0.01\n,\n \n0.99\n,\n \n0.01\n)\n\n\n\nplt\n.\ntitle\n(\n\"Logit function\"\n)\n\n\nplt\n.\nxlabel\n(\n\"p\"\n)\n\n\nplt\n.\nylabel\n(\n\"$\\ln(p/(1-p))$\"\n)\n\n\n\nplt\n.\nplot\n(\np_\n,\n \nnp\n.\nlog\n(\np_\n \n/\n \n(\n1\n \n-\n \np_\n)));\n\n\n\n\n\n\n\n\n\n\n\n\nIn logistic regression \nhypothesis\n is defined as the inverse function of logit - \nlogistic function\n: \nh(\\vec x) = \\frac{1}{1 + e^{-\\vec w \\cdot \\vec x}}\nh(\\vec x) = \\frac{1}{1 + e^{-\\vec w \\cdot \\vec x}}\n\n\n\n\n\n\nThus, the probability of \n1\n1\n is given by \nP(y = 1~|~\\vec x, \\vec w) = h(\\vec x)\nP(y = 1~|~\\vec x, \\vec w) = h(\\vec x)\n\n\n\n\n\n\nand the probability of \n0\n0\n is given by \nP(y = 0~|~\\vec x, \\vec w) = 1 - h(\\vec x)\nP(y = 0~|~\\vec x, \\vec w) = 1 - h(\\vec x)\n\n\n\n\n\n\nx_\n \n=\n \nnp\n.\narange\n(\n-\n10\n,\n \n10\n,\n \n0.1\n)\n\n\n\nplt\n.\ntitle\n(\n\"Logistic function\"\n)\n\n\nplt\n.\nxlabel\n(\n\"x\"\n)\n\n\nplt\n.\nylabel\n(\n\"$1/(1 + e^{-x})$\"\n)\n\n\n\nplt\n.\nplot\n(\nx_\n,\n \n1\n \n/\n \n(\n1\n \n+\n \nnp\n.\nexp\n(\n-\nx_\n)));\n\n\n\n\n\n\n\n\nCost function\n\u00b6\n\n\n\n\n\n\nThe probablity mass function (PMS) for \ny\ny\n (for given \n\\vec w\n\\vec w\n): \np(y~|~\\vec x, \\vec w) = h(\\vec x)^y\\cdot \\left(1 - h(\\vec x)\\right)^{1 - y}\np(y~|~\\vec x, \\vec w) = h(\\vec x)^y\\cdot \\left(1 - h(\\vec x)\\right)^{1 - y}\n\n\n\n\n\n\nThe \nlikelihood function\n is PMS considered as a function of \n\\vec w\n\\vec w\n (for fixed \ny\ny\n)\n\n\n\n\n\n\nThus, for a single data point \n(\\vec x^{(i)}, y^{(i)})\n(\\vec x^{(i)}, y^{(i)})\n: \nl(\\vec w) = h(\\vec x^{(i)})^{y^{(i)}}\\cdot \\left(1 - h(\\vec x^{(i)})\\right)^{1 - y^{(i)}}\nl(\\vec w) = h(\\vec x^{(i)})^{y^{(i)}}\\cdot \\left(1 - h(\\vec x^{(i)})\\right)^{1 - y^{(i)}}\n\n\n\n\n\n\nAnd for the whole dataset: \nl(\\vec w) = \\prod\\limits_{i=1}^N h(\\vec x^{(i)})^{y^{(i)}}\\cdot \\left(1 - h(\\vec x^{(i)})\\right)^{1 - y^{(i)}}\nl(\\vec w) = \\prod\\limits_{i=1}^N h(\\vec x^{(i)})^{y^{(i)}}\\cdot \\left(1 - h(\\vec x^{(i)})\\right)^{1 - y^{(i)}}\n\n\n\n\n\n\nThe goal is to maximize likelihood w.r.t to \n\\vec w\n\\vec w\n, which is the same as maximizing \nlog-likelihood\n: \n\\ln\\left(l(\\vec w)\\right) = \\sum\\limits_{i=1}^N\\left[y^{(i)}\\ln\\left(h(\\vec x^{(i)})\\right) + \\left(1 - y^{(i)}\\right)\\ln\\left(1 - h(\\vec x^{(i)})\\right)\\right]\n\\ln\\left(l(\\vec w)\\right) = \\sum\\limits_{i=1}^N\\left[y^{(i)}\\ln\\left(h(\\vec x^{(i)})\\right) + \\left(1 - y^{(i)}\\right)\\ln\\left(1 - h(\\vec x^{(i)})\\right)\\right]\n\n\n\n\n\n\nWhich is the same as minimizing the cost function \nL(\\vec w) = -\\frac{1}{N}\\ln\\left(l(\\vec w)\\right)\nL(\\vec w) = -\\frac{1}{N}\\ln\\left(l(\\vec w)\\right)\n\n\n\n\n\n\nOnce again using gradient descent method we can update weights using: \nw_j \\rightarrow w_j - \\alpha\\frac{\\partial L(\\vec w)}{\\partial w_j} = w_j - \\frac{\\alpha}{N}\\sum\\limits_{i=1}^{N}\\left(y^{(i)} - h(\\vec x^{(i)})\\right)x^{(i)}_j\nw_j \\rightarrow w_j - \\alpha\\frac{\\partial L(\\vec w)}{\\partial w_j} = w_j - \\frac{\\alpha}{N}\\sum\\limits_{i=1}^{N}\\left(y^{(i)} - h(\\vec x^{(i)})\\right)x^{(i)}_j\n\n\n\n\n\n\nQuick proof\n\u00b6\n\n\n\n\nFirst, lets consider the derivative of \nh\nh\n\n\n\n\n$\\begin{eqnarray}\n\\frac{\\partial h(\\vec x)}{\\partial w_j} & = & \\frac{\\partial}{\\partial w_j}\\left(1 + e^{-\\vec w \\cdot \\vec x}\\right)^{-1} \\\\\n                                        & = & e^{-\\vec w \\cdot \\vec x}\\left(1 + e^{-\\vec w \\cdot \\vec x}\\right)^{-2}x_j \\\\\n                                        & = & (1 + e^{-\\vec w \\cdot \\vec x} - 1)\\left(1 + e^{-\\vec w \\cdot \\vec x}\\right)^{-2}x_j \\\\\n                                        & = & \\left(1 + e^{-\\vec w \\cdot \\vec x}\\right)^{-1}x_j - \\left(1 + e^{-\\vec w \\cdot \\vec x}\\right)^{-2}x_j\\\\\n                                        & = & h(\\vec x)x_j - h^2(\\vec x)x_j \\\\\n                                        & = & h(\\vec x)\\left(1 - h(\\vec x)\\right)x_j\n\\end{eqnarray}$\n\n\n\n\n\n\n\n\n\nThus, \n\\frac{\\partial}{\\partial w_j}\\ln\\left(h(\\vec x)\\right) = \\left(1 - h(\\vec x)\\right)x_j\n\\frac{\\partial}{\\partial w_j}\\ln\\left(h(\\vec x)\\right) = \\left(1 - h(\\vec x)\\right)x_j\n\n\n\n\n\n\nand \n\\frac{\\partial}{\\partial w_j}\\ln\\left(1 - h(\\vec x)\\right) = -h(\\vec x)x_j\n\\frac{\\partial}{\\partial w_j}\\ln\\left(1 - h(\\vec x)\\right) = -h(\\vec x)x_j\n\n\n\n\n\n\nFinally, we have \n\\frac{\\partial}{\\partial w_j}\\left[y\\ln\\left(h(\\vec x)\\right) + (1 - y)\\ln\\left(1 - h(\\vec x)\\right)\\right] = \\left[y\\left(1 - h(\\vec x)\\right) - (1 - y)h(\\vec x)\\right]x_j = \\left[y - h(\\vec x)\\right]x_j\n\\frac{\\partial}{\\partial w_j}\\left[y\\ln\\left(h(\\vec x)\\right) + (1 - y)\\ln\\left(1 - h(\\vec x)\\right)\\right] = \\left[y\\left(1 - h(\\vec x)\\right) - (1 - y)h(\\vec x)\\right]x_j = \\left[y - h(\\vec x)\\right]x_j\n\n\n\n\n\n\nExample\n\u00b6\n\n\n\n\n\n\nLets consider the following dataset\n\n\n\n\n\n\nx\nx\n - number of hours spent studying machine learning\n\n\n\n\n\n\ny\ny\n - student passed (1) or failed (0) the exam\n\n\n\n\n\n\n\n\n\n\nN\n \n=\n \n50\n  \n# number of students per class\n\n\n\nX\n \n=\n \nnp\n.\nconcatenate\n((\nnp\n.\nrandom\n.\nrandom\n((\nN\n))\n \n*\n \n35\n,\n\n                   \n30\n \n+\n \nnp\n.\nrandom\n.\nrandom\n((\nN\n))\n \n*\n \n25\n))\n\n\n\nY\n \n=\n \nnp\n.\nconcatenate\n(([\n0\n]\n*\nN\n,\n \n[\n1\n]\n*\nN\n))\n\n\n\nplt\n.\nxlabel\n(\n\"Study time [h]\"\n)\n\n\nplt\n.\nylabel\n(\n\"Success\"\n)\n\n\n\nplt\n.\nscatter\n(\nX\n,\n \nY\n);\n\n\n\n\n\n\n\n\n\n\nOnce again lets use \ntheano\n\n\n\n\nimport\n \ntheano\n\n\nimport\n \ntheano.tensor\n \nas\n \nT\n\n\n\nx\n \n=\n \nT\n.\nvector\n(\n'x'\n)\n \n# feature vector\n\n\ny\n \n=\n \nT\n.\nvector\n(\n'y'\n)\n \n# target vector\n\n\n\na\n \n=\n \ntheano\n.\nshared\n(\nnp\n.\nrandom\n.\nrandn\n(),\n \nname\n \n=\n \n'w'\n)\n \n# weights initialized randomly\n\n\nb\n \n=\n \ntheano\n.\nshared\n(\nnp\n.\nrandom\n.\nrandn\n(),\n \nname\n \n=\n \n'b'\n)\n\n\n\nhypo\n \n=\n \n1\n \n/\n \n(\n1\n \n+\n \nT\n.\nexp\n(\n-\nT\n.\ndot\n(\nx\n,\n \na\n)\n \n-\n \nb\n))\n              \n# hyphothesis\n\n\nxent\n \n=\n \n-\n \ny\n \n*\n \nT\n.\nlog\n(\nhypo\n)\n \n-\n \n(\n1\n \n-\n \ny\n)\n \n*\n \nT\n.\nlog\n(\n1\n \n-\n \nhypo\n)\n  \n# cross-entropy loss function\n\n\ncost\n \n=\n \nxent\n.\nsum\n()\n                                     \n# cost function\n\n\ngrad_a\n,\n \ngrad_b\n \n=\n \nT\n.\ngrad\n(\ncost\n,\n \n[\na\n,\n \nb\n])\n                 \n# gradients\n\n\n\nalpha\n \n=\n \n0.01\n \n# learning rate\n\n\n\n# at each training step we update weights:\n\n\n# w -> w - alpha * grad_w and b -> b - alpha * grad_b\n\n\ntrain\n \n=\n \ntheano\n.\nfunction\n(\ninputs\n \n=\n \n[\nx\n,\ny\n],\n\n                        \noutputs\n \n=\n \ncost\n,\n\n                        \nupdates\n \n=\n \n((\na\n,\n \na\n \n-\n \nalpha\n \n*\n \ngrad_a\n),\n\n                                   \n(\nb\n,\n \nb\n \n-\n \nalpha\n \n*\n \ngrad_b\n)))\n\n\n\n\n\n\n\n\nFor the training we will scale features to \n[0, 1]\n[0, 1]\n, which helps gradient descent to converge faster\n\n\n\n\nx_min\n \n=\n \nmin\n(\nX\n)\n\n\nx_max\n \n=\n \nmax\n(\nX\n)\n\n\n\ns\n \n=\n \nlambda\n \nx\n:\n \n(\nx\n \n-\n \nx_min\n)\n \n/\n \n(\nx_max\n \n-\n \nx_min\n)\n  \n# scale\n\n\n\n\n\n\n\n\nNow, we train the model on normalized data\n\n\n\n\nn_epochs\n \n=\n \n1000\n\n\n\n[\ntrain\n(\ns\n(\nX\n),\n \nY\n)\n \nfor\n \n_\n \nin\n \nrange\n(\nn_epochs\n)]\n\n\n\nplt\n.\nxlabel\n(\n\"Study time [h]\"\n)\n\n\nplt\n.\nylabel\n(\n\"Success\"\n)\n\n\n\nplt\n.\nscatter\n(\nX\n,\n \nY\n)\n\n\n\nh_\n \n=\n \nnp\n.\narange\n(\n0\n,\n \n60\n,\n \n0.01\n)\n\n\n\nplt\n.\nplot\n(\nh_\n,\n \n1\n \n/\n \n(\n1\n \n+\n \nnp\n.\nexp\n(\n-\ns\n(\nh_\n)\n*\na\n.\nget_value\n()\n \n-\n \nb\n.\nget_value\n())),\n \n'C1'\n)\n\n\n\nplt\n.\nplot\n([\n0\n,\n \n60\n],\n \n[\n0.5\n,\n \n0.5\n],\n \n'C2--'\n);\n\n\n\n\n\n\n\n\n\n\n\n\nThe orange line gives the probability of success as a function of study time\n\n\n\n\n\n\nTo classify a student one can make a cut at \n0.5\n0.5\n\n\n\n\n\n\nMultinominal logistic regression\n\u00b6\n\n\n\n\n\n\nSo far, we considered dependent variable to be binary - it is time to generalize LR to \nK\nK\n possible classes\n\n\n\n\n\n\nLets consider dataset \n\\left\\{(\\vec x^{(i)}, y^{(i)}); i = 1,\\cdots, N\\right\\}\n\\left\\{(\\vec x^{(i)}, y^{(i)}); i = 1,\\cdots, N\\right\\}\n\n\n\n\n\n\nwhere independent variables \n\\vec x^{(i)} = (1, x^{(i)}_1,\\cdots, x^{(i)}_n)\n\\vec x^{(i)} = (1, x^{(i)}_1,\\cdots, x^{(i)}_n)\n\n\n\n\n\n\nand dependent variables \ny^{(i)} \\in \\{1, \\cdots, K\\}\ny^{(i)} \\in \\{1, \\cdots, K\\}\n\n\n\n\n\n\nLogit approach\n\u00b6\n\n\n\n\n\n\nThe one way to go is to prepare \nK - 1\nK - 1\n binary classifiers w.r.t. to a chosen class as a pivot\n\n\n\n\n\n\nThe odds of \ny\ny\n being \nj \\in \\left\\{1, \\cdots, K-1\\right\\}\nj \\in \\left\\{1, \\cdots, K-1\\right\\}\n (chosing \nK\nK\n as a pivot) is given by: \n\\frac{P(y = j)}{P(y = K)}\n\\frac{P(y = j)}{P(y = K)}\n\n\n\n\n\n\nwhich leads to \nK-1\nK-1\n equations with different weights for each possible outcome:\n\n\n\n\n\n\n$\\begin{eqnarray}\n\\ln\\frac{P(y = 1)}{P(y = K)} = \\vec w_1 \\cdot \\vec x & \\Rightarrow & P(y=1) = P(y=K)e^{\\vec w_1 \\cdot \\vec x} \\\\\n\\ln\\frac{P(y = 2)}{P(y = K)} = \\vec w_2 \\cdot \\vec x & \\Rightarrow & P(y=2) = P(y=K)e^{\\vec w_2 \\cdot \\vec x} \\\\\n                                                     & ... & \\\\\n\\ln\\frac{P(y = K-1)}{P(y = K)} = \\vec w_{K-1} \\cdot \\vec x & \\Rightarrow & P(y=K-1) = P(y=K)e^{\\vec w_{K-1} \\cdot \\vec x} \\\\\n\\end{eqnarray}$\n\n\n\n\n\n\n\n\n\nAnd as they have to sum to \n1\n1\n we get: \nP(y=K) = 1 - \\sum\\limits_{i=1}^{K-1}P(y=K)e^{\\vec w_i \\cdot \\vec x} \\Rightarrow P(y = K) = \\frac{1}{1 + \\sum\\limits_{i=1}^{K-1}e^{\\vec w_i \\cdot \\vec x}}\nP(y=K) = 1 - \\sum\\limits_{i=1}^{K-1}P(y=K)e^{\\vec w_i \\cdot \\vec x} \\Rightarrow P(y = K) = \\frac{1}{1 + \\sum\\limits_{i=1}^{K-1}e^{\\vec w_i \\cdot \\vec x}}\n\n\n\n\n\n\nThus, the probability of \ny = j\ny = j\n, for \nj \\in \\left\\{1, \\cdots, K-1\\right\\}\nj \\in \\left\\{1, \\cdots, K-1\\right\\}\n is given by: \nP(y=j) = \\frac{e^{\\vec w_j \\cdot \\vec x}}{1 + \\sum\\limits_{i=1}^{K-1}e^{\\vec w_i \\cdot \\vec x}}\nP(y=j) = \\frac{e^{\\vec w_j \\cdot \\vec x}}{1 + \\sum\\limits_{i=1}^{K-1}e^{\\vec w_i \\cdot \\vec x}}\n\n\n\n\n\n\nCheck binary case\n\u00b6\n\n\n\n\n\n\nLets consider binary classification with \n0\n0\n being a pivot\n\n\n\n\n\n\nThen we have: \np = P(y=1) = \\frac{e^{\\vec w \\cdot \\vec x}}{1 + e^{\\vec w \\cdot \\vec x}} = \\frac{1}{1 + e^{-\\vec w \\cdot \\vec x}}\np = P(y=1) = \\frac{e^{\\vec w \\cdot \\vec x}}{1 + e^{\\vec w \\cdot \\vec x}} = \\frac{1}{1 + e^{-\\vec w \\cdot \\vec x}}\n\n\n\n\n\n\nso the same result as before\n\n\n\n\n\n\nSoftmax approach\n\u00b6\n\n\n\n\nAn alternative approach (which works for any number of classes) is to consider each class separately with its own parameters set and include the normalization factor ensuring that we get a probability distribution:\n\n\n\n\n$\\begin{eqnarray}\n\\ln P(y = 1) = \\vec w_1 \\cdot \\vec x - \\ln Z & \\Rightarrow & P(y=1) = \\frac{1}{Z}e^{\\vec w_1 \\cdot \\vec x} \\\\\n\\ln P(y = 2) = \\vec w_2 \\cdot \\vec x - \\ln Z & \\Rightarrow & P(y=2) = \\frac{1}{Z}e^{\\vec w_2 \\cdot \\vec x} \\\\\n                                                     & ... & \\\\\n\\ln P(y = K) = \\vec w_K \\cdot \\vec x - \\ln Z & \\Rightarrow & P(y=K) = \\frac{1}{Z}e^{\\vec w_K \\cdot \\vec x} \\\\\n\\end{eqnarray}$\n\n\n\n\n\n\n\n\n\nAs they have to sum to \n1\n1\n: \n\\frac{1}{Z}\\sum\\limits_{i=1}^Ke^{\\vec w_i \\cdot \\vec x} = 1 \\Rightarrow Z = \\sum\\limits_{i=1}^Ke^{\\vec w_i \\cdot \\vec x}\n\\frac{1}{Z}\\sum\\limits_{i=1}^Ke^{\\vec w_i \\cdot \\vec x} = 1 \\Rightarrow Z = \\sum\\limits_{i=1}^Ke^{\\vec w_i \\cdot \\vec x}\n\n\n\n\n\n\nThus, the probability of \ny = j\ny = j\n, for \nj \\in \\left\\{1, \\cdots, K\\right\\}\nj \\in \\left\\{1, \\cdots, K\\right\\}\n is given by: \nP(y=j) = \\frac{e^{\\vec w_j \\cdot \\vec x}}{\\sum\\limits_{i=1}^{K}e^{\\vec w_i \\cdot \\vec x}}\nP(y=j) = \\frac{e^{\\vec w_j \\cdot \\vec x}}{\\sum\\limits_{i=1}^{K}e^{\\vec w_i \\cdot \\vec x}}\n\n\n\n\n\n\nwhich is called \nsoftmax function\n\n\n\n\n\n\nCheck binary case\n\u00b6\n\n\n\n\nFor \ny \\in \\left\\{0, 1\\right\\}\ny \\in \\left\\{0, 1\\right\\}\n we have:\n\n\n\n\n$\\begin{eqnarray}\nP(y = 0) & = & \\frac{e^{\\vec w_0 \\cdot \\vec x}}{e^{\\vec w_0 \\cdot \\vec x} + e^{\\vec w_1 \\cdot \\vec x}} \\\\\nP(y = 1) & = & \\frac{e^{\\vec w_1 \\cdot \\vec x}}{e^{\\vec w_0 \\cdot \\vec x} + e^{\\vec w_1 \\cdot \\vec x}}\n\\end{eqnarray}$\n\n\n\n\n\n\n\n\n\nPlease note, that this model is overspecified! \n\\rightarrow\n\\rightarrow\n \nP(y = 0) + P(y = 1) = 1\nP(y = 0) + P(y = 1) = 1\n (always)\n\n\n\n\n\n\nThat means, that once we have one probability the other is given, so we can choose one of \n\\vec w_i\n\\vec w_i\n arbitrary - lets choose \n\\vec w_0 = 0\n\\vec w_0 = 0\n (and \n\\vec w_1 \\equiv \\vec w\n\\vec w_1 \\equiv \\vec w\n): \nP(y = 1) = \\frac{e^{\\vec w \\cdot \\vec x}}{1 + e^{\\vec w\\cdot x}} = \\frac{1}{1 + e^{-\\vec w \\cdot x}}\nP(y = 1) = \\frac{e^{\\vec w \\cdot \\vec x}}{1 + e^{\\vec w\\cdot x}} = \\frac{1}{1 + e^{-\\vec w \\cdot x}}\n\n\n\n\n\n\nCost function\n\u00b6\n\n\n\n\n\n\nLets recall that in our notation we have:\n\n\n\n\n\n\ndataset: \n\\left\\{(\\vec x^{(i)}, y^{(i)}); i = 1,\\cdots, N\\right\\}\n\\left\\{(\\vec x^{(i)}, y^{(i)}); i = 1,\\cdots, N\\right\\}\n\n\n\n\n\n\nfeatures: \n\\vec x^{(i)} = (1, x^{(i)}_1,\\cdots, x^{(i)}_n)\n\\vec x^{(i)} = (1, x^{(i)}_1,\\cdots, x^{(i)}_n)\n\n\n\n\n\n\ntargets: \ny^{(i)} \\in \\{1, \\cdots, K\\}\ny^{(i)} \\in \\{1, \\cdots, K\\}\n\n\n\n\n\n\n\n\n\n\nFor every possible outcome we have a corresponding vector of weights \n\\vec w_j\n\\vec w_j\n (for \nj = 1, \\cdots, K\nj = 1, \\cdots, K\n) - so in fact we have a matrix of parameters (\nW\nW\n)\n\n\n\n\n\n\nThe hypothesis is given by a vector: \nh(\\vec x) = \\left[\\begin{array}{c}\\frac{e^{\\vec w_1 \\cdot \\vec x}}{\\sum\\limits_{i=1}^{K}e^{\\vec w_i \\cdot \\vec x}} \\\\ \\frac{e^{\\vec w_2 \\cdot \\vec x}}{\\sum\\limits_{i=1}^{K}e^{\\vec w_i \\cdot \\vec x}} \\\\ ... \\\\ \\frac{e^{\\vec w_K \\cdot \\vec x}}{\\sum\\limits_{i=1}^{K}e^{\\vec w_i \\cdot \\vec x}}\\end{array}\\right]\nh(\\vec x) = \\left[\\begin{array}{c}\\frac{e^{\\vec w_1 \\cdot \\vec x}}{\\sum\\limits_{i=1}^{K}e^{\\vec w_i \\cdot \\vec x}} \\\\ \\frac{e^{\\vec w_2 \\cdot \\vec x}}{\\sum\\limits_{i=1}^{K}e^{\\vec w_i \\cdot \\vec x}} \\\\ ... \\\\ \\frac{e^{\\vec w_K \\cdot \\vec x}}{\\sum\\limits_{i=1}^{K}e^{\\vec w_i \\cdot \\vec x}}\\end{array}\\right]\n\n\n\n\n\n\nThe prediction for unseen sample is done using \nargmax\n function\n\n\n\n\n\n\nAs before we define the cost function as the negative log-likelihood: \nL(W) = -\\frac{1}{N}\\sum\\limits_{i=1}^N\\ln\\left[\\frac{e^{\\vec w_{y^{(i)}} \\cdot \\vec x^{(i)}}}{\\sum\\limits_{j=1}^{K}e^{\\vec w_j \\cdot \\vec x^{(i)}}}\\right] = -\\frac{1}{N}\\sum\\limits_{i=1}^N\\left[\\vec w_{y^{(i)}} \\cdot \\vec x^{(i)} - \\ln\\sum\\limits_{j=1}^{K}e^{\\vec w_j \\cdot \\vec x^{(i)}}\\right]\nL(W) = -\\frac{1}{N}\\sum\\limits_{i=1}^N\\ln\\left[\\frac{e^{\\vec w_{y^{(i)}} \\cdot \\vec x^{(i)}}}{\\sum\\limits_{j=1}^{K}e^{\\vec w_j \\cdot \\vec x^{(i)}}}\\right] = -\\frac{1}{N}\\sum\\limits_{i=1}^N\\left[\\vec w_{y^{(i)}} \\cdot \\vec x^{(i)} - \\ln\\sum\\limits_{j=1}^{K}e^{\\vec w_j \\cdot \\vec x^{(i)}}\\right]\n\n\n\n\n\n\nGradient\n\u00b6\n\n\n\n\n\n\nWe need to calculate the partial derivative for each parameter \nw_{ab}\nw_{ab}\n \n\n\n\n\n\n\na = 1, \\cdots, K\na = 1, \\cdots, K\n (possible outcome \ny\ny\n)\n\n\n\n\n\n\nb = 0, \\cdots, n\nb = 0, \\cdots, n\n (\n\\vec x\n\\vec x\n coordinate)\n\n\n\n\n\n\n\n\n\n\nFor the first term we have: \n\\frac{\\partial}{\\partial w_{ab}} \\vec w_{y^{(i)}} \\cdot \\vec x^{(i)} = [y^{(i)} = a]x^{(i)}_b\n\\frac{\\partial}{\\partial w_{ab}} \\vec w_{y^{(i)}} \\cdot \\vec x^{(i)} = [y^{(i)} = a]x^{(i)}_b\n\n\n\n\n\n\nFor the second term we have:\n\n\n\n\n\n\n$\\begin{eqnarray}\n\\frac{\\partial}{\\partial w_{ab}} \\ln\\sum\\limits_{j=1}^{K}e^{\\vec w_j \\cdot \\vec x^{(i)}} & = & \\frac{\\sum\\limits_{j=1}^{K}e^{\\vec w_j \\cdot \\vec x^{(i)}} \\cdot [y^{(i)} = a] \\cdot x^{(i)}_b}{\\sum\\limits_{j=1}^{K}e^{\\vec w_j \\cdot \\vec x^{(i)}}} \\\\ & = & \\sum\\limits_{j=1}^{K}\\left[\\frac{e^{\\vec w_j \\cdot \\vec x^{(i)}}}{\\sum\\limits_{j=1}^{K}e^{\\vec w_j \\cdot \\vec x^{(i)}}}\\cdot [y^{(i)} = a] \\cdot x^{(i)}_b\\right]\\\\ & = & \\sum\\limits_{j=1}^{K}\\left[P(y = j~|~\\vec x^{(i)})\\cdot [y^{(i)} = a] \\cdot x^{(i)}_b\\right] \\\\ & = & P(y=a~|~\\vec x^{(i)})x^{(i)}_b\n\\end{eqnarray}$\n\n\n\n\n\n\n\n\n\nFinally, the gradient of the cost function is given by: \n\\frac{\\partial L(W)}{\\partial w_{ab}} = -\\frac{1}{N}\\sum\\limits_{i=1}^N\\left[[y^{(i)}=a]x^{(i)}_b - P(y=a~|~\\vec x^{(i)})x^{(i)}_b\\right]\n\\frac{\\partial L(W)}{\\partial w_{ab}} = -\\frac{1}{N}\\sum\\limits_{i=1}^N\\left[[y^{(i)}=a]x^{(i)}_b - P(y=a~|~\\vec x^{(i)})x^{(i)}_b\\right]\n\n\n\n\n\n\nAnd for every iteration of the gradient descent method weights are updated according to: \nw_{ab} \\rightarrow w_{ab} - \\alpha\\frac{\\partial L(W)}{\\partial w_{ab}}\nw_{ab} \\rightarrow w_{ab} - \\alpha\\frac{\\partial L(W)}{\\partial w_{ab}}\n\n\n\n\n\n\nExample\n\u00b6\n\n\n\n\n\n\nLet a student have two features:\n\n\n\n\n\n\ninitial knowledge: \nx_1 \\in [0, 100]\nx_1 \\in [0, 100]\n\n\n\n\n\n\nhours spent studying: \nx_2 \\in [0, 50]\nx_2 \\in [0, 50]\n\n\n\n\n\n\n\n\n\n\nAnd, based on these two features, a grade can be assigned to a student:\n\n\n\n\ntarget: \ny \\in \\left\\{2, 3, 4, 5\\right\\}\ny \\in \\left\\{2, 3, 4, 5\\right\\}\n\n\n\n\n\n\n\n\nDataset\n\u00b6\n\n\ndef\n \ngrade\n(\ninit_know\n,\n \nstudy_time\n):\n\n  \n\"\"\"Arbitrary grading system.\"\"\"\n\n  \nscore\n \n=\n \nnp\n.\nrandom\n.\nnormal\n(\ninit_know\n \n+\n \n2\n*\nstudy_time\n,\n \n5\n)\n\n\n  \nif\n \nscore\n \n>\n \n90\n:\n \nreturn\n \n3\n    \n# bdb\n\n  \nelif\n \nscore\n \n>\n \n70\n:\n \nreturn\n \n2\n  \n# db\n\n  \nelif\n \nscore\n \n>\n \n50\n:\n \nreturn\n \n1\n  \n# dst\n\n  \nelse\n:\n \nreturn\n \n0\n             \n# ndst\n\n\n\n\n\n\n\n\nThe training set\n\n\n\n\nN\n \n=\n \n1000\n  \n# number of students\n\n\n\nX\n \n=\n \nnp\n.\nrandom\n.\nsample\n((\nN\n,\n \n2\n))\n \n*\n \n[\n100\n,\n \n50\n]\n\n\nY\n \n=\n \nnp\n.\narray\n([\ngrade\n(\n*\nstudent\n)\n \nfor\n \nstudent\n \nin\n \nX\n],\n \ndtype\n=\n'int32'\n)\n\n\n\n\n\n\nplt\n.\nxlabel\n(\n\"Initial knowledge\"\n)\n\n\nplt\n.\nylabel\n(\n\"Study time\"\n)\n\n\n\nfor\n \nstudent\n,\n \ng\n \nin\n \nzip\n(\nX\n,\n \nY\n):\n\n  \nplt\n.\nscatter\n(\n*\nstudent\n,\n \ncolor\n=\n'C'\n+\nstr\n(\ng\n),\n \nmarker\n=\n'.'\n)\n\n\n\n\n\n\n\n\nData preparation\n\u00b6\n\n\n\n\nFor the training process we scale features to \n[0, 1]\n[0, 1]\n - otherwise \ninitial knowledge\n would weight more!\n\n\n\n\nX_train\n \n=\n \nnp\n.\nmultiply\n(\nX\n,\n \nnp\n.\narray\n([\n1\n/\n100\n,\n \n1\n/\n50\n]))\n\n\n\n\n\n\n\n\nLets add 1 for bias term to the dataset\n\n\n\n\nX_train\n \n=\n \nnp\n.\nhstack\n((\nnp\n.\nones\n((\nN\n,\n \n1\n)),\n \nX_train\n))\n\n\n\n\n\n\n\n\nHow does it look?\n\n\n\n\nprint\n(\n\"Original:\"\n,\n \nX\n[:\n5\n],\n \n\"Preprocessed:\"\n,\n \nX_train\n[:\n5\n],\n \nsep\n=\n\"\n\\n\\n\n\"\n)\n\n\n\n\n\n\nOriginal:\n\n[[ 7.47319311 25.86221876]\n [21.66198394 33.34522121]\n [69.87399886  4.53759853]\n [75.83844581 22.05176574]\n [17.38539272 47.02800299]]\n\nPreprocessed:\n\n[[1.         0.07473193 0.51724438]\n [1.         0.21661984 0.66690442]\n [1.         0.69873999 0.09075197]\n [1.         0.75838446 0.44103531]\n [1.         0.17385393 0.94056006]]\n\n\n\n\n\nTraining\n\u00b6\n\n\n\n\nThe implementation of MLR in \ntheano\n\n\n\n\nimport\n \ntheano\n\n\nimport\n \ntheano.tensor\n \nas\n \nT\n\n\n\nx\n \n=\n \nT\n.\nmatrix\n(\n'x'\n)\n  \n# feature vectors\n\n\ny\n \n=\n \nT\n.\nivector\n(\n'y'\n)\n \n# target vector\n\n\n\nW\n \n=\n \ntheano\n.\nshared\n(\nnp\n.\nrandom\n.\nrandn\n(\n3\n,\n \n4\n))\n  \n# weight matrix (2 features + bias,\n\n                                          \n#                4 possible outcomes)\n\n\n\nhypo\n \n=\n \nT\n.\nnnet\n.\nsoftmax\n(\nT\n.\ndot\n(\nx\n,\nW\n))\n                     \n# hyphothesis\n\n\ncost\n \n=\n \n-\nT\n.\nmean\n(\nT\n.\nlog\n(\nhypo\n)[\nT\n.\narange\n(\ny\n.\nshape\n[\n0\n]),\n \ny\n])\n  \n# cost function\n\n\ngrad_W\n \n=\n \nT\n.\ngrad\n(\ncost\n=\ncost\n,\n \nwrt\n=\nW\n)\n                     \n# gradients\n\n\n\nalpha\n \n=\n \n0.5\n \n# learning rate\n\n\n\n# define a training step\n\n\ntrain\n \n=\n \ntheano\n.\nfunction\n(\ninputs\n \n=\n \n[\nx\n,\ny\n],\n\n                        \noutputs\n \n=\n \ncost\n,\n\n                        \nupdates\n \n=\n \n[(\nW\n,\n \nW\n \n-\n \nalpha\n \n*\n \ngrad_W\n)]\n\n                       \n)\n\n\n\n# predict a class label\n\n\npredict\n \n=\n \ntheano\n.\nfunction\n(\ninputs\n=\n[\nx\n],\n \noutputs\n=\nT\n.\nargmax\n(\nhypo\n,\n \naxis\n=\n1\n))\n\n\n\n\n\n\n\n\nThe training process on normalized data\n\n\n\n\nn_epochs\n \n=\n \n10000\n\n\nacc_train\n \n=\n \n[]\n  \n# accuracy on training dataset\n\n\n\nfor\n \n_\n \nin\n \nrange\n(\nn_epochs\n):\n\n  \n# do a single step of gradient descent\n\n  \ntrain\n(\nX_train\n,\n \nY\n)\n\n  \n# calculate accuracy with current set of weights\n\n  \nacc_train\n.\nappend\n((\nY\n \n==\n \npredict\n(\nX_train\n))\n.\nsum\n()\n \n/\n \nY\n.\nshape\n[\n0\n])\n\n\n\n\n\n\nplt\n.\nxlabel\n(\n\"Epoch\"\n)\n\n\nplt\n.\nylabel\n(\n\"Cost\"\n)\n\n\n\nplt\n.\nplot\n(\nrange\n(\nlen\n(\nacc_train\n)),\n \nacc_train\n);\n\n\n\n\n\n\n\n\nValidation\n\u00b6\n\n\n\n\nFirst we need \nunseen\n data for testing\n\n\n\n\n# another set of students\n\n\nX_test\n \n=\n \nnp\n.\nrandom\n.\nsample\n((\nN\n,\n \n2\n))\n \n*\n \n[\n100\n,\n \n50\n]\n\n\nY_test\n \n=\n \nnp\n.\narray\n([\ngrade\n(\n*\nstudent\n)\n \nfor\n \nstudent\n \nin\n \nX_test\n],\n \ndtype\n=\n'int32'\n)\n\n\n\n# normalize and add bias \n\n\nX_test_normalized\n \n=\n \nnp\n.\nmultiply\n(\nX_test\n,\n \nnp\n.\narray\n([\n1\n/\n100\n,\n \n1\n/\n50\n]))\n\n\nX_test_normalized\n \n=\n \nnp\n.\nhstack\n((\nnp\n.\nones\n((\nN\n,\n \n1\n)),\n \nX_test_normalized\n))\n\n\n\n\n\n\n\n\nTo predict a grade we use the function \npredict\n defined earlier\n\n\n\n\nY_pred\n \n=\n \npredict\n(\nX_test_normalized\n)\n\n\n\n\n\n\n\n\nWe can visualize the prediction\n\n\n\n\nplt\n.\nxlabel\n(\n\"Initial knowledge\"\n)\n\n\nplt\n.\nylabel\n(\n\"Study time\"\n)\n\n\n\nfor\n \nstudent\n,\n \ng\n \nin\n \nzip\n(\nX_test\n,\n \nY_pred\n):\n\n  \nplt\n.\nscatter\n(\n*\nstudent\n,\n \ncolor\n=\n'C'\n+\nstr\n(\ng\n),\n \nmarker\n=\n'.'\n)\n\n\n\n\n\n\n\n\n\n\nand calculate the accuracy\n\n\n\n\n(\nY_test\n \n==\n \nY_pred\n)\n.\nsum\n()\n \n/\n \nY_test\n.\nshape\n[\n0\n]\n\n\n\n\n\n\n0.906\n\n\n\n\n\nSoftmax visualization\n\u00b6\n\n\n\n\n\n\nLets visualize what the model has just learned\n\n\n\n\n\n\nFirst we need an easy way to calculate softmax output for a student\n\n\n\n\n\n\nsoftmax\n \n=\n \ntheano\n.\nfunction\n(\ninputs\n=\n[\nx\n],\n \noutputs\n=\nhypo\n)\n\n\n\n\n\n\n\n\nNow, we can use it on the validation dataset\n\n\n\n\nprobs\n \n=\n \nsoftmax\n(\nX_test_normalized\n)\n\n\n\n\n\n\n\n\nFor every sample softmax returns an array of the probabilities of belonging to each class\n\n\n\n\nprint\n(\nprobs\n.\nshape\n)\n\n\n\n\n\n\n(1000, 4)\n\n\n\n\n\n\n\nWe can plot each class separately\n\n\n\n\nfrom\n \nmpl_toolkits.mplot3d\n \nimport\n \nAxes3D\n\n\n\ngrades\n \n=\n \n(\n\"ndst\"\n,\n \n\"dst\"\n,\n \n\"db\"\n,\n \n\"bdb\"\n)\n\n\n\nfor\n \ni\n \nin\n \nrange\n(\n4\n):\n\n  \nfig\n \n=\n \nplt\n.\nfigure\n()\n\n  \nax\n \n=\n \nfig\n.\nadd_subplot\n(\n111\n,\n \nprojection\n=\n'3d'\n)\n\n\n  \nax\n.\nset_xlabel\n(\n\"Initial knowledge\"\n,\n  \nlabelpad\n=\n20\n)\n\n  \nax\n.\nset_ylabel\n(\n\"Study time\"\n,\n  \nlabelpad\n=\n20\n)\n\n\n  \nax\n.\nset_title\n(\n\"Grade: \"\n \n+\n \ngrades\n[\ni\n])\n\n\n  \nax\n.\nscatter\n(\nX_test\n.\nT\n[\n0\n],\n \nX_test\n.\nT\n[\n1\n],\n \nprobs\n.\nT\n[\ni\n],\n \nmarker\n=\n'.'\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Networks\n\u00b6\n\n\n\n\n\n\nHere are some helpful functions to draw neural networks\n\n\n\n\n\n\nLets just skip them - it is just bunch of matplotlib\n\n\n\n\n\n\n# the functions below grabbed from http://www.astroml.org/book_figures/appendix/fig_neural_network.html\n\n\n\nradius\n \n=\n \n0.3\n\n\n\narrow_kwargs\n \n=\n \ndict\n(\nhead_width\n=\n0.05\n,\n \nfc\n=\n'black'\n)\n\n\n\ndef\n \ndraw_connecting_arrow\n(\nax\n,\n \ncirc1\n,\n \nrad1\n,\n \ncirc2\n,\n \nrad2\n):\n\n    \ntheta\n \n=\n \nnp\n.\narctan2\n(\ncirc2\n[\n1\n]\n \n-\n \ncirc1\n[\n1\n],\n\n                       \ncirc2\n[\n0\n]\n \n-\n \ncirc1\n[\n0\n])\n\n\n    \nstarting_point\n \n=\n \n(\ncirc1\n[\n0\n]\n \n+\n \nrad1\n \n*\n \nnp\n.\ncos\n(\ntheta\n),\n\n                      \ncirc1\n[\n1\n]\n \n+\n \nrad1\n \n*\n \nnp\n.\nsin\n(\ntheta\n))\n\n\n    \nlength\n \n=\n \n(\ncirc2\n[\n0\n]\n \n-\n \ncirc1\n[\n0\n]\n \n-\n \n(\nrad1\n \n+\n \n1.4\n \n*\n \nrad2\n)\n \n*\n \nnp\n.\ncos\n(\ntheta\n),\n\n              \ncirc2\n[\n1\n]\n \n-\n \ncirc1\n[\n1\n]\n \n-\n \n(\nrad1\n \n+\n \n1.4\n \n*\n \nrad2\n)\n \n*\n \nnp\n.\nsin\n(\ntheta\n))\n\n\n    \nax\n.\narrow\n(\nstarting_point\n[\n0\n],\n \nstarting_point\n[\n1\n],\n\n             \nlength\n[\n0\n],\n \nlength\n[\n1\n],\n \n**\narrow_kwargs\n)\n\n\n\n\ndef\n \ndraw_circle\n(\nax\n,\n \ncenter\n,\n \nradius\n):\n\n    \ncirc\n \n=\n \nplt\n.\nCircle\n(\ncenter\n,\n \nradius\n,\n \nfill\n=\nFalse\n,\n \nlw\n=\n2\n)\n\n    \nax\n.\nadd_patch\n(\ncirc\n)\n\n\n\n\n\n\n# based on borrowed function we can create a new one to draw NN\n\n\n\ndef\n \ndraw_net\n(\ninput_size\n,\n \noutput_size\n,\n \nhidden_layers\n=\n[],\n \nw\n=\n6\n,\n \nh\n=\n4\n):\n\n  \n\"\"\"Draw a network\"\"\"\n\n  \nx\n \n=\n \n0\n  \n# initial layer position\n\n\n  \nax\n \n=\n \nplt\n.\nsubplot\n()\n\n  \nax\n.\nset_aspect\n(\n'equal'\n)\n\n  \nax\n.\naxis\n(\n'off'\n)\n\n\n  \nax\n.\nset_xlim\n([\n-\n2\n,\n \n-\n2\n \n+\n \nw\n])\n\n  \nax\n.\nset_ylim\n([\n-\nh\n \n/\n \n2\n \n,\n \nh\n \n/\n \n2\n \n+\n \n1\n])\n\n\n  \n# set y position  \n\n  \ny_input\n \n=\n \nnp\n.\narange\n(\n-\n(\ninput_size\n \n-\n \n1\n)\n \n/\n \n2\n,\n \n(\ninput_size\n \n+\n \n1\n)\n \n/\n \n2\n,\n \n1\n)\n\n  \ny_output\n \n=\n \nnp\n.\narange\n(\n-\n(\noutput_size\n \n-\n \n1\n)\n \n/\n \n2\n,\n \n(\noutput_size\n \n+\n \n1\n)\n \n/\n \n2\n,\n \n1\n)\n\n  \ny_hidden\n \n=\n \n[\nnp\n.\narange\n(\n-\n(\nn\n \n-\n \n1\n)\n \n/\n \n2\n,\n \n(\nn\n \n+\n \n1\n)\n \n/\n \n2\n,\n \n1\n)\n \nfor\n \nn\n \nin\n \nhidden_layers\n]\n\n\n  \n# draw input layer\n\n  \nplt\n.\ntext\n(\nx\n,\n \nh\n \n/\n \n2\n \n+\n \n0.5\n,\n \n\"Input\n\\n\nLayer\"\n,\n \nha\n=\n'center'\n,\n \nva\n=\n'top'\n,\n \nfontsize\n=\n16\n)\n\n\n  \nfor\n \ni\n,\n \ny\n \nin\n \nenumerate\n(\ny_input\n):\n\n    \ndraw_circle\n(\nax\n,\n \n(\nx\n,\n \ny\n),\n \nradius\n)\n\n    \nax\n.\ntext\n(\nx\n \n-\n \n0.9\n,\n \ny\n,\n \n'$x_\n%i\n$'\n \n%\n \n(\ninput_size\n \n-\n \n1\n \n-\n \ni\n),\n\n            \nha\n=\n'right'\n,\n \nva\n=\n'center'\n,\n \nfontsize\n=\n16\n)\n\n    \ndraw_connecting_arrow\n(\nax\n,\n \n(\nx\n \n-\n \n0.9\n,\n \ny\n),\n \n0.1\n,\n \n(\nx\n,\n \ny\n),\n \nradius\n)\n\n\n  \nlast_layer\n \n=\n \ny_input\n  \n# last layer y positions\n\n\n  \n# draw hidden layers\n\n  \nfor\n \nys\n \nin\n \ny_hidden\n:\n\n    \n# shift x\n\n    \nx\n \n+=\n \n2\n\n    \nplt\n.\ntext\n(\nx\n,\n \nh\n \n/\n \n2\n \n+\n \n0.5\n,\n \n\"Hidden\n\\n\nLayer\"\n,\n \nha\n=\n'center'\n,\n \nva\n=\n'top'\n,\n \nfontsize\n=\n16\n)\n\n\n    \n# draw neurons for each hidden layer\n\n    \nfor\n \ni\n,\n \ny1\n \nin\n \nenumerate\n(\nys\n):\n\n      \ndraw_circle\n(\nax\n,\n \n(\nx\n,\n \ny1\n),\n \nradius\n)\n\n\n      \n# connect a neuron with all neurons from previous layer\n\n      \nif\n \ni\n \n!=\n \nlen\n(\nys\n)\n \n-\n \n1\n:\n \n# skip bias\n\n        \nfor\n \ny2\n \nin\n \nlast_layer\n:\n\n          \ndraw_connecting_arrow\n(\nax\n,\n \n(\nx\n \n-\n \n2\n,\n \ny2\n),\n \nradius\n,\n \n(\nx\n,\n \ny1\n),\n \nradius\n)\n\n\n    \n# update last layer\n\n    \nlast_layer\n \n=\n \nys\n\n\n  \nx\n \n+=\n \n2\n  \n# update position for output layer\n\n\n  \n# draw output layer\n\n  \nplt\n.\ntext\n(\nx\n,\n \nh\n \n/\n \n2\n \n+\n \n0.5\n,\n \n\"Output\n\\n\nLayer\"\n,\n \nha\n=\n'center'\n,\n \nva\n=\n'top'\n,\n \nfontsize\n=\n16\n)\n\n\n  \nfor\n \ni\n,\n \ny1\n \nin\n \nenumerate\n(\ny_output\n):\n\n    \ndraw_circle\n(\nax\n,\n \n(\nx\n,\n \ny1\n),\n \nradius\n)\n\n    \nax\n.\ntext\n(\nx\n \n+\n \n0.8\n,\n \ny1\n,\n \n'Output'\n,\n \nha\n=\n'left'\n,\n \nva\n=\n'center'\n,\n \nfontsize\n=\n16\n)\n\n    \ndraw_connecting_arrow\n(\nax\n,\n \n(\nx\n,\n \ny1\n),\n \nradius\n,\n \n(\nx\n \n+\n \n0.8\n,\n \ny1\n),\n \n0.1\n)\n\n\n    \n# connect each output neuron with all neurons from previous layer\n\n    \nfor\n \ny2\n \nin\n \nlast_layer\n:\n\n      \ndraw_connecting_arrow\n(\nax\n,\n \n(\nx\n \n-\n \n2\n,\n \ny2\n),\n \nradius\n,\n \n(\nx\n,\n \ny1\n),\n \nradius\n)\n\n\n\n\n\n\nNeuron\n\u00b6\n\n\n\n\n\n\nAs mentioned at the beginning, we are going to discuss feedforward neural networks\n\n\n\n\n\n\nLets start with a single neuron\n\n\n\n\n\n\nWhat we were doing so far\n\n\n\n\n\n\nWe had some training samples with \nN\nN\n features\n\n\n\n\n\n\nWe assumed linear model\n\n\n\n\n\n\nWe connected features with outcomes using linear, logistic or softmax function\n\n\n\n\n\n\n\n\n\n\nThus, we considered somthing like this:\n\n\n\n\n\n\ndraw_net\n(\n3\n,\n \n1\n)\n\n\n\n\n\n\n\n\n\n\n\n\nOutput circle\n represents a neuron\n\n\n\n\n\n\nArrows represent connections (weights)\n\n\n\n\n\n\nA neuron is defined by an activation function, e.g.:\n\n\n\n\n\n\nbinary step\n\n\n\n\n\n\nlogistic function\n\n\n\n\n\n\nhyperbolic tangent\n\n\n\n\n\n\nrectified linear unit (ReLU)\n\n\n\n\n\n\n\n\n\n\nimport\n \nmath\n\n\n\ndef\n \nbinary_step\n(\nx\n):\n \nreturn\n \n0\n \nif\n \nx\n \n<\n \n0\n \nelse\n \n1\n\n\n\ndef\n \nlogistic\n(\nx\n):\n \nreturn\n \n1\n \n/\n \n(\n1\n \n+\n \nmath\n.\nexp\n(\n-\nx\n))\n\n\n\ndef\n \ntanh\n(\nx\n):\n \nreturn\n \nmath\n.\ntanh\n(\nx\n)\n\n\n\ndef\n \nrelu\n(\nx\n):\n \nreturn\n \n0\n \nif\n \nx\n \n<\n \n0\n \nelse\n \nx\n\n\n\nx\n \n=\n \nnp\n.\nlinspace\n(\n-\n5\n,\n \n5\n,\n \n100\n)\n\n\n\nbs\n \n=\n \n[\nbinary_step\n(\nx_\n)\n \nfor\n \nx_\n \nin\n \nx\n]\n\n\nlf\n \n=\n \n[\nlogistic\n(\nx_\n)\n \nfor\n \nx_\n \nin\n \nx\n]\n\n\nth\n \n=\n \n[\ntanh\n(\nx_\n)\n \nfor\n \nx_\n \nin\n \nx\n]\n\n\nre\n \n=\n \n[\nrelu\n(\nx_\n)\n \nfor\n \nx_\n \nin\n \nx\n]\n\n\n\n_\n,\n \n((\nax1\n,\n \nax2\n),\n \n(\nax3\n,\n \nax4\n))\n \n=\n \nplt\n.\nsubplots\n(\n2\n,\n \n2\n,\n \nfigsize\n=\n(\n10\n,\n10\n))\n\n\n\nax1\n.\nset_title\n(\n\"Binary step\"\n)\n\n\nax2\n.\nset_title\n(\n\"TanH\"\n)\n\n\nax3\n.\nset_title\n(\n\"Logistic\"\n)\n\n\nax4\n.\nset_title\n(\n\"ReLU\"\n)\n\n\n\nax1\n.\nplot\n(\nx\n,\n \nbs\n)\n\n\nax2\n.\nplot\n(\nx\n,\n \nlf\n)\n\n\nax3\n.\nplot\n(\nx\n,\n \nth\n)\n\n\nax4\n.\nplot\n(\nx\n,\n \nre\n);\n\n\n\n\n\n\n\n\nNeural networks\n\u00b6\n\n\n\n\n\n\nImagine that the output of a neuron is an input for another neuron\n\n\n\n\n\n\nThis way we can create another layer of neurons (hidden layer) which would be an input for the output layer\n\n\n\n\n\n\ndraw_net\n(\n3\n,\n \n1\n,\n \n[\n5\n],\n \nw\n=\n9\n,\n \nh\n=\n6\n)\n\n\n\n\n\n\n\n\n\n\nOr we could get carried away\n\n\n\n\ndraw_net\n(\n3\n,\n \n1\n,\n \n[\n5\n,\n \n7\n,\n \n9\n,\n \n5\n],\n \nw\n=\n14\n,\n \nh\n=\n10\n)\n\n\n\n\n\n\n\n\n\n\n\n\nThis way we can solve non-linear problems\n\n\n\n\n\n\nIn general, the more the problem is complex the more neurons we need\n\n\n\n\n\n\nThe numbers of hidden layers and hidden neurons are \nhyperparameters\n of the model\n\n\n\n\n\n\nIf the NN is too small - underfitting\n\n\n\n\n\n\nIt the NN is too large - overfitting\n\n\n\n\n\n\n\n\n\n\nPlase note, that we may have also many possible outcomes through e.g. softmax\n\n\n\n\n\n\ndraw_net\n(\n3\n,\n \n4\n,\n \n[\n5\n,\n \n7\n,\n \n9\n,\n \n5\n],\n \nw\n=\n14\n,\n \nh\n=\n10\n)\n\n\n\n\n\n\n\n\n\n\nHow to train this monster?\n\n\n\n\nBackpropagation\n\u00b6\n\n\n\n\n\n\nWe can use the gradient descent method, but now we need to propagate the error through all layers:\n\n\n\n\n\n\nInput -> forward propagation -> error\n\n\n\n\n\n\nBackpropagate the error -> update weights\n\n\n\n\n\n\n\n\n\n\nLets see how it works on a simple example\n\n\n\n\n\n\ndraw_net\n(\n3\n,\n \n2\n,\n \n[\n3\n],\n \nw\n=\n9\n,\n \nh\n=\n4\n)\n\n\n\n\n\n\n\n\n\n\n\n\nThe input is given by: \nx_0 = 1\nx_0 = 1\n, \nx_1\nx_1\n, \nx_2\nx_2\n\n\n\n\n\n\nThere are 2 hidden neurons + bias: \nh_0 = 1\nh_0 = 1\n, \nh_1\nh_1\n, \nh_2\nh_2\n\n\n\n\n\n\nAnd two possible outcomes: \no_1\no_1\n, \no_2\no_2\n\n\n\n\n\n\nThe input layer is connected with the hidden layer by weights: \nw^{(1)}_{ij}\nw^{(1)}_{ij}\n, where \ni = 0, 1, 2\ni = 0, 1, 2\n and \nj = 1, 2\nj = 1, 2\n, e.g.\n\n\n\n\nw^{(1)}_{12}\nw^{(1)}_{12}\n is the weight connecting \nx_1\nx_1\n and \nh_2\nh_2\n\n\n\n\n\n\n\n\nThe hidden layer is connected with the output layer by weights: \nw^{(2)}_{ij}\nw^{(2)}_{ij}\n\n\n\n\n\n\nThe input for a neuron \nh_k\nh_k\n is given by: \nh_{k, in} = w^{(1)}_{0k} + w^{(1)}_{1k} \\cdot x_1 + w^{(1)}_{2k} \\cdot x_2\nh_{k, in} = w^{(1)}_{0k} + w^{(1)}_{1k} \\cdot x_1 + w^{(1)}_{2k} \\cdot x_2\n\n\n\n\n\n\nAnd the output: \nh_{k, out} = \\left(1 + \\exp(-h_{k, in})\\right)^{-1}\nh_{k, out} = \\left(1 + \\exp(-h_{k, in})\\right)^{-1}\n\n\n\n\n\n\nThe input for the outcome \no_k\no_k\n is given by: \no_{k, in} = w^{(2)}_{0k} + w^{(2)}_{1k} \\cdot h_{1, out} + w^{(2)}_{2k} \\cdot h_{2, out}\no_{k, in} = w^{(2)}_{0k} + w^{(2)}_{1k} \\cdot h_{1, out} + w^{(2)}_{2k} \\cdot h_{2, out}\n\n\n\n\n\n\nAnd the output: \no_{k, out} = \\left(1 + \\exp(-o_{k, in})\\right)^{-1}\no_{k, out} = \\left(1 + \\exp(-o_{k, in})\\right)^{-1}\n\n\n\n\n\n\nLets define the cost function: \nL(w) = \\frac{1}{2}\\left(o_{1, true} - o_{1, out}\\right)^2 + \\frac{1}{2}\\left(o_{2, true} - o_{2, out}\\right)^2\nL(w) = \\frac{1}{2}\\left(o_{1, true} - o_{1, out}\\right)^2 + \\frac{1}{2}\\left(o_{2, true} - o_{2, out}\\right)^2\n\n\n\n\n\n\nTo update weights using the gradient descent method we need to calculate \n\\partial L(w) / \\partial w^{(a)}_{ij}\n\\partial L(w) / \\partial w^{(a)}_{ij}\n\n\n\n\n\n\nAs an example, let consider updating \nw^{(2)}_{11}\nw^{(2)}_{11}\n: \n\\frac{\\partial L(w)}{\\partial w^{(2)}_{11}} = \\frac{\\partial L(w)}{\\partial o_{1, out}}\\cdot\\frac{\\partial o_{1, out}}{\\partial o_{1, in}}\\cdot\\frac{\\partial o_{1, in}}{\\partial w^{(2)}_{11}}\n\\frac{\\partial L(w)}{\\partial w^{(2)}_{11}} = \\frac{\\partial L(w)}{\\partial o_{1, out}}\\cdot\\frac{\\partial o_{1, out}}{\\partial o_{1, in}}\\cdot\\frac{\\partial o_{1, in}}{\\partial w^{(2)}_{11}}\n\n\n\n\n\n\nAND, OR vs XOR\n\u00b6\n\n\n\n\n\n\nI will assume that basic logic gates do not need introduction\n\n\n\n\n\n\nThe point here is that AND and OR are linearly separable, and XOR is not\n\n\n\n\n\n\nX\n \n=\n \n[[\n0\n,\n0\n],\n \n[\n0\n,\n1\n],\n \n[\n1\n,\n0\n],\n \n[\n1\n,\n1\n]]\n\n\nY_and\n \n=\n \n[\n0\n,\n \n0\n,\n \n0\n,\n \n1\n]\n\n\nY_or\n \n=\n \n[\n0\n,\n \n1\n,\n \n1\n,\n \n1\n]\n\n\nY_xor\n \n=\n \n[\n0\n,\n \n1\n,\n \n1\n,\n \n0\n]\n\n\n\ntitles\n \n=\n \n(\n\"AND\"\n,\n \n\"OR\"\n,\n \n\"XOR\"\n)\n\n\n\nfor\n \ni\n,\n \nY\n \nin\n \nenumerate\n([\nY_and\n,\n \nY_or\n,\n \nY_xor\n]):\n\n  \nax\n \n=\n \nplt\n.\nsubplot\n(\n131\n \n+\n \ni\n)\n\n\n  \nax\n.\nset_xlim\n([\n-\n0.5\n,\n \n1.5\n])\n\n  \nax\n.\nset_ylim\n([\n-\n0.5\n,\n \n1.5\n])\n\n\n  \nax\n.\nset_aspect\n(\n'equal'\n)\n\n\n  \nplt\n.\ntitle\n(\ntitles\n[\ni\n])\n\n  \nplt\n.\nscatter\n(\n*\nzip\n(\n*\nX\n),\n \nc\n=\nY\n)\n\n\n  \nif\n \ni\n \n==\n \n0\n:\n\n    \nplt\n.\nplot\n([\n0\n,\n \n1.5\n],\n \n[\n1.5\n,\n \n0\n])\n\n  \nelif\n \ni\n \n==\n \n1\n:\n\n    \nplt\n.\nplot\n([\n-\n0.5\n,\n \n1\n],\n \n[\n1\n,\n \n-\n0.5\n])\n\n  \nelse\n:\n\n    \nplt\n.\ntext\n(\n0.5\n,\n \n0.5\n,\n \ns\n=\n\"?\"\n,\n \nfontsize\n=\n16\n,\n \nha\n=\n'center'\n,\n \nva\n=\n'center'\n)\n\n\n\nplt\n.\ntight_layout\n()\n\n\n\n\n\n\n\n\nSingle neuron approach\n\u00b6\n\n\ndraw_net\n(\n3\n,\n \n1\n)\n\n\n\n\n\n\n\n\n\n\n\n\nLet logistic function be our hypothesis: \nh(x_1, x_2) = \\left(1 + \\exp(-w_0 - w_1 \\cdot x_1 - w_2 \\cdot x_2)\\right)^{-1}\nh(x_1, x_2) = \\left(1 + \\exp(-w_0 - w_1 \\cdot x_1 - w_2 \\cdot x_2)\\right)^{-1}\n\n\n\n\n\n\nFor AND gate we want \nh(0, 0) = h(0, 1) = h(1, 0) = 0\nh(0, 0) = h(0, 1) = h(1, 0) = 0\n and \nh(1, 1) = 1\nh(1, 1) = 1\n\n\n\n\n\n\nThe example is so simple, that we could guess weights:\n\n\n\n\n\n\nw_0 << 0\nw_0 << 0\n\n\n\n\n\n\nw_0 + w_1 << 0\nw_0 + w_1 << 0\n\n\n\n\n\n\nw_0 + w_2 << 0\nw_0 + w_2 << 0\n\n\n\n\n\n\nw_0 + w_1 + w_2 >> 0\nw_0 + w_1 + w_2 >> 0\n\n\n\n\n\n\n\n\n\n\nBut lets build a neuron\n\n\n\n\n\n\nimport\n \ntheano\n\n\nimport\n \ntheano.tensor\n \nas\n \nT\n\n\n\nx\n \n=\n \nT\n.\nmatrix\n(\n'x'\n)\n \n# feature vector\n\n\ny\n \n=\n \nT\n.\nvector\n(\n'y'\n)\n \n# target vector\n\n\n\nw\n \n=\n \ntheano\n.\nshared\n(\nnp\n.\nrandom\n.\nrandn\n(\n2\n),\n \nname\n \n=\n \n'w'\n)\n \n# weights initialized randomly\n\n\nb\n \n=\n \ntheano\n.\nshared\n(\nnp\n.\nrandom\n.\nrandn\n(),\n \nname\n \n=\n \n'b'\n)\n  \n# bias term\n\n\n\nhypo\n \n=\n \n1\n \n/\n \n(\n1\n \n+\n \nT\n.\nexp\n(\n-\nT\n.\ndot\n(\nx\n,\n \nw\n)\n \n-\n \nb\n))\n              \n# hyphothesis\n\n\nxent\n \n=\n \n-\n \ny\n \n*\n \nT\n.\nlog\n(\nhypo\n)\n \n-\n \n(\n1\n \n-\n \ny\n)\n \n*\n \nT\n.\nlog\n(\n1\n \n-\n \nhypo\n)\n  \n# cross-entropy loss function\n\n\ncost\n \n=\n \nxent\n.\nsum\n()\n                                     \n# cost function\n\n\ngrad_w\n,\n \ngrad_b\n \n=\n \nT\n.\ngrad\n(\ncost\n,\n \n[\nw\n,\n \nb\n])\n                 \n# gradients\n\n\n\nalpha\n \n=\n \n0.1\n \n# learning rate\n\n\n\n# at each training step we update weights:\n\n\n# w -> w - alpha * grad_w and b -> b - alpha * grad_b\n\n\ntrain\n \n=\n \ntheano\n.\nfunction\n(\ninputs\n \n=\n \n[\nx\n,\ny\n],\n\n                        \noutputs\n \n=\n \ncost\n,\n\n                        \nupdates\n \n=\n \n((\nw\n,\n \nw\n \n-\n \nalpha\n \n*\n \ngrad_w\n),\n\n                                   \n(\nb\n,\n \nb\n \n-\n \nalpha\n \n*\n \ngrad_b\n)))\n\n\n\npredict\n \n=\n \ntheano\n.\nfunction\n(\ninputs\n=\n[\nx\n],\n \noutputs\n=\nhypo\n)\n\n\n\n\n\n\n\n\nTrain for all gates and save prediction\n\n\n\n\nN\n \n=\n \n1000\n\n\n\ngates\n \n=\n \n(\n\"AND\"\n,\n \n\"OR\"\n,\n \n\"XOR\"\n)\n\n\ngates_pred\n \n=\n \n{}\n\n\n\nfor\n \ngate\n,\n \ndata\n \nin\n \nzip\n(\ngates\n,\n \n(\nY_and\n,\n \nY_or\n,\n \nY_xor\n)):\n\n  \n# reset weights\n\n  \nw\n.\nset_value\n(\nnp\n.\nrandom\n.\nrandn\n(\n2\n))\n\n  \nb\n.\nset_value\n(\nnp\n.\nrandom\n.\nrandn\n())\n\n\n  \n# train neuron\n\n  \n[\ntrain\n(\nX\n,\n \ndata\n)\n \nfor\n \n_\n \nin\n \nrange\n(\nN\n)]\n\n  \ngates_pred\n[\ngate\n]\n \n=\n \npredict\n(\nX\n)\n\n\n\n\n\n\n\n\nLets see the result\n\n\n\n\nfor\n \ngate\n \nin\n \ngates\n:\n\n  \nfor\n \ni\n,\n \n(\nx1\n,\n \nx2\n)\n \nin\n \nenumerate\n(\nX\n):\n\n    \nprint\n(\n\"{} {} {} -> {}\"\n.\nformat\n(\nx1\n,\n \ngate\n,\n \nx2\n,\n \ngates_pred\n[\ngate\n][\ni\n]))\n\n  \nprint\n()\n\n\n\n\n\n\n0 AND 0 -> 0.00018774340087294284\n0 AND 1 -> 0.04835388458963121\n1 AND 0 -> 0.04834664667043076\n1 AND 1 -> 0.9321880416392287\n\n0 OR 0 -> 0.05096342740264947\n0 OR 1 -> 0.9795405137359009\n1 OR 0 -> 0.9798688809365104\n1 OR 1 -> 0.9999769570570585\n\n0 XOR 0 -> 0.49999999640124027\n0 XOR 1 -> 0.49999999943423534\n1 XOR 0 -> 0.4999999994362604\n1 XOR 1 -> 0.5000000024692554\n\n\n\n\n\n\n\nAs one could / should expect a linear classifier can not work for non-linear problems (like XOR gate)\n\n\n\n\nNeural network approach\n\u00b6\n\n\n\n\nPlease note, that XOR is not linear, but it can be expressed in terms or linear problems combination\n\n\n\n\nx XOR y = (x AND NOT y) OR (y AND NOT x)\n\n\n\n\n\n\n\nLets consider NN with two hidden neurons\n\n\n\n\ndraw_net\n(\n3\n,\n \n1\n,\n \n[\n3\n],\n \nw\n=\n8\n)\n\n\n\n\n\n\n\n\n\n\nLets use \ntheano\n last time to build the network\n\n\n\n\nimport\n \ntheano\n\n\nimport\n \ntheano.tensor\n \nas\n \nT\n\n\n\nx\n \n=\n \nT\n.\nmatrix\n(\n'x'\n)\n \n# feature vector\n\n\ny\n \n=\n \nT\n.\nvector\n(\n'y'\n)\n \n# target vector\n\n\n\n# first layer's weights (including bias)\n\n\nw1\n \n=\n \ntheano\n.\nshared\n(\nnp\n.\nrandom\n.\nrandn\n(\n3\n,\n2\n),\n \nname\n \n=\n \n'w1'\n)\n\n\n# second layer's weights (including bias)\n\n\nw2\n \n=\n \ntheano\n.\nshared\n(\nnp\n.\nrandom\n.\nrandn\n(\n3\n),\n \nname\n \n=\n \n'w2'\n)\n\n\n\nh\n \n=\n \nT\n.\nnnet\n.\nsigmoid\n(\nT\n.\ndot\n(\nx\n,\n \nw1\n[:\n2\n,])\n \n+\n \nw1\n[\n2\n,])\n  \n# hidden layer\n\n\no\n \n=\n \nT\n.\nnnet\n.\nsigmoid\n(\nT\n.\ndot\n(\nh\n,\n \nw2\n[:\n2\n,])\n \n+\n \nw2\n[\n2\n,])\n  \n# output layer\n\n\n\nxent\n \n=\n \n-\n \ny\n \n*\n \nT\n.\nlog\n(\no\n)\n \n-\n \n(\n1\n \n-\n \ny\n)\n \n*\n \nT\n.\nlog\n(\n1\n \n-\n \no\n)\n  \n# cross-entropy loss function\n\n\ncost\n \n=\n \nxent\n.\nsum\n()\n                               \n# cost function\n\n\ngrad_w1\n,\n \ngrad_w2\n \n=\n \nT\n.\ngrad\n(\ncost\n,\n \n[\nw1\n,\n \nw2\n])\n       \n# gradients\n\n\n\nalpha\n \n=\n \n0.1\n \n# learning rate\n\n\n\n# at each training step we update weights:\n\n\n# w -> w - alpha * grad_w and b -> b - alpha * grad_b\n\n\ntrain\n \n=\n \ntheano\n.\nfunction\n(\ninputs\n \n=\n \n[\nx\n,\ny\n],\n\n                        \noutputs\n \n=\n \ncost\n,\n\n                        \nupdates\n \n=\n \n((\nw1\n,\n \nw1\n \n-\n \nalpha\n \n*\n \ngrad_w1\n),\n\n                                   \n(\nw2\n,\n \nw2\n \n-\n \nalpha\n \n*\n \ngrad_w2\n)))\n\n\n\npredict\n \n=\n \ntheano\n.\nfunction\n(\ninputs\n=\n[\nx\n],\n \noutputs\n=\no\n)\n\n\n\n\n\n\n\n\nTrain on XOR and print prediction\n\n\n\n\n[\ntrain\n(\nX\n,\n \nY_xor\n)\n \nfor\n \n_\n \nin\n \nrange\n(\n10000\n)]\n\n\nprediction\n \n=\n \npredict\n(\nX\n)\n\n\n\nfor\n \ni\n,\n \n(\nx1\n,\n \nx2\n)\n \nin\n \nenumerate\n(\nX\n):\n\n  \nprint\n(\n\"{} XOR {} -> {}\"\n.\nformat\n(\nx1\n,\n \nx2\n,\n \nprediction\n[\ni\n]))\n\n\n\n\n\n\n0 XOR 0 -> 0.005114740815278212\n0 XOR 1 -> 0.9932581408757912\n1 XOR 0 -> 0.9931902289336407\n1 XOR 1 -> 0.004574250556087016\n\n\n\n\n\nAgain the same, but with tensorflow\n\u00b6\n\n\n\n\n\n\nLets try to solve XOR gate using \ntensorflow\n\n\n\n\n\n\nIn comments there is \ntheano\n code\n\n\n\n\n\n\nimport\n \ntensorflow\n \nas\n \ntf\n\n\n\n# x = T.matrix('x') # feature vector\n\n\n# y = T.vector('y') # target vector\n\n\nx\n \n=\n \ntf\n.\nplaceholder\n(\ntf\n.\nfloat32\n,\n \n[\n4\n,\n \n2\n])\n\n\ny\n \n=\n \ntf\n.\nplaceholder\n(\ntf\n.\nfloat32\n,\n \n[\n4\n,\n \n1\n])\n\n\n\n# w1 = theano.shared(np.random.randn(3,2), name = 'w1')\n\n\n# w2 = theano.shared(np.random.randn(3), name = 'w2')\n\n\nw1\n \n=\n \ntf\n.\nVariable\n(\ntf\n.\nrandom_normal\n([\n3\n,\n \n2\n]),\n \nname\n=\n'w1'\n)\n\n\nw2\n \n=\n \ntf\n.\nVariable\n(\ntf\n.\nrandom_normal\n([\n3\n,\n \n1\n]),\n \nname\n=\n'w2'\n)\n\n\n\n# h = T.nnet.sigmoid(T.dot(x, w1[:2,]) + w1[2,])\n\n\n# o = T.nnet.sigmoid(T.dot(h, w2[:2,]) + w2[2,])\n\n\nh\n \n=\n \ntf\n.\nsigmoid\n(\ntf\n.\nadd\n(\ntf\n.\nmatmul\n(\nx\n,\n \nw1\n[:\n2\n,]),\n \nw1\n[\n2\n,]))\n\n\no\n \n=\n \ntf\n.\nsigmoid\n(\ntf\n.\nadd\n(\ntf\n.\nmatmul\n(\nh\n,\n \nw2\n[:\n2\n,]),\n \nw2\n[\n2\n,]))\n\n\n\n#xent = - y * tf.log(o) - (1 - y) * tf.log(1 - o)\n\n\nxent\n \n=\n \ntf\n.\nlosses\n.\nlog_loss\n(\ny\n,\n \no\n)\n\n\ncost\n \n=\n \ntf\n.\nreduce_mean\n(\nxent\n)\n\n\n\nopt\n \n=\n \ntf\n.\ntrain\n.\nGradientDescentOptimizer\n(\n0.1\n)\n.\nminimize\n(\ncost\n)\n\n\n\ninit\n \n=\n \ntf\n.\nglobal_variables_initializer\n()\n\n\n\n\n\n\nX\n \n=\n \n[[\n0\n,\n0\n],\n \n[\n1\n,\n0\n],\n \n[\n0\n,\n1\n],\n \n[\n1\n,\n1\n]]\n\n\nY_xor\n \n=\n \n[[\n0\n],\n \n[\n1\n],\n \n[\n1\n],\n \n[\n0\n]]\n\n\n\nwith\n \ntf\n.\nSession\n()\n \nas\n \nsess\n:\n\n  \nsess\n.\nrun\n(\ninit\n)\n\n\n  \n[\nsess\n.\nrun\n(\nopt\n,\n \nfeed_dict\n=\n{\nx\n:\n \nX\n,\n \ny\n:\n \nY_xor\n})\n \nfor\n \n_\n \nin\n \nrange\n(\n10000\n)]\n\n\n  \nprint\n(\nsess\n.\nrun\n(\no\n,\n \nfeed_dict\n=\n{\nx\n:\n \nX\n}))\n\n\n\n\n\n\n[[0.01071231]\n [0.98791456]\n [0.98790956]\n [0.01878399]]\n\n\n\n\n\nSimple regression with NN\n\u00b6\n\n\n\n\nLets consider a dataset generated from \nnoised\n sinus distribution\n\n\n\n\nfrom\n \nmath\n \nimport\n \nsin\n,\n \ncos\n,\n \npi\n,\n \nexp\n\n\n\ndef\n \nget_dataset\n(\nN\n=\n20\n,\n \nsigma\n=\n0.1\n):\n\n  \n\"\"\"Generate N training samples\"\"\"\n\n  \n# X is a set of random points from [-1, 1]\n\n  \nX\n \n=\n \n2\n \n*\n \nnp\n.\nrandom\n.\nsample\n(\nN\n)\n \n-\n \n1\n\n  \n# Y are corresponding target values (with noise included)\n\n  \nY\n \n=\n \nnp\n.\narray\n([\nsin\n(\npi\n*\nx\n)\n \n+\n \nnp\n.\nrandom\n.\nnormal\n(\n0\n,\n \nsigma\n)\n \nfor\n \nx\n \nin\n \nX\n])\n\n\n  \nreturn\n \nX\n,\n \nY\n\n\n\n# plot a sample\n\n\nX\n,\n \nY\n \n=\n \nget_dataset\n(\n100\n,\n \n0.25\n)\n\n\n\nx_\n \n=\n \nnp\n.\narange\n(\n-\n1\n,\n \n1\n,\n \n0.01\n)\n\n\n\nplt\n.\nscatter\n(\nX\n,\n \nY\n,\n \ncolor\n=\n'C1'\n)\n\n\nplt\n.\nplot\n(\nx_\n,\n \nnp\n.\nsin\n(\nnp\n.\npi\n \n*\n \nx_\n),\n \n'C0--'\n);\n\n\n\n\n\n\n\n\n\n\n\n\nWe represent hidden neurons via logistic function (3 should be enough)\n\n\n\n\n\n\nThe output is just a sum of three hidden neurons outputs and bias term\n\n\n\n\n\n\ndraw_net\n(\n2\n,\n \n1\n,\n \n[\n4\n],\n \nw\n=\n10\n)\n\n\n\n\n\n\n\n\n\n\nLets implement above network in \ntensorflow\n\n\n\n\nimport\n \ntensorflow\n \nas\n \ntf\n\n\n\nx\n \n=\n \ntf\n.\nplaceholder\n(\ntf\n.\nfloat32\n,\n \n[\nNone\n,\n \n1\n])\n\n\ny\n \n=\n \ntf\n.\nplaceholder\n(\ntf\n.\nfloat32\n,\n \n[\nNone\n,\n \n1\n])\n\n\n\nw1\n \n=\n \ntf\n.\nVariable\n(\ntf\n.\nrandom_normal\n([\n1\n,\n \n3\n]),\n \nname\n=\n'w1'\n)\n\n\nw2\n \n=\n \ntf\n.\nVariable\n(\ntf\n.\nrandom_normal\n([\n3\n,\n \n1\n]),\n \nname\n=\n'w2'\n)\n\n\n\nb1\n \n=\n \ntf\n.\nVariable\n(\ntf\n.\nrandom_normal\n([\n3\n]),\n \nname\n=\n'b1'\n)\n\n\nb2\n \n=\n \ntf\n.\nVariable\n(\ntf\n.\nrandom_normal\n([\n1\n]),\n \nname\n=\n'b2'\n)\n\n\n\nh\n \n=\n \ntf\n.\nnn\n.\nsigmoid\n(\ntf\n.\nadd\n(\ntf\n.\nmatmul\n(\nx\n,\n \nw1\n),\n \nb1\n))\n\n\no\n \n=\n \ntf\n.\nadd\n(\ntf\n.\nmatmul\n(\nh\n,\n \nw2\n),\n \nb2\n)\n\n\n\nxent\n \n=\n \ntf\n.\nlosses\n.\nmean_squared_error\n(\ny\n,\n \no\n)\n\n\ncost\n \n=\n \ntf\n.\nreduce_mean\n(\nxent\n)\n\n\n\nopt\n \n=\n \ntf\n.\ntrain\n.\nGradientDescentOptimizer\n(\n0.25\n)\n.\nminimize\n(\ncost\n)\n\n\n\ninit\n \n=\n \ntf\n.\nglobal_variables_initializer\n()\n\n\n\n\n\n\n\n\nWe need to reshape out training data\n\n\n\n\nX_train\n \n=\n \nnp\n.\nreshape\n(\nX\n,\n \n(\n-\n1\n,\n1\n))\n\n\nY_train\n \n=\n \nnp\n.\nreshape\n(\nY\n,\n \n(\n-\n1\n,\n1\n))\n\n\n\nprint\n(\n\"Original\"\n,\n \nX\n[:\n5\n],\n \nY\n[:\n5\n],\n \nsep\n=\n'\n\\n\\n\n'\n)\n\n\nprint\n(\n\"\n\\n\nReshaped\"\n,\n \nX_train\n[:\n5\n],\n \nY_train\n[:\n5\n],\n \nsep\n=\n'\n\\n\\n\n'\n)\n\n\n\n\n\n\nOriginal\n\n[ 0.52226483  0.80798072  0.15974077 -0.50709742  0.78635136]\n\n[ 0.97538575  0.83448142  0.4117488  -0.89203221  0.38857674]\n\nReshaped\n\n[[ 0.52226483]\n [ 0.80798072]\n [ 0.15974077]\n [-0.50709742]\n [ 0.78635136]]\n\n[[ 0.97538575]\n [ 0.83448142]\n [ 0.4117488 ]\n [-0.89203221]\n [ 0.38857674]]\n\n\n\n\n\n\n\nAnd we can train the model\n\n\n\n\nX_test\n \n=\n \nnp\n.\narange\n(\n-\n1\n,\n \n1\n,\n \n0.01\n)\n.\nreshape\n(\n-\n1\n,\n1\n)\n\n\n\nwith\n \ntf\n.\nSession\n()\n \nas\n \nsess\n:\n\n  \nsess\n.\nrun\n(\ninit\n)\n\n\n  \n[\nsess\n.\nrun\n(\nopt\n,\n \nfeed_dict\n=\n{\nx\n:\n \nX_train\n,\n \ny\n:\n \nY_train\n})\n \nfor\n \n_\n \nin\n \nrange\n(\n10000\n)]\n\n\n  \nprediction\n \n=\n \nsess\n.\nrun\n(\no\n,\n \nfeed_dict\n=\n{\nx\n:\n \nX_test\n})\n\n\n\n\n\n\nplt\n.\nscatter\n(\nX_test\n,\n \nprediction\n,\n \ncolor\n=\n'C2'\n,\n \nlabel\n=\n'NN'\n)\n\n\nplt\n.\nscatter\n(\nX\n,\n \nY\n,\n \ncolor\n=\n'C1'\n,\n \nlabel\n=\n'Data'\n)\n\n\nplt\n.\nplot\n(\nx_\n,\n \nnp\n.\nsin\n(\nnp\n.\npi\n \n*\n \nx_\n),\n \n'C0--'\n,\n \nlabel\n=\n'Truth'\n)\n\n\n\nplt\n.\nlegend\n();\n\n\n\n\n\n\n\n\nMore examples\n\u00b6\n\n\n\n\nThe lecturer was too lazy to prepare more examples, but we can play in \ntensorflow playground\n, which has awesome visualizations\n\n\n\n\nMNIST\n\u00b6\n\n\n\n\nTHE MNIST DATABASE of handwritten digits\n\n\n\n\n\n\nThe MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n\n\nIt is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.\n\n\n\n\n\n\nWe can grab MNIST dataset using \ntensorflow.examples.tutorials.mnist\n\n\n\n\nimport\n \ntensorflow\n \nas\n \ntf\n\n\nfrom\n \ntensorflow.examples.tutorials.mnist\n \nimport\n \ninput_data\n\n\n\n# to avoid warnings printed in the notebook\n\n\ntf\n.\nlogging\n.\nset_verbosity\n(\ntf\n.\nlogging\n.\nERROR\n)\n\n\n\n# one hot -> label 0-9 -> 0...01, 0...10, ...\n\n\nmnist\n \n=\n \ninput_data\n.\nread_data_sets\n(\n\"/tmp/\"\n,\n \none_hot\n=\nTrue\n)\n\n\n\n\n\n\nSuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\nExtracting /tmp/train-images-idx3-ubyte.gz\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\nExtracting /tmp/train-labels-idx1-ubyte.gz\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\nExtracting /tmp/t10k-images-idx3-ubyte.gz\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\nExtracting /tmp/t10k-labels-idx1-ubyte.gz\n\n\n\n\n\n\n\nLets see how data looks like\n\n\n\n\nprint\n(\nmnist\n.\ntrain\n.\nimages\n.\nshape\n)\n\n\n\n\n\n\n(55000, 784)\n\n\n\n\n\nfor\n \ni\n \nin\n \nrange\n(\n4\n):\n\n  \nplt\n.\nsubplot\n(\n221\n \n+\n \ni\n)\n\n\n  \n# random training sample\n\n  \nindex\n \n=\n \nnp\n.\nrandom\n.\nrandint\n(\nlen\n(\nmnist\n.\ntrain\n.\nimages\n))\n\n\n  \n# train.images contains images in a form of a vector\n\n  \n# so we reshape it back to 28x28\n\n  \nplt\n.\nimshow\n(\nmnist\n.\ntrain\n.\nimages\n[\nindex\n]\n.\nreshape\n(\n28\n,\n \n28\n),\n \ncmap\n=\n'gray'\n)\n\n\n  \n# train.labels contains labels in one hot format\n\n  \nplt\n.\ntitle\n(\nmnist\n.\ntrain\n.\nlabels\n[\nindex\n])\n\n\n\nplt\n.\ntight_layout\n();\n\n\n\n\n\n\n\n\n\n\n\n\nToday we solve this classification problem with \nsoftmax\n\n\n\n\n\n\nDuring next lecture we apply convolutional NN on MNIST dataset\n\n\n\n\n\n\nLets start building the model\n\n\n\n\n\n\nx\n \n=\n \ntf\n.\nplaceholder\n(\ntf\n.\nfloat32\n,\n \n[\nNone\n,\n \n784\n])\n  \n# img -> 28x28 -> 784\n\n\ny\n \n=\n \ntf\n.\nplaceholder\n(\ntf\n.\nfloat32\n,\n \n[\nNone\n,\n \n10\n])\n   \n# 10 classes\n\n\n\nW\n \n=\n \ntf\n.\nVariable\n(\ntf\n.\nzeros\n([\n784\n,\n \n10\n]))\n  \n# weights\n\n\nb\n \n=\n \ntf\n.\nVariable\n(\ntf\n.\nzeros\n([\n10\n]))\n       \n# bias\n\n\n\nout\n \n=\n \ntf\n.\nnn\n.\nsoftmax\n(\ntf\n.\nmatmul\n(\nx\n,\n \nW\n)\n \n+\n \nb\n)\n\n\n\n\n\n\n\n\nDefine the loss function and optimizer\n\n\n\n\ncross_entropy\n \n=\n \ntf\n.\nreduce_mean\n(\n\n    \ntf\n.\nnn\n.\nsoftmax_cross_entropy_with_logits\n(\nlabels\n=\ny\n,\n \nlogits\n=\nout\n)\n\n\n)\n\n\n\ntrain_step\n \n=\n \ntf\n.\ntrain\n.\nGradientDescentOptimizer\n(\n0.5\n)\n.\nminimize\n(\ncross_entropy\n)\n\n\n\n\n\n\n\n\nTrain the model\n\n\n\n\n# create a session\n\n\nsess\n \n=\n  \ntf\n.\nSession\n()\n\n\n# initialize weights\n\n\nsess\n.\nrun\n(\ntf\n.\nglobal_variables_initializer\n())\n\n\n\nfor\n \n_\n \nin\n \nrange\n(\n10000\n):\n\n  \n# here instead of updating weights after the whole training set\n\n  \n# we use batch size 100 (more about that in the next section)\n\n  \nbatch_xs\n,\n \nbatch_ys\n \n=\n \nmnist\n.\ntrain\n.\nnext_batch\n(\n100\n)\n\n\n  \n# train_step is minimizing cross_entropy with learning rate 0.5 using GD\n\n  \n# we pass small batches to placeholders x and y\n\n  \nsess\n.\nrun\n(\ntrain_step\n,\n \nfeed_dict\n=\n{\nx\n:\n \nbatch_xs\n,\n \ny\n:\n \nbatch_ys\n})\n\n\n\n\n\n\n\n\nValidate the model\n\n\n\n\n# argmax returns the index of the heighest index in a tensor\n\n\n# equal returns True / False if prediction is equal/not equal to true label\n\n\n# cast would convert True/False to 1/0, so we can calculate the average\n\n\ncorrect_prediction\n \n=\n \ntf\n.\nequal\n(\ntf\n.\nargmax\n(\nout\n,\n \n1\n),\n \ntf\n.\nargmax\n(\ny\n,\n \n1\n))\n\n\naccuracy\n \n=\n \ntf\n.\nreduce_mean\n(\ntf\n.\ncast\n(\ncorrect_prediction\n,\n \ntf\n.\nfloat32\n))\n\n\n\nprint\n(\nsess\n.\nrun\n(\naccuracy\n,\n \nfeed_dict\n=\n{\nx\n:\n \nmnist\n.\ntest\n.\nimages\n,\n \ny\n:\n \nmnist\n.\ntest\n.\nlabels\n}))\n\n\n\nsess\n.\nclose\n()\n\n\n\n\n\n\n0.9241\n\n\n\n\n\n\n\n\n\nIt is pretty crazy what you can do with modern ML/DL frameworks\n\n\n\n\n\n\nWe have just learned our computer to recognize handwritten digits with a few lines of code\n\n\n\n\n\n\nDuring next lecture we will improve the accuracy using deep neural networks\n\n\n\n\n\n\nGradient descent variations\n\u00b6\n\n\n\n\n\n\nIn the \ngradient descent\n (GD) method weights are updated after a full loop over training data: \nW \\rightarrow W - \\alpha\\cdot\\nabla_W L(W)\nW \\rightarrow W - \\alpha\\cdot\\nabla_W L(W)\n\n\n\n\n\n\nIn the \nstochastic gradient descent\n (SGD) method weights are updated for each training sample: \nW \\rightarrow W - \\alpha\\cdot\\nabla_W L(W; x^{(i)}; y^{(i)})\nW \\rightarrow W - \\alpha\\cdot\\nabla_W L(W; x^{(i)}; y^{(i)})\n\n\n\n\n\n\nNote, that SGD is also called \nonline learning\n\n\n\n\n\n\nFor the large dataset it is likely that GD would recompute gradients for similar examples before an update\n\n\n\n\n\n\nSGD perform frequent updates but with a high variance, so objective function fluctuates\n\n\n\n\n\n\nIt may help to escape local minima\n\n\n\n\n\n\nThe common method, which is somehow between GD and SGD, is \nmini-batch gradient descent\n (MBGD) - the one we used for MNIST example: \nW \\rightarrow W - \\alpha\\cdot\\nabla_W L(W; x^{(i; i+n)}; y^{(i; i+n)})\nW \\rightarrow W - \\alpha\\cdot\\nabla_W L(W; x^{(i; i+n)}; y^{(i; i+n)})\n\n\n\n\n\n\nSGD on MNIST\n\u00b6\n\n\n\n\nLets see how the loss function looks like when applying SGD for training the network on MNIST data\n\n\n\n\n# create a session\n\n\nsess\n \n=\n  \ntf\n.\nSession\n()\n\n\n# initialize weights\n\n\nsess\n.\nrun\n(\ntf\n.\nglobal_variables_initializer\n())\n\n\n\ntest_loss\n \n=\n \n[]\n  \n# placeholder for loss value per iteration\n\n\n\nfor\n \n_\n \nin\n \nrange\n(\n10000\n):\n\n  \n# SGD -> batch size = 1\n\n  \nbatch_xs\n,\n \nbatch_ys\n \n=\n \nmnist\n.\ntrain\n.\nnext_batch\n(\n1\n)\n  \n  \n# update weights\n\n  \nsess\n.\nrun\n(\ntrain_step\n,\n \nfeed_dict\n=\n{\nx\n:\n \nbatch_xs\n,\n \ny\n:\n \nbatch_ys\n})\n\n  \n# calculate loss funtion on test samples\n\n  \nloss\n \n=\n \nsess\n.\nrun\n(\ncross_entropy\n,\n\n                  \nfeed_dict\n=\n{\nx\n:\n \nmnist\n.\ntest\n.\nimages\n,\n \ny\n:\n \nmnist\n.\ntest\n.\nlabels\n})\n\n  \n# save it  \n\n  \ntest_loss\n.\nappend\n(\nloss\n)\n\n\n\n\n\n\nplt\n.\nplot\n(\nnp\n.\narange\n(\n0\n,\n \n10000\n,\n \n1\n),\n \ntest_loss\n);\n\n\n\n\n\n\n\n\nMomentum\n\u00b6\n\n\n\n\nOnline learning may help to escape local minima\n\n\n\n\nx\n \n=\n \nnp\n.\nlinspace\n(\n-\n2\n,\n \n2\n,\n \n100\n)\n\n\ny1\n \n=\n \nx\n**\n2\n\n\ny2\n \n=\n \nnp\n.\narray\n([\na\n**\n2\n \n+\n \nnp\n.\nsin\n(\n5\n*\na\n)\n \nfor\n \na\n \nin\n \nx\n])\n\n\n\nplt\n.\nsubplot\n(\n121\n)\n\n\nplt\n.\nplot\n(\nx\n,\n \ny1\n)\n\n\nplt\n.\nscatter\n([\n-\n1.5\n],\n \n[\n3\n],\n \nc\n=\n'k'\n,\n \ns\n=\n300\n)\n\n\n\nplt\n.\nsubplot\n(\n122\n)\n\n\nplt\n.\nplot\n(\nx\n,\n \ny2\n)\n\n\nplt\n.\nscatter\n([\n-\n1.45\n],\n \n[\n1.75\n],\n \nc\n=\n'k'\n,\n \ns\n=\n300\n);\n\n\n\n\n\n\n\n\n\n\nAnother technique that can help escape local minima is to use a momentum term\n\n\n\n\n$\\begin{eqnarray}\nv_t & = & \\gamma\\cdot v_{t-1} + \\alpha\\cdot\\nabla_W L(W; x^{(i)}; y^{(i)}) \\\\\nW &\\rightarrow& W - v_t\n\\end{eqnarray}$\n\n\n\n\n\n\n\n\n\nIt is like pushing a ball down a hill and ball accumulates momentum\n\n\n\n\n\n\n\\gamma\n\\gamma\n is another hyperparameter, usually set around \n0.9\n0.9\n\n\n\n\n\n\nOne may want a smarter ball, which can predict its future position\n\n\n\n\n\n\nThe future position is approximate by \nW - \\gamma\\cdot v_{t-1}\nW - \\gamma\\cdot v_{t-1}\n\n\n\n\n\n\nNesterov accelerated gradient\n calculates gradient \nnot\n w.r.t current weights, but w.r.t approximated future  values:\n\n\n\n\n\n\n$\\begin{eqnarray}\nv_t & = & \\gamma\\cdot v_{t-1} + \\alpha\\cdot\\nabla_W L(W - \\gamma\\cdot v_{t-1} ; x^{(i)}; y^{(i)}) \\\\\nW &\\rightarrow& W - v_t\n\\end{eqnarray}$\n\n\n\n\n\nAdaptive models\n\u00b6\n\n\n\n\n\n\nA drawback of \nregular\n gradient descent methods is a constant learning rate, which needs to be tuned by hand\n\n\n\n\n\n\nif too small the training process is long\n\n\n\n\n\n\nif too large the minimum may be skipped\n\n\n\n\n\n\n\n\n\n\nThere are several algorithms which adapt the learning rate during training\n\n\n\n\n\n\nAdagrad, Adadelta, Adam are presented here, but please note there are more available\n\n\n\n\n\n\nAdagrad\n\u00b6\n\n\n\n\n\n\nLet \ng_{ti}\ng_{ti}\n be the gradient of the objective function w.r.t the weight \nW_i\nW_i\n at time step \nt\nt\n: \ng_{ti} = \\nabla_WL(W_{t,i})\ng_{ti} = \\nabla_WL(W_{t,i})\n\n\n\n\n\n\nIn this notation, SGD step for a parameter \ni\ni\n can be written as:  \nW_{t+1, i} = W_{t,i} - \\alpha g_{ti}\nW_{t+1, i} = W_{t,i} - \\alpha g_{ti}\n\n\n\n\n\n\nAdagrad modifies the learning rate \n\\alpha\n\\alpha\n based on previous gradients: \nW_{t+1, i} = W_{t,i} - \\frac{\\alpha}{\\sqrt{G_{t,ii} + \\varepsilon}} g_{ti}\nW_{t+1, i} = W_{t,i} - \\frac{\\alpha}{\\sqrt{G_{t,ii} + \\varepsilon}} g_{ti}\n\n\n\n\n\n\nWhere \nG_{t}\nG_{t}\n is a diagonal matrix, where \ni, i\ni, i\n elements are equal to the sum of all gradients w.r.t \nW_i\nW_i\n up to step \nt\nt\n\n\n\n\n\n\n\\varepsilon\n\\varepsilon\n is just to ensure denominator not equal zero\n\n\n\n\n\n\nSince \nG_t\nG_t\n is diagonal, we can write the general formula using element-wise product: \nW_{t+1} = W_{t}- \\frac{\\alpha}{\\sqrt{G_{t} + \\varepsilon}} \\odot g_{t}\nW_{t+1} = W_{t}- \\frac{\\alpha}{\\sqrt{G_{t} + \\varepsilon}} \\odot g_{t}\n\n\n\n\n\n\nNote, that each weight has now its own training rate; they just share initial value \n\\alpha\n\\alpha\n\n\n\n\n\n\nThere is no need to tune \n\\alpha\n\\alpha\n anymore; usually one set it up to be around \n0.01\n0.01\n and let Adagrad do the job\n\n\n\n\n\n\nThe disadvantage of Adagrad is that it accumulates squares of gradients in the denominator - at some point training rate is so small that the model is unable to learn anythng new\n\n\n\n\n\n\nAdadelta\n\u00b6\n\n\n\n\n\n\nAdadelta is an extension of Adagrad\n\n\n\n\n\n\nIts goal is to reduce the speed of training rate decreasing\n\n\n\n\n\n\nThe idea is to accumulate just a few last gradients instead all of them\n\n\n\n\n\n\nSince it is highly inefficient, Adagrad implements this as an exponentially decaying average of all the squared gradients\n\n\n\n\n\n\nLet \nE[g^2]_t\nE[g^2]_t\n be the average of the squared greadients up to step \nt\nt\n\n\n\n\n\n\nLet \n\\gamma\n\\gamma\n be decay constant (similar to momentum), then: \nE[g^2]_t = \\gamma\\cdot E[g^2]_{t-1} + (1 - \\gamma)\\cdot g_t^2\nE[g^2]_t = \\gamma\\cdot E[g^2]_{t-1} + (1 - \\gamma)\\cdot g_t^2\n\n\n\n\n\n\nAnd the training step is given by: \nW_{t+1} = W_{t}- \\frac{\\alpha}{\\sqrt{E[g^2]_t + \\varepsilon}}  g_{t}\nW_{t+1} = W_{t}- \\frac{\\alpha}{\\sqrt{E[g^2]_t + \\varepsilon}}  g_{t}\n\n\n\n\n\n\nPlease note, that the denominator is just root mean squared (RMS) of the gradient:  \nW_{t+1} = W_{t}- \\frac{\\alpha}{RMS[g]_t}  g_{t}\nW_{t+1} = W_{t}- \\frac{\\alpha}{RMS[g]_t}  g_{t}\n\n\n\n\n\n\nThe author also note that in GD-like methods the update has different hypothetical units than weights itself: \nunits~of~\\Delta w \\sim units~of~g \\sim units~of~\\frac{\\partial L}{\\partial w} \\sim \\frac{1}{units~of~w}\nunits~of~\\Delta w \\sim units~of~g \\sim units~of~\\frac{\\partial L}{\\partial w} \\sim \\frac{1}{units~of~w}\n\n\n\n\n\n\nInspired by second order optimization method that using Hessian information (like Newton's method): \nunits~of~\\Delta w \\sim units~of~H^{-1}g \\sim units~of~\\frac{\\frac{\\partial L}{\\partial w}}{\\frac{\\partial^2L}{\\partial w^2}} \\sim units~of~w\nunits~of~\\Delta w \\sim units~of~H^{-1}g \\sim units~of~\\frac{\\frac{\\partial L}{\\partial w}}{\\frac{\\partial^2L}{\\partial w^2}} \\sim units~of~w\n\n\n\n\n\n\nthe nominator is replaced by RMS of of weights updates (assuming diagonal Hessian): \n\\Delta w = \\frac{\\frac{\\partial L}{\\partial w}}{\\frac{\\partial^2 L}{\\partial w^2}} \\Rightarrow \\frac{1}{\\frac{\\partial^2 L}{\\partial w^2}} = \\frac{\\Delta w}{\\frac{\\partial L}{\\partial w}}\n\\Delta w = \\frac{\\frac{\\partial L}{\\partial w}}{\\frac{\\partial^2 L}{\\partial w^2}} \\Rightarrow \\frac{1}{\\frac{\\partial^2 L}{\\partial w^2}} = \\frac{\\Delta w}{\\frac{\\partial L}{\\partial w}}\n\n\n\n\n\n\nSince current update is not known until step is done, the following approximation is used: \n\\Delta W_t = - \\frac{RMS[\\Delta W]_{t-1}}{RMS[g]_t}  g_{t}\n\\Delta W_t = - \\frac{RMS[\\Delta W]_{t-1}}{RMS[g]_t}  g_{t}\n\n\n\n\n\n\nPlease note, that Adagrad does not require initial training rate!\n\n\n\n\n\n\nAlthough, it is still not parameter-free model as one need to set up \n\\gamma\n\\gamma\n and \n\\varepsilon\n\\varepsilon\n\n\n\n\n\n\nAdam\n\u00b6\n\n\n\n\n\n\nSimilar to Adedelta Adam uses exponentially decaying average of past sqaured gradients \nv_t\nv_t\n (notation from the original paper): \nv_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2)\\cdot g_t^2\nv_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2)\\cdot g_t^2\n\n\n\n\n\n\nAnd similar to SGD with momentum Adam also keeps information about past gradients: \nm_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1)\\cdot g_t\nm_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1)\\cdot g_t\n\n\n\n\n\n\nm_t\nm_t\n and \nv_t\nv_t\n are initialized with zeros\n\n\n\n\n\n\nFor \n\\beta_{1,2}\\sim 1\n\\beta_{1,2}\\sim 1\n training is slow at the beginning\n\n\n\n\n\n\nThus, bias-corrected first and second moments are introduced:\n\n\n\n\n\n\n$\\begin{eqnarray}\n\\hat m_t & = & \\frac{m_t}{1 - \\beta_1^t} \\\\\n\\hat v_t & = & \\frac{v_t}{1 - \\beta_2^t}\n\\end{eqnarray}$\n\n\n\n\n\n\n\n\n\nAnd the update rule is given by: \n\\Delta W_{t+1} = W_t - \\frac{\\alpha}{\\sqrt{\\hat v_t} + \\varepsilon}\\hat m_t\n\\Delta W_{t+1} = W_t - \\frac{\\alpha}{\\sqrt{\\hat v_t} + \\varepsilon}\\hat m_t\n\n\n\n\n\n\nThe authors suggest \n\\beta_1 \\approx 0.9\n\\beta_1 \\approx 0.9\n, \n\\beta_2 \\approx 0.999\n\\beta_2 \\approx 0.999\n, and \n\\varepsilon\\approx 10^{-8}\n\\varepsilon\\approx 10^{-8}\n\n\n\n\n\n\nRegularization\n\u00b6\n\n\n\n\n\n\nAs mentioned during last lecture, regularization is any modification to learning algorithm made to prevent overfitting\n\n\n\n\n\n\nThe common method is to add \nregularization term\n (\nregularizer\n) to a loss function \nL(W)\nL(W)\n: \nL(W) \\rightarrow L(W) + \\alpha R(W)\nL(W) \\rightarrow L(W) + \\alpha R(W)\n\n\n\n\n\n\nTwo common regularizer are \nL1\nL1\n or \nL2\nL2\n norms\n\n\n\n\n\n\nRidge\n regression uses \nL2\nL2\n regularization: \nR(W) = \\sum\\limits_i W_i^2\nR(W) = \\sum\\limits_i W_i^2\n\n\n\n\n\n\nLasso\n regression uses \nL1\nL1\n regularization: \nR(W) = \\sum\\limits_i |W_i|\nR(W) = \\sum\\limits_i |W_i|\n\n\n\n\n\n\nWhy to penalize the magnitude of weights?\n\n\n\n\n\n\nLets consider a simple example\n\n\n\n\n\n\nTo regularize or not to regularize\n\u00b6\n\n\n\n\nLets consider again \nsinus\n dataset\n\n\n\n\nfrom\n \nmath\n \nimport\n \nsin\n,\n \ncos\n,\n \npi\n,\n \nexp\n\n\n\ndef\n \nget_dataset\n(\nN\n=\n20\n,\n \nsigma\n=\n0.1\n):\n\n  \n\"\"\"Generate N training samples\"\"\"\n\n  \n# X is a set of random points from [-1, 1]\n\n  \nX\n \n=\n \n2\n \n*\n \nnp\n.\nrandom\n.\nsample\n(\nN\n)\n \n-\n \n1\n\n  \n# Y are corresponding target values (with noise included)\n\n  \nY\n \n=\n \nnp\n.\narray\n([\nsin\n(\npi\n*\nx\n)\n \n+\n \nnp\n.\nrandom\n.\nnormal\n(\n0\n,\n \nsigma\n)\n \nfor\n \nx\n \nin\n \nX\n])\n\n\n  \nreturn\n \nX\n,\n \nY\n\n\n\n# plot a sample\n\n\nX\n,\n \nY\n \n=\n \nget_dataset\n(\n50\n)\n\n\n\nx_\n \n=\n \nnp\n.\narange\n(\n-\n1\n,\n \n1\n,\n \n0.01\n)\n\n\n\nplt\n.\nscatter\n(\nX\n,\n \nY\n,\n \ncolor\n=\n'C1'\n)\n\n\nplt\n.\nplot\n(\nx_\n,\n \nnp\n.\nsin\n(\nnp\n.\npi\n \n*\n \nx_\n),\n \n'C0--'\n);\n\n\n\n\n\n\n\n\n\n\nLets fit data to polynomial of order 20\n\n\n\n\nN\n \n=\n \n20\n  \n# polynomial order\n\n\n\n# add powers of x \n\n\nX_train\n \n=\n \n[[\nx\n**\ni\n \nfor\n \ni\n \nin\n \nrange\n(\n1\n,\n \nN\n)]\n \nfor\n \nx\n \nin\n \nX\n]\n\n\n\nfrom\n \nsklearn.linear_model\n \nimport\n \nLinearRegression\n\n\n\nreg\n \n=\n \nLinearRegression\n()\n\n\nreg\n.\nfit\n(\nX_train\n,\n \nY\n);\n\n\n\n\n\n\n\n\nAnd plot prediction together with training data\n\n\n\n\nX_test\n \n=\n \nnp\n.\nlinspace\n(\n-\n1\n,\n \n1\n,\n \n100\n)\n\n\nY_test\n \n=\n \nreg\n.\npredict\n([[\nx\n**\ni\n \nfor\n \ni\n \nin\n \nrange\n(\n1\n,\nN\n)]\n \nfor\n \nx\n \nin\n \nX_test\n])\n\n\n\nplt\n.\nylim\n([\n-\n1.5\n,\n \n1.5\n])\n\n\n\nplt\n.\nscatter\n(\nX\n,\n \nY\n,\n \ncolor\n=\n'C1'\n)\n\n\nplt\n.\nplot\n(\nX_test\n,\n \nY_test\n,\n \n'C0'\n);\n\n\n\n\n\n\n\n\n\n\n\n\nIt is clearly overfitted\n\n\n\n\n\n\nLets do the same using Ridge regression\n\n\n\n\n\n\nfrom\n \nsklearn.linear_model\n \nimport\n \nRidge\n\n\n\nreg_l2\n \n=\n \nRidge\n(\nalpha\n=\n0.1\n)\n\n\nreg_l2\n.\nfit\n(\nX_train\n,\n \nY\n)\n\n\n\nY_test\n \n=\n \nreg_l2\n.\npredict\n([[\nx\n**\ni\n \nfor\n \ni\n \nin\n \nrange\n(\n1\n,\nN\n)]\n \nfor\n \nx\n \nin\n \nX_test\n])\n\n\n\nplt\n.\nylim\n([\n-\n1.5\n,\n \n1.5\n])\n\n\n\nplt\n.\nscatter\n(\nX\n,\n \nY\n,\n \ncolor\n=\n'C1'\n)\n\n\nplt\n.\nplot\n(\nX_test\n,\n \nY_test\n,\n \n'C0'\n);\n\n\n\n\n\n\n\n\n\n\n\n\nPlay on your own with \n\\lambda\n\\lambda\n to see how it affects the result\n\n\n\n\n\n\nNow, take a look at coefficients without regularizer\n\n\n\n\n\n\nreg\n.\ncoef_\n\n\n\n\n\n\narray([ 2.60931184e+00, -1.34651207e+00,  2.04999890e+01,  9.00515668e+00,\n       -4.18472051e+02, -1.27117676e+02,  3.14310368e+03,  1.10582631e+03,\n       -1.24897897e+04, -4.36716920e+03,  2.86984815e+04,  8.89018057e+03,\n       -3.94741356e+04, -9.78295086e+03,  3.21343881e+04,  5.54204833e+03,\n       -1.43111414e+04, -1.26843444e+03,  2.69454755e+03])\n\n\n\n\n\n\n\n\n\nNote, that coefficients increase drastically for large powers of \nx\nx\n\n\n\n\n\n\nHigh weights mean that a model put a lot of emphasis on a given features, which leads to overfitting\n\n\n\n\n\n\nThat is why we need to put some constraints on the magnitude of weights\n\n\n\n\n\n\nNow, lets see how coefficients look like with L2 regularizer\n\n\n\n\n\n\nreg_l2\n.\ncoef_\n\n\n\n\n\n\narray([ 2.3350277 , -0.00324186, -1.57606849,  0.04982775, -1.0117878 ,\n       -0.025911  , -0.4412411 , -0.04283921, -0.10340078, -0.03155621,\n        0.07582327, -0.01138888,  0.16245585,  0.01121341,  0.19718257,\n        0.03442763,  0.20359299,  0.05769045,  0.19518384])\n\n\n\n\n\n\n\n\n\nRidge regression causes coefficient shrinkage and reduces model complexity\n\n\n\n\n\n\nLets repeat the same for Lasso regression\n\n\n\n\n\n\nfrom\n \nsklearn.linear_model\n \nimport\n \nLasso\n\n\n\nreg_l1\n \n=\n \nLasso\n(\nalpha\n=\n0.001\n)\n\n\nreg_l1\n.\nfit\n(\nX_train\n,\n \nY\n)\n\n\n\nY_test\n \n=\n \nreg_l1\n.\npredict\n([[\nx\n**\ni\n \nfor\n \ni\n \nin\n \nrange\n(\n1\n,\nN\n)]\n \nfor\n \nx\n \nin\n \nX_test\n])\n\n\n\nplt\n.\nylim\n([\n-\n1.5\n,\n \n1.5\n])\n\n\n\nplt\n.\nscatter\n(\nX\n,\n \nY\n,\n \ncolor\n=\n'C1'\n)\n\n\nplt\n.\nplot\n(\nX_test\n,\n \nY_test\n,\n \n'C0'\n);\n\n\n\n\n\n\n\n\n\n\n\n\nOnce again we got a nice fit\n\n\n\n\n\n\nBut there is a difference - see the coefficients\n\n\n\n\n\n\nreg_l1\n.\ncoef_\n\n\n\n\n\n\narray([ 2.779238  , -0.        , -3.20859903, -0.        , -0.        ,\n       -0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.1980417 ,  0.        ,  0.22056433,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ])\n\n\n\n\n\n\n\n\n\nNote, that in opposite to Ridge not all features are taken into account as some weights ended up being zero\n\n\n\n\n\n\nThus, Lasso regression additionally performs features selection, which is useful for data with many features\n\n\n\n\n\n\nSummary\n\u00b6\n\n\n\n\n\n\nNeural networks are one of the most popular machine learning method these days\n\n\n\n\n\n\nNote, that NN has much more hyperparameters than models described so far:\n\n\n\n\n\n\nnumber of hidden layers\n\n\n\n\n\n\nnumber of neurons\n\n\n\n\n\n\nactivation functions\n\n\n\n\n\n\nlearning algorithm settings (model, learning rate, momentum etc)\n\n\n\n\n\n\nregularization method and its parameters\n\n\n\n\n\n\nbatch size in mini-batch gradient descent\n\n\n\n\n\n\n\n\n\n\nIt is crucial to use cross-validation for hyperparameter tuning\n\n\n\n\n\n\nNext week \"we need to go deeper\"",
            "title": "Neural Networks"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#neural-network",
            "text": "Artificial neural network (in particular deep NN) is the most popular machine learning method these days    They are inspired by human brains (at least initially)    Artifical neuron is a mathematical function    Neurons are connected with each other (kind of synapses)    Usually connections have some weights    Today, feedforward neural networks (multilayer perceptrons) will be discussed    However, before we go there, lets start with linear and logistic regression    # our standard imports: matplotlib and numpy  import   matplotlib.pyplot   as   plt  import   numpy   as   np  # just to overwrite default colab style  plt . style . use ( 'default' )  plt . style . use ( 'seaborn-talk' )",
            "title": "Neural Network"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#linear-regression",
            "text": "Lets consider dataset  \\left\\{(\\vec x^{(i)}, y^{(i)}); i = 1,\\cdots, N\\right\\} \\left\\{(\\vec x^{(i)}, y^{(i)}); i = 1,\\cdots, N\\right\\}    where explanatory variables ( features )  \\vec x^{(i)} = (x^{(i)}_1,\\cdots, x^{(i)}_n) \\in \\mathcal{R}^n \\vec x^{(i)} = (x^{(i)}_1,\\cdots, x^{(i)}_n) \\in \\mathcal{R}^n    and dependent variables ( targets )  y^{(i)} \\in \\mathcal{R} y^{(i)} \\in \\mathcal{R}    Lets define the  hypothesis :  h(\\vec x) = w_0 + w_1x_1 + \\cdots w_nx_n h(\\vec x) = w_0 + w_1x_1 + \\cdots w_nx_n    In other words we claim that  y^{(i)} y^{(i)}  can be calculated from  h(\\vec x^{(i)}) h(\\vec x^{(i)}) , if we know  weights   w_i w_i    For convenience lets set  x^{(i)}_0 = 1 x^{(i)}_0 = 1 , so we can rewrite the hypothesis:  h(\\vec x) = \\sum_{i=0}^nw_ix_i = \\vec w \\cdot \\vec x h(\\vec x) = \\sum_{i=0}^nw_ix_i = \\vec w \\cdot \\vec x",
            "title": "Linear regression"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#normal-equation",
            "text": "One could find weights using normal equation    Let  Y^T = (y^{(1)}, \\cdots, y^{(N)}) Y^T = (y^{(1)}, \\cdots, y^{(N)}) ,  X^T = \\left((\\vec x^{(1)})^T, \\cdots, (\\vec x^{(N)})^T\\right) X^T = \\left((\\vec x^{(1)})^T, \\cdots, (\\vec x^{(N)})^T\\right)  and  W^T = (w_0, \\cdots, w_n) W^T = (w_0, \\cdots, w_n)    Then we can write a matrix equation:  Y = XW Y = XW    The normal equation (minimizing the sum of the square differences between left and right sides):  X^TY = X^TXW X^TY = X^TXW    Thus, one could find weights by calculating:  W = (X^TX)^{-1}X^TY W = (X^TX)^{-1}X^TY    Doable, but computational expensive",
            "title": "Normal equation"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#gradient-descent",
            "text": "Gradient descent is an iterative algorithm for finding the minimum    For linear regression the  cost function  (or  loss function ) is given by mean squared error:  L(\\vec w) = \\frac{1}{2N}\\sum\\limits_{i=1}^N\\left(h(\\vec x^{(i)}) - y^{(i)}\\right)^2 L(\\vec w) = \\frac{1}{2N}\\sum\\limits_{i=1}^N\\left(h(\\vec x^{(i)}) - y^{(i)}\\right)^2    It measures the quality of given set of parameters / weights    Please note, that  \\frac{1}{2} \\frac{1}{2}  is added for convenience to MSE definition    In gradient descent method weights are updated w.r.t. the gradient of cost function:  w_j \\rightarrow w_j - \\alpha\\frac{\\partial L(\\vec w)}{\\partial w_j} = w_j - \\frac{\\alpha}{N}\\sum\\limits_{i=1}^{N}\\left(h(\\vec x^{(i)}) - y^{(i)}\\right)x^{(i)}_j w_j \\rightarrow w_j - \\alpha\\frac{\\partial L(\\vec w)}{\\partial w_j} = w_j - \\frac{\\alpha}{N}\\sum\\limits_{i=1}^{N}\\left(h(\\vec x^{(i)}) - y^{(i)}\\right)x^{(i)}_j    Where  \\alpha \\alpha  is training rate",
            "title": "Gradient descent"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#example",
            "text": "Lets generate some fake data according to  y = ax + b y = ax + b  for given slope and intercept    And add some noise to  y y    ### SETTINGS ###  N   =   100   # number of samples  a   =   0.50   # slope  b   =   0.50   # y-intercept  s   =   0.25   # sigma  ### GENERATE SAMPLES ###  X   =   ( 10.0   *   np . random . sample ( N ))                                 # features  Y   =   [( a   *   X [ i ]   +   b )   +   np . random . normal ( 0 , s )   for   i   in   range ( N )]   # targets  ### PLOT SAMPLES ###  plt . xlabel ( 'Feature' )  plt . ylabel ( 'Target' )  plt . scatter ( X ,   Y ,   marker = '.' );     It is time to learn about new framework    Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. It can use GPUs and perform efficient symbolic differentiation.   ! pip   install   theano     In this example the hypothesis is given by:  h(x) = \\vec w \\cdot \\vec x = ax + b h(x) = \\vec w \\cdot \\vec x = ax + b    where  \\vec w = (b, a) \\vec w = (b, a)  and  \\vec x = (1, x) \\vec x = (1, x)    Lets first create symbolic variable for:    feature vector  X = (x_1, \\cdots, x_N) X = (x_1, \\cdots, x_N)    target  Y = (y_1, \\cdots, y_N) Y = (y_1, \\cdots, y_N)    weights  a a  and  b b  (or  w_1 w_1  and  w_0 w_0 )      import   theano  import   theano.tensor   as   T  x   =   T . vector ( 'x' )   # feature vector  y   =   T . vector ( 'y' )   # target vector  # weights initialized randomly  #a = theano.shared(np.random.randn(), name = 'w')  #b = theano.shared(np.random.randn(), name = 'b')  # initial weights by hand for demonstration (random may be to close)  a   =   theano . shared ( - 0.5 ,   name   =   'w' )  b   =   theano . shared ( 1.0 ,   name   =   'b' )     Having that, we can define:    hypothesis    cost function    gradients      pred   =   T . dot ( x ,   a )   +   b                   # hyphothesis  cost   =   T . sum ( T . pow ( pred   -   y ,   2 ))   /   N     # cost function  grad_a ,   grad_b   =   T . grad ( cost ,   [ a ,   b ])    # gradients    And finally, we define gradient descent method (which also returns the value of the cost function)   alpha   =   0.005   # learning rate  # at each training step we update weights:  # w -> w - alpha * grad_w and b -> b - alpha * grad_b  train   =   theano . function ( inputs   =   [ x , y ], \n                         outputs   =   cost , \n                         updates   =   (( a ,   a   -   alpha   *   grad_a ), \n                                    ( b ,   b   -   alpha   *   grad_b )))    Each training step involves the full cycle on training data ( epoch )   n_epochs   =   1000    # number of training steps / epochs  costs   =   []         # to keep track on the value of cost function on each step  weights   =   []       # to store few set of weights  keep   =   ( 0 ,   10 ,   100 ,   500 ,   1000 )    # save result for some epochs passed  for   i   in   range ( n_epochs   +   1 ): \n   if   i   in   keep : \n     weights . append (( a . get_value (),   b . get_value ())) \n\n   costs . append ( train ( X ,   Y ))    Finally, we can visualize the results   plt . figure ( figsize = ( 10 ,   15 ))  n_rows   =   3  n_cols   =   2  for   i ,   ( a_ ,   b_ )   in   enumerate ( weights ): \n   plt . subplot ( n_rows ,   n_cols ,   i + 1 ) \n\n   plt . title ( 'Epoch  %i : y =  %.2f  x +  %.2f '   %   ( keep [ i ],   a_ ,   b_ )) \n   plt . xlabel ( 'Feature' ) \n   plt . ylabel ( 'Target' ) \n\n   x_   =   np . arange ( 0 ,   10 ,   0.1 ) \n\n   plt . plot ( x_ ,   a_ * x_   +   b_ ,   color = 'C1' ) \n   plt . scatter ( X ,   Y ,   marker = '.' )  plt . subplot ( n_rows ,   n_cols ,   len ( weights )   +   1 )  plt . title ( \"Cost function\" )  plt . xlabel ( \"Epoch\" )  plt . ylabel ( \"L\" )  plt . ylim ([ 0 , 0.2 ])  plt . plot ( range ( len ( costs )),   costs )  plt . tight_layout ();",
            "title": "Example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#logistic-regression",
            "text": "Logistic regression is used when dependent variable (target) is categorical    Lets consider dataset  \\left\\{(\\vec x^{(i)}, y^{(i)}); i = 1,\\cdots, N\\right\\} \\left\\{(\\vec x^{(i)}, y^{(i)}); i = 1,\\cdots, N\\right\\}    where independent variables  \\vec x^{(i)} = (1, x^{(i)}_1,\\cdots, x^{(i)}_n) \\vec x^{(i)} = (1, x^{(i)}_1,\\cdots, x^{(i)}_n)    and dependent variables (we start with binary case)  y^{(i)} \\in \\{0, 1\\} y^{(i)} \\in \\{0, 1\\}",
            "title": "Logistic regression"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#hypothesis",
            "text": "The dependent variable follows Bernoulli distribution ( 1 1  with probability  p p ,  0 0  with probability  1-p 1-p )    We want to link the independent variable with Bernoulli distribution    The  logit function   translates  a linear combination  \\vec w \\cdot \\vec x \\vec w \\cdot \\vec x  (which can result in any value) into probability distribution:  logit(p) = \\ln(odds) = \\ln\\left(\\frac{p}{1-p}\\right) = \\vec w \\cdot \\vec x logit(p) = \\ln(odds) = \\ln\\left(\\frac{p}{1-p}\\right) = \\vec w \\cdot \\vec x    p_   =   np . arange ( 0.01 ,   0.99 ,   0.01 )  plt . title ( \"Logit function\" )  plt . xlabel ( \"p\" )  plt . ylabel ( \"$\\ln(p/(1-p))$\" )  plt . plot ( p_ ,   np . log ( p_   /   ( 1   -   p_ )));      In logistic regression  hypothesis  is defined as the inverse function of logit -  logistic function :  h(\\vec x) = \\frac{1}{1 + e^{-\\vec w \\cdot \\vec x}} h(\\vec x) = \\frac{1}{1 + e^{-\\vec w \\cdot \\vec x}}    Thus, the probability of  1 1  is given by  P(y = 1~|~\\vec x, \\vec w) = h(\\vec x) P(y = 1~|~\\vec x, \\vec w) = h(\\vec x)    and the probability of  0 0  is given by  P(y = 0~|~\\vec x, \\vec w) = 1 - h(\\vec x) P(y = 0~|~\\vec x, \\vec w) = 1 - h(\\vec x)    x_   =   np . arange ( - 10 ,   10 ,   0.1 )  plt . title ( \"Logistic function\" )  plt . xlabel ( \"x\" )  plt . ylabel ( \"$1/(1 + e^{-x})$\" )  plt . plot ( x_ ,   1   /   ( 1   +   np . exp ( - x_ )));",
            "title": "Hypothesis"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#cost-function",
            "text": "The probablity mass function (PMS) for  y y  (for given  \\vec w \\vec w ):  p(y~|~\\vec x, \\vec w) = h(\\vec x)^y\\cdot \\left(1 - h(\\vec x)\\right)^{1 - y} p(y~|~\\vec x, \\vec w) = h(\\vec x)^y\\cdot \\left(1 - h(\\vec x)\\right)^{1 - y}    The  likelihood function  is PMS considered as a function of  \\vec w \\vec w  (for fixed  y y )    Thus, for a single data point  (\\vec x^{(i)}, y^{(i)}) (\\vec x^{(i)}, y^{(i)}) :  l(\\vec w) = h(\\vec x^{(i)})^{y^{(i)}}\\cdot \\left(1 - h(\\vec x^{(i)})\\right)^{1 - y^{(i)}} l(\\vec w) = h(\\vec x^{(i)})^{y^{(i)}}\\cdot \\left(1 - h(\\vec x^{(i)})\\right)^{1 - y^{(i)}}    And for the whole dataset:  l(\\vec w) = \\prod\\limits_{i=1}^N h(\\vec x^{(i)})^{y^{(i)}}\\cdot \\left(1 - h(\\vec x^{(i)})\\right)^{1 - y^{(i)}} l(\\vec w) = \\prod\\limits_{i=1}^N h(\\vec x^{(i)})^{y^{(i)}}\\cdot \\left(1 - h(\\vec x^{(i)})\\right)^{1 - y^{(i)}}    The goal is to maximize likelihood w.r.t to  \\vec w \\vec w , which is the same as maximizing  log-likelihood :  \\ln\\left(l(\\vec w)\\right) = \\sum\\limits_{i=1}^N\\left[y^{(i)}\\ln\\left(h(\\vec x^{(i)})\\right) + \\left(1 - y^{(i)}\\right)\\ln\\left(1 - h(\\vec x^{(i)})\\right)\\right] \\ln\\left(l(\\vec w)\\right) = \\sum\\limits_{i=1}^N\\left[y^{(i)}\\ln\\left(h(\\vec x^{(i)})\\right) + \\left(1 - y^{(i)}\\right)\\ln\\left(1 - h(\\vec x^{(i)})\\right)\\right]    Which is the same as minimizing the cost function  L(\\vec w) = -\\frac{1}{N}\\ln\\left(l(\\vec w)\\right) L(\\vec w) = -\\frac{1}{N}\\ln\\left(l(\\vec w)\\right)    Once again using gradient descent method we can update weights using:  w_j \\rightarrow w_j - \\alpha\\frac{\\partial L(\\vec w)}{\\partial w_j} = w_j - \\frac{\\alpha}{N}\\sum\\limits_{i=1}^{N}\\left(y^{(i)} - h(\\vec x^{(i)})\\right)x^{(i)}_j w_j \\rightarrow w_j - \\alpha\\frac{\\partial L(\\vec w)}{\\partial w_j} = w_j - \\frac{\\alpha}{N}\\sum\\limits_{i=1}^{N}\\left(y^{(i)} - h(\\vec x^{(i)})\\right)x^{(i)}_j",
            "title": "Cost function"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#quick-proof",
            "text": "First, lets consider the derivative of  h h   $\\begin{eqnarray}\n\\frac{\\partial h(\\vec x)}{\\partial w_j} & = & \\frac{\\partial}{\\partial w_j}\\left(1 + e^{-\\vec w \\cdot \\vec x}\\right)^{-1} \\\\\n                                        & = & e^{-\\vec w \\cdot \\vec x}\\left(1 + e^{-\\vec w \\cdot \\vec x}\\right)^{-2}x_j \\\\\n                                        & = & (1 + e^{-\\vec w \\cdot \\vec x} - 1)\\left(1 + e^{-\\vec w \\cdot \\vec x}\\right)^{-2}x_j \\\\\n                                        & = & \\left(1 + e^{-\\vec w \\cdot \\vec x}\\right)^{-1}x_j - \\left(1 + e^{-\\vec w \\cdot \\vec x}\\right)^{-2}x_j\\\\\n                                        & = & h(\\vec x)x_j - h^2(\\vec x)x_j \\\\\n                                        & = & h(\\vec x)\\left(1 - h(\\vec x)\\right)x_j\n\\end{eqnarray}$     Thus,  \\frac{\\partial}{\\partial w_j}\\ln\\left(h(\\vec x)\\right) = \\left(1 - h(\\vec x)\\right)x_j \\frac{\\partial}{\\partial w_j}\\ln\\left(h(\\vec x)\\right) = \\left(1 - h(\\vec x)\\right)x_j    and  \\frac{\\partial}{\\partial w_j}\\ln\\left(1 - h(\\vec x)\\right) = -h(\\vec x)x_j \\frac{\\partial}{\\partial w_j}\\ln\\left(1 - h(\\vec x)\\right) = -h(\\vec x)x_j    Finally, we have  \\frac{\\partial}{\\partial w_j}\\left[y\\ln\\left(h(\\vec x)\\right) + (1 - y)\\ln\\left(1 - h(\\vec x)\\right)\\right] = \\left[y\\left(1 - h(\\vec x)\\right) - (1 - y)h(\\vec x)\\right]x_j = \\left[y - h(\\vec x)\\right]x_j \\frac{\\partial}{\\partial w_j}\\left[y\\ln\\left(h(\\vec x)\\right) + (1 - y)\\ln\\left(1 - h(\\vec x)\\right)\\right] = \\left[y\\left(1 - h(\\vec x)\\right) - (1 - y)h(\\vec x)\\right]x_j = \\left[y - h(\\vec x)\\right]x_j",
            "title": "Quick proof"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#example_1",
            "text": "Lets consider the following dataset    x x  - number of hours spent studying machine learning    y y  - student passed (1) or failed (0) the exam      N   =   50    # number of students per class  X   =   np . concatenate (( np . random . random (( N ))   *   35 , \n                    30   +   np . random . random (( N ))   *   25 ))  Y   =   np . concatenate (([ 0 ] * N ,   [ 1 ] * N ))  plt . xlabel ( \"Study time [h]\" )  plt . ylabel ( \"Success\" )  plt . scatter ( X ,   Y );     Once again lets use  theano   import   theano  import   theano.tensor   as   T  x   =   T . vector ( 'x' )   # feature vector  y   =   T . vector ( 'y' )   # target vector  a   =   theano . shared ( np . random . randn (),   name   =   'w' )   # weights initialized randomly  b   =   theano . shared ( np . random . randn (),   name   =   'b' )  hypo   =   1   /   ( 1   +   T . exp ( - T . dot ( x ,   a )   -   b ))                # hyphothesis  xent   =   -   y   *   T . log ( hypo )   -   ( 1   -   y )   *   T . log ( 1   -   hypo )    # cross-entropy loss function  cost   =   xent . sum ()                                       # cost function  grad_a ,   grad_b   =   T . grad ( cost ,   [ a ,   b ])                   # gradients  alpha   =   0.01   # learning rate  # at each training step we update weights:  # w -> w - alpha * grad_w and b -> b - alpha * grad_b  train   =   theano . function ( inputs   =   [ x , y ], \n                         outputs   =   cost , \n                         updates   =   (( a ,   a   -   alpha   *   grad_a ), \n                                    ( b ,   b   -   alpha   *   grad_b )))    For the training we will scale features to  [0, 1] [0, 1] , which helps gradient descent to converge faster   x_min   =   min ( X )  x_max   =   max ( X )  s   =   lambda   x :   ( x   -   x_min )   /   ( x_max   -   x_min )    # scale    Now, we train the model on normalized data   n_epochs   =   1000  [ train ( s ( X ),   Y )   for   _   in   range ( n_epochs )]  plt . xlabel ( \"Study time [h]\" )  plt . ylabel ( \"Success\" )  plt . scatter ( X ,   Y )  h_   =   np . arange ( 0 ,   60 ,   0.01 )  plt . plot ( h_ ,   1   /   ( 1   +   np . exp ( - s ( h_ ) * a . get_value ()   -   b . get_value ())),   'C1' )  plt . plot ([ 0 ,   60 ],   [ 0.5 ,   0.5 ],   'C2--' );      The orange line gives the probability of success as a function of study time    To classify a student one can make a cut at  0.5 0.5",
            "title": "Example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#multinominal-logistic-regression",
            "text": "So far, we considered dependent variable to be binary - it is time to generalize LR to  K K  possible classes    Lets consider dataset  \\left\\{(\\vec x^{(i)}, y^{(i)}); i = 1,\\cdots, N\\right\\} \\left\\{(\\vec x^{(i)}, y^{(i)}); i = 1,\\cdots, N\\right\\}    where independent variables  \\vec x^{(i)} = (1, x^{(i)}_1,\\cdots, x^{(i)}_n) \\vec x^{(i)} = (1, x^{(i)}_1,\\cdots, x^{(i)}_n)    and dependent variables  y^{(i)} \\in \\{1, \\cdots, K\\} y^{(i)} \\in \\{1, \\cdots, K\\}",
            "title": "Multinominal logistic regression"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#logit-approach",
            "text": "The one way to go is to prepare  K - 1 K - 1  binary classifiers w.r.t. to a chosen class as a pivot    The odds of  y y  being  j \\in \\left\\{1, \\cdots, K-1\\right\\} j \\in \\left\\{1, \\cdots, K-1\\right\\}  (chosing  K K  as a pivot) is given by:  \\frac{P(y = j)}{P(y = K)} \\frac{P(y = j)}{P(y = K)}    which leads to  K-1 K-1  equations with different weights for each possible outcome:    $\\begin{eqnarray}\n\\ln\\frac{P(y = 1)}{P(y = K)} = \\vec w_1 \\cdot \\vec x & \\Rightarrow & P(y=1) = P(y=K)e^{\\vec w_1 \\cdot \\vec x} \\\\\n\\ln\\frac{P(y = 2)}{P(y = K)} = \\vec w_2 \\cdot \\vec x & \\Rightarrow & P(y=2) = P(y=K)e^{\\vec w_2 \\cdot \\vec x} \\\\\n                                                     & ... & \\\\\n\\ln\\frac{P(y = K-1)}{P(y = K)} = \\vec w_{K-1} \\cdot \\vec x & \\Rightarrow & P(y=K-1) = P(y=K)e^{\\vec w_{K-1} \\cdot \\vec x} \\\\\n\\end{eqnarray}$     And as they have to sum to  1 1  we get:  P(y=K) = 1 - \\sum\\limits_{i=1}^{K-1}P(y=K)e^{\\vec w_i \\cdot \\vec x} \\Rightarrow P(y = K) = \\frac{1}{1 + \\sum\\limits_{i=1}^{K-1}e^{\\vec w_i \\cdot \\vec x}} P(y=K) = 1 - \\sum\\limits_{i=1}^{K-1}P(y=K)e^{\\vec w_i \\cdot \\vec x} \\Rightarrow P(y = K) = \\frac{1}{1 + \\sum\\limits_{i=1}^{K-1}e^{\\vec w_i \\cdot \\vec x}}    Thus, the probability of  y = j y = j , for  j \\in \\left\\{1, \\cdots, K-1\\right\\} j \\in \\left\\{1, \\cdots, K-1\\right\\}  is given by:  P(y=j) = \\frac{e^{\\vec w_j \\cdot \\vec x}}{1 + \\sum\\limits_{i=1}^{K-1}e^{\\vec w_i \\cdot \\vec x}} P(y=j) = \\frac{e^{\\vec w_j \\cdot \\vec x}}{1 + \\sum\\limits_{i=1}^{K-1}e^{\\vec w_i \\cdot \\vec x}}",
            "title": "Logit approach"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#check-binary-case",
            "text": "Lets consider binary classification with  0 0  being a pivot    Then we have:  p = P(y=1) = \\frac{e^{\\vec w \\cdot \\vec x}}{1 + e^{\\vec w \\cdot \\vec x}} = \\frac{1}{1 + e^{-\\vec w \\cdot \\vec x}} p = P(y=1) = \\frac{e^{\\vec w \\cdot \\vec x}}{1 + e^{\\vec w \\cdot \\vec x}} = \\frac{1}{1 + e^{-\\vec w \\cdot \\vec x}}    so the same result as before",
            "title": "Check binary case"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#softmax-approach",
            "text": "An alternative approach (which works for any number of classes) is to consider each class separately with its own parameters set and include the normalization factor ensuring that we get a probability distribution:   $\\begin{eqnarray}\n\\ln P(y = 1) = \\vec w_1 \\cdot \\vec x - \\ln Z & \\Rightarrow & P(y=1) = \\frac{1}{Z}e^{\\vec w_1 \\cdot \\vec x} \\\\\n\\ln P(y = 2) = \\vec w_2 \\cdot \\vec x - \\ln Z & \\Rightarrow & P(y=2) = \\frac{1}{Z}e^{\\vec w_2 \\cdot \\vec x} \\\\\n                                                     & ... & \\\\\n\\ln P(y = K) = \\vec w_K \\cdot \\vec x - \\ln Z & \\Rightarrow & P(y=K) = \\frac{1}{Z}e^{\\vec w_K \\cdot \\vec x} \\\\\n\\end{eqnarray}$     As they have to sum to  1 1 :  \\frac{1}{Z}\\sum\\limits_{i=1}^Ke^{\\vec w_i \\cdot \\vec x} = 1 \\Rightarrow Z = \\sum\\limits_{i=1}^Ke^{\\vec w_i \\cdot \\vec x} \\frac{1}{Z}\\sum\\limits_{i=1}^Ke^{\\vec w_i \\cdot \\vec x} = 1 \\Rightarrow Z = \\sum\\limits_{i=1}^Ke^{\\vec w_i \\cdot \\vec x}    Thus, the probability of  y = j y = j , for  j \\in \\left\\{1, \\cdots, K\\right\\} j \\in \\left\\{1, \\cdots, K\\right\\}  is given by:  P(y=j) = \\frac{e^{\\vec w_j \\cdot \\vec x}}{\\sum\\limits_{i=1}^{K}e^{\\vec w_i \\cdot \\vec x}} P(y=j) = \\frac{e^{\\vec w_j \\cdot \\vec x}}{\\sum\\limits_{i=1}^{K}e^{\\vec w_i \\cdot \\vec x}}    which is called  softmax function",
            "title": "Softmax approach"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#check-binary-case_1",
            "text": "For  y \\in \\left\\{0, 1\\right\\} y \\in \\left\\{0, 1\\right\\}  we have:   $\\begin{eqnarray}\nP(y = 0) & = & \\frac{e^{\\vec w_0 \\cdot \\vec x}}{e^{\\vec w_0 \\cdot \\vec x} + e^{\\vec w_1 \\cdot \\vec x}} \\\\\nP(y = 1) & = & \\frac{e^{\\vec w_1 \\cdot \\vec x}}{e^{\\vec w_0 \\cdot \\vec x} + e^{\\vec w_1 \\cdot \\vec x}}\n\\end{eqnarray}$     Please note, that this model is overspecified!  \\rightarrow \\rightarrow   P(y = 0) + P(y = 1) = 1 P(y = 0) + P(y = 1) = 1  (always)    That means, that once we have one probability the other is given, so we can choose one of  \\vec w_i \\vec w_i  arbitrary - lets choose  \\vec w_0 = 0 \\vec w_0 = 0  (and  \\vec w_1 \\equiv \\vec w \\vec w_1 \\equiv \\vec w ):  P(y = 1) = \\frac{e^{\\vec w \\cdot \\vec x}}{1 + e^{\\vec w\\cdot x}} = \\frac{1}{1 + e^{-\\vec w \\cdot x}} P(y = 1) = \\frac{e^{\\vec w \\cdot \\vec x}}{1 + e^{\\vec w\\cdot x}} = \\frac{1}{1 + e^{-\\vec w \\cdot x}}",
            "title": "Check binary case"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#cost-function_1",
            "text": "Lets recall that in our notation we have:    dataset:  \\left\\{(\\vec x^{(i)}, y^{(i)}); i = 1,\\cdots, N\\right\\} \\left\\{(\\vec x^{(i)}, y^{(i)}); i = 1,\\cdots, N\\right\\}    features:  \\vec x^{(i)} = (1, x^{(i)}_1,\\cdots, x^{(i)}_n) \\vec x^{(i)} = (1, x^{(i)}_1,\\cdots, x^{(i)}_n)    targets:  y^{(i)} \\in \\{1, \\cdots, K\\} y^{(i)} \\in \\{1, \\cdots, K\\}      For every possible outcome we have a corresponding vector of weights  \\vec w_j \\vec w_j  (for  j = 1, \\cdots, K j = 1, \\cdots, K ) - so in fact we have a matrix of parameters ( W W )    The hypothesis is given by a vector:  h(\\vec x) = \\left[\\begin{array}{c}\\frac{e^{\\vec w_1 \\cdot \\vec x}}{\\sum\\limits_{i=1}^{K}e^{\\vec w_i \\cdot \\vec x}} \\\\ \\frac{e^{\\vec w_2 \\cdot \\vec x}}{\\sum\\limits_{i=1}^{K}e^{\\vec w_i \\cdot \\vec x}} \\\\ ... \\\\ \\frac{e^{\\vec w_K \\cdot \\vec x}}{\\sum\\limits_{i=1}^{K}e^{\\vec w_i \\cdot \\vec x}}\\end{array}\\right] h(\\vec x) = \\left[\\begin{array}{c}\\frac{e^{\\vec w_1 \\cdot \\vec x}}{\\sum\\limits_{i=1}^{K}e^{\\vec w_i \\cdot \\vec x}} \\\\ \\frac{e^{\\vec w_2 \\cdot \\vec x}}{\\sum\\limits_{i=1}^{K}e^{\\vec w_i \\cdot \\vec x}} \\\\ ... \\\\ \\frac{e^{\\vec w_K \\cdot \\vec x}}{\\sum\\limits_{i=1}^{K}e^{\\vec w_i \\cdot \\vec x}}\\end{array}\\right]    The prediction for unseen sample is done using  argmax  function    As before we define the cost function as the negative log-likelihood:  L(W) = -\\frac{1}{N}\\sum\\limits_{i=1}^N\\ln\\left[\\frac{e^{\\vec w_{y^{(i)}} \\cdot \\vec x^{(i)}}}{\\sum\\limits_{j=1}^{K}e^{\\vec w_j \\cdot \\vec x^{(i)}}}\\right] = -\\frac{1}{N}\\sum\\limits_{i=1}^N\\left[\\vec w_{y^{(i)}} \\cdot \\vec x^{(i)} - \\ln\\sum\\limits_{j=1}^{K}e^{\\vec w_j \\cdot \\vec x^{(i)}}\\right] L(W) = -\\frac{1}{N}\\sum\\limits_{i=1}^N\\ln\\left[\\frac{e^{\\vec w_{y^{(i)}} \\cdot \\vec x^{(i)}}}{\\sum\\limits_{j=1}^{K}e^{\\vec w_j \\cdot \\vec x^{(i)}}}\\right] = -\\frac{1}{N}\\sum\\limits_{i=1}^N\\left[\\vec w_{y^{(i)}} \\cdot \\vec x^{(i)} - \\ln\\sum\\limits_{j=1}^{K}e^{\\vec w_j \\cdot \\vec x^{(i)}}\\right]",
            "title": "Cost function"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#gradient",
            "text": "We need to calculate the partial derivative for each parameter  w_{ab} w_{ab}      a = 1, \\cdots, K a = 1, \\cdots, K  (possible outcome  y y )    b = 0, \\cdots, n b = 0, \\cdots, n  ( \\vec x \\vec x  coordinate)      For the first term we have:  \\frac{\\partial}{\\partial w_{ab}} \\vec w_{y^{(i)}} \\cdot \\vec x^{(i)} = [y^{(i)} = a]x^{(i)}_b \\frac{\\partial}{\\partial w_{ab}} \\vec w_{y^{(i)}} \\cdot \\vec x^{(i)} = [y^{(i)} = a]x^{(i)}_b    For the second term we have:    $\\begin{eqnarray}\n\\frac{\\partial}{\\partial w_{ab}} \\ln\\sum\\limits_{j=1}^{K}e^{\\vec w_j \\cdot \\vec x^{(i)}} & = & \\frac{\\sum\\limits_{j=1}^{K}e^{\\vec w_j \\cdot \\vec x^{(i)}} \\cdot [y^{(i)} = a] \\cdot x^{(i)}_b}{\\sum\\limits_{j=1}^{K}e^{\\vec w_j \\cdot \\vec x^{(i)}}} \\\\ & = & \\sum\\limits_{j=1}^{K}\\left[\\frac{e^{\\vec w_j \\cdot \\vec x^{(i)}}}{\\sum\\limits_{j=1}^{K}e^{\\vec w_j \\cdot \\vec x^{(i)}}}\\cdot [y^{(i)} = a] \\cdot x^{(i)}_b\\right]\\\\ & = & \\sum\\limits_{j=1}^{K}\\left[P(y = j~|~\\vec x^{(i)})\\cdot [y^{(i)} = a] \\cdot x^{(i)}_b\\right] \\\\ & = & P(y=a~|~\\vec x^{(i)})x^{(i)}_b\n\\end{eqnarray}$     Finally, the gradient of the cost function is given by:  \\frac{\\partial L(W)}{\\partial w_{ab}} = -\\frac{1}{N}\\sum\\limits_{i=1}^N\\left[[y^{(i)}=a]x^{(i)}_b - P(y=a~|~\\vec x^{(i)})x^{(i)}_b\\right] \\frac{\\partial L(W)}{\\partial w_{ab}} = -\\frac{1}{N}\\sum\\limits_{i=1}^N\\left[[y^{(i)}=a]x^{(i)}_b - P(y=a~|~\\vec x^{(i)})x^{(i)}_b\\right]    And for every iteration of the gradient descent method weights are updated according to:  w_{ab} \\rightarrow w_{ab} - \\alpha\\frac{\\partial L(W)}{\\partial w_{ab}} w_{ab} \\rightarrow w_{ab} - \\alpha\\frac{\\partial L(W)}{\\partial w_{ab}}",
            "title": "Gradient"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#example_2",
            "text": "Let a student have two features:    initial knowledge:  x_1 \\in [0, 100] x_1 \\in [0, 100]    hours spent studying:  x_2 \\in [0, 50] x_2 \\in [0, 50]      And, based on these two features, a grade can be assigned to a student:   target:  y \\in \\left\\{2, 3, 4, 5\\right\\} y \\in \\left\\{2, 3, 4, 5\\right\\}",
            "title": "Example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#dataset",
            "text": "def   grade ( init_know ,   study_time ): \n   \"\"\"Arbitrary grading system.\"\"\" \n   score   =   np . random . normal ( init_know   +   2 * study_time ,   5 ) \n\n   if   score   >   90 :   return   3      # bdb \n   elif   score   >   70 :   return   2    # db \n   elif   score   >   50 :   return   1    # dst \n   else :   return   0               # ndst    The training set   N   =   1000    # number of students  X   =   np . random . sample (( N ,   2 ))   *   [ 100 ,   50 ]  Y   =   np . array ([ grade ( * student )   for   student   in   X ],   dtype = 'int32' )   plt . xlabel ( \"Initial knowledge\" )  plt . ylabel ( \"Study time\" )  for   student ,   g   in   zip ( X ,   Y ): \n   plt . scatter ( * student ,   color = 'C' + str ( g ),   marker = '.' )",
            "title": "Dataset"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#data-preparation",
            "text": "For the training process we scale features to  [0, 1] [0, 1]  - otherwise  initial knowledge  would weight more!   X_train   =   np . multiply ( X ,   np . array ([ 1 / 100 ,   1 / 50 ]))    Lets add 1 for bias term to the dataset   X_train   =   np . hstack (( np . ones (( N ,   1 )),   X_train ))    How does it look?   print ( \"Original:\" ,   X [: 5 ],   \"Preprocessed:\" ,   X_train [: 5 ],   sep = \" \\n\\n \" )   Original:\n\n[[ 7.47319311 25.86221876]\n [21.66198394 33.34522121]\n [69.87399886  4.53759853]\n [75.83844581 22.05176574]\n [17.38539272 47.02800299]]\n\nPreprocessed:\n\n[[1.         0.07473193 0.51724438]\n [1.         0.21661984 0.66690442]\n [1.         0.69873999 0.09075197]\n [1.         0.75838446 0.44103531]\n [1.         0.17385393 0.94056006]]",
            "title": "Data preparation"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#training",
            "text": "The implementation of MLR in  theano   import   theano  import   theano.tensor   as   T  x   =   T . matrix ( 'x' )    # feature vectors  y   =   T . ivector ( 'y' )   # target vector  W   =   theano . shared ( np . random . randn ( 3 ,   4 ))    # weight matrix (2 features + bias, \n                                           #                4 possible outcomes)  hypo   =   T . nnet . softmax ( T . dot ( x , W ))                       # hyphothesis  cost   =   - T . mean ( T . log ( hypo )[ T . arange ( y . shape [ 0 ]),   y ])    # cost function  grad_W   =   T . grad ( cost = cost ,   wrt = W )                       # gradients  alpha   =   0.5   # learning rate  # define a training step  train   =   theano . function ( inputs   =   [ x , y ], \n                         outputs   =   cost , \n                         updates   =   [( W ,   W   -   alpha   *   grad_W )] \n                        )  # predict a class label  predict   =   theano . function ( inputs = [ x ],   outputs = T . argmax ( hypo ,   axis = 1 ))    The training process on normalized data   n_epochs   =   10000  acc_train   =   []    # accuracy on training dataset  for   _   in   range ( n_epochs ): \n   # do a single step of gradient descent \n   train ( X_train ,   Y ) \n   # calculate accuracy with current set of weights \n   acc_train . append (( Y   ==   predict ( X_train )) . sum ()   /   Y . shape [ 0 ])   plt . xlabel ( \"Epoch\" )  plt . ylabel ( \"Cost\" )  plt . plot ( range ( len ( acc_train )),   acc_train );",
            "title": "Training"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#validation",
            "text": "First we need  unseen  data for testing   # another set of students  X_test   =   np . random . sample (( N ,   2 ))   *   [ 100 ,   50 ]  Y_test   =   np . array ([ grade ( * student )   for   student   in   X_test ],   dtype = 'int32' )  # normalize and add bias   X_test_normalized   =   np . multiply ( X_test ,   np . array ([ 1 / 100 ,   1 / 50 ]))  X_test_normalized   =   np . hstack (( np . ones (( N ,   1 )),   X_test_normalized ))    To predict a grade we use the function  predict  defined earlier   Y_pred   =   predict ( X_test_normalized )    We can visualize the prediction   plt . xlabel ( \"Initial knowledge\" )  plt . ylabel ( \"Study time\" )  for   student ,   g   in   zip ( X_test ,   Y_pred ): \n   plt . scatter ( * student ,   color = 'C' + str ( g ),   marker = '.' )     and calculate the accuracy   ( Y_test   ==   Y_pred ) . sum ()   /   Y_test . shape [ 0 ]   0.906",
            "title": "Validation"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#softmax-visualization",
            "text": "Lets visualize what the model has just learned    First we need an easy way to calculate softmax output for a student    softmax   =   theano . function ( inputs = [ x ],   outputs = hypo )    Now, we can use it on the validation dataset   probs   =   softmax ( X_test_normalized )    For every sample softmax returns an array of the probabilities of belonging to each class   print ( probs . shape )   (1000, 4)   We can plot each class separately   from   mpl_toolkits.mplot3d   import   Axes3D  grades   =   ( \"ndst\" ,   \"dst\" ,   \"db\" ,   \"bdb\" )  for   i   in   range ( 4 ): \n   fig   =   plt . figure () \n   ax   =   fig . add_subplot ( 111 ,   projection = '3d' ) \n\n   ax . set_xlabel ( \"Initial knowledge\" ,    labelpad = 20 ) \n   ax . set_ylabel ( \"Study time\" ,    labelpad = 20 ) \n\n   ax . set_title ( \"Grade: \"   +   grades [ i ]) \n\n   ax . scatter ( X_test . T [ 0 ],   X_test . T [ 1 ],   probs . T [ i ],   marker = '.' )",
            "title": "Softmax visualization"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#neural-networks",
            "text": "Here are some helpful functions to draw neural networks    Lets just skip them - it is just bunch of matplotlib    # the functions below grabbed from http://www.astroml.org/book_figures/appendix/fig_neural_network.html  radius   =   0.3  arrow_kwargs   =   dict ( head_width = 0.05 ,   fc = 'black' )  def   draw_connecting_arrow ( ax ,   circ1 ,   rad1 ,   circ2 ,   rad2 ): \n     theta   =   np . arctan2 ( circ2 [ 1 ]   -   circ1 [ 1 ], \n                        circ2 [ 0 ]   -   circ1 [ 0 ]) \n\n     starting_point   =   ( circ1 [ 0 ]   +   rad1   *   np . cos ( theta ), \n                       circ1 [ 1 ]   +   rad1   *   np . sin ( theta )) \n\n     length   =   ( circ2 [ 0 ]   -   circ1 [ 0 ]   -   ( rad1   +   1.4   *   rad2 )   *   np . cos ( theta ), \n               circ2 [ 1 ]   -   circ1 [ 1 ]   -   ( rad1   +   1.4   *   rad2 )   *   np . sin ( theta )) \n\n     ax . arrow ( starting_point [ 0 ],   starting_point [ 1 ], \n              length [ 0 ],   length [ 1 ],   ** arrow_kwargs )  def   draw_circle ( ax ,   center ,   radius ): \n     circ   =   plt . Circle ( center ,   radius ,   fill = False ,   lw = 2 ) \n     ax . add_patch ( circ )   # based on borrowed function we can create a new one to draw NN  def   draw_net ( input_size ,   output_size ,   hidden_layers = [],   w = 6 ,   h = 4 ): \n   \"\"\"Draw a network\"\"\" \n   x   =   0    # initial layer position \n\n   ax   =   plt . subplot () \n   ax . set_aspect ( 'equal' ) \n   ax . axis ( 'off' ) \n\n   ax . set_xlim ([ - 2 ,   - 2   +   w ]) \n   ax . set_ylim ([ - h   /   2   ,   h   /   2   +   1 ]) \n\n   # set y position   \n   y_input   =   np . arange ( - ( input_size   -   1 )   /   2 ,   ( input_size   +   1 )   /   2 ,   1 ) \n   y_output   =   np . arange ( - ( output_size   -   1 )   /   2 ,   ( output_size   +   1 )   /   2 ,   1 ) \n   y_hidden   =   [ np . arange ( - ( n   -   1 )   /   2 ,   ( n   +   1 )   /   2 ,   1 )   for   n   in   hidden_layers ] \n\n   # draw input layer \n   plt . text ( x ,   h   /   2   +   0.5 ,   \"Input \\n Layer\" ,   ha = 'center' ,   va = 'top' ,   fontsize = 16 ) \n\n   for   i ,   y   in   enumerate ( y_input ): \n     draw_circle ( ax ,   ( x ,   y ),   radius ) \n     ax . text ( x   -   0.9 ,   y ,   '$x_ %i $'   %   ( input_size   -   1   -   i ), \n             ha = 'right' ,   va = 'center' ,   fontsize = 16 ) \n     draw_connecting_arrow ( ax ,   ( x   -   0.9 ,   y ),   0.1 ,   ( x ,   y ),   radius ) \n\n   last_layer   =   y_input    # last layer y positions \n\n   # draw hidden layers \n   for   ys   in   y_hidden : \n     # shift x \n     x   +=   2 \n     plt . text ( x ,   h   /   2   +   0.5 ,   \"Hidden \\n Layer\" ,   ha = 'center' ,   va = 'top' ,   fontsize = 16 ) \n\n     # draw neurons for each hidden layer \n     for   i ,   y1   in   enumerate ( ys ): \n       draw_circle ( ax ,   ( x ,   y1 ),   radius ) \n\n       # connect a neuron with all neurons from previous layer \n       if   i   !=   len ( ys )   -   1 :   # skip bias \n         for   y2   in   last_layer : \n           draw_connecting_arrow ( ax ,   ( x   -   2 ,   y2 ),   radius ,   ( x ,   y1 ),   radius ) \n\n     # update last layer \n     last_layer   =   ys \n\n   x   +=   2    # update position for output layer \n\n   # draw output layer \n   plt . text ( x ,   h   /   2   +   0.5 ,   \"Output \\n Layer\" ,   ha = 'center' ,   va = 'top' ,   fontsize = 16 ) \n\n   for   i ,   y1   in   enumerate ( y_output ): \n     draw_circle ( ax ,   ( x ,   y1 ),   radius ) \n     ax . text ( x   +   0.8 ,   y1 ,   'Output' ,   ha = 'left' ,   va = 'center' ,   fontsize = 16 ) \n     draw_connecting_arrow ( ax ,   ( x ,   y1 ),   radius ,   ( x   +   0.8 ,   y1 ),   0.1 ) \n\n     # connect each output neuron with all neurons from previous layer \n     for   y2   in   last_layer : \n       draw_connecting_arrow ( ax ,   ( x   -   2 ,   y2 ),   radius ,   ( x ,   y1 ),   radius )",
            "title": "Neural Networks"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#neuron",
            "text": "As mentioned at the beginning, we are going to discuss feedforward neural networks    Lets start with a single neuron    What we were doing so far    We had some training samples with  N N  features    We assumed linear model    We connected features with outcomes using linear, logistic or softmax function      Thus, we considered somthing like this:    draw_net ( 3 ,   1 )      Output circle  represents a neuron    Arrows represent connections (weights)    A neuron is defined by an activation function, e.g.:    binary step    logistic function    hyperbolic tangent    rectified linear unit (ReLU)      import   math  def   binary_step ( x ):   return   0   if   x   <   0   else   1  def   logistic ( x ):   return   1   /   ( 1   +   math . exp ( - x ))  def   tanh ( x ):   return   math . tanh ( x )  def   relu ( x ):   return   0   if   x   <   0   else   x  x   =   np . linspace ( - 5 ,   5 ,   100 )  bs   =   [ binary_step ( x_ )   for   x_   in   x ]  lf   =   [ logistic ( x_ )   for   x_   in   x ]  th   =   [ tanh ( x_ )   for   x_   in   x ]  re   =   [ relu ( x_ )   for   x_   in   x ]  _ ,   (( ax1 ,   ax2 ),   ( ax3 ,   ax4 ))   =   plt . subplots ( 2 ,   2 ,   figsize = ( 10 , 10 ))  ax1 . set_title ( \"Binary step\" )  ax2 . set_title ( \"TanH\" )  ax3 . set_title ( \"Logistic\" )  ax4 . set_title ( \"ReLU\" )  ax1 . plot ( x ,   bs )  ax2 . plot ( x ,   lf )  ax3 . plot ( x ,   th )  ax4 . plot ( x ,   re );",
            "title": "Neuron"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#neural-networks_1",
            "text": "Imagine that the output of a neuron is an input for another neuron    This way we can create another layer of neurons (hidden layer) which would be an input for the output layer    draw_net ( 3 ,   1 ,   [ 5 ],   w = 9 ,   h = 6 )     Or we could get carried away   draw_net ( 3 ,   1 ,   [ 5 ,   7 ,   9 ,   5 ],   w = 14 ,   h = 10 )      This way we can solve non-linear problems    In general, the more the problem is complex the more neurons we need    The numbers of hidden layers and hidden neurons are  hyperparameters  of the model    If the NN is too small - underfitting    It the NN is too large - overfitting      Plase note, that we may have also many possible outcomes through e.g. softmax    draw_net ( 3 ,   4 ,   [ 5 ,   7 ,   9 ,   5 ],   w = 14 ,   h = 10 )     How to train this monster?",
            "title": "Neural networks"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#backpropagation",
            "text": "We can use the gradient descent method, but now we need to propagate the error through all layers:    Input -> forward propagation -> error    Backpropagate the error -> update weights      Lets see how it works on a simple example    draw_net ( 3 ,   2 ,   [ 3 ],   w = 9 ,   h = 4 )      The input is given by:  x_0 = 1 x_0 = 1 ,  x_1 x_1 ,  x_2 x_2    There are 2 hidden neurons + bias:  h_0 = 1 h_0 = 1 ,  h_1 h_1 ,  h_2 h_2    And two possible outcomes:  o_1 o_1 ,  o_2 o_2    The input layer is connected with the hidden layer by weights:  w^{(1)}_{ij} w^{(1)}_{ij} , where  i = 0, 1, 2 i = 0, 1, 2  and  j = 1, 2 j = 1, 2 , e.g.   w^{(1)}_{12} w^{(1)}_{12}  is the weight connecting  x_1 x_1  and  h_2 h_2     The hidden layer is connected with the output layer by weights:  w^{(2)}_{ij} w^{(2)}_{ij}    The input for a neuron  h_k h_k  is given by:  h_{k, in} = w^{(1)}_{0k} + w^{(1)}_{1k} \\cdot x_1 + w^{(1)}_{2k} \\cdot x_2 h_{k, in} = w^{(1)}_{0k} + w^{(1)}_{1k} \\cdot x_1 + w^{(1)}_{2k} \\cdot x_2    And the output:  h_{k, out} = \\left(1 + \\exp(-h_{k, in})\\right)^{-1} h_{k, out} = \\left(1 + \\exp(-h_{k, in})\\right)^{-1}    The input for the outcome  o_k o_k  is given by:  o_{k, in} = w^{(2)}_{0k} + w^{(2)}_{1k} \\cdot h_{1, out} + w^{(2)}_{2k} \\cdot h_{2, out} o_{k, in} = w^{(2)}_{0k} + w^{(2)}_{1k} \\cdot h_{1, out} + w^{(2)}_{2k} \\cdot h_{2, out}    And the output:  o_{k, out} = \\left(1 + \\exp(-o_{k, in})\\right)^{-1} o_{k, out} = \\left(1 + \\exp(-o_{k, in})\\right)^{-1}    Lets define the cost function:  L(w) = \\frac{1}{2}\\left(o_{1, true} - o_{1, out}\\right)^2 + \\frac{1}{2}\\left(o_{2, true} - o_{2, out}\\right)^2 L(w) = \\frac{1}{2}\\left(o_{1, true} - o_{1, out}\\right)^2 + \\frac{1}{2}\\left(o_{2, true} - o_{2, out}\\right)^2    To update weights using the gradient descent method we need to calculate  \\partial L(w) / \\partial w^{(a)}_{ij} \\partial L(w) / \\partial w^{(a)}_{ij}    As an example, let consider updating  w^{(2)}_{11} w^{(2)}_{11} :  \\frac{\\partial L(w)}{\\partial w^{(2)}_{11}} = \\frac{\\partial L(w)}{\\partial o_{1, out}}\\cdot\\frac{\\partial o_{1, out}}{\\partial o_{1, in}}\\cdot\\frac{\\partial o_{1, in}}{\\partial w^{(2)}_{11}} \\frac{\\partial L(w)}{\\partial w^{(2)}_{11}} = \\frac{\\partial L(w)}{\\partial o_{1, out}}\\cdot\\frac{\\partial o_{1, out}}{\\partial o_{1, in}}\\cdot\\frac{\\partial o_{1, in}}{\\partial w^{(2)}_{11}}",
            "title": "Backpropagation"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#and-or-vs-xor",
            "text": "I will assume that basic logic gates do not need introduction    The point here is that AND and OR are linearly separable, and XOR is not    X   =   [[ 0 , 0 ],   [ 0 , 1 ],   [ 1 , 0 ],   [ 1 , 1 ]]  Y_and   =   [ 0 ,   0 ,   0 ,   1 ]  Y_or   =   [ 0 ,   1 ,   1 ,   1 ]  Y_xor   =   [ 0 ,   1 ,   1 ,   0 ]  titles   =   ( \"AND\" ,   \"OR\" ,   \"XOR\" )  for   i ,   Y   in   enumerate ([ Y_and ,   Y_or ,   Y_xor ]): \n   ax   =   plt . subplot ( 131   +   i ) \n\n   ax . set_xlim ([ - 0.5 ,   1.5 ]) \n   ax . set_ylim ([ - 0.5 ,   1.5 ]) \n\n   ax . set_aspect ( 'equal' ) \n\n   plt . title ( titles [ i ]) \n   plt . scatter ( * zip ( * X ),   c = Y ) \n\n   if   i   ==   0 : \n     plt . plot ([ 0 ,   1.5 ],   [ 1.5 ,   0 ]) \n   elif   i   ==   1 : \n     plt . plot ([ - 0.5 ,   1 ],   [ 1 ,   - 0.5 ]) \n   else : \n     plt . text ( 0.5 ,   0.5 ,   s = \"?\" ,   fontsize = 16 ,   ha = 'center' ,   va = 'center' )  plt . tight_layout ()",
            "title": "AND, OR vs XOR"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#single-neuron-approach",
            "text": "draw_net ( 3 ,   1 )      Let logistic function be our hypothesis:  h(x_1, x_2) = \\left(1 + \\exp(-w_0 - w_1 \\cdot x_1 - w_2 \\cdot x_2)\\right)^{-1} h(x_1, x_2) = \\left(1 + \\exp(-w_0 - w_1 \\cdot x_1 - w_2 \\cdot x_2)\\right)^{-1}    For AND gate we want  h(0, 0) = h(0, 1) = h(1, 0) = 0 h(0, 0) = h(0, 1) = h(1, 0) = 0  and  h(1, 1) = 1 h(1, 1) = 1    The example is so simple, that we could guess weights:    w_0 << 0 w_0 << 0    w_0 + w_1 << 0 w_0 + w_1 << 0    w_0 + w_2 << 0 w_0 + w_2 << 0    w_0 + w_1 + w_2 >> 0 w_0 + w_1 + w_2 >> 0      But lets build a neuron    import   theano  import   theano.tensor   as   T  x   =   T . matrix ( 'x' )   # feature vector  y   =   T . vector ( 'y' )   # target vector  w   =   theano . shared ( np . random . randn ( 2 ),   name   =   'w' )   # weights initialized randomly  b   =   theano . shared ( np . random . randn (),   name   =   'b' )    # bias term  hypo   =   1   /   ( 1   +   T . exp ( - T . dot ( x ,   w )   -   b ))                # hyphothesis  xent   =   -   y   *   T . log ( hypo )   -   ( 1   -   y )   *   T . log ( 1   -   hypo )    # cross-entropy loss function  cost   =   xent . sum ()                                       # cost function  grad_w ,   grad_b   =   T . grad ( cost ,   [ w ,   b ])                   # gradients  alpha   =   0.1   # learning rate  # at each training step we update weights:  # w -> w - alpha * grad_w and b -> b - alpha * grad_b  train   =   theano . function ( inputs   =   [ x , y ], \n                         outputs   =   cost , \n                         updates   =   (( w ,   w   -   alpha   *   grad_w ), \n                                    ( b ,   b   -   alpha   *   grad_b )))  predict   =   theano . function ( inputs = [ x ],   outputs = hypo )    Train for all gates and save prediction   N   =   1000  gates   =   ( \"AND\" ,   \"OR\" ,   \"XOR\" )  gates_pred   =   {}  for   gate ,   data   in   zip ( gates ,   ( Y_and ,   Y_or ,   Y_xor )): \n   # reset weights \n   w . set_value ( np . random . randn ( 2 )) \n   b . set_value ( np . random . randn ()) \n\n   # train neuron \n   [ train ( X ,   data )   for   _   in   range ( N )] \n   gates_pred [ gate ]   =   predict ( X )    Lets see the result   for   gate   in   gates : \n   for   i ,   ( x1 ,   x2 )   in   enumerate ( X ): \n     print ( \"{} {} {} -> {}\" . format ( x1 ,   gate ,   x2 ,   gates_pred [ gate ][ i ])) \n   print ()   0 AND 0 -> 0.00018774340087294284\n0 AND 1 -> 0.04835388458963121\n1 AND 0 -> 0.04834664667043076\n1 AND 1 -> 0.9321880416392287\n\n0 OR 0 -> 0.05096342740264947\n0 OR 1 -> 0.9795405137359009\n1 OR 0 -> 0.9798688809365104\n1 OR 1 -> 0.9999769570570585\n\n0 XOR 0 -> 0.49999999640124027\n0 XOR 1 -> 0.49999999943423534\n1 XOR 0 -> 0.4999999994362604\n1 XOR 1 -> 0.5000000024692554   As one could / should expect a linear classifier can not work for non-linear problems (like XOR gate)",
            "title": "Single neuron approach"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#neural-network-approach",
            "text": "Please note, that XOR is not linear, but it can be expressed in terms or linear problems combination   x XOR y = (x AND NOT y) OR (y AND NOT x)   Lets consider NN with two hidden neurons   draw_net ( 3 ,   1 ,   [ 3 ],   w = 8 )     Lets use  theano  last time to build the network   import   theano  import   theano.tensor   as   T  x   =   T . matrix ( 'x' )   # feature vector  y   =   T . vector ( 'y' )   # target vector  # first layer's weights (including bias)  w1   =   theano . shared ( np . random . randn ( 3 , 2 ),   name   =   'w1' )  # second layer's weights (including bias)  w2   =   theano . shared ( np . random . randn ( 3 ),   name   =   'w2' )  h   =   T . nnet . sigmoid ( T . dot ( x ,   w1 [: 2 ,])   +   w1 [ 2 ,])    # hidden layer  o   =   T . nnet . sigmoid ( T . dot ( h ,   w2 [: 2 ,])   +   w2 [ 2 ,])    # output layer  xent   =   -   y   *   T . log ( o )   -   ( 1   -   y )   *   T . log ( 1   -   o )    # cross-entropy loss function  cost   =   xent . sum ()                                 # cost function  grad_w1 ,   grad_w2   =   T . grad ( cost ,   [ w1 ,   w2 ])         # gradients  alpha   =   0.1   # learning rate  # at each training step we update weights:  # w -> w - alpha * grad_w and b -> b - alpha * grad_b  train   =   theano . function ( inputs   =   [ x , y ], \n                         outputs   =   cost , \n                         updates   =   (( w1 ,   w1   -   alpha   *   grad_w1 ), \n                                    ( w2 ,   w2   -   alpha   *   grad_w2 )))  predict   =   theano . function ( inputs = [ x ],   outputs = o )    Train on XOR and print prediction   [ train ( X ,   Y_xor )   for   _   in   range ( 10000 )]  prediction   =   predict ( X )  for   i ,   ( x1 ,   x2 )   in   enumerate ( X ): \n   print ( \"{} XOR {} -> {}\" . format ( x1 ,   x2 ,   prediction [ i ]))   0 XOR 0 -> 0.005114740815278212\n0 XOR 1 -> 0.9932581408757912\n1 XOR 0 -> 0.9931902289336407\n1 XOR 1 -> 0.004574250556087016",
            "title": "Neural network approach"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#again-the-same-but-with-tensorflow",
            "text": "Lets try to solve XOR gate using  tensorflow    In comments there is  theano  code    import   tensorflow   as   tf  # x = T.matrix('x') # feature vector  # y = T.vector('y') # target vector  x   =   tf . placeholder ( tf . float32 ,   [ 4 ,   2 ])  y   =   tf . placeholder ( tf . float32 ,   [ 4 ,   1 ])  # w1 = theano.shared(np.random.randn(3,2), name = 'w1')  # w2 = theano.shared(np.random.randn(3), name = 'w2')  w1   =   tf . Variable ( tf . random_normal ([ 3 ,   2 ]),   name = 'w1' )  w2   =   tf . Variable ( tf . random_normal ([ 3 ,   1 ]),   name = 'w2' )  # h = T.nnet.sigmoid(T.dot(x, w1[:2,]) + w1[2,])  # o = T.nnet.sigmoid(T.dot(h, w2[:2,]) + w2[2,])  h   =   tf . sigmoid ( tf . add ( tf . matmul ( x ,   w1 [: 2 ,]),   w1 [ 2 ,]))  o   =   tf . sigmoid ( tf . add ( tf . matmul ( h ,   w2 [: 2 ,]),   w2 [ 2 ,]))  #xent = - y * tf.log(o) - (1 - y) * tf.log(1 - o)  xent   =   tf . losses . log_loss ( y ,   o )  cost   =   tf . reduce_mean ( xent )  opt   =   tf . train . GradientDescentOptimizer ( 0.1 ) . minimize ( cost )  init   =   tf . global_variables_initializer ()   X   =   [[ 0 , 0 ],   [ 1 , 0 ],   [ 0 , 1 ],   [ 1 , 1 ]]  Y_xor   =   [[ 0 ],   [ 1 ],   [ 1 ],   [ 0 ]]  with   tf . Session ()   as   sess : \n   sess . run ( init ) \n\n   [ sess . run ( opt ,   feed_dict = { x :   X ,   y :   Y_xor })   for   _   in   range ( 10000 )] \n\n   print ( sess . run ( o ,   feed_dict = { x :   X }))   [[0.01071231]\n [0.98791456]\n [0.98790956]\n [0.01878399]]",
            "title": "Again the same, but with tensorflow"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#simple-regression-with-nn",
            "text": "Lets consider a dataset generated from  noised  sinus distribution   from   math   import   sin ,   cos ,   pi ,   exp  def   get_dataset ( N = 20 ,   sigma = 0.1 ): \n   \"\"\"Generate N training samples\"\"\" \n   # X is a set of random points from [-1, 1] \n   X   =   2   *   np . random . sample ( N )   -   1 \n   # Y are corresponding target values (with noise included) \n   Y   =   np . array ([ sin ( pi * x )   +   np . random . normal ( 0 ,   sigma )   for   x   in   X ]) \n\n   return   X ,   Y  # plot a sample  X ,   Y   =   get_dataset ( 100 ,   0.25 )  x_   =   np . arange ( - 1 ,   1 ,   0.01 )  plt . scatter ( X ,   Y ,   color = 'C1' )  plt . plot ( x_ ,   np . sin ( np . pi   *   x_ ),   'C0--' );      We represent hidden neurons via logistic function (3 should be enough)    The output is just a sum of three hidden neurons outputs and bias term    draw_net ( 2 ,   1 ,   [ 4 ],   w = 10 )     Lets implement above network in  tensorflow   import   tensorflow   as   tf  x   =   tf . placeholder ( tf . float32 ,   [ None ,   1 ])  y   =   tf . placeholder ( tf . float32 ,   [ None ,   1 ])  w1   =   tf . Variable ( tf . random_normal ([ 1 ,   3 ]),   name = 'w1' )  w2   =   tf . Variable ( tf . random_normal ([ 3 ,   1 ]),   name = 'w2' )  b1   =   tf . Variable ( tf . random_normal ([ 3 ]),   name = 'b1' )  b2   =   tf . Variable ( tf . random_normal ([ 1 ]),   name = 'b2' )  h   =   tf . nn . sigmoid ( tf . add ( tf . matmul ( x ,   w1 ),   b1 ))  o   =   tf . add ( tf . matmul ( h ,   w2 ),   b2 )  xent   =   tf . losses . mean_squared_error ( y ,   o )  cost   =   tf . reduce_mean ( xent )  opt   =   tf . train . GradientDescentOptimizer ( 0.25 ) . minimize ( cost )  init   =   tf . global_variables_initializer ()    We need to reshape out training data   X_train   =   np . reshape ( X ,   ( - 1 , 1 ))  Y_train   =   np . reshape ( Y ,   ( - 1 , 1 ))  print ( \"Original\" ,   X [: 5 ],   Y [: 5 ],   sep = ' \\n\\n ' )  print ( \" \\n Reshaped\" ,   X_train [: 5 ],   Y_train [: 5 ],   sep = ' \\n\\n ' )   Original\n\n[ 0.52226483  0.80798072  0.15974077 -0.50709742  0.78635136]\n\n[ 0.97538575  0.83448142  0.4117488  -0.89203221  0.38857674]\n\nReshaped\n\n[[ 0.52226483]\n [ 0.80798072]\n [ 0.15974077]\n [-0.50709742]\n [ 0.78635136]]\n\n[[ 0.97538575]\n [ 0.83448142]\n [ 0.4117488 ]\n [-0.89203221]\n [ 0.38857674]]   And we can train the model   X_test   =   np . arange ( - 1 ,   1 ,   0.01 ) . reshape ( - 1 , 1 )  with   tf . Session ()   as   sess : \n   sess . run ( init ) \n\n   [ sess . run ( opt ,   feed_dict = { x :   X_train ,   y :   Y_train })   for   _   in   range ( 10000 )] \n\n   prediction   =   sess . run ( o ,   feed_dict = { x :   X_test })   plt . scatter ( X_test ,   prediction ,   color = 'C2' ,   label = 'NN' )  plt . scatter ( X ,   Y ,   color = 'C1' ,   label = 'Data' )  plt . plot ( x_ ,   np . sin ( np . pi   *   x_ ),   'C0--' ,   label = 'Truth' )  plt . legend ();",
            "title": "Simple regression with NN"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#more-examples",
            "text": "The lecturer was too lazy to prepare more examples, but we can play in  tensorflow playground , which has awesome visualizations",
            "title": "More examples"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#mnist",
            "text": "THE MNIST DATABASE of handwritten digits    The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.  It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.    We can grab MNIST dataset using  tensorflow.examples.tutorials.mnist   import   tensorflow   as   tf  from   tensorflow.examples.tutorials.mnist   import   input_data  # to avoid warnings printed in the notebook  tf . logging . set_verbosity ( tf . logging . ERROR )  # one hot -> label 0-9 -> 0...01, 0...10, ...  mnist   =   input_data . read_data_sets ( \"/tmp/\" ,   one_hot = True )   Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\nExtracting /tmp/train-images-idx3-ubyte.gz\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\nExtracting /tmp/train-labels-idx1-ubyte.gz\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\nExtracting /tmp/t10k-images-idx3-ubyte.gz\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\nExtracting /tmp/t10k-labels-idx1-ubyte.gz   Lets see how data looks like   print ( mnist . train . images . shape )   (55000, 784)  for   i   in   range ( 4 ): \n   plt . subplot ( 221   +   i ) \n\n   # random training sample \n   index   =   np . random . randint ( len ( mnist . train . images )) \n\n   # train.images contains images in a form of a vector \n   # so we reshape it back to 28x28 \n   plt . imshow ( mnist . train . images [ index ] . reshape ( 28 ,   28 ),   cmap = 'gray' ) \n\n   # train.labels contains labels in one hot format \n   plt . title ( mnist . train . labels [ index ])  plt . tight_layout ();      Today we solve this classification problem with  softmax    During next lecture we apply convolutional NN on MNIST dataset    Lets start building the model    x   =   tf . placeholder ( tf . float32 ,   [ None ,   784 ])    # img -> 28x28 -> 784  y   =   tf . placeholder ( tf . float32 ,   [ None ,   10 ])     # 10 classes  W   =   tf . Variable ( tf . zeros ([ 784 ,   10 ]))    # weights  b   =   tf . Variable ( tf . zeros ([ 10 ]))         # bias  out   =   tf . nn . softmax ( tf . matmul ( x ,   W )   +   b )    Define the loss function and optimizer   cross_entropy   =   tf . reduce_mean ( \n     tf . nn . softmax_cross_entropy_with_logits ( labels = y ,   logits = out )  )  train_step   =   tf . train . GradientDescentOptimizer ( 0.5 ) . minimize ( cross_entropy )    Train the model   # create a session  sess   =    tf . Session ()  # initialize weights  sess . run ( tf . global_variables_initializer ())  for   _   in   range ( 10000 ): \n   # here instead of updating weights after the whole training set \n   # we use batch size 100 (more about that in the next section) \n   batch_xs ,   batch_ys   =   mnist . train . next_batch ( 100 ) \n\n   # train_step is minimizing cross_entropy with learning rate 0.5 using GD \n   # we pass small batches to placeholders x and y \n   sess . run ( train_step ,   feed_dict = { x :   batch_xs ,   y :   batch_ys })    Validate the model   # argmax returns the index of the heighest index in a tensor  # equal returns True / False if prediction is equal/not equal to true label  # cast would convert True/False to 1/0, so we can calculate the average  correct_prediction   =   tf . equal ( tf . argmax ( out ,   1 ),   tf . argmax ( y ,   1 ))  accuracy   =   tf . reduce_mean ( tf . cast ( correct_prediction ,   tf . float32 ))  print ( sess . run ( accuracy ,   feed_dict = { x :   mnist . test . images ,   y :   mnist . test . labels }))  sess . close ()   0.9241    It is pretty crazy what you can do with modern ML/DL frameworks    We have just learned our computer to recognize handwritten digits with a few lines of code    During next lecture we will improve the accuracy using deep neural networks",
            "title": "MNIST"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#gradient-descent-variations",
            "text": "In the  gradient descent  (GD) method weights are updated after a full loop over training data:  W \\rightarrow W - \\alpha\\cdot\\nabla_W L(W) W \\rightarrow W - \\alpha\\cdot\\nabla_W L(W)    In the  stochastic gradient descent  (SGD) method weights are updated for each training sample:  W \\rightarrow W - \\alpha\\cdot\\nabla_W L(W; x^{(i)}; y^{(i)}) W \\rightarrow W - \\alpha\\cdot\\nabla_W L(W; x^{(i)}; y^{(i)})    Note, that SGD is also called  online learning    For the large dataset it is likely that GD would recompute gradients for similar examples before an update    SGD perform frequent updates but with a high variance, so objective function fluctuates    It may help to escape local minima    The common method, which is somehow between GD and SGD, is  mini-batch gradient descent  (MBGD) - the one we used for MNIST example:  W \\rightarrow W - \\alpha\\cdot\\nabla_W L(W; x^{(i; i+n)}; y^{(i; i+n)}) W \\rightarrow W - \\alpha\\cdot\\nabla_W L(W; x^{(i; i+n)}; y^{(i; i+n)})",
            "title": "Gradient descent variations"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#sgd-on-mnist",
            "text": "Lets see how the loss function looks like when applying SGD for training the network on MNIST data   # create a session  sess   =    tf . Session ()  # initialize weights  sess . run ( tf . global_variables_initializer ())  test_loss   =   []    # placeholder for loss value per iteration  for   _   in   range ( 10000 ): \n   # SGD -> batch size = 1 \n   batch_xs ,   batch_ys   =   mnist . train . next_batch ( 1 )   \n   # update weights \n   sess . run ( train_step ,   feed_dict = { x :   batch_xs ,   y :   batch_ys }) \n   # calculate loss funtion on test samples \n   loss   =   sess . run ( cross_entropy , \n                   feed_dict = { x :   mnist . test . images ,   y :   mnist . test . labels }) \n   # save it   \n   test_loss . append ( loss )   plt . plot ( np . arange ( 0 ,   10000 ,   1 ),   test_loss );",
            "title": "SGD on MNIST"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#momentum",
            "text": "Online learning may help to escape local minima   x   =   np . linspace ( - 2 ,   2 ,   100 )  y1   =   x ** 2  y2   =   np . array ([ a ** 2   +   np . sin ( 5 * a )   for   a   in   x ])  plt . subplot ( 121 )  plt . plot ( x ,   y1 )  plt . scatter ([ - 1.5 ],   [ 3 ],   c = 'k' ,   s = 300 )  plt . subplot ( 122 )  plt . plot ( x ,   y2 )  plt . scatter ([ - 1.45 ],   [ 1.75 ],   c = 'k' ,   s = 300 );     Another technique that can help escape local minima is to use a momentum term   $\\begin{eqnarray}\nv_t & = & \\gamma\\cdot v_{t-1} + \\alpha\\cdot\\nabla_W L(W; x^{(i)}; y^{(i)}) \\\\\nW &\\rightarrow& W - v_t\n\\end{eqnarray}$     It is like pushing a ball down a hill and ball accumulates momentum    \\gamma \\gamma  is another hyperparameter, usually set around  0.9 0.9    One may want a smarter ball, which can predict its future position    The future position is approximate by  W - \\gamma\\cdot v_{t-1} W - \\gamma\\cdot v_{t-1}    Nesterov accelerated gradient  calculates gradient  not  w.r.t current weights, but w.r.t approximated future  values:    $\\begin{eqnarray}\nv_t & = & \\gamma\\cdot v_{t-1} + \\alpha\\cdot\\nabla_W L(W - \\gamma\\cdot v_{t-1} ; x^{(i)}; y^{(i)}) \\\\\nW &\\rightarrow& W - v_t\n\\end{eqnarray}$",
            "title": "Momentum"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#adaptive-models",
            "text": "A drawback of  regular  gradient descent methods is a constant learning rate, which needs to be tuned by hand    if too small the training process is long    if too large the minimum may be skipped      There are several algorithms which adapt the learning rate during training    Adagrad, Adadelta, Adam are presented here, but please note there are more available",
            "title": "Adaptive models"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#adagrad",
            "text": "Let  g_{ti} g_{ti}  be the gradient of the objective function w.r.t the weight  W_i W_i  at time step  t t :  g_{ti} = \\nabla_WL(W_{t,i}) g_{ti} = \\nabla_WL(W_{t,i})    In this notation, SGD step for a parameter  i i  can be written as:   W_{t+1, i} = W_{t,i} - \\alpha g_{ti} W_{t+1, i} = W_{t,i} - \\alpha g_{ti}    Adagrad modifies the learning rate  \\alpha \\alpha  based on previous gradients:  W_{t+1, i} = W_{t,i} - \\frac{\\alpha}{\\sqrt{G_{t,ii} + \\varepsilon}} g_{ti} W_{t+1, i} = W_{t,i} - \\frac{\\alpha}{\\sqrt{G_{t,ii} + \\varepsilon}} g_{ti}    Where  G_{t} G_{t}  is a diagonal matrix, where  i, i i, i  elements are equal to the sum of all gradients w.r.t  W_i W_i  up to step  t t    \\varepsilon \\varepsilon  is just to ensure denominator not equal zero    Since  G_t G_t  is diagonal, we can write the general formula using element-wise product:  W_{t+1} = W_{t}- \\frac{\\alpha}{\\sqrt{G_{t} + \\varepsilon}} \\odot g_{t} W_{t+1} = W_{t}- \\frac{\\alpha}{\\sqrt{G_{t} + \\varepsilon}} \\odot g_{t}    Note, that each weight has now its own training rate; they just share initial value  \\alpha \\alpha    There is no need to tune  \\alpha \\alpha  anymore; usually one set it up to be around  0.01 0.01  and let Adagrad do the job    The disadvantage of Adagrad is that it accumulates squares of gradients in the denominator - at some point training rate is so small that the model is unable to learn anythng new",
            "title": "Adagrad"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#adadelta",
            "text": "Adadelta is an extension of Adagrad    Its goal is to reduce the speed of training rate decreasing    The idea is to accumulate just a few last gradients instead all of them    Since it is highly inefficient, Adagrad implements this as an exponentially decaying average of all the squared gradients    Let  E[g^2]_t E[g^2]_t  be the average of the squared greadients up to step  t t    Let  \\gamma \\gamma  be decay constant (similar to momentum), then:  E[g^2]_t = \\gamma\\cdot E[g^2]_{t-1} + (1 - \\gamma)\\cdot g_t^2 E[g^2]_t = \\gamma\\cdot E[g^2]_{t-1} + (1 - \\gamma)\\cdot g_t^2    And the training step is given by:  W_{t+1} = W_{t}- \\frac{\\alpha}{\\sqrt{E[g^2]_t + \\varepsilon}}  g_{t} W_{t+1} = W_{t}- \\frac{\\alpha}{\\sqrt{E[g^2]_t + \\varepsilon}}  g_{t}    Please note, that the denominator is just root mean squared (RMS) of the gradient:   W_{t+1} = W_{t}- \\frac{\\alpha}{RMS[g]_t}  g_{t} W_{t+1} = W_{t}- \\frac{\\alpha}{RMS[g]_t}  g_{t}    The author also note that in GD-like methods the update has different hypothetical units than weights itself:  units~of~\\Delta w \\sim units~of~g \\sim units~of~\\frac{\\partial L}{\\partial w} \\sim \\frac{1}{units~of~w} units~of~\\Delta w \\sim units~of~g \\sim units~of~\\frac{\\partial L}{\\partial w} \\sim \\frac{1}{units~of~w}    Inspired by second order optimization method that using Hessian information (like Newton's method):  units~of~\\Delta w \\sim units~of~H^{-1}g \\sim units~of~\\frac{\\frac{\\partial L}{\\partial w}}{\\frac{\\partial^2L}{\\partial w^2}} \\sim units~of~w units~of~\\Delta w \\sim units~of~H^{-1}g \\sim units~of~\\frac{\\frac{\\partial L}{\\partial w}}{\\frac{\\partial^2L}{\\partial w^2}} \\sim units~of~w    the nominator is replaced by RMS of of weights updates (assuming diagonal Hessian):  \\Delta w = \\frac{\\frac{\\partial L}{\\partial w}}{\\frac{\\partial^2 L}{\\partial w^2}} \\Rightarrow \\frac{1}{\\frac{\\partial^2 L}{\\partial w^2}} = \\frac{\\Delta w}{\\frac{\\partial L}{\\partial w}} \\Delta w = \\frac{\\frac{\\partial L}{\\partial w}}{\\frac{\\partial^2 L}{\\partial w^2}} \\Rightarrow \\frac{1}{\\frac{\\partial^2 L}{\\partial w^2}} = \\frac{\\Delta w}{\\frac{\\partial L}{\\partial w}}    Since current update is not known until step is done, the following approximation is used:  \\Delta W_t = - \\frac{RMS[\\Delta W]_{t-1}}{RMS[g]_t}  g_{t} \\Delta W_t = - \\frac{RMS[\\Delta W]_{t-1}}{RMS[g]_t}  g_{t}    Please note, that Adagrad does not require initial training rate!    Although, it is still not parameter-free model as one need to set up  \\gamma \\gamma  and  \\varepsilon \\varepsilon",
            "title": "Adadelta"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#adam",
            "text": "Similar to Adedelta Adam uses exponentially decaying average of past sqaured gradients  v_t v_t  (notation from the original paper):  v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2)\\cdot g_t^2 v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2)\\cdot g_t^2    And similar to SGD with momentum Adam also keeps information about past gradients:  m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1)\\cdot g_t m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1)\\cdot g_t    m_t m_t  and  v_t v_t  are initialized with zeros    For  \\beta_{1,2}\\sim 1 \\beta_{1,2}\\sim 1  training is slow at the beginning    Thus, bias-corrected first and second moments are introduced:    $\\begin{eqnarray}\n\\hat m_t & = & \\frac{m_t}{1 - \\beta_1^t} \\\\\n\\hat v_t & = & \\frac{v_t}{1 - \\beta_2^t}\n\\end{eqnarray}$     And the update rule is given by:  \\Delta W_{t+1} = W_t - \\frac{\\alpha}{\\sqrt{\\hat v_t} + \\varepsilon}\\hat m_t \\Delta W_{t+1} = W_t - \\frac{\\alpha}{\\sqrt{\\hat v_t} + \\varepsilon}\\hat m_t    The authors suggest  \\beta_1 \\approx 0.9 \\beta_1 \\approx 0.9 ,  \\beta_2 \\approx 0.999 \\beta_2 \\approx 0.999 , and  \\varepsilon\\approx 10^{-8} \\varepsilon\\approx 10^{-8}",
            "title": "Adam"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#regularization",
            "text": "As mentioned during last lecture, regularization is any modification to learning algorithm made to prevent overfitting    The common method is to add  regularization term  ( regularizer ) to a loss function  L(W) L(W) :  L(W) \\rightarrow L(W) + \\alpha R(W) L(W) \\rightarrow L(W) + \\alpha R(W)    Two common regularizer are  L1 L1  or  L2 L2  norms    Ridge  regression uses  L2 L2  regularization:  R(W) = \\sum\\limits_i W_i^2 R(W) = \\sum\\limits_i W_i^2    Lasso  regression uses  L1 L1  regularization:  R(W) = \\sum\\limits_i |W_i| R(W) = \\sum\\limits_i |W_i|    Why to penalize the magnitude of weights?    Lets consider a simple example",
            "title": "Regularization"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#to-regularize-or-not-to-regularize",
            "text": "Lets consider again  sinus  dataset   from   math   import   sin ,   cos ,   pi ,   exp  def   get_dataset ( N = 20 ,   sigma = 0.1 ): \n   \"\"\"Generate N training samples\"\"\" \n   # X is a set of random points from [-1, 1] \n   X   =   2   *   np . random . sample ( N )   -   1 \n   # Y are corresponding target values (with noise included) \n   Y   =   np . array ([ sin ( pi * x )   +   np . random . normal ( 0 ,   sigma )   for   x   in   X ]) \n\n   return   X ,   Y  # plot a sample  X ,   Y   =   get_dataset ( 50 )  x_   =   np . arange ( - 1 ,   1 ,   0.01 )  plt . scatter ( X ,   Y ,   color = 'C1' )  plt . plot ( x_ ,   np . sin ( np . pi   *   x_ ),   'C0--' );     Lets fit data to polynomial of order 20   N   =   20    # polynomial order  # add powers of x   X_train   =   [[ x ** i   for   i   in   range ( 1 ,   N )]   for   x   in   X ]  from   sklearn.linear_model   import   LinearRegression  reg   =   LinearRegression ()  reg . fit ( X_train ,   Y );    And plot prediction together with training data   X_test   =   np . linspace ( - 1 ,   1 ,   100 )  Y_test   =   reg . predict ([[ x ** i   for   i   in   range ( 1 , N )]   for   x   in   X_test ])  plt . ylim ([ - 1.5 ,   1.5 ])  plt . scatter ( X ,   Y ,   color = 'C1' )  plt . plot ( X_test ,   Y_test ,   'C0' );      It is clearly overfitted    Lets do the same using Ridge regression    from   sklearn.linear_model   import   Ridge  reg_l2   =   Ridge ( alpha = 0.1 )  reg_l2 . fit ( X_train ,   Y )  Y_test   =   reg_l2 . predict ([[ x ** i   for   i   in   range ( 1 , N )]   for   x   in   X_test ])  plt . ylim ([ - 1.5 ,   1.5 ])  plt . scatter ( X ,   Y ,   color = 'C1' )  plt . plot ( X_test ,   Y_test ,   'C0' );      Play on your own with  \\lambda \\lambda  to see how it affects the result    Now, take a look at coefficients without regularizer    reg . coef_   array([ 2.60931184e+00, -1.34651207e+00,  2.04999890e+01,  9.00515668e+00,\n       -4.18472051e+02, -1.27117676e+02,  3.14310368e+03,  1.10582631e+03,\n       -1.24897897e+04, -4.36716920e+03,  2.86984815e+04,  8.89018057e+03,\n       -3.94741356e+04, -9.78295086e+03,  3.21343881e+04,  5.54204833e+03,\n       -1.43111414e+04, -1.26843444e+03,  2.69454755e+03])    Note, that coefficients increase drastically for large powers of  x x    High weights mean that a model put a lot of emphasis on a given features, which leads to overfitting    That is why we need to put some constraints on the magnitude of weights    Now, lets see how coefficients look like with L2 regularizer    reg_l2 . coef_   array([ 2.3350277 , -0.00324186, -1.57606849,  0.04982775, -1.0117878 ,\n       -0.025911  , -0.4412411 , -0.04283921, -0.10340078, -0.03155621,\n        0.07582327, -0.01138888,  0.16245585,  0.01121341,  0.19718257,\n        0.03442763,  0.20359299,  0.05769045,  0.19518384])    Ridge regression causes coefficient shrinkage and reduces model complexity    Lets repeat the same for Lasso regression    from   sklearn.linear_model   import   Lasso  reg_l1   =   Lasso ( alpha = 0.001 )  reg_l1 . fit ( X_train ,   Y )  Y_test   =   reg_l1 . predict ([[ x ** i   for   i   in   range ( 1 , N )]   for   x   in   X_test ])  plt . ylim ([ - 1.5 ,   1.5 ])  plt . scatter ( X ,   Y ,   color = 'C1' )  plt . plot ( X_test ,   Y_test ,   'C0' );      Once again we got a nice fit    But there is a difference - see the coefficients    reg_l1 . coef_   array([ 2.779238  , -0.        , -3.20859903, -0.        , -0.        ,\n       -0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.1980417 ,  0.        ,  0.22056433,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ])    Note, that in opposite to Ridge not all features are taken into account as some weights ended up being zero    Thus, Lasso regression additionally performs features selection, which is useful for data with many features",
            "title": "To regularize or not to regularize"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_04_nn/introduction_to_machine_learning_04_nn/#summary",
            "text": "Neural networks are one of the most popular machine learning method these days    Note, that NN has much more hyperparameters than models described so far:    number of hidden layers    number of neurons    activation functions    learning algorithm settings (model, learning rate, momentum etc)    regularization method and its parameters    batch size in mini-batch gradient descent      It is crucial to use cross-validation for hyperparameter tuning    Next week \"we need to go deeper\"",
            "title": "Summary"
        }
    ]
}