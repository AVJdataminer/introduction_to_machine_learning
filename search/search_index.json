{
    "docs": [
        {
            "location": "/",
            "text": "Introduction to Machine Learning\n\u00b6\n\n\n\n\nNote\n\n\nThe content of this website was automatically generated from Jupyter notebooks.\n\n\nHopefully, nothing has been broken.\n\n\n\n\nLectures notes are prepared in \nColaboratory\n, which is a Jupyter notebook environment running in the cloud and storing notebooks in Google Drive. It makes it easy to collaborate on a project in Jupyter notebooks and provides free computing power.\n\n\nThis website was created for your convenience. Feel free to use Jupyter notebooks though:\n\n\n\n\n\n\nIntroduction (\nColaboratory\n or \nGitHub\n)\n\n\n\n\n\n\nk-Nearest Neighbors (\nColaboratory\n or \nGitHub\n)\n\n\n\n\n\n\nFirst ML problem\n\n\n\n\n\n\nNearest Neighbor algorithm\n\n\n\n\n\n\nk-Nearest Neighbors algorithm\n\n\n\n\n\n\nHyperparameters\n\n\n\n\n\n\nIris dataset\n\n\n\n\n\n\nMNIST\n\n\n\n\n\n\nRegression with kNN\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\nDecision trees (\nColaboratory\n or \nGitHub\n)\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\nID3 and C4.5 algorithms\n\n\n\n\n\n\nCART\n\n\n\n\n\n\nBias-variance trade-off\n\n\n\n\n\n\nEnsemble learning\n\n\n\n\n\n\nSummary",
            "title": "Home"
        },
        {
            "location": "/#introduction-to-machine-learning",
            "text": "Note  The content of this website was automatically generated from Jupyter notebooks.  Hopefully, nothing has been broken.   Lectures notes are prepared in  Colaboratory , which is a Jupyter notebook environment running in the cloud and storing notebooks in Google Drive. It makes it easy to collaborate on a project in Jupyter notebooks and provides free computing power.  This website was created for your convenience. Feel free to use Jupyter notebooks though:    Introduction ( Colaboratory  or  GitHub )    k-Nearest Neighbors ( Colaboratory  or  GitHub )    First ML problem    Nearest Neighbor algorithm    k-Nearest Neighbors algorithm    Hyperparameters    Iris dataset    MNIST    Regression with kNN    Summary      Decision trees ( Colaboratory  or  GitHub )    Introduction    ID3 and C4.5 algorithms    CART    Bias-variance trade-off    Ensemble learning    Summary",
            "title": "Introduction to Machine Learning"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/",
            "text": "Introduction to Machine Learning\n\u00b6\n\n\nLecturer details\n\u00b6\n\n\n\n\n\n\nTomasz Golan\n\n\n\n\n\n\nemail: \ntomasz.golan@uwr.edu.pl\n\n\n\n\n\n\nroom@ift: 438\n\n\n\n\n\n\nphone: +48 71 375-9405\n\n\n\n\n\n\nconsultations:\n\n\n\n\n\n\nMonday 11-12\n\n\n\n\n\n\nThursday 16-17 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture details\n\u00b6\n\n\nPlan\n\u00b6\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\nk-Nearest Neighbors\n\n\n\n\n\n\nDecision Tree\n\n\n\n\n\n\nSupport Vector Machine\n\n\n\n\n\n\nMultilayer Perceptron\n\n\n\n\n\n\nDeep Learning\n\n\n\n\n\n\nLiterature\n\u00b6\n\n\n\n\n\n\n\u201cDeep Learning\u201d by Ian Goodfellow, Yoshua Bengio, Aaron Courville\n\n\n\n\n\n\n\u201cPattern Recognition and Machine Learning\u201d by Christopher Bishop\n\n\n\n\n\n\nRecommended prerequisite knowledge\n\u00b6\n\n\n\n\n\n\nLinear algebra\n\n\n\n\n\n\nCalculus\n\n\n\n\n\n\nPython\n\n\n\n\n\n\nExam\n\u00b6\n\n\n\n\n\n\nIn the form of the presentation\n\n\n\n\n\n\nIndividual or group project\n\n\n\n\n\n\nAt least one machine learning algorithm must be used\n\n\n\n\n\n\nWith the model description included\n\n\n\n\n\n\nuseful (but not interesting) functions\n\u00b6\n\n\n\n\n\n\nHere, I just define some functions used for making demo plots during the introduction.\n\n\n\n\n\n\nFeel free to look at them later (especially if you are not familiar with \nnumpy\n and \nmatplotlib\n).\n\n\n\n\n\n\nBut now let's skip them.\n\n\n\n\n\n\n# numpy and matplotlib will be used a lot during the lecture\n\n\n# if you are familiar with these libraries you may skip this part\n\n\n# if not - extended comments were added to make it easier to understand\n\n\n\n# it is kind of standard to import numpy as np and pyplot as plt\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\nimport\n \nmatplotlib.pyplot\n \nas\n \nplt\n\n\n\n# used later to apply different colors in for loops\n\n\nmpl_colors\n \n=\n \n(\n'r'\n,\n \n'b'\n,\n \n'g'\n,\n \n'c'\n,\n \n'm'\n,\n \n'y'\n,\n \n'k'\n,\n \n'w'\n)\n\n\n\n# just to overwrite default colab style\n\n\nplt\n.\nstyle\n.\nuse\n(\n'default'\n)\n\n\nplt\n.\nstyle\n.\nuse\n(\n'seaborn-talk'\n)\n\n\n\n\ndef\n \ngenerate_random_points\n(\nsize\n=\n10\n,\n \nlow\n=\n0\n,\n \nhigh\n=\n1\n):\n\n  \n\"\"\"Generate a set of random 2D points\n\n\n\n  size -- number of points to generate\n\n\n  low  -- min value\n\n\n  high -- max value\n\n\n  \"\"\"\n\n  \n# random_sample([size]) returns random numbers with shape defined by size\n\n  \n# e.g.\n\n  \n# >>> np.random.random_sample((2, 3))\n\n  \n#\n\n  \n# array([[ 0.44013807,  0.77358569,  0.64338619],\n\n  \n#        [ 0.54363868,  0.31855232,  0.16791031]])\n\n  \n#\n\n  \nreturn\n \n(\nhigh\n \n-\n \nlow\n)\n \n*\n \nnp\n.\nrandom\n.\nrandom_sample\n((\nsize\n,\n \n2\n))\n \n+\n \nlow\n\n\n\n\ndef\n \ninit_plot\n(\nx_range\n=\nNone\n,\n \ny_range\n=\nNone\n,\n \nx_label\n=\n\"$x_1$\"\n,\n \ny_label\n=\n\"$x_2$\"\n):\n\n  \n\"\"\"Set axes limits and labels\n\n\n\n  x_range -- [min x, max x]\n\n\n  y_range -- [min y, max y]\n\n\n  x_label -- string\n\n\n  y_label -- string\n\n\n  \"\"\"\n\n\n  \n# subplots returns figure and axes\n\n  \n# (in general you may want many axes on one figure)\n\n  \n# we do not need fig here\n\n  \n# but we will apply changes (including adding points) to axes\n\n  \n_\n,\n \nax\n \n=\n \nplt\n.\nsubplots\n(\ndpi\n=\n70\n)\n\n\n  \n# set grid style and color\n\n  \nax\n.\ngrid\n(\nc\n=\n'0.70'\n,\n \nlinestyle\n=\n':'\n)\n\n\n  \n# set axes limits (x_range and y_range is a list with two elements)\n\n  \nax\n.\nset_xlim\n(\nx_range\n)\n \n  \nax\n.\nset_ylim\n(\ny_range\n)\n\n\n  \n# set axes labels\n\n  \nax\n.\nset_xlabel\n(\nx_label\n)\n\n  \nax\n.\nset_ylabel\n(\ny_label\n)\n\n\n  \n# return axes so we can continue modyfing them later\n\n  \nreturn\n \nax\n\n\n\n\ndef\n \nplot_random_points\n(\nstyle\n=\nNone\n,\n \ncolor\n=\nNone\n):\n\n  \n\"\"\"Generate and plot two (separated) sets of random points\n\n\n\n  style -- latter group points style (default as first)\n\n\n  color -- latter group color (default as first)\n\n\n  \"\"\"\n\n\n  \n# create a plot with x and y ranges from 0 to 2.5\n\n  \nax\n \n=\n \ninit_plot\n([\n0\n,\n \n2.5\n],\n \n[\n0\n,\n \n2.5\n])\n\n\n  \n# add two different sets of random points\n\n  \n# first set = 5 points from [0.5, 1.0]x[0.5, 1.0]\n\n  \n# second set = 5 points from [1.5, 2.0]x[1.5, 2.0]\n\n  \n# generate_random_points return a numpy array in the format like\n\n  \n# [[x1, y1], [x2, y2], ..., [xn, yn]]\n\n  \n# pyplot.plt take separately arrays with X and Y, like\n\n  \n# plot([x1, x2, x3], [y1, y2, y3])\n\n  \n# thus, we transpose numpy array to the format\n\n  \n# [[x1, x2, ..., xn], [y1, y2, ..., yn]]\n\n  \n# and unpack it with *\n\n  \nax\n.\nplot\n(\n*\ngenerate_random_points\n(\n5\n,\n \n0.5\n,\n \n1.0\n)\n.\nT\n,\n \n'ro'\n)\n\n  \nax\n.\nplot\n(\n*\ngenerate_random_points\n(\n5\n,\n \n1.5\n,\n \n2.0\n)\n.\nT\n,\n \nstyle\n \nor\n \n'ro'\n)\n\n\n  \nreturn\n \nax\n\n\n\n\ndef\n \nplot_an_example\n(\nstyle\n=\nNone\n,\n \ncolor\n=\nNone\n,\n \nlabel\n=\n\"Class\"\n):\n\n  \n\"\"\"Plot an example of supervised or unsupervised learning\"\"\"\n\n  \nax\n \n=\n \nplot_random_points\n(\nstyle\n,\n \ncolor\n)\n\n\n  \n# circle areas related to each set of points\n\n  \n# pyplot.Circle((x, y), r); (x, y) - the center of a circle; r - radius\n\n  \n# lw - line width\n\n  \nax\n.\nadd_artist\n(\nplt\n.\nCircle\n((\n0.75\n,\n \n0.75\n),\n \n0.5\n,\n \nfill\n=\n0\n,\n \ncolor\n=\n'r'\n,\n \nlw\n=\n2\n))\n\n  \nax\n.\nadd_artist\n(\nplt\n.\nCircle\n((\n1.75\n,\n \n1.75\n),\n \n0.5\n,\n \nfill\n=\n0\n,\n \ncolor\n=\ncolor\n \nor\n \n'r'\n,\n \nlw\n=\n2\n))\n\n\n  \n# put group labels\n\n  \n# pyplot.text just put arbitrary text in given coordinates\n\n  \nax\n.\ntext\n(\n0.65\n,\n \n1.4\n,\n \nlabel\n \n+\n \n\" I\"\n,\n \nfontdict\n=\n{\n'color'\n:\n \n'r'\n})\n\n  \nax\n.\ntext\n(\n1.65\n,\n \n1.1\n,\n \nlabel\n \n+\n \n\" II\"\n,\n \nfontdict\n=\n{\n'color'\n:\n \ncolor\n \nor\n \n'r'\n})\n\n\n\n\n\n\nIntroduction\n\u00b6\n\n\nWhat is machine learning?\n\u00b6\n\n\n+-------------------------------------------------------------------------+\n|                                                                         |\n|  Any technique which enables                                            |\n|  computers to mimic human                      Artificial Intelligence  |\n|  intelligence                                                           |\n|                                                                         |\n|     +-------------------------------------------------------------------+\n|     |                                                                   |\n|     |   Statistical techniques which                                    |\n|     |   enable computers to improve               Machine Learning      |\n|     |   with experience (subset of AI)                                  |\n|     |                                                                   |\n|     |       +-----------------------------------------------------------+\n|     |       |                                                           |\n|     |       |  Subset of ML which makes                                 |\n|     |       |  the computations using              Deep Learning        |\n|     |       |  multi-layer neural networks                              |\n|     |       |                                                           |\n+-----+-------+-----------------------------------------------------------+\n\n\n\n\n\nSupervised learning\n\u00b6\n\n\n\n\n\n\nProblems: classification, regression\n\n\n\n\n\n\nLet \n\\vec x_i \\in X\n\\vec x_i \\in X\n be feature vectors\n\n\n\n\n\n\nLet \ny_i \\in Y\ny_i \\in Y\n be class labels\n\n\n\n\n\n\nLet \nh: X \\rightarrow Y\nh: X \\rightarrow Y\n be hypothesis\n\n\n\n\n\n\nFind \nh(\\vec x)\nh(\\vec x)\n given \nN\nN\n training examples \n\\left\\{(\\vec x_1, y_1), ..., (\\vec x_N, y_N)\\right\\}\n\\left\\{(\\vec x_1, y_1), ..., (\\vec x_N, y_N)\\right\\}\n\n\n\n\n\n\nplot_an_example\n(\nstyle\n=\n'bs'\n,\n \ncolor\n=\n'b'\n);\n\n\n\n\n\n\n\n\nUnsupervised learning\n\u00b6\n\n\n\n\n\n\nIn opposite to supervised learning data is not labeled\n\n\n\n\n\n\nProblems: clustering, association\n\n\n\n\n\n\nFor example: k-means clustering, self-organizing maps\n\n\n\n\n\n\nplot_an_example\n(\nlabel\n=\n\"Cluster\"\n);\n\n\n\n\n\n\n\n\nExample: Supervised vs Unsupervised\n\u00b6\n\n\n\n\n\n\nHaving \nN\nN\n photos of different animals\n\n\n\n\n\n\nSupervised task (requires labeled data)\n\n\n\n\n\n\n\n\nTrain an algorithm to recognise given species on a photo.\n\n\nOutput: There is X on a photo.\n\n\n\n\n\n\nUnsupervised task\n\n\n\n\n\n\nTrain an algorithm to group animals with similar features.\n\n\nOutput: No idea what it is, but it looks similar to these animals.\n\n\n\n\nReinforcement learning\n\u00b6\n\n\n                +---------+\n                |         |\n       +--------+  AGENT  | <------+\n       |        |         |        |\n       |        +---------+        |\n       |                           | Observation\nAction |                           |\n       |                           | Reward\n       |     +---------------+     |\n       |     |               |     |\n       +---> |  ENVIRONMENT  +-----+\n             |               |\n             +---------------+\n\n\n\n\n\nML applications\n\u00b6\n\n\n\n\n\n\nImage recognition\n\n\n\n\n\n\nGoogle Maps\n - finding licence plates and faces; extracting street names and building numbers\n\n\n\n\n\n\nFacebook\n - recognising similar faces\n\n\n\n\n\n\n\n\n\n\nSpeech recognition\n\n\n\n\n\n\nMicrosoft\n - Cortana\n\n\n\n\n\n\nApple\n - Siri\n\n\n\n\n\n\n\n\n\n\nNatural Language Processing\n\n\n\n\n\n\nGoogle Translate\n - machine translation\n\n\n\n\n\n\nNext Game of Thrones Book\n - language modeling\n\n\n\n\n\n\n\n\n\n\nMisc\n\n\n\n\n\n\nPayPal\n - fraud alert\n\n\n\n\n\n\nNetflix\n, \nAmazon\n - recommendation system\n\n\n\n\n\n\nArt\n\n\n\n\n\n\n\n\n\n\nAlphaGo\n\n\n\n\n\n\n\n\n\n\nML Fails\n\u00b6\n\n\n\n\n\n\nAmazon's Alexa - TV broadcast caused many orders around San Diego when presenter said \nI love the little girl, saying 'Alexa ordered me a dollhouse'.\n\n\n\n\n\n\nAmazon's Alexa - when a kid asked for his favorite song \nDigger, Digger\n Alexa's respond was: \nYou want to hear a station for porn detected \u2026 hot chick amateur girl sexy.\n\n\n\n\n\n\nMicrosoft's Tay chatbot learned from tweets how to be racist\n\n\n\n\n\n\n\n\n\n\nPassport checker rejects Asian's photo because \neyes are closed\n\n\n\n\n\n\n\n\nSo make sure you can not relate to this\n\n\n\n\n\n\nML Frameworks\n\u00b6\n\n\n\n\n\n\nTensorflow\n by Google - Python (and somewhat in C/C++)\n\n\n\n\n\n\nCaffe\n by Berkeley Vision and Learning Center - C/C++, Python, MATLAB, Command line interface\n\n\n\n\n\n\nTorch\n by many - Lua and C/C++\n\n\n\n\n\n\nTheano\n by University of Montreal - Python (development stopped in 2017)\n\n\n\n\n\n\nscikit-learn\n by many - Python\n\n\n\n\n\n\nand many others",
            "title": "Introduction"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#introduction-to-machine-learning",
            "text": "",
            "title": "Introduction to Machine Learning"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#lecturer-details",
            "text": "Tomasz Golan    email:  tomasz.golan@uwr.edu.pl    room@ift: 438    phone: +48 71 375-9405    consultations:    Monday 11-12    Thursday 16-17",
            "title": "Lecturer details"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#lecture-details",
            "text": "",
            "title": "Lecture details"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#plan",
            "text": "Introduction    k-Nearest Neighbors    Decision Tree    Support Vector Machine    Multilayer Perceptron    Deep Learning",
            "title": "Plan"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#literature",
            "text": "\u201cDeep Learning\u201d by Ian Goodfellow, Yoshua Bengio, Aaron Courville    \u201cPattern Recognition and Machine Learning\u201d by Christopher Bishop",
            "title": "Literature"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#recommended-prerequisite-knowledge",
            "text": "Linear algebra    Calculus    Python",
            "title": "Recommended prerequisite knowledge"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#exam",
            "text": "In the form of the presentation    Individual or group project    At least one machine learning algorithm must be used    With the model description included",
            "title": "Exam"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#useful-but-not-interesting-functions",
            "text": "Here, I just define some functions used for making demo plots during the introduction.    Feel free to look at them later (especially if you are not familiar with  numpy  and  matplotlib ).    But now let's skip them.    # numpy and matplotlib will be used a lot during the lecture  # if you are familiar with these libraries you may skip this part  # if not - extended comments were added to make it easier to understand  # it is kind of standard to import numpy as np and pyplot as plt  import   numpy   as   np  import   matplotlib.pyplot   as   plt  # used later to apply different colors in for loops  mpl_colors   =   ( 'r' ,   'b' ,   'g' ,   'c' ,   'm' ,   'y' ,   'k' ,   'w' )  # just to overwrite default colab style  plt . style . use ( 'default' )  plt . style . use ( 'seaborn-talk' )  def   generate_random_points ( size = 10 ,   low = 0 ,   high = 1 ): \n   \"\"\"Generate a set of random 2D points    size -- number of points to generate    low  -- min value    high -- max value    \"\"\" \n   # random_sample([size]) returns random numbers with shape defined by size \n   # e.g. \n   # >>> np.random.random_sample((2, 3)) \n   # \n   # array([[ 0.44013807,  0.77358569,  0.64338619], \n   #        [ 0.54363868,  0.31855232,  0.16791031]]) \n   # \n   return   ( high   -   low )   *   np . random . random_sample (( size ,   2 ))   +   low  def   init_plot ( x_range = None ,   y_range = None ,   x_label = \"$x_1$\" ,   y_label = \"$x_2$\" ): \n   \"\"\"Set axes limits and labels    x_range -- [min x, max x]    y_range -- [min y, max y]    x_label -- string    y_label -- string    \"\"\" \n\n   # subplots returns figure and axes \n   # (in general you may want many axes on one figure) \n   # we do not need fig here \n   # but we will apply changes (including adding points) to axes \n   _ ,   ax   =   plt . subplots ( dpi = 70 ) \n\n   # set grid style and color \n   ax . grid ( c = '0.70' ,   linestyle = ':' ) \n\n   # set axes limits (x_range and y_range is a list with two elements) \n   ax . set_xlim ( x_range )  \n   ax . set_ylim ( y_range ) \n\n   # set axes labels \n   ax . set_xlabel ( x_label ) \n   ax . set_ylabel ( y_label ) \n\n   # return axes so we can continue modyfing them later \n   return   ax  def   plot_random_points ( style = None ,   color = None ): \n   \"\"\"Generate and plot two (separated) sets of random points    style -- latter group points style (default as first)    color -- latter group color (default as first)    \"\"\" \n\n   # create a plot with x and y ranges from 0 to 2.5 \n   ax   =   init_plot ([ 0 ,   2.5 ],   [ 0 ,   2.5 ]) \n\n   # add two different sets of random points \n   # first set = 5 points from [0.5, 1.0]x[0.5, 1.0] \n   # second set = 5 points from [1.5, 2.0]x[1.5, 2.0] \n   # generate_random_points return a numpy array in the format like \n   # [[x1, y1], [x2, y2], ..., [xn, yn]] \n   # pyplot.plt take separately arrays with X and Y, like \n   # plot([x1, x2, x3], [y1, y2, y3]) \n   # thus, we transpose numpy array to the format \n   # [[x1, x2, ..., xn], [y1, y2, ..., yn]] \n   # and unpack it with * \n   ax . plot ( * generate_random_points ( 5 ,   0.5 ,   1.0 ) . T ,   'ro' ) \n   ax . plot ( * generate_random_points ( 5 ,   1.5 ,   2.0 ) . T ,   style   or   'ro' ) \n\n   return   ax  def   plot_an_example ( style = None ,   color = None ,   label = \"Class\" ): \n   \"\"\"Plot an example of supervised or unsupervised learning\"\"\" \n   ax   =   plot_random_points ( style ,   color ) \n\n   # circle areas related to each set of points \n   # pyplot.Circle((x, y), r); (x, y) - the center of a circle; r - radius \n   # lw - line width \n   ax . add_artist ( plt . Circle (( 0.75 ,   0.75 ),   0.5 ,   fill = 0 ,   color = 'r' ,   lw = 2 )) \n   ax . add_artist ( plt . Circle (( 1.75 ,   1.75 ),   0.5 ,   fill = 0 ,   color = color   or   'r' ,   lw = 2 )) \n\n   # put group labels \n   # pyplot.text just put arbitrary text in given coordinates \n   ax . text ( 0.65 ,   1.4 ,   label   +   \" I\" ,   fontdict = { 'color' :   'r' }) \n   ax . text ( 1.65 ,   1.1 ,   label   +   \" II\" ,   fontdict = { 'color' :   color   or   'r' })",
            "title": "useful (but not interesting) functions"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#introduction",
            "text": "",
            "title": "Introduction"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#what-is-machine-learning",
            "text": "+-------------------------------------------------------------------------+\n|                                                                         |\n|  Any technique which enables                                            |\n|  computers to mimic human                      Artificial Intelligence  |\n|  intelligence                                                           |\n|                                                                         |\n|     +-------------------------------------------------------------------+\n|     |                                                                   |\n|     |   Statistical techniques which                                    |\n|     |   enable computers to improve               Machine Learning      |\n|     |   with experience (subset of AI)                                  |\n|     |                                                                   |\n|     |       +-----------------------------------------------------------+\n|     |       |                                                           |\n|     |       |  Subset of ML which makes                                 |\n|     |       |  the computations using              Deep Learning        |\n|     |       |  multi-layer neural networks                              |\n|     |       |                                                           |\n+-----+-------+-----------------------------------------------------------+",
            "title": "What is machine learning?"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#supervised-learning",
            "text": "Problems: classification, regression    Let  \\vec x_i \\in X \\vec x_i \\in X  be feature vectors    Let  y_i \\in Y y_i \\in Y  be class labels    Let  h: X \\rightarrow Y h: X \\rightarrow Y  be hypothesis    Find  h(\\vec x) h(\\vec x)  given  N N  training examples  \\left\\{(\\vec x_1, y_1), ..., (\\vec x_N, y_N)\\right\\} \\left\\{(\\vec x_1, y_1), ..., (\\vec x_N, y_N)\\right\\}    plot_an_example ( style = 'bs' ,   color = 'b' );",
            "title": "Supervised learning"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#unsupervised-learning",
            "text": "In opposite to supervised learning data is not labeled    Problems: clustering, association    For example: k-means clustering, self-organizing maps    plot_an_example ( label = \"Cluster\" );",
            "title": "Unsupervised learning"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#example-supervised-vs-unsupervised",
            "text": "Having  N N  photos of different animals    Supervised task (requires labeled data)     Train an algorithm to recognise given species on a photo.  Output: There is X on a photo.    Unsupervised task    Train an algorithm to group animals with similar features.  Output: No idea what it is, but it looks similar to these animals.",
            "title": "Example: Supervised vs Unsupervised"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#reinforcement-learning",
            "text": "+---------+\n                |         |\n       +--------+  AGENT  | <------+\n       |        |         |        |\n       |        +---------+        |\n       |                           | Observation\nAction |                           |\n       |                           | Reward\n       |     +---------------+     |\n       |     |               |     |\n       +---> |  ENVIRONMENT  +-----+\n             |               |\n             +---------------+",
            "title": "Reinforcement learning"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#ml-applications",
            "text": "Image recognition    Google Maps  - finding licence plates and faces; extracting street names and building numbers    Facebook  - recognising similar faces      Speech recognition    Microsoft  - Cortana    Apple  - Siri      Natural Language Processing    Google Translate  - machine translation    Next Game of Thrones Book  - language modeling      Misc    PayPal  - fraud alert    Netflix ,  Amazon  - recommendation system    Art      AlphaGo",
            "title": "ML applications"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#ml-fails",
            "text": "Amazon's Alexa - TV broadcast caused many orders around San Diego when presenter said  I love the little girl, saying 'Alexa ordered me a dollhouse'.    Amazon's Alexa - when a kid asked for his favorite song  Digger, Digger  Alexa's respond was:  You want to hear a station for porn detected \u2026 hot chick amateur girl sexy.    Microsoft's Tay chatbot learned from tweets how to be racist      Passport checker rejects Asian's photo because  eyes are closed     So make sure you can not relate to this",
            "title": "ML Fails"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_00_intro/introduction_to_machine_learning_00_intro/#ml-frameworks",
            "text": "Tensorflow  by Google - Python (and somewhat in C/C++)    Caffe  by Berkeley Vision and Learning Center - C/C++, Python, MATLAB, Command line interface    Torch  by many - Lua and C/C++    Theano  by University of Montreal - Python (development stopped in 2017)    scikit-learn  by many - Python    and many others",
            "title": "ML Frameworks"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/",
            "text": "k-Nearest Neighbors\n\u00b6\n\n\nuseful (but not interesting) functions\n\u00b6\n\n\n\n\n\n\nHere, I just define some functions used for making demo plots during the introduction.\n\n\n\n\n\n\nFeel free to look at them later (especially if you are not familiar with \nnumpy\n and \nmatplotlib\n).\n\n\n\n\n\n\nBut now let's skip them.\n\n\n\n\n\n\n# numpy and matplotlib will be used a lot during the lecture\n\n\n# if you are familiar with these libraries you may skip this part\n\n\n# if not - extended comments were added to make it easier to understand\n\n\n\n# it is kind of standard to import numpy as np and pyplot as plt\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\nimport\n \nmatplotlib.pyplot\n \nas\n \nplt\n\n\n\n# used later to apply different colors in for loops\n\n\nmpl_colors\n \n=\n \n(\n'r'\n,\n \n'b'\n,\n \n'g'\n,\n \n'c'\n,\n \n'm'\n,\n \n'y'\n,\n \n'k'\n,\n \n'w'\n)\n\n\n\n# just to overwrite default colab style\n\n\nplt\n.\nstyle\n.\nuse\n(\n'default'\n)\n\n\nplt\n.\nstyle\n.\nuse\n(\n'seaborn-talk'\n)\n\n\n\n\ndef\n \ngenerate_random_points\n(\nsize\n=\n10\n,\n \nlow\n=\n0\n,\n \nhigh\n=\n1\n):\n\n  \n\"\"\"Generate a set of random 2D points\n\n\n\n  size -- number of points to generate\n\n\n  low  -- min value\n\n\n  high -- max value\n\n\n  \"\"\"\n\n  \n# random_sample([size]) returns random numbers with shape defined by size\n\n  \n# e.g.\n\n  \n# >>> np.random.random_sample((2, 3))\n\n  \n#\n\n  \n# array([[ 0.44013807,  0.77358569,  0.64338619],\n\n  \n#        [ 0.54363868,  0.31855232,  0.16791031]])\n\n  \n#\n\n  \nreturn\n \n(\nhigh\n \n-\n \nlow\n)\n \n*\n \nnp\n.\nrandom\n.\nrandom_sample\n((\nsize\n,\n \n2\n))\n \n+\n \nlow\n\n\n\n\ndef\n \ninit_plot\n(\nx_range\n=\nNone\n,\n \ny_range\n=\nNone\n,\n \nx_label\n=\n\"$x_1$\"\n,\n \ny_label\n=\n\"$x_2$\"\n):\n\n  \n\"\"\"Set axes limits and labels\n\n\n\n  x_range -- [min x, max x]\n\n\n  y_range -- [min y, max y]\n\n\n  x_label -- string\n\n\n  y_label -- string\n\n\n  \"\"\"\n\n\n  \n# subplots returns figure and axes\n\n  \n# (in general you may want many axes on one figure)\n\n  \n# we do not need fig here\n\n  \n# but we will apply changes (including adding points) to axes\n\n  \n_\n,\n \nax\n \n=\n \nplt\n.\nsubplots\n(\ndpi\n=\n70\n)\n\n\n  \n# set grid style and color\n\n  \nax\n.\ngrid\n(\nc\n=\n'0.70'\n,\n \nlinestyle\n=\n':'\n)\n\n\n  \n# set axes limits (x_range and y_range is a list with two elements)\n\n  \nax\n.\nset_xlim\n(\nx_range\n)\n \n  \nax\n.\nset_ylim\n(\ny_range\n)\n\n\n  \n# set axes labels\n\n  \nax\n.\nset_xlabel\n(\nx_label\n)\n\n  \nax\n.\nset_ylabel\n(\ny_label\n)\n\n\n  \n# return axes so we can continue modyfing them later\n\n  \nreturn\n \nax\n\n\n\n\ndef\n \nplot_random_points\n(\nstyle\n=\nNone\n,\n \ncolor\n=\nNone\n):\n\n  \n\"\"\"Generate and plot two (separated) sets of random points\n\n\n\n  style -- latter group points style (default as first)\n\n\n  color -- latter group color (default as first)\n\n\n  \"\"\"\n\n\n  \n# create a plot with x and y ranges from 0 to 2.5\n\n  \nax\n \n=\n \ninit_plot\n([\n0\n,\n \n2.5\n],\n \n[\n0\n,\n \n2.5\n])\n\n\n  \n# add two different sets of random points\n\n  \n# first set = 5 points from [0.5, 1.0]x[0.5, 1.0]\n\n  \n# second set = 5 points from [1.5, 2.0]x[1.5, 2.0]\n\n  \n# generate_random_points return a numpy array in the format like\n\n  \n# [[x1, y1], [x2, y2], ..., [xn, yn]]\n\n  \n# pyplot.plt take separately arrays with X and Y, like\n\n  \n# plot([x1, x2, x3], [y1, y2, y3])\n\n  \n# thus, we transpose numpy array to the format\n\n  \n# [[x1, x2, ..., xn], [y1, y2, ..., yn]]\n\n  \n# and unpack it with *\n\n  \nax\n.\nplot\n(\n*\ngenerate_random_points\n(\n5\n,\n \n0.5\n,\n \n1.0\n)\n.\nT\n,\n \n'ro'\n)\n\n  \nax\n.\nplot\n(\n*\ngenerate_random_points\n(\n5\n,\n \n1.5\n,\n \n2.0\n)\n.\nT\n,\n \nstyle\n \nor\n \n'ro'\n)\n\n\n  \nreturn\n \nax\n\n\n\n\ndef\n \nplot_an_example\n(\nstyle\n=\nNone\n,\n \ncolor\n=\nNone\n,\n \nlabel\n=\n\"Class\"\n):\n\n  \n\"\"\"Plot an example of supervised or unsupervised learning\"\"\"\n\n  \nax\n \n=\n \nplot_random_points\n(\nstyle\n,\n \ncolor\n)\n\n\n  \n# circle areas related to each set of points\n\n  \n# pyplot.Circle((x, y), r); (x, y) - the center of a circle; r - radius\n\n  \n# lw - line width\n\n  \nax\n.\nadd_artist\n(\nplt\n.\nCircle\n((\n0.75\n,\n \n0.75\n),\n \n0.5\n,\n \nfill\n=\n0\n,\n \ncolor\n=\n'r'\n,\n \nlw\n=\n2\n))\n\n  \nax\n.\nadd_artist\n(\nplt\n.\nCircle\n((\n1.75\n,\n \n1.75\n),\n \n0.5\n,\n \nfill\n=\n0\n,\n \ncolor\n=\ncolor\n \nor\n \n'r'\n,\n \nlw\n=\n2\n))\n\n\n  \n# put group labels\n\n  \n# pyplot.text just put arbitrary text in given coordinates\n\n  \nax\n.\ntext\n(\n0.65\n,\n \n1.4\n,\n \nlabel\n \n+\n \n\" I\"\n,\n \nfontdict\n=\n{\n'color'\n:\n \n'r'\n})\n\n  \nax\n.\ntext\n(\n1.65\n,\n \n1.1\n,\n \nlabel\n \n+\n \n\" II\"\n,\n \nfontdict\n=\n{\n'color'\n:\n \ncolor\n \nor\n \n'r'\n})\n\n\n\n\n\n\nOur first ML problem\n\u00b6\n\n\n\n\n\n\nTwo classes: red circles and blue squares (\ntraining\n samples)\n\n\n\n\n\n\nWhere does the green triangle (\ntest\n sample) belong?\n\n\n\n\n\n\nX1\n \n=\n \ngenerate_random_points\n(\n20\n,\n \n0\n,\n \n1\n)\n\n\nX2\n \n=\n \ngenerate_random_points\n(\n20\n,\n \n1\n,\n \n2\n)\n\n\n\nnew_point\n \n=\n \ngenerate_random_points\n(\n1\n,\n \n0\n,\n \n2\n)\n\n\n\nplot\n \n=\n \ninit_plot\n([\n0\n,\n \n2\n],\n \n[\n0\n,\n \n2\n])\n  \n# [0, 2] x [0, 2]\n\n\n\nplot\n.\nplot\n(\n*\nX1\n.\nT\n,\n \n'ro'\n,\n \n*\nX2\n.\nT\n,\n \n'bs'\n,\n \n*\nnew_point\n.\nT\n,\n \n'g^'\n);\n\n\n\n\n\n\n\n\nNearest Neighbor\n\u00b6\n\n\n\n\n\n\nThe nearest neigbor classifier \ncompares\n a test sample with all training samples to predict a label (class).\n\n\n\n\n\n\nHow to compare two samples?\n\n\n\n\n\n\nL1 distance: \nd(\\vec x_1, \\vec x_2) = \\sum\\limits_i |x_1^i - x_2^i|\nd(\\vec x_1, \\vec x_2) = \\sum\\limits_i |x_1^i - x_2^i|\n\n\n\n\n\n\nL2 distance: \nd(\\vec x_1, \\vec x_2) = \\sqrt{\\sum\\limits_i (x_1^i - x_2^i)^2}\nd(\\vec x_1, \\vec x_2) = \\sqrt{\\sum\\limits_i (x_1^i - x_2^i)^2}\n\n\n\n\n\n\nnote: in practice square root is ignored (becasue is monotonic function)\n\n\n\n\n\n\nL2 is less forgiving than L1 - prefers many small disagreements than one big one\n\n\n\n\n\n\n\n\n\n\ncosine distance (cosine similarity): \nd(\\vec x_1, \\vec x_2) = \\frac{x_1 \\cdot x_1}{||\\vec x_1|| \\cdot ||\\vec x_2||}\nd(\\vec x_1, \\vec x_2) = \\frac{x_1 \\cdot x_1}{||\\vec x_1|| \\cdot ||\\vec x_2||}\n\n\n\n\n\n\nChebyshev distance: \nd(\\vec x_1, \\vec x_2) = max_i(|x_1^i - x_2^i|)\nd(\\vec x_1, \\vec x_2) = max_i(|x_1^i - x_2^i|)\n\n\n\n\n\n\nand many others\n\n\n\n\n\n\n\n\n\n\nThe closest one determines the test sample label\n\n\n\n\n\n\nImplementation\n\u00b6\n\n\n\n\n\n\nThe implementation of nearest neighbor algorithm is pretty straightforward\n\n\n\n\n\n\nThere is no real training process here - we just need to remember all training feature vectors and corresponding labels\n\n\n\n\n\n\nTo predict a label for new sample we just need to find the label of the closest point from training samples\n\n\n\n\n\n\nclass\n \nNearestNeighbor\n():\n\n  \n\"\"\"Nearest Neighbor Classifier\"\"\"\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \ndistance\n=\n0\n):\n\n    \n\"\"\"Set distance definition: 0 - L1, 1 - L2\"\"\"\n\n    \nif\n \ndistance\n \n==\n \n0\n:\n\n      \nself\n.\ndistance\n \n=\n \nnp\n.\nabs\n     \n# absolute value\n\n    \nelif\n \ndistance\n \n==\n \n1\n:\n\n      \nself\n.\ndistance\n \n=\n \nnp\n.\nsquare\n  \n# square root\n\n    \nelse\n:\n\n      \nraise\n \nException\n(\n\"Distance not defined.\"\n)\n\n\n\n  \ndef\n \ntrain\n(\nself\n,\n \nx\n,\n \ny\n):\n\n    \n\"\"\"Train the classifier (here simply save training data)\n\n\n\n    x -- feature vectors (N x D)\n\n\n    y -- labels (N x 1)\n\n\n    \"\"\"\n\n    \nself\n.\nx_train\n \n=\n \nx\n\n    \nself\n.\ny_train\n \n=\n \ny\n\n\n\n  \ndef\n \npredict\n(\nself\n,\n \nx\n):\n\n    \n\"\"\"Predict and return labels for each feature vector from x\n\n\n\n    x -- feature vectors (N x D)\n\n\n    \"\"\"\n\n    \npredictions\n \n=\n \n[]\n  \n# placeholder for N labels\n\n\n    \n# loop over all test samples\n\n    \nfor\n \nx_test\n \nin\n \nx\n:\n\n      \n# array of distances between current test and all training samples\n\n      \ndistances\n \n=\n \nnp\n.\nsum\n(\nself\n.\ndistance\n(\nself\n.\nx_train\n \n-\n \nx_test\n),\n \naxis\n=\n1\n)\n\n\n      \n# get the closest one\n\n      \nmin_index\n \n=\n \nnp\n.\nargmin\n(\ndistances\n)\n\n\n      \n# add corresponding label\n\n      \npredictions\n.\nappend\n(\nself\n.\ny_train\n[\nmin_index\n])\n\n\n    \nreturn\n \npredictions\n\n\n\n\n\n\nThe magic of numpy\n\u00b6\n\n\n\n\n\n\nNumPy is irreplacable tool for numerical operations on arrays\n\n\n\n\n\n\nUsing numpy we could easily find all distances using one line\n\n\n\n\n\n\ndistances = np.sum(self.distance(self.x_train - x_test), axis=1)\n\n\n\n\n\n\n\nHere is how it works\n\n\n\n\n# let's create an array with 5x2 shape\n\n\na\n \n=\n \nnp\n.\nrandom\n.\nrandom_sample\n((\n5\n,\n \n2\n))\n\n\n\n# and another array with 1x2 shape\n\n\nb\n \n=\n \nnp\n.\narray\n([[\n1.\n,\n \n1.\n]])\n\n\n\nprint\n(\na\n,\n \nb\n,\n \nsep\n=\n\"\n\\n\\n\n\"\n)\n\n\n\n\n\n\n[[0.79036457 0.36571819]\n [0.76743991 0.08439684]\n [0.56876884 0.97967839]\n [0.77020776 0.21238365]\n [0.94235534 0.73884472]]\n\n[[1. 1.]]\n\n\n\n\n\n# subtract arguments (element-wise)\n\n\n# note, that at least one dimension must be the same \n\n\nprint\n(\na\n \n-\n \nb\n)\n\n\n\n\n\n\n[[-0.20963543 -0.63428181]\n [-0.23256009 -0.91560316]\n [-0.43123116 -0.02032161]\n [-0.22979224 -0.78761635]\n [-0.05764466 -0.26115528]]\n\n\n\n\n\n# numpy.abs calculates absolute value (element-wise)\n\n\nprint\n(\nnp\n.\nabs\n(\na\n \n-\n \nb\n))\n\n\n\n\n\n\n[[0.20963543 0.63428181]\n [0.23256009 0.91560316]\n [0.43123116 0.02032161]\n [0.22979224 0.78761635]\n [0.05764466 0.26115528]]\n\n\n\n\n\n# sum all elements\n\n\nnp\n.\nsum\n(\nnp\n.\nabs\n(\na\n \n-\n \nb\n))\n\n\n\n\n\n\n3.7798417848539096\n\n\n\n\n\n# sum elements over a given axis\n\n\nnp\n.\nsum\n(\nnp\n.\nabs\n(\na\n \n-\n \nb\n),\n \naxis\n=\n0\n)\n\n\n\n\n\n\narray([1.16086358, 2.61897821])\n\n\n\n\n\nnp\n.\nsum\n(\nnp\n.\nabs\n(\na\n \n-\n \nb\n),\n \naxis\n=\n1\n)\n\n\n\n\n\n\narray([0.84391724, 1.14816326, 0.45155276, 1.01740859, 0.31879994])\n\n\n\n\n\nAnalysis\n\u00b6\n\n\n\n\n\n\nBefore we start using \nNearestNeighbor\n let's create a simple mini-framework to apply NN and visualize results easily\n\n\n\n\n\n\nWe want to initilize \nNearestNeighbor\n with some feature vectors (and automatically assign labels for each class)\n\n\n\n\n\n\nWe want our test samples to be a grid of uniformly distributed points\n\n\n\n\n\n\nWe want methods to process test data and to make a plots with final results\n\n\n\n\n\n\nclass\n \nAnalysis\n():\n\n  \n\"\"\"Apply NearestNeighbor to generated (uniformly) test samples.\"\"\"\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \n*\nx\n,\n \ndistance\n):\n\n    \n\"\"\"Generate labels and initilize classifier\n\n\n\n    x -- feature vectors arrays\n\n\n    distance -- 0 for L1, 1 for L2    \n\n\n    \"\"\"\n\n    \n# get number of classes\n\n    \nself\n.\nnof_classes\n \n=\n \nlen\n(\nx\n)\n\n\n    \n# create lables array\n\n    \n# np.ones creates an array of given shape filled with 1 of given type\n\n    \n# we apply consecutive integer numbers as class labels\n\n    \n# ravel return flatten array\n\n    \ny\n \n=\n \n[\ni\n \n*\n \nnp\n.\nones\n(\n_x\n.\nshape\n[\n0\n],\n \ndtype\n=\nnp\n.\nint\n)\n \nfor\n \ni\n,\n \n_x\n \nin\n \nenumerate\n(\nx\n)]\n\n    \ny\n \n=\n \nnp\n.\narray\n(\ny\n)\n.\nravel\n()\n\n\n    \n# save training samples to plot them later\n\n    \nself\n.\nx_train\n \n=\n \nx\n\n\n    \n# merge feature vector arrays for NearestNeighbor\n\n    \nx\n \n=\n \nnp\n.\nconcatenate\n(\nx\n,\n \naxis\n=\n0\n)\n\n\n    \n# train classifier\n\n    \nself\n.\nnn\n \n=\n \nNearestNeighbor\n(\ndistance\n)\n\n    \nself\n.\nnn\n.\ntrain\n(\nx\n,\n \ny\n)\n\n\n\n  \ndef\n \nprepare_test_samples\n(\nself\n,\n \nlow\n=\n0\n,\n \nhigh\n=\n2\n,\n \nstep\n=\n0.01\n):\n\n    \n\"\"\"Generate a grid with test points (from low to high with step)\"\"\"\n\n    \n# remember range\n\n    \nself\n.\nrange\n \n=\n \n[\nlow\n,\n \nhigh\n]\n\n\n    \n# start with grid of points from [low, high] x [low, high]\n\n    \ngrid\n \n=\n \nnp\n.\nmgrid\n[\nlow\n:\nhigh\n+\nstep\n:\nstep\n,\n \nlow\n:\nhigh\n+\nstep\n:\nstep\n]\n\n\n    \n# convert to an array of 2D points\n\n    \nself\n.\nx_test\n \n=\n \nnp\n.\nvstack\n([\ngrid\n[\n0\n]\n.\nravel\n(),\n \ngrid\n[\n1\n]\n.\nravel\n()])\n.\nT\n\n\n\n  \ndef\n \nanalyse\n(\nself\n):\n\n    \n\"\"\"Run classifier on test samples and split them according to labels.\"\"\"\n\n\n    \n# find labels for test samples \n\n    \nself\n.\ny_test\n \n=\n \nself\n.\nnn\n.\npredict\n(\nself\n.\nx_test\n)\n\n\n    \nself\n.\nclassified\n \n=\n \n[]\n  \n# [class I test points, class II test ...]\n\n\n    \n# loop over available labels\n\n    \nfor\n \nlabel\n \nin\n \nrange\n(\nself\n.\nnof_classes\n):\n\n      \n# if i-th label == current label -> add test[i]\n\n      \nclass_i\n \n=\n \nnp\n.\narray\n([\nself\n.\nx_test\n[\ni\n]\n \\\n                          \nfor\n \ni\n,\n \nl\n \nin\n \nenumerate\n(\nself\n.\ny_test\n)\n \\\n                          \nif\n \nl\n \n==\n \nlabel\n])\n\n      \nself\n.\nclassified\n.\nappend\n(\nclass_i\n)\n\n\n\n  \ndef\n \nplot\n(\nself\n,\n \nt\n=\n''\n):\n\n    \n\"\"\"Visualize the result of classification\"\"\"\n\n    \nplot\n \n=\n \ninit_plot\n(\nself\n.\nrange\n,\n \nself\n.\nrange\n)\n\n    \nplot\n.\nset_title\n(\nt\n)\n\n    \nplot\n.\ngrid\n(\nFalse\n)\n\n\n    \n# plot training samples\n\n    \nfor\n \ni\n,\n \nx\n \nin\n \nenumerate\n(\nself\n.\nx_train\n):\n\n      \nplot\n.\nplot\n(\n*\nx\n.\nT\n,\n \nmpl_colors\n[\ni\n]\n \n+\n \n'o'\n)\n\n\n    \n# plot test samples\n\n    \nfor\n \ni\n,\n \nx\n \nin\n \nenumerate\n(\nself\n.\nclassified\n):\n\n      \nplot\n.\nplot\n(\n*\nx\n.\nT\n,\n \nmpl_colors\n[\ni\n]\n \n+\n \n','\n)\n\n\n\n\n\n\nL1 test\n\u00b6\n\n\nl1\n \n=\n \nAnalysis\n(\nX1\n,\n \nX2\n,\n \ndistance\n=\n0\n)\n\n\nl1\n.\nprepare_test_samples\n()\n\n\nl1\n.\nanalyse\n()\n\n\nl1\n.\nplot\n()\n\n\n\n\n\n\n\n\nL2 Test\n\u00b6\n\n\nl2\n \n=\n \nAnalysis\n(\nX1\n,\n \nX2\n,\n \ndistance\n=\n1\n)\n\n\nl2\n.\nprepare_test_samples\n()\n\n\nl2\n.\nanalyse\n()\n\n\nl2\n.\nplot\n()\n\n\n\n\n\n\n\n\nMulticlass classification\n\u00b6\n\n\n\n\n\n\nTraining samples from 4 squares:\n\n\n\n\n[0, 1] x [0, 1]\n\n\n[0, 1] x [1, 2]\n\n\n[1, 2] x [0, 1]\n\n\n[1, 2] x [1, 2]\n\n\n\n\n\n\n\n\nWe expect 4 squares created by test samples grid\n\n\n\n\n\n\nHow does it depend on the size of training samples?\n\n\n\n\n\n\ndef\n \ngenerate4\n(\nn\n=\n50\n):\n\n  \n\"\"\"Generate 4 sets of random points.\"\"\"\n\n\n  \n# points from [0, 1] x [0, 1]\n\n  \nX1\n \n=\n \ngenerate_random_points\n(\nn\n,\n \n0\n,\n \n1\n)\n\n  \n# points from [1, 2] x [1, 2]\n\n  \nX2\n \n=\n \ngenerate_random_points\n(\nn\n,\n \n1\n,\n \n2\n)\n\n  \n# points from [0, 1] x [1, 2]\n\n  \nX3\n \n=\n \nnp\n.\narray\n([[\nx\n,\n \ny\n+\n1\n]\n \nfor\n \nx\n,\ny\n \nin\n \ngenerate_random_points\n(\nn\n,\n \n0\n,\n \n1\n)])\n\n  \n# points from [1, 2] x [0, 1]\n\n  \nX4\n \n=\n \nnp\n.\narray\n([[\nx\n,\n \ny\n-\n1\n]\n \nfor\n \nx\n,\ny\n \nin\n \ngenerate_random_points\n(\nn\n,\n \n1\n,\n \n2\n)])\n\n\n  \nreturn\n \nX1\n,\n \nX2\n,\n \nX3\n,\n \nX4\n\n\n\n\n\n\n# loop over no. of training samples\n\n\nfor\n \nn\n \nin\n \n(\n5\n,\n \n10\n,\n \n50\n,\n \n100\n):\n\n  \n# generate 4 sets of random points (each one with n samples)\n\n  \n# unpack them when passing to Analysis\n\n  \nc4\n \n=\n \nAnalysis\n(\n*\ngenerate4\n(\nn\n),\n \ndistance\n=\n1\n)\n\n  \nc4\n.\nprepare_test_samples\n()\n\n  \nc4\n.\nanalyse\n()\n\n  \nc4\n.\nplot\n(\n\"No. of samples = {}\"\n.\nformat\n(\nn\n))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMessage 01: size matters!\n\n\nNoise\n\u00b6\n\n\n\n\n\n\nData are rarely perfect and you may expect some training samples to have unsual features\n\n\n\n\n\n\nFeatures shared by a majority of training samples are more important than a single occurrence\n\n\n\n\n\n\nLet's add some noise to our data and see how Nearest Neighbor deal with it\n\n\n\n\n\n\n# generate 4 classes of 2D points\n\n\nX1\n,\n \nX2\n,\n \nX3\n,\n \nX4\n \n=\n \ngenerate4\n()\n\n\n\n# add some noise by applying gaussian to every point coordinates\n\n\nnoise\n \n=\n \nlambda\n \nx\n,\n \ny\n:\n \n[\nnp\n.\nrandom\n.\nnormal\n(\nx\n,\n \n0.1\n),\n \nnp\n.\nrandom\n.\nnormal\n(\ny\n,\n \n0.1\n)]\n\n\n\nX1\n \n=\n \nnp\n.\narray\n([\nnoise\n(\nx\n,\n \ny\n)\n \nfor\n \nx\n,\n \ny\n \nin\n \nX1\n])\n\n\nX2\n \n=\n \nnp\n.\narray\n([\nnoise\n(\nx\n,\n \ny\n)\n \nfor\n \nx\n,\n \ny\n \nin\n \nX2\n])\n\n\nX3\n \n=\n \nnp\n.\narray\n([\nnoise\n(\nx\n,\n \ny\n)\n \nfor\n \nx\n,\n \ny\n \nin\n \nX3\n])\n\n\nX4\n \n=\n \nnp\n.\narray\n([\nnoise\n(\nx\n,\n \ny\n)\n \nfor\n \nx\n,\n \ny\n \nin\n \nX4\n])\n\n\n\n# perform analysis\n\n\nc4\n \n=\n \nAnalysis\n(\nX1\n,\n \nX2\n,\n \nX3\n,\n \nX4\n,\n \ndistance\n=\n1\n)\n\n\nc4\n.\nprepare_test_samples\n()\n\n\nc4\n.\nanalyse\n()\n\n\nc4\n.\nplot\n()\n\n\n\n\n\n\n\n\nOverfitting\n\u00b6\n\n\n\n\n\n\nThe above is an example of overfitting\n\n\n\n\nperfectly describe training data\n\n\nlose the generalization ability\n\n\n\n\n\n\n\n\nIn general you want to extract all common features from training samples, but neglect characteristic features of single sample\n\n\n\n\n\n\nMessage 02: avoid overfitting!\n\n\nAccuracy\n\u00b6\n\n\n\n\nAccuracy defines the fraction of (unseen) samples which are correctly classify by the algorithm \n\n\n\n\naccuracy\n \n=\n \n0\n\n\n\n# loop over (sample, reconstructed label)\n\n\nfor\n \nsample\n,\n \nlabel\n \nin\n \nzip\n(\nc4\n.\nx_test\n,\n \nc4\n.\ny_test\n):\n\n  \n# determine true label\n\n  \nif\n \nsample\n[\n0\n]\n \n<\n \n1\n \nand\n \nsample\n[\n1\n]\n \n<\n \n1\n:\n\n    \ntrue_label\n \n=\n \n0\n\n  \nelif\n \nsample\n[\n0\n]\n \n>\n \n1\n \nand\n \nsample\n[\n1\n]\n \n>\n \n1\n:\n\n    \ntrue_label\n \n=\n \n1\n\n  \nelif\n \nsample\n[\n0\n]\n \n<\n \n1\n \nand\n \nsample\n[\n1\n]\n \n>\n \n1\n:\n\n    \ntrue_label\n \n=\n \n2\n\n  \nelse\n:\n\n    \ntrue_label\n \n=\n \n3\n\n\n  \nif\n \ntrue_label\n \n==\n \nlabel\n:\n \naccuracy\n \n+=\n \n1\n\n\n\naccuracy\n \n/=\n \nlen\n(\nc4\n.\nx_test\n)\n\n\n\nprint\n(\naccuracy\n)\n\n\n\n\n\n\n0.924878097076805\n\n\n\n\n\n\n\n\n\nPlease note, that this is a toy model - in the case of real problems there is no way to determine true labels (otherwise there is no point to use ML methods...)\n\n\n\n\n\n\nTo measure accuracy of the model one usually splits data into:\n\n\n\n\n\n\ntraining samples (usually about 80%)\n\n\n\n\n\n\ntest samples (usually about 20%)\n\n\n\n\n\n\n\n\n\n\nAfter the model is trained on training samples, the accuracy is measured on test samples\n\n\n\n\n\n\nMessage 03: keep some data for testing!\n\n\nk-Nearest Neighbors\n\u00b6\n\n\n\n\nInstead of letting one closest neighbor to decide, let \nk\n nearest neghbors to vote\n\n\n\n\nImplementation\n\u00b6\n\n\n\n\n\n\nWe can base the implementation on \nNearestNeighbor\n, but\n\n\n\n\n\n\nThe \nconstructor\n has an extra parameter \nk\n\n\n\n\n\n\nand we need to override \npredict\n method\n\n\n\n\n\n\nclass\n \nkNearestNeighbors\n(\nNearestNeighbor\n):\n\n  \n\"\"\"k-Nearest Neighbor Classifier\"\"\"\n\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \nk\n=\n1\n,\n \ndistance\n=\n0\n):\n\n    \n\"\"\"Set distance definition: 0 - L1, 1 - L2\"\"\"\n\n    \nsuper\n()\n.\n__init__\n(\ndistance\n)\n\n    \nself\n.\nk\n \n=\n \nk\n\n\n\n  \ndef\n \npredict\n(\nself\n,\n \nx\n):\n\n    \n\"\"\"Predict and return labels for each feature vector from x\n\n\n\n    x -- feature vectors (N x D)\n\n\n    \"\"\"\n\n    \npredictions\n \n=\n \n[]\n  \n# placeholder for N labels\n\n\n    \n# no. of classes = max label (labels starts from 0)\n\n    \nnof_classes\n \n=\n \nnp\n.\namax\n(\nself\n.\ny_train\n)\n \n+\n \n1\n\n\n    \n# loop over all test samples\n\n    \nfor\n \nx_test\n \nin\n \nx\n:\n\n      \n# array of distances between current test and all training samples\n\n      \ndistances\n \n=\n \nnp\n.\nsum\n(\nself\n.\ndistance\n(\nself\n.\nx_train\n \n-\n \nx_test\n),\n \naxis\n=\n1\n)\n\n\n      \n# placeholder for labels votes\n\n      \nvotes\n \n=\n \nnp\n.\nzeros\n(\nnof_classes\n,\n \ndtype\n=\nnp\n.\nint\n)\n\n\n      \n# find k closet neighbors and vote\n\n      \n# argsort returns the indices that would sort an array\n\n      \n# so indices of nearest neighbors\n\n      \n# we take self.k first\n\n      \nfor\n \nneighbor_id\n \nin\n \nnp\n.\nargsort\n(\ndistances\n)[:\nself\n.\nk\n]:\n\n        \n# this is a label corresponding to one of the closest neighbor\n\n        \nneighbor_label\n \n=\n \nself\n.\ny_train\n[\nneighbor_id\n]\n\n        \n# which updates votes array\n\n        \nvotes\n[\nneighbor_label\n]\n \n+=\n \n1\n\n\n      \n# predicted label is the one with most votes\n\n      \npredictions\n.\nappend\n(\nnp\n.\nargmax\n(\nvotes\n))\n\n\n    \nreturn\n \npredictions\n\n\n\n\n\n\nkAnalysis\n\u00b6\n\n\n\n\nWe also create \nkAnalysis\n based on \nAnalysis\n for visualization of kNN results\n\n\n\n\nclass\n \nkAnalysis\n(\nAnalysis\n):\n\n  \n\"\"\"Apply kNearestNeighbor to generated (uniformly) test samples.\"\"\"\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \n*\nx\n,\n \nk\n=\n1\n,\n \ndistance\n=\n1\n):\n\n    \n\"\"\"Generate labels and initilize classifier\n\n\n\n    x -- feature vectors arrays\n\n\n    k -- number of nearest neighbors\n\n\n    distance -- 0 for L1, 1 for L2    \n\n\n    \"\"\"\n\n    \n# get number of classes\n\n    \nself\n.\nnof_classes\n \n=\n \nlen\n(\nx\n)\n\n\n    \n# create lables array\n\n    \ny\n \n=\n \n[\ni\n \n*\n \nnp\n.\nones\n(\n_x\n.\nshape\n[\n0\n],\n \ndtype\n=\nnp\n.\nint\n)\n \nfor\n \ni\n,\n \n_x\n \nin\n \nenumerate\n(\nx\n)]\n\n    \ny\n \n=\n \nnp\n.\narray\n(\ny\n)\n.\nravel\n()\n\n\n    \n# save training samples to plot them later\n\n    \nself\n.\nx_train\n \n=\n \nx\n\n\n    \n# merge feature vector arrays for NearestNeighbor\n\n    \nx\n \n=\n \nnp\n.\nconcatenate\n(\nx\n,\n \naxis\n=\n0\n)\n\n\n    \n# train classifier (knn this time)\n\n    \nself\n.\nnn\n \n=\n \nkNearestNeighbors\n(\nk\n,\n \ndistance\n)\n\n    \nself\n.\nnn\n.\ntrain\n(\nx\n,\n \ny\n)\n\n\n\n\n\n\nSanity check\n\u00b6\n\n\n\n\nk-Nearest Neighbor classifier with \nk = 1\n must give exactly the same results as Nearest Neighbor\n\n\n\n\n# apply kNN with k=1 on the same set of training samples\n\n\nknn\n \n=\n \nkAnalysis\n(\nX1\n,\n \nX2\n,\n \nX3\n,\n \nX4\n,\n \nk\n=\n1\n,\n \ndistance\n=\n1\n)\n\n\nknn\n.\nprepare_test_samples\n()\n\n\nknn\n.\nanalyse\n()\n\n\nknn\n.\nplot\n()\n\n\n\n\n\n\n\n\nk-Test\n\u00b6\n\n\n\n\n\n\nFor \nk = 1\n kNN is likely to overfit the problem\n\n\n\n\n\n\nAlthough, it does not mean that higher \nk\n is better!\n\n\n\n\n\n\nNow, let's see how different values of \nk\n affects the result\n\n\n\n\n\n\nLater, we will learn how to find optimal value of \nk\n for given problem\n\n\n\n\n\n\n# training size = 50\n\n\n# let's check a few values between 1 and 50\n\n\nfor\n \nk\n \nin\n \n(\n1\n,\n \n5\n,\n \n10\n,\n \n50\n):\n\n  \nknn\n \n=\n \nkAnalysis\n(\nX1\n,\n \nX2\n,\n \nX3\n,\n \nX4\n,\n \nk\n=\nk\n,\n \ndistance\n=\n1\n)\n\n  \nknn\n.\nprepare_test_samples\n()\n\n  \nknn\n.\nanalyse\n()\n\n  \nknn\n.\nplot\n(\n\"k = {}\"\n.\nformat\n(\nk\n))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHyperparameters\n\u00b6\n\n\n\n\n\n\nML model may have some hyperparameters - parameters set before training\n\n\n\n\n\n\nPlease note, ML algorithm may have also parameters which are set during training\n\n\n\n\n\n\nIn the case of kNN there are two hyperparameters:\n\n\n\n\n\n\nnumber of nearest neihgbors (\nk\n)\n\n\n\n\n\n\nthe definition of distance\n\n\n\n\n\n\n\n\n\n\nThe choice of hyperparameters values highly depends on a problem\n\n\n\n\n\n\nThe wrong choice of hyperparameters may lead to underfitting or overfitting\n\n\n\n\n\n\nOver-, under-fitting example\n\u00b6\n\n\n# generate random data from x^2 function (with some noise)\n\n\ndata\n \n=\n \nnp\n.\narray\n([[\nx\n,\n \nnp\n.\nrandom\n.\nnormal\n(\nx\n**\n2\n,\n \n0.1\n)]\n \\\n                 \nfor\n \nx\n \nin\n \n2\n*\nnp\n.\nrandom\n.\nrandom\n(\n10\n)\n \n-\n \n1\n])\n\n\n\nplot\n \n=\n \ninit_plot\n([\n-\n1\n,\n \n1\n],\n \n[\n-\n1\n,\n \n1\n])\n\n\nplot\n.\nplot\n(\n*\ndata\n.\nT\n,\n \n'o'\n);\n\n\n\n\n\n\n\n\n\n\n\n\nLet's try to fit this data to a polynomial\n\n\n\n\n\n\nThe degree is a hyperparamter (which defines number of coefficients)\n\n\n\n\n\n\n# loop over degrees of polynomial\n\n\n# data is x^2, so let's try degrees 1, 2, 10\n\n\nfor\n \nn\n \nin\n \n(\n1\n,\n \n2\n,\n \n10\n):\n\n  \n# polyfit returns an array with polynomial coefficients\n\n  \n# poly1d is a polynomial class\n\n  \nf\n \n=\n \nnp\n.\npoly1d\n(\nnp\n.\npolyfit\n(\n*\ndata\n.\nT\n,\n \nn\n))\n\n\n  \n# returns an array with 100 uniformly distributed numbers from -1 to 1\n\n  \nx\n \n=\n \nnp\n.\nlinspace\n(\n-\n1\n,\n \n1\n,\n \n100\n)\n\n\n  \nplot\n \n=\n \ninit_plot\n([\n-\n1\n,\n \n1\n],\n \n[\n-\n1\n,\n \n1\n])\n\n  \nplot\n.\nset_title\n(\n\"n = {}\"\n.\nformat\n(\nn\n))\n\n  \nplot\n.\nplot\n(\n*\ndata\n.\nT\n,\n \n'o'\n,\n \nx\n,\n \nf\n(\nx\n))\n\n\n\n\n\n\n/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RankWarning: Polyfit may be poorly conditioned\n  after removing the cwd from sys.path.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor \nn = 1\n we clearly underfit the data as we do not have enough parameters to describe the complexity of the problem\n\n\n\n\n\n\nFor \nn = 2\n we have appropriate capacity (as we actually generated data form \nx^2\nx^2\n function)\n\n\n\n\n\n\nFor \nn = 10\n we overfit the data - training samples are described perfectly, but we clearly lost the generalization ability\n\n\n\n\n\n\nMessage 04: right choice of hyperparameters is crucial!\n\n\nValidation dataset\n\u00b6\n\n\n\n\n\n\nOne splits data into training and test samples\n\n\n\n\n\n\ntraining samples are used to optimize model parameters\n\n\n\n\n\n\ntest samples are used to measure accuracy\n\n\n\n\n\n\nthere is no rule of thumb on how to split dataset\n\n\n\n\n\n\n\n\n\n\nIf a model has some hyperparameters the part of training set is used for valitation samples:\n\n\n\n\n\n\ntraining samples - tuning model parameters\n\n\n\n\n\n\nvalidation samples - tuning hyperparameters\n\n\n\n\n\n\n\n\n\n\n                  +---------------------+      +------------------------+\n+----------+      |                     |      |                        |\n|          |      | Measure accuracy on |      | Measure final accuracy |\n| Training | +--> |                     | +--> |                        |\n|          |      | validation samples  |      | on test samples        |\n+----------+      |                     |      |                        |\n     ^            +----------+----------+      +------------------------+\n     |                       |\n     |      Change           | \n     +-----------------------+\n         hyperparameters\n\n\n\n\n\nIris dataset\n\u00b6\n\n\n\n\n\n\nThe data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. \nsrc\n\n\n\n\n\n\nAttribute Information:\n\n\n\n\n\n\nsepal length in cm\n\n\n\n\n\n\nsepal width in cm\n\n\n\n\n\n\npetal length in cm\n\n\n\n\n\n\npetal width in cm\n\n\n\n\n\n\nclass: \n\n\n\n\n\n\nIris Setosa\n\n\n\n\n\n\nIris Versicolour\n\n\n\n\n\n\nIris Virginica\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoad dataset\n\u00b6\n\n\n\n\nWe use \npandas\n for data manipulation - it is super handy and supports many formats\n\n\n\n\nimport\n \npandas\n \nas\n \npd\n\n\n\n# columns names - can be used to access columns later\n\n\ncolumns\n \n=\n \n[\n\"Sepal Length\"\n,\n \n\"Sepal Width\"\n,\n\n           \n\"Petal Length\"\n,\n \n\"Petal Width\"\n,\n\n           \n\"Class\"\n]\n\n\n\n# iris.data is a csv file\n\n\nsrc\n \n=\n \n\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n\n\n\n# load the file with pandas.read_csv \n\n\n# it will name columns as defined in columns list\n\n\n# so one can access a column through index or name\n\n\niris_data\n \n=\n \npd\n.\nread_csv\n(\nsrc\n,\n \nheader\n=\nNone\n,\n \nnames\n=\ncolumns\n)\n\n\n\n\n\n\niris_data\n.\nhead\n()\n  \n# print a few first entries\n\n\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nSepal Length\n\n      \nSepal Width\n\n      \nPetal Length\n\n      \nPetal Width\n\n      \nClass\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n5.1\n\n      \n3.5\n\n      \n1.4\n\n      \n0.2\n\n      \nIris-setosa\n\n    \n\n    \n\n      \n1\n\n      \n4.9\n\n      \n3.0\n\n      \n1.4\n\n      \n0.2\n\n      \nIris-setosa\n\n    \n\n    \n\n      \n2\n\n      \n4.7\n\n      \n3.2\n\n      \n1.3\n\n      \n0.2\n\n      \nIris-setosa\n\n    \n\n    \n\n      \n3\n\n      \n4.6\n\n      \n3.1\n\n      \n1.5\n\n      \n0.2\n\n      \nIris-setosa\n\n    \n\n    \n\n      \n4\n\n      \n5.0\n\n      \n3.6\n\n      \n1.4\n\n      \n0.2\n\n      \nIris-setosa\n\n    \n\n  \n\n\n\n\n\n\n\nVisualize dataset\n\u00b6\n\n\n\n\n\n\npandas\n offers plotting through \nmatplotlib\n integration\n\n\n\n\n\n\nLet's visualize Iris data\n\n\n\n\n\n\nLet's keep the code short - sorry if it is hard to follow\n\n\n\n\n\n\n# to extract rows with class column == class_name\n\n\nextract\n \n=\n \nlambda\n \nclass_name\n:\n \niris_data\n.\nloc\n[\niris_data\n[\n'Class'\n]\n \n==\n \nclass_name\n]\n\n\n\n# axes settings - part = Sepal or Petal; x = Length, y = Width\n\n\nset_ax\n \n=\n \nlambda\n \npart\n:\n \n{\n\"x\"\n:\n \npart\n \n+\n \n\" Length\"\n,\n\n                       \n\"y\"\n:\n \npart\n \n+\n \n\" Width\"\n,\n\n                       \n\"kind\"\n:\n \n\"scatter\"\n}\n\n\n\n# add iris type / sepal or petal / color to existing axis\n\n\nplot\n \n=\n \nlambda\n \nclass_name\n,\n \npart\n,\n \ncolor\n,\n \naxis\n:\n \\\n  \nextract\n(\nclass_name\n)\n.\nplot\n(\n**\nset_ax\n(\npart\n),\n\n                           \ncolor\n=\ncolor\n,\n\n                           \nlabel\n=\nclass_name\n,\n\n                           \nax\n=\naxis\n)\n\n\n\n# plot all Iris types (sepal or petal) on existing axis\n\n\nplot_all\n \n=\n \nlambda\n \npart\n,\n \naxis\n:\n \\\n  \n[\nplot\n(\niris\n,\n \npart\n,\n \nmpl_colors\n[\ni\n],\n \naxis\n)\n \\\n   \nfor\n \ni\n,\n \niris\n \nin\n \nenumerate\n(\nset\n(\niris_data\n[\n'Class'\n]))]\n \n\n\n\n\n\n# with pyplot.subplots we can create many plots on one figure\n\n\n# here we create 2 plots - 1 row and 2 columns\n\n\n# thus, subplots returns figure, axes of 1st plot, axes for 2nd plot\n\n\n_\n,\n \n(\nax1\n,\n \nax2\n)\n \n=\n \nplt\n.\nsubplots\n(\n1\n,\n \n2\n,\n \nfigsize\n=\n(\n9\n,\n4\n))\n\n\n\n# using messy lambda we can plot all Iris types at once\n\n\n# Petal data on 1st plots and Sepal data on 2nd plot\n\n\nplot_all\n(\n\"Petal\"\n,\n \nax1\n)\n\n\nplot_all\n(\n\"Sepal\"\n,\n \nax2\n)\n\n\n\n# tight_layout adjust subplots params so they fit into figure ares\n\n\nplt\n.\ntight_layout\n()\n\n\n\n\n\n\n\n\nPrepare feature vectors and labels\n\u00b6\n\n\n\n\n\n\nFirst step is to prepare data - we need feature vectors with corresponding labels\n\n\n\n\n\n\nIn this case every sample's feature vector is 4D (sepal length, sepal width, petal length, petal width) and is labeled with one of three classes (Iris Setosa, Iris Versicolour, Iris Virginica)\n\n\n\n\n\n\n# every Iris has 4 features (forming our 4D feature vectors)\n\n\n# pandaoc.DataFrame.iloc allows us access data through indices\n\n\n# we create an array with feature vectors by taking all rows for first 4 columns\n\n\nX\n \n=\n \niris_data\n.\niloc\n[:,\n \n:\n4\n]\n\n\n\n# it is still pandoc.DataFrame object - pretty handy\n\n\nX\n.\nhead\n()\n\n\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nSepal Length\n\n      \nSepal Width\n\n      \nPetal Length\n\n      \nPetal Width\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n5.1\n\n      \n3.5\n\n      \n1.4\n\n      \n0.2\n\n    \n\n    \n\n      \n1\n\n      \n4.9\n\n      \n3.0\n\n      \n1.4\n\n      \n0.2\n\n    \n\n    \n\n      \n2\n\n      \n4.7\n\n      \n3.2\n\n      \n1.3\n\n      \n0.2\n\n    \n\n    \n\n      \n3\n\n      \n4.6\n\n      \n3.1\n\n      \n1.5\n\n      \n0.2\n\n    \n\n    \n\n      \n4\n\n      \n5.0\n\n      \n3.6\n\n      \n1.4\n\n      \n0.2\n\n    \n\n  \n\n\n\n\n\n\n\n\n\npandas.DataFrame\n object are handy to manipulate data, but at the end of the day we want to perform algebra with \nnumpy\n\n\n\n\n# create numpy array (matrix) for further processing\n\n\nX\n \n=\n \nnp\n.\narray\n(\nX\n)\n\n\n\n# print a few first entries\n\n\nprint\n(\nX\n[:\n5\n])\n\n\n\n\n\n\n[[5.1 3.5 1.4 0.2]\n [4.9 3.  1.4 0.2]\n [4.7 3.2 1.3 0.2]\n [4.6 3.1 1.5 0.2]\n [5.  3.6 1.4 0.2]]\n\n\n\n\n\n\n\nfrom the las column (\"Class\") we create our labels\n\n\n\n\n# as mentioned before, we can access DataFrame object through column labels\n\n\nY\n \n=\n \nnp\n.\narray\n(\niris_data\n[\n\"Class\"\n])\n\n\n\n# print a few first entries\n\n\nprint\n(\nY\n[:\n5\n])\n\n\n\n\n\n\n['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa']\n\n\n\n\n\nPrepare test dataset\n\u00b6\n\n\n\n\n\n\nLet's use 80% for training and 20% for testing\n\n\n\n\n\n\nWe, obviously, can not just take last 20% of samples for testing because our data is ordered\n\n\n\n\n\n\nBut we can randomly select 20% of samples\n\n\n\n\n\n\nEasy to do by hand, but let's start to use some ML frameworks\n\n\n\n\n\n\nfrom\n \nsklearn.model_selection\n \nimport\n \ntrain_test_split\n \nas\n \nsplit\n\n\n\n# train_test_split: Split arrays or matrices into random train and test subsets\n\n\nX_train\n,\n \nX_test\n,\n \nY_train\n,\n \nY_test\n \n=\n \nsplit\n(\nX\n,\n \nY\n,\n \ntest_size\n=\n0.2\n)\n\n\n\n# let's use 20% of training samples for validation\n\n\nX_train\n,\n \nX_valid\n,\n \nY_train\n,\n \nY_valid\n \n=\n \nsplit\n(\nX_train\n,\n \nY_train\n,\n \ntest_size\n=\n0.2\n)\n\n\n\n# check how many sample we have\n\n\nprint\n(\nX_train\n.\nshape\n[\n0\n],\n \nX_valid\n.\nshape\n[\n0\n],\n \nX_test\n.\nshape\n[\n0\n])\n\n\n\n\n\n\n96 24 30\n\n\n\n\n\nkNN from scikit-learn\n\u00b6\n\n\n\n\n\n\nscikit-learn\n has already implemented k-Nearest Neighbor algorithm (which is more flexible than the one implemented during this lecture)\n\n\n\n\n\n\nLet's see how \ncomplicated\n is using one of ML frameworks with Python\n\n\n\n\n\n\nfrom\n \nsklearn.neighbors\n \nimport\n \nKNeighborsClassifier\n\n\n\n# create knn classifier with k = 48\n\n\nknn\n \n=\n \nKNeighborsClassifier\n(\nn_neighbors\n=\n48\n)\n\n\n\n# train the model\n\n\nknn\n.\nfit\n(\nX_train\n,\n \nY_train\n)\n\n\n\n# predict labels for test samples\n\n\nY_pred\n \n=\n \nknn\n.\npredict\n(\nX_valid\n)\n\n\n\n\n\n\nAccuracy\n\u00b6\n\n\n\n\nFirst let's print true labels along with predicted ones\n\n\n\n\n# use bold if true != predicted\n\n\nfor\n \ntrue\n,\n \npred\n \nin\n \nzip\n(\nY_valid\n,\n \nY_pred\n):\n\n  \nif\n \npred\n \n==\n \ntrue\n:\n\n    \nprint\n(\n\"{}\n\\t\n -> {}\"\n.\nformat\n(\ntrue\n,\n \npred\n))\n\n  \nelse\n:\n\n    \nprint\n(\n\"\n\\033\n[1m{}\n\\t\n -> {}\n\\033\n[0m\"\n.\nformat\n(\ntrue\n,\n \npred\n))\n\n\n\n\n\n\nIris-setosa  -> Iris-setosa\nIris-versicolor  -> Iris-versicolor\nIris-setosa  -> Iris-setosa\nIris-versicolor  -> Iris-versicolor\nIris-virginica   -> Iris-virginica\nIris-virginica   -> Iris-virginica\n\u001b[1mIris-versicolor  -> Iris-virginica\u001b[0m\nIris-virginica   -> Iris-virginica\nIris-versicolor  -> Iris-versicolor\nIris-setosa  -> Iris-setosa\nIris-virginica   -> Iris-virginica\nIris-versicolor  -> Iris-versicolor\nIris-virginica   -> Iris-virginica\n\u001b[1mIris-virginica   -> Iris-versicolor\u001b[0m\nIris-virginica   -> Iris-virginica\nIris-virginica   -> Iris-virginica\nIris-versicolor  -> Iris-versicolor\nIris-setosa  -> Iris-setosa\nIris-setosa  -> Iris-setosa\nIris-virginica   -> Iris-virginica\n\u001b[1mIris-virginica   -> Iris-versicolor\u001b[0m\nIris-setosa  -> Iris-setosa\nIris-versicolor  -> Iris-versicolor\n\u001b[1mIris-virginica   -> Iris-versicolor\u001b[0m\n\n\n\n\n\n\n\nWe can easily calculate accuracy by hand as it is just a number of correctly predicted labels divided by no. of samples\n\n\n\n\n# Y_valid == Y_pred -> array of True/False (if two elements are equal or not)\n\n\n# (Y_valid == Y_pred).sum() -> number of Trues\n\n\n# Y_valid.shape[0] -> number of validation samples\n\n\naccuracy\n \n=\n \n(\nY_valid\n \n==\n \nY_pred\n)\n.\nsum\n()\n \n/\n \nY_valid\n.\nshape\n[\n0\n]\n\n\n\nprint\n(\naccuracy\n)\n\n\n\n\n\n\n0.8333333333333334\n\n\n\n\n\n\n\nBut we can also use \nscikit-learn\n function \naccuracy_score\n\n\n\n\nfrom\n \nsklearn.metrics\n \nimport\n \naccuracy_score\n\n\n\nprint\n(\naccuracy_score\n(\nY_valid\n,\n \nY_pred\n))\n\n\n\n\n\n\n0.8333333333333334\n\n\n\n\n\nk-dependence of the accuracy\n\u00b6\n\n\n\n\n\n\nLet's use validation set to determine the best hyperparameter \nk\n\n\n\n\n\n\nWe will run kNN for various values of \nk\n and measure accuracy\n\n\n\n\n\n\nThis will allow us to find the optimal value of \nk\n\n\n\n\n\n\nAnd check the accuracy on the test dataset\n\n\n\n\n\n\nscores\n \n=\n \n[]\n  \n# placeholder for accuracy\n\n\n\nmax_k\n \n=\n \n85\n  \n# maximum number of voters\n\n\n\n# loop over different values of k\n\n\nfor\n \nk\n \nin\n \nrange\n(\n1\n,\n \nmax_k\n):\n\n  \n# create knn classifier with k = k\n\n  \nknn\n \n=\n \nKNeighborsClassifier\n(\nn_neighbors\n=\nk\n)\n\n\n  \n# train the model\n\n  \nknn\n.\nfit\n(\nX_train\n,\n \nY_train\n)\n\n\n  \n# predict labels for test samples\n\n  \nY_pred\n \n=\n \nknn\n.\npredict\n(\nX_valid\n)\n\n\n  \n# add accuracy to score table\n\n  \nscores\n.\nappend\n(\naccuracy_score\n(\nY_valid\n,\n \nY_pred\n))\n\n\n\n\n\n\n\n\nNow, we can plot accuracy as a function of \nk\n\n\n\n\ndef\n \nk_accuracy_plot\n(\nmax_k\n=\n85\n):\n\n  \n\"\"\"Just plot settings\"\"\"\n\n  \nplt\n.\ngrid\n(\nTrue\n)\n\n  \nplt\n.\nxlabel\n(\n\"k\"\n)\n\n  \nplt\n.\nylabel\n(\n\"Accuracy\"\n)\n\n  \nplt\n.\nxlim\n([\n0\n,\n \nmax_k\n \n+\n \n5\n])\n\n  \nplt\n.\nylim\n([\n0\n,\n \n1\n])\n\n  \nplt\n.\nxticks\n(\nrange\n(\n0\n,\n \nmax_k\n \n+\n \n5\n,\n \n5\n))\n\n\n  \nreturn\n \nplt\n\n\n\nk_accuracy_plot\n()\n.\nplot\n(\nrange\n(\n1\n,\n \nmax_k\n),\n \nscores\n);\n\n\n\n\n\n\n\n\n\n\nAnd check the accuracy measured on the test samples\n\n\n\n\nknn\n \n=\n \nKNeighborsClassifier\n(\nn_neighbors\n=\n9\n)\n\n\nknn\n.\nfit\n(\nX_train\n,\n \nY_train\n)\n\n\nY_pred\n \n=\n \nknn\n.\npredict\n(\nX_test\n)\n\n\n\nprint\n(\naccuracy_score\n(\nY_test\n,\n \nY_pred\n))\n\n\n\n\n\n\n0.9666666666666667\n\n\n\n\n\n\n\n\n\nThe accuracy plot is not smooth\n\n\n\n\n\n\nIt is common if one does not have enough validation samples\n\n\n\n\n\n\nBut there is another way to measure accuracy dependence on hyperparameters\n\n\n\n\n\n\nCross-validation\n\u00b6\n\n\n         Split training samples into N folds\n\n +-------+   +-------+   +-------+         +-------+\n |       |   |       |   |       |         |       |\n |   1   |   |   2   |   |   3   |   ...   |   N   |\n |       |   |       |   |       |         |       |\n +-------+   +-------+   +-------+         +-------+\n\nTake one fold as validation set and train on N-1 folds\n\n +-------+   +-------+   +-------+         +-------+\n |*******|   |       |   |       |         |       |\n |*******|   |   2   |   |   3   |   ...   |   N   |\n |*******|   |       |   |       |         |       |\n +-------+   +-------+   +-------+         +-------+\n\n         Take the next one as validation set\n\n +-------+   +-------+   +-------+         +-------+\n |       |   |*******|   |       |         |       |\n |   1   |   |*******|   |   3   |   ...   |   N   |\n |       |   |*******|   |       |         |       |\n +-------+   +-------+   +-------+         +-------+\n\n          Repeat the procedure for all folds\n\n +-------+   +-------+   +-------+         +-------+\n |       |   |       |   |       |         |*******|\n |   1   |   |   2   |   |   3   |   ...   |*******|\n |       |   |       |   |       |         |*******|\n +-------+   +-------+   +-------+         +-------+\n\n            And average out the accuracy\n\n\n\n\n\n\n\nOnce again \nscikit-learn\n has already implemented the procedure we need\n\n\n\n\nfrom\n \nsklearn.model_selection\n \nimport\n \ncross_val_score\n\n\n\n# this time we do not create dedicated validation set\n\n\nX_train\n,\n \nX_test\n,\n \nY_train\n,\n \nY_test\n \n=\n \nsplit\n(\nX\n,\n \nY\n,\n \ntest_size\n=\n0.2\n)\n\n\n\navg_scores\n \n=\n \n[]\n  \n# average score for different k\n\n\n\nnof_folds\n \n=\n \n10\n\n\n\n# loop over different values of k\n\n\nfor\n \nk\n \nin\n \nrange\n(\n1\n,\n \nmax_k\n):\n\n  \n# create knn classifier with k = k\n\n  \nknn\n \n=\n \nKNeighborsClassifier\n(\nn_neighbors\n=\nk\n)\n\n\n  \n# cross-validate knn on our training sample with nof_folds\n\n  \nscores\n \n=\n \ncross_val_score\n(\nknn\n,\n \nX_train\n,\n \nY_train\n,\n\n                           \ncv\n=\nnof_folds\n,\n \nscoring\n=\n'accuracy'\n)\n\n\n  \n# add avg accuracy to score table\n\n  \navg_scores\n.\nappend\n(\nscores\n.\nmean\n())\n\n\n\n\n\n\nk_accuracy_plot\n()\n.\nplot\n(\nrange\n(\n1\n,\n \nmax_k\n),\n \navg_scores\n);\n\n\n\n\n\n\n\n\n\n\n\n\nIn theory, k-fold cross-validation is the way to go (especially if a dataset is small)\n\n\n\n\n\n\nIn practice, people tend to use a single validation split as it is not that computational expensive\n\n\n\n\n\n\nData normalization\n\u00b6\n\n\n\n\n\n\nSometimes there is a need to preprocess data before training\n\n\n\n\n\n\nLet's imagine Iris sepal data is in cm but petal data in mm\n\n\n\n\n\n\n# original data - both in cm\n\n\nprint\n(\nX\n[:\n5\n])\n\n\n\n\n\n\n[[5.1 3.5 1.4 0.2]\n [4.9 3.  1.4 0.2]\n [4.7 3.2 1.3 0.2]\n [4.6 3.1 1.5 0.2]\n [5.  3.6 1.4 0.2]]\n\n\n\n\n\n# make a copy of X\n\n\nXmm\n \n=\n \nX\n.\ncopy\n()\n\n\n\n# and multiply last two columns by 0.1\n\n\nXmm\n[:,\n2\n:]\n \n*=\n \n0.1\n\n\n\n# and we have our fake Iris data with petal length/width in mm\n\n\nprint\n(\nXmm\n[:\n5\n])\n\n\n\n\n\n\n[[5.1  3.5  0.14 0.02]\n [4.9  3.   0.14 0.02]\n [4.7  3.2  0.13 0.02]\n [4.6  3.1  0.15 0.02]\n [5.   3.6  0.14 0.02]]\n\n\n\n\n\n\n\nLet's compare result of the same classifier on both dataset\n\n\n\n\ndef\n \nget_accuracy\n(\nX\n,\n \nY\n,\n \nk\n=\n10\n):\n\n  \n\"\"\"Make training and test datasets and process through kNN\"\"\"\n\n\n  \n# prepare training / test samples\n\n  \nX_train\n,\n \nX_test\n,\n \nY_train\n,\n \nY_test\n \n=\n \nsplit\n(\nX\n,\n \nY\n,\n \ntest_size\n=\n0.2\n)\n\n\n  \n# create a kNN with k = k\n\n  \nknn\n \n=\n \nKNeighborsClassifier\n(\nn_neighbors\n=\nk\n)\n\n\n  \n# get prediction for original dataset\n\n  \nknn\n.\nfit\n(\nX_train\n,\n \nY_train\n)\n\n  \nY_pred\n \n=\n \nknn\n.\npredict\n(\nX_test\n)\n\n\n  \nreturn\n \naccuracy_score\n(\nY_test\n,\n \nY_pred\n)\n\n\n\ncm\n \n=\n \nget_accuracy\n(\nX\n,\n \nY\n)\n\n\nmm\n \n=\n \nget_accuracy\n(\nXmm\n,\n \nY\n)\n\n\n\nprint\n(\n\"Accuracy:\n\\n\\t\nboth in cm: {}\n\\n\\t\npetal in mm: {}\"\n.\nformat\n(\ncm\n,\n \nmm\n))\n\n\n\n\n\n\nAccuracy:\n    both in cm: 1.0\n    petal in mm: 0.7\n\n\n\n\n\n\n\n\n\nIt is kind of obvious here - petal information will barely contribute to the distance\n\n\n\n\n\n\nHowever, it is not always obvious if some features are not suppressed by the way data is normalized\n\n\n\n\n\n\nMessage 05: be aware of data normalization!\n\n\nMNIST\n\u00b6\n\n\n\n\n\n\nTHE MNIST DATABASE of handwritten digits\n\n\n\n\n\n\nThe MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n\n\n\n\n\n\nIt is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.\n\n\n\n\n\n\n\n\n\n\n\n\nTo make it simpler (and faster) let's use digits toy dataset which comes with \nscikit-learn\n \nsrc\n\n\n\n\n\n\nEach datapoint is a 8x8 image of a digit.\n\n\n\n\n\n\nAbout 180 samples per class (digit)\n\n\n\n\n\n\nTotal number of samples 1797\n\n\n\n\n\n\nLoad digits\n\u00b6\n\n\nfrom\n \nsklearn.datasets\n \nimport\n \nload_digits\n\n\n\ndigits\n \n=\n \nload_digits\n()\n\n\n\nprint\n(\ndigits\n.\ndata\n.\nshape\n)\n\n\n\n\n\n\n(1797, 64)\n\n\n\n\n\n\n\n\n\ndigits.images\n is a \nnumpy\n array with 1797 \nnumpy\n arrays 8x8 (feature vectors) representing digits\n\n\n\n\n\n\ndigits.target\n is a \nnumpy\n array with 1797 integer numbers (class labels)\n\n\n\n\n\n\nthe code below allow us to visualize a random digits from the dataset\n\n\n\n\n\n\n# set grayscale\n\n\nplt\n.\ngray\n()\n\n\n\n# get some random index from 0 to dataset size\n\n\nrandom_index\n \n=\n \nnp\n.\nrandom\n.\nrandint\n(\n1796\n)\n\n\n\n\n# draw random digit\n\n\nplt\n.\nmatshow\n(\ndigits\n.\nimages\n[\nrandom_index\n])\n\n\n\n# and print the matrix\n\n\nplt\n.\ntext\n(\n8\n,\n \n5\n,\n \ndigits\n.\nimages\n[\nrandom_index\n],\n\n         \nfontdict\n=\n{\n'family'\n:\n \n'monospace'\n,\n \n'size'\n:\n \n16\n})\n\n\n\n# and the label\n\n\nplt\n.\ntext\n(\n10\n,\n \n1\n,\n \n\"This is: {}\"\n.\nformat\n(\ndigits\n.\ntarget\n[\nrandom_index\n]),\n\n         \nfontdict\n=\n{\n'family'\n:\n \n'monospace'\n,\n \n'size'\n:\n \n16\n});\n\n\n\n\n\n\n<matplotlib.figure.Figure at 0x7faccc90a048>\n\n\n\n\n\n\n\nDistance between images\n\u00b6\n\n\n  TEST      TRAIN    PIXEL-WISE\n| 4 2 0     2 5 8 |   |2 3 8|\n| 5 3 9  -  2 8 1 | = |3 5 8|  ->  38\n| 0 2 3     1 4 9 |   |1 2 6|\n\n\n\n\n\nPrepare data\n\u00b6\n\n\n\n\n\n\nWe need to split dataset to training and test samples\n\n\n\n\n\n\nHowever, images are in 8x8 format and we have to flatten them first\n\n\n\n\n\n\n# the original shape of an image\n\n\nprint\n(\ndigits\n.\nimages\n.\nshape\n)\n\n\n\n\n\n\n(1797, 8, 8)\n\n\n\n\n\n# numpy.reshape is handy here\n\n\nprint\n(\ndigits\n.\nimages\n.\nreshape\n((\n1797\n,\n \n-\n1\n))\n.\nshape\n)\n\n\n\n\n\n\n(1797, 64)\n\n\n\n\n\n\n\n\n\nPlease note -1 in new shape\n\n\n\n\n\n\nnumpy.reshape\n allows us to pass one \nunknown\n dimension which can be determined automatically\n\n\n\n\n\n\nThus, the above is equivalent to\n\n\n\n\n\n\nprint\n(\ndigits\n.\nimages\n.\nreshape\n((\n1797\n,\n \n64\n))\n.\nshape\n)\n\n\n\n\n\n\n(1797, 64)\n\n\n\n\n\nprint\n(\ndigits\n.\nimages\n.\nreshape\n((\n-\n1\n,\n \n64\n))\n.\nshape\n)\n\n\n\n\n\n\n(1797, 64)\n\n\n\n\n\n\n\nAs before, we can split our dataset using \nsklearn.model_selection.train_test_split\n\n\n\n\ndata_train\n,\n \ndata_test\n,\n \nlabel_train\n,\n \nlabel_test\n \n=\n \\\n  \nsplit\n(\ndigits\n.\nimages\n.\nreshape\n((\n1797\n,\n \n-\n1\n)),\n \ndigits\n.\ntarget\n,\n \ntest_size\n=\n0.2\n)\n\n\n\n\n\n\nCross-validation\n\u00b6\n\n\n\n\nWe perform cross-validation on training samples to determine the best \nk\n (as for the Iris dataset)\n\n\n\n\navg_scores\n \n=\n \n[]\n  \n# average score for different k\n\n\n\nmax_k\n \n=\n \n50\n\n\nnof_folds\n \n=\n \n10\n\n\n\n# loop over different values of k\n\n\nfor\n \nk\n \nin\n \nrange\n(\n1\n,\n \nmax_k\n):\n\n  \n# create knn classifier with k = k\n\n  \nknn\n \n=\n \nKNeighborsClassifier\n(\nn_neighbors\n=\nk\n)\n\n\n  \n# cross-validate knn on our training sample with nof_folds\n\n  \nscores\n \n=\n \ncross_val_score\n(\nknn\n,\n \ndata_train\n,\n \nlabel_train\n,\n\n                           \ncv\n=\nnof_folds\n,\n \nscoring\n=\n'accuracy'\n)\n\n\n  \n# add avg accuracy to score table\n\n  \navg_scores\n.\nappend\n(\nscores\n.\nmean\n())\n\n\n\n\n\n\nplt\n.\ngrid\n(\nTrue\n)\n\n\nplt\n.\nxlabel\n(\n\"k\"\n)\n\n\nplt\n.\nylabel\n(\n\"Accuracy\"\n)\n\n\nplt\n.\nxlim\n([\n0\n,\n \nmax_k\n])\n\n\nplt\n.\nylim\n([\n0\n,\n \n1\n])\n\n\nplt\n.\nxticks\n(\nrange\n(\n0\n,\n \nmax_k\n,\n \n5\n))\n\n\n\nplt\n.\nplot\n(\nrange\n(\n1\n,\n \nmax_k\n),\n \navg_scores\n);\n\n\n\n\n\n\n\n\n\n\n\n\nWe used nearly the same procedure as for the Iris dataset\n\n\n\n\n\n\nNote, that digits toy dataset prefer different \nk\n\n\n\n\n\n\nThis is the idea of ML - the same algorithm can solve different problems if train on different data\n\n\n\n\n\n\nNowadays, in ML field \ndata is more important than algorithms\n (we have good algorithms already) \n\n\n\n\n\n\nFinal test\n\u00b6\n\n\n\n\nLet's take the bes \nk\n and check how the classifier works on test samples\n\n\n\n\nfrom\n \nsklearn.metrics\n \nimport\n \naccuracy_score\n\n\n\nknn\n \n=\n \nKNeighborsClassifier\n(\nn_neighbors\n=\n1\n)\n\n\nknn\n.\nfit\n(\ndata_train\n,\n \nlabel_train\n)\n\n\nprediction\n \n=\n \nknn\n.\npredict\n(\ndata_test\n)\n\n\n\nprint\n(\naccuracy_score\n(\nlabel_test\n,\n \nprediction\n))\n\n\n\n\n\n\n0.9888888888888889\n\n\n\n\n\n\n\nWe can take a look at misclassified digits\n\n\n\n\nfor\n \ni\n,\n \n(\ntrue\n,\n \npredict\n)\n \nin\n \nenumerate\n(\nzip\n(\nlabel_test\n,\n \nprediction\n)):\n\n  \nif\n \ntrue\n \n!=\n \npredict\n:\n\n    \ndigit\n \n=\n \ndata_test\n[\ni\n]\n.\nreshape\n((\n8\n,\n \n8\n))\n  \n# reshape again to 8x8\n\n    \nplt\n.\nmatshow\n(\ndigit\n)\n                    \n# for matshow\n\n    \nplt\n.\ntitle\n(\n\"{} predicted as {}\"\n.\nformat\n(\ntrue\n,\n \npredict\n))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression with kNN\n\u00b6\n\n\n\n\n\n\nIt is also possible to do regression using k-Nearest Neighbors\n\n\n\n\n\n\nfind \nk\n nearest neighbors from training samples\n\n\n\n\n\n\ncalculate the predicted value using inverse distance weighting method\n\n\n\n\n\n\n\n\ny_{pred}(\\vec x) = \\frac{\\sum\\limits_i w_i(\\vec x) y_{train, i}}{\\sum\\limits_i w_i(\\vec x_i)}\n\n\ny_{pred}(\\vec x) = \\frac{\\sum\\limits_i w_i(\\vec x) y_{train, i}}{\\sum\\limits_i w_i(\\vec x_i)}\n\n\n\n\n\n\n\n\nwhere \nw_i(\\vec x) = \\frac{1}{d(\\vec x, \\vec x_{train, i})}\nw_i(\\vec x) = \\frac{1}{d(\\vec x, \\vec x_{train, i})}\n\n\n\n\n\n\nNote, that \ny_{pred}(\\vec x) = y_{train, i}\ny_{pred}(\\vec x) = y_{train, i}\n if \nd(\\vec x, \\vec x_{train, i}) = 0\nd(\\vec x, \\vec x_{train, i}) = 0\n\n\n\n\n\n\n\n\n\n\nGenearate some fake data\n\u00b6\n\n\n\n\n\n\nLet's grab some random points from the sine function\n\n\n\n\n\n\nAnd add some noise to make it more like real data\n\n\n\n\n\n\ndata_size\n \n=\n \n50\n\n\n\n# generate and sort *data_size* numbers from 0 to 4pi \n\n\nx_train\n \n=\n \n4\n \n*\n \nnp\n.\npi\n \n*\n \nnp\n.\nsort\n(\nnp\n.\nrandom\n.\nrand\n(\ndata_size\n,\n \n1\n),\n \naxis\n=\n0\n)\n\n\n\n# let's fit to sine  \n\n\ny_train\n \n=\n \nnp\n.\nsin\n(\nx_train\n)\n.\nravel\n()\n\n\n\n# add some noise to the data\n\n\ny_train\n \n=\n \nnp\n.\narray\n([\nnp\n.\nrandom\n.\nnormal\n(\ny\n,\n \n0.05\n)\n \nfor\n \ny\n \nin\n \ny_train\n])\n\n\n\nplt\n.\nplot\n(\nx_train\n,\n \ny_train\n,\n \n'ro'\n);\n\n\n\n\n\n\n\n\nMake a fit\n\u00b6\n\n\n\n\n\n\nIn general, one should do cross-validation to determine the best \nk\n\n\n\n\n\n\nWe will skip this part during the lecture (feel free to check this at home though!)\n\n\n\n\n\n\nLet's just check how kNN fit works for a few different values of \nk\n\n\n\n\n\n\nComment on \nnumpy.newaxis\n\u00b6\n\n\n# let's create a 1D numpy array\n\n\nD1\n \n=\n \nnp\n.\narray\n([\n1\n,\n \n2\n,\n \n3\n,\n \n4\n])\n\n\n\nprint\n(\nD1\n)\n\n\n\n\n\n\n[1 2 3 4]\n\n\n\n\n\n# we can easily add another dimension using numpy.newaxis\n\n\nD2\n \n=\n \nD1\n[:,\n \nnp\n.\nnewaxis\n]\n\n\n\nprint\n(\nD2\n)\n\n\n\n\n\n\n[[1]\n [2]\n [3]\n [4]]\n\n\n\n\n\nAnd back to the task\n\u00b6\n\n\n\n\nWe use kNN regressor from \nscikit-learn\n (from intro: \nWhat I really do...\n)\n\n\n\n\nfrom\n \nsklearn.neighbors\n \nimport\n \nKNeighborsRegressor\n\n\n\n# first we need test sample\n\n\nx_test\n \n=\n \nnp\n.\nlinspace\n(\n0\n,\n \n4\n*\nnp\n.\npi\n,\n \n100\n)[:,\n \nnp\n.\nnewaxis\n]\n\n\n\nfor\n \ni\n,\n \nk\n \nin\n \nenumerate\n((\n1\n,\n \n5\n,\n \n10\n,\n \n20\n)):\n\n  \n# weights=distance - weight using distances\n\n  \nknn\n \n=\n \nKNeighborsRegressor\n(\nk\n,\n \nweights\n=\n'distance'\n)\n\n\n  \n# calculate y_test for all points in x_test\n\n  \ny_test\n \n=\n \nknn\n.\nfit\n(\nx_train\n,\n \ny_train\n)\n.\npredict\n(\nx_test\n)\n\n\n  \nplt\n.\nsubplot\n(\n2\n,\n \n2\n,\n \ni\n \n+\n \n1\n)\n\n\n  \nplt\n.\ntitle\n(\n\"k = {}\"\n.\nformat\n(\nk\n))\n\n\n  \nplt\n.\nplot\n(\nx_train\n,\n \ny_train\n,\n \n'ro'\n,\n \nx_test\n,\n \ny_test\n,\n \n'g'\n);\n\n\n\nplt\n.\ntight_layout\n()\n\n\n\n\n\n\n\n\nSummary\n\u00b6\n\n\n\n\n\n\nWe have learned first ML algorithm - k-Nearest Neighbors\n\n\n\n\n\n\nIt has some pros:\n\n\n\n\n\n\neasy to understand and implement\n\n\n\n\n\n\nno time needed for training - may be used for initial analysis before one reaches for some \nheavier\n tool\n\n\n\n\n\n\nsolves nonlinear problems \n\n\n\n\n\n\nlimited number of hyperparameters\n\n\n\n\n\n\nno parameters!\n\n\n\n\n\n\nat the end of this lecture we will deal with tens of hyperparameters and thousands of parameters\n\n\n\n\n\n\n\n\n\n\nAlthough cons make it hard to use in practice\n\n\n\n\n\n\ntraining data must be kept for the whole time (so called \nlazy training\n)\n\n\n\n\n\n\nimagine having GB of training samples and you want to make mobile app\n\n\n\n\n\n\nother algorithms allows to discard training samples once the model is trained (\neager learning\n) - usually it means long training process but super fast classification (which is what we really want)\n\n\n\n\n\n\n\n\n\n\ndistance-comparing is not suitable for all data - a picture of a cat on a blue background (e.g. sky) can be close to a ship on a sea (because background pixels vote too)\n\n\n\n\ne.g. for \nCIFAR-10\n (60k pictures, 10 classes, more about that later) vanilla kNN get less than 40% accuracy\n\n\n\n\n\n\n\n\nstill better than random guessing (10%), but convolutional neural networks get >95%\n\n\n\n\n\n\n\n\n\n\nStill, we have learned from kNN a few important things:\n\n\n\n\n\n\nData is important (both size and quality)\n\n\n\n\n\n\nSometimes data requires preprocessing\n\n\n\n\n\n\nWrong choice of hyperparameters may lead to under- or over-fitting\n\n\n\n\n\n\nUse validation samples to tune the model\n\n\n\n\n\n\nAnd \nDO NOT\n touch test samples until you are done!",
            "title": "k-Nearest Neighbors"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#k-nearest-neighbors",
            "text": "",
            "title": "k-Nearest Neighbors"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#useful-but-not-interesting-functions",
            "text": "Here, I just define some functions used for making demo plots during the introduction.    Feel free to look at them later (especially if you are not familiar with  numpy  and  matplotlib ).    But now let's skip them.    # numpy and matplotlib will be used a lot during the lecture  # if you are familiar with these libraries you may skip this part  # if not - extended comments were added to make it easier to understand  # it is kind of standard to import numpy as np and pyplot as plt  import   numpy   as   np  import   matplotlib.pyplot   as   plt  # used later to apply different colors in for loops  mpl_colors   =   ( 'r' ,   'b' ,   'g' ,   'c' ,   'm' ,   'y' ,   'k' ,   'w' )  # just to overwrite default colab style  plt . style . use ( 'default' )  plt . style . use ( 'seaborn-talk' )  def   generate_random_points ( size = 10 ,   low = 0 ,   high = 1 ): \n   \"\"\"Generate a set of random 2D points    size -- number of points to generate    low  -- min value    high -- max value    \"\"\" \n   # random_sample([size]) returns random numbers with shape defined by size \n   # e.g. \n   # >>> np.random.random_sample((2, 3)) \n   # \n   # array([[ 0.44013807,  0.77358569,  0.64338619], \n   #        [ 0.54363868,  0.31855232,  0.16791031]]) \n   # \n   return   ( high   -   low )   *   np . random . random_sample (( size ,   2 ))   +   low  def   init_plot ( x_range = None ,   y_range = None ,   x_label = \"$x_1$\" ,   y_label = \"$x_2$\" ): \n   \"\"\"Set axes limits and labels    x_range -- [min x, max x]    y_range -- [min y, max y]    x_label -- string    y_label -- string    \"\"\" \n\n   # subplots returns figure and axes \n   # (in general you may want many axes on one figure) \n   # we do not need fig here \n   # but we will apply changes (including adding points) to axes \n   _ ,   ax   =   plt . subplots ( dpi = 70 ) \n\n   # set grid style and color \n   ax . grid ( c = '0.70' ,   linestyle = ':' ) \n\n   # set axes limits (x_range and y_range is a list with two elements) \n   ax . set_xlim ( x_range )  \n   ax . set_ylim ( y_range ) \n\n   # set axes labels \n   ax . set_xlabel ( x_label ) \n   ax . set_ylabel ( y_label ) \n\n   # return axes so we can continue modyfing them later \n   return   ax  def   plot_random_points ( style = None ,   color = None ): \n   \"\"\"Generate and plot two (separated) sets of random points    style -- latter group points style (default as first)    color -- latter group color (default as first)    \"\"\" \n\n   # create a plot with x and y ranges from 0 to 2.5 \n   ax   =   init_plot ([ 0 ,   2.5 ],   [ 0 ,   2.5 ]) \n\n   # add two different sets of random points \n   # first set = 5 points from [0.5, 1.0]x[0.5, 1.0] \n   # second set = 5 points from [1.5, 2.0]x[1.5, 2.0] \n   # generate_random_points return a numpy array in the format like \n   # [[x1, y1], [x2, y2], ..., [xn, yn]] \n   # pyplot.plt take separately arrays with X and Y, like \n   # plot([x1, x2, x3], [y1, y2, y3]) \n   # thus, we transpose numpy array to the format \n   # [[x1, x2, ..., xn], [y1, y2, ..., yn]] \n   # and unpack it with * \n   ax . plot ( * generate_random_points ( 5 ,   0.5 ,   1.0 ) . T ,   'ro' ) \n   ax . plot ( * generate_random_points ( 5 ,   1.5 ,   2.0 ) . T ,   style   or   'ro' ) \n\n   return   ax  def   plot_an_example ( style = None ,   color = None ,   label = \"Class\" ): \n   \"\"\"Plot an example of supervised or unsupervised learning\"\"\" \n   ax   =   plot_random_points ( style ,   color ) \n\n   # circle areas related to each set of points \n   # pyplot.Circle((x, y), r); (x, y) - the center of a circle; r - radius \n   # lw - line width \n   ax . add_artist ( plt . Circle (( 0.75 ,   0.75 ),   0.5 ,   fill = 0 ,   color = 'r' ,   lw = 2 )) \n   ax . add_artist ( plt . Circle (( 1.75 ,   1.75 ),   0.5 ,   fill = 0 ,   color = color   or   'r' ,   lw = 2 )) \n\n   # put group labels \n   # pyplot.text just put arbitrary text in given coordinates \n   ax . text ( 0.65 ,   1.4 ,   label   +   \" I\" ,   fontdict = { 'color' :   'r' }) \n   ax . text ( 1.65 ,   1.1 ,   label   +   \" II\" ,   fontdict = { 'color' :   color   or   'r' })",
            "title": "useful (but not interesting) functions"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#our-first-ml-problem",
            "text": "Two classes: red circles and blue squares ( training  samples)    Where does the green triangle ( test  sample) belong?    X1   =   generate_random_points ( 20 ,   0 ,   1 )  X2   =   generate_random_points ( 20 ,   1 ,   2 )  new_point   =   generate_random_points ( 1 ,   0 ,   2 )  plot   =   init_plot ([ 0 ,   2 ],   [ 0 ,   2 ])    # [0, 2] x [0, 2]  plot . plot ( * X1 . T ,   'ro' ,   * X2 . T ,   'bs' ,   * new_point . T ,   'g^' );",
            "title": "Our first ML problem"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#nearest-neighbor",
            "text": "The nearest neigbor classifier  compares  a test sample with all training samples to predict a label (class).    How to compare two samples?    L1 distance:  d(\\vec x_1, \\vec x_2) = \\sum\\limits_i |x_1^i - x_2^i| d(\\vec x_1, \\vec x_2) = \\sum\\limits_i |x_1^i - x_2^i|    L2 distance:  d(\\vec x_1, \\vec x_2) = \\sqrt{\\sum\\limits_i (x_1^i - x_2^i)^2} d(\\vec x_1, \\vec x_2) = \\sqrt{\\sum\\limits_i (x_1^i - x_2^i)^2}    note: in practice square root is ignored (becasue is monotonic function)    L2 is less forgiving than L1 - prefers many small disagreements than one big one      cosine distance (cosine similarity):  d(\\vec x_1, \\vec x_2) = \\frac{x_1 \\cdot x_1}{||\\vec x_1|| \\cdot ||\\vec x_2||} d(\\vec x_1, \\vec x_2) = \\frac{x_1 \\cdot x_1}{||\\vec x_1|| \\cdot ||\\vec x_2||}    Chebyshev distance:  d(\\vec x_1, \\vec x_2) = max_i(|x_1^i - x_2^i|) d(\\vec x_1, \\vec x_2) = max_i(|x_1^i - x_2^i|)    and many others      The closest one determines the test sample label",
            "title": "Nearest Neighbor"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#implementation",
            "text": "The implementation of nearest neighbor algorithm is pretty straightforward    There is no real training process here - we just need to remember all training feature vectors and corresponding labels    To predict a label for new sample we just need to find the label of the closest point from training samples    class   NearestNeighbor (): \n   \"\"\"Nearest Neighbor Classifier\"\"\" \n\n   def   __init__ ( self ,   distance = 0 ): \n     \"\"\"Set distance definition: 0 - L1, 1 - L2\"\"\" \n     if   distance   ==   0 : \n       self . distance   =   np . abs       # absolute value \n     elif   distance   ==   1 : \n       self . distance   =   np . square    # square root \n     else : \n       raise   Exception ( \"Distance not defined.\" ) \n\n\n   def   train ( self ,   x ,   y ): \n     \"\"\"Train the classifier (here simply save training data)      x -- feature vectors (N x D)      y -- labels (N x 1)      \"\"\" \n     self . x_train   =   x \n     self . y_train   =   y \n\n\n   def   predict ( self ,   x ): \n     \"\"\"Predict and return labels for each feature vector from x      x -- feature vectors (N x D)      \"\"\" \n     predictions   =   []    # placeholder for N labels \n\n     # loop over all test samples \n     for   x_test   in   x : \n       # array of distances between current test and all training samples \n       distances   =   np . sum ( self . distance ( self . x_train   -   x_test ),   axis = 1 ) \n\n       # get the closest one \n       min_index   =   np . argmin ( distances ) \n\n       # add corresponding label \n       predictions . append ( self . y_train [ min_index ]) \n\n     return   predictions",
            "title": "Implementation"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#the-magic-of-numpy",
            "text": "NumPy is irreplacable tool for numerical operations on arrays    Using numpy we could easily find all distances using one line    distances = np.sum(self.distance(self.x_train - x_test), axis=1)   Here is how it works   # let's create an array with 5x2 shape  a   =   np . random . random_sample (( 5 ,   2 ))  # and another array with 1x2 shape  b   =   np . array ([[ 1. ,   1. ]])  print ( a ,   b ,   sep = \" \\n\\n \" )   [[0.79036457 0.36571819]\n [0.76743991 0.08439684]\n [0.56876884 0.97967839]\n [0.77020776 0.21238365]\n [0.94235534 0.73884472]]\n\n[[1. 1.]]  # subtract arguments (element-wise)  # note, that at least one dimension must be the same   print ( a   -   b )   [[-0.20963543 -0.63428181]\n [-0.23256009 -0.91560316]\n [-0.43123116 -0.02032161]\n [-0.22979224 -0.78761635]\n [-0.05764466 -0.26115528]]  # numpy.abs calculates absolute value (element-wise)  print ( np . abs ( a   -   b ))   [[0.20963543 0.63428181]\n [0.23256009 0.91560316]\n [0.43123116 0.02032161]\n [0.22979224 0.78761635]\n [0.05764466 0.26115528]]  # sum all elements  np . sum ( np . abs ( a   -   b ))   3.7798417848539096  # sum elements over a given axis  np . sum ( np . abs ( a   -   b ),   axis = 0 )   array([1.16086358, 2.61897821])  np . sum ( np . abs ( a   -   b ),   axis = 1 )   array([0.84391724, 1.14816326, 0.45155276, 1.01740859, 0.31879994])",
            "title": "The magic of numpy"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#analysis",
            "text": "Before we start using  NearestNeighbor  let's create a simple mini-framework to apply NN and visualize results easily    We want to initilize  NearestNeighbor  with some feature vectors (and automatically assign labels for each class)    We want our test samples to be a grid of uniformly distributed points    We want methods to process test data and to make a plots with final results    class   Analysis (): \n   \"\"\"Apply NearestNeighbor to generated (uniformly) test samples.\"\"\" \n\n   def   __init__ ( self ,   * x ,   distance ): \n     \"\"\"Generate labels and initilize classifier      x -- feature vectors arrays      distance -- 0 for L1, 1 for L2          \"\"\" \n     # get number of classes \n     self . nof_classes   =   len ( x ) \n\n     # create lables array \n     # np.ones creates an array of given shape filled with 1 of given type \n     # we apply consecutive integer numbers as class labels \n     # ravel return flatten array \n     y   =   [ i   *   np . ones ( _x . shape [ 0 ],   dtype = np . int )   for   i ,   _x   in   enumerate ( x )] \n     y   =   np . array ( y ) . ravel () \n\n     # save training samples to plot them later \n     self . x_train   =   x \n\n     # merge feature vector arrays for NearestNeighbor \n     x   =   np . concatenate ( x ,   axis = 0 ) \n\n     # train classifier \n     self . nn   =   NearestNeighbor ( distance ) \n     self . nn . train ( x ,   y ) \n\n\n   def   prepare_test_samples ( self ,   low = 0 ,   high = 2 ,   step = 0.01 ): \n     \"\"\"Generate a grid with test points (from low to high with step)\"\"\" \n     # remember range \n     self . range   =   [ low ,   high ] \n\n     # start with grid of points from [low, high] x [low, high] \n     grid   =   np . mgrid [ low : high + step : step ,   low : high + step : step ] \n\n     # convert to an array of 2D points \n     self . x_test   =   np . vstack ([ grid [ 0 ] . ravel (),   grid [ 1 ] . ravel ()]) . T \n\n\n   def   analyse ( self ): \n     \"\"\"Run classifier on test samples and split them according to labels.\"\"\" \n\n     # find labels for test samples  \n     self . y_test   =   self . nn . predict ( self . x_test ) \n\n     self . classified   =   []    # [class I test points, class II test ...] \n\n     # loop over available labels \n     for   label   in   range ( self . nof_classes ): \n       # if i-th label == current label -> add test[i] \n       class_i   =   np . array ([ self . x_test [ i ]  \\\n                           for   i ,   l   in   enumerate ( self . y_test )  \\\n                           if   l   ==   label ]) \n       self . classified . append ( class_i ) \n\n\n   def   plot ( self ,   t = '' ): \n     \"\"\"Visualize the result of classification\"\"\" \n     plot   =   init_plot ( self . range ,   self . range ) \n     plot . set_title ( t ) \n     plot . grid ( False ) \n\n     # plot training samples \n     for   i ,   x   in   enumerate ( self . x_train ): \n       plot . plot ( * x . T ,   mpl_colors [ i ]   +   'o' ) \n\n     # plot test samples \n     for   i ,   x   in   enumerate ( self . classified ): \n       plot . plot ( * x . T ,   mpl_colors [ i ]   +   ',' )",
            "title": "Analysis"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#l1-test",
            "text": "l1   =   Analysis ( X1 ,   X2 ,   distance = 0 )  l1 . prepare_test_samples ()  l1 . analyse ()  l1 . plot ()",
            "title": "L1 test"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#l2-test",
            "text": "l2   =   Analysis ( X1 ,   X2 ,   distance = 1 )  l2 . prepare_test_samples ()  l2 . analyse ()  l2 . plot ()",
            "title": "L2 Test"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#multiclass-classification",
            "text": "Training samples from 4 squares:   [0, 1] x [0, 1]  [0, 1] x [1, 2]  [1, 2] x [0, 1]  [1, 2] x [1, 2]     We expect 4 squares created by test samples grid    How does it depend on the size of training samples?    def   generate4 ( n = 50 ): \n   \"\"\"Generate 4 sets of random points.\"\"\" \n\n   # points from [0, 1] x [0, 1] \n   X1   =   generate_random_points ( n ,   0 ,   1 ) \n   # points from [1, 2] x [1, 2] \n   X2   =   generate_random_points ( n ,   1 ,   2 ) \n   # points from [0, 1] x [1, 2] \n   X3   =   np . array ([[ x ,   y + 1 ]   for   x , y   in   generate_random_points ( n ,   0 ,   1 )]) \n   # points from [1, 2] x [0, 1] \n   X4   =   np . array ([[ x ,   y - 1 ]   for   x , y   in   generate_random_points ( n ,   1 ,   2 )]) \n\n   return   X1 ,   X2 ,   X3 ,   X4   # loop over no. of training samples  for   n   in   ( 5 ,   10 ,   50 ,   100 ): \n   # generate 4 sets of random points (each one with n samples) \n   # unpack them when passing to Analysis \n   c4   =   Analysis ( * generate4 ( n ),   distance = 1 ) \n   c4 . prepare_test_samples () \n   c4 . analyse () \n   c4 . plot ( \"No. of samples = {}\" . format ( n ))       Message 01: size matters!",
            "title": "Multiclass classification"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#noise",
            "text": "Data are rarely perfect and you may expect some training samples to have unsual features    Features shared by a majority of training samples are more important than a single occurrence    Let's add some noise to our data and see how Nearest Neighbor deal with it    # generate 4 classes of 2D points  X1 ,   X2 ,   X3 ,   X4   =   generate4 ()  # add some noise by applying gaussian to every point coordinates  noise   =   lambda   x ,   y :   [ np . random . normal ( x ,   0.1 ),   np . random . normal ( y ,   0.1 )]  X1   =   np . array ([ noise ( x ,   y )   for   x ,   y   in   X1 ])  X2   =   np . array ([ noise ( x ,   y )   for   x ,   y   in   X2 ])  X3   =   np . array ([ noise ( x ,   y )   for   x ,   y   in   X3 ])  X4   =   np . array ([ noise ( x ,   y )   for   x ,   y   in   X4 ])  # perform analysis  c4   =   Analysis ( X1 ,   X2 ,   X3 ,   X4 ,   distance = 1 )  c4 . prepare_test_samples ()  c4 . analyse ()  c4 . plot ()",
            "title": "Noise"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#overfitting",
            "text": "The above is an example of overfitting   perfectly describe training data  lose the generalization ability     In general you want to extract all common features from training samples, but neglect characteristic features of single sample    Message 02: avoid overfitting!",
            "title": "Overfitting"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#accuracy",
            "text": "Accuracy defines the fraction of (unseen) samples which are correctly classify by the algorithm    accuracy   =   0  # loop over (sample, reconstructed label)  for   sample ,   label   in   zip ( c4 . x_test ,   c4 . y_test ): \n   # determine true label \n   if   sample [ 0 ]   <   1   and   sample [ 1 ]   <   1 : \n     true_label   =   0 \n   elif   sample [ 0 ]   >   1   and   sample [ 1 ]   >   1 : \n     true_label   =   1 \n   elif   sample [ 0 ]   <   1   and   sample [ 1 ]   >   1 : \n     true_label   =   2 \n   else : \n     true_label   =   3 \n\n   if   true_label   ==   label :   accuracy   +=   1  accuracy   /=   len ( c4 . x_test )  print ( accuracy )   0.924878097076805    Please note, that this is a toy model - in the case of real problems there is no way to determine true labels (otherwise there is no point to use ML methods...)    To measure accuracy of the model one usually splits data into:    training samples (usually about 80%)    test samples (usually about 20%)      After the model is trained on training samples, the accuracy is measured on test samples    Message 03: keep some data for testing!",
            "title": "Accuracy"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#k-nearest-neighbors_1",
            "text": "Instead of letting one closest neighbor to decide, let  k  nearest neghbors to vote",
            "title": "k-Nearest Neighbors"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#implementation_1",
            "text": "We can base the implementation on  NearestNeighbor , but    The  constructor  has an extra parameter  k    and we need to override  predict  method    class   kNearestNeighbors ( NearestNeighbor ): \n   \"\"\"k-Nearest Neighbor Classifier\"\"\" \n\n\n   def   __init__ ( self ,   k = 1 ,   distance = 0 ): \n     \"\"\"Set distance definition: 0 - L1, 1 - L2\"\"\" \n     super () . __init__ ( distance ) \n     self . k   =   k \n\n\n   def   predict ( self ,   x ): \n     \"\"\"Predict and return labels for each feature vector from x      x -- feature vectors (N x D)      \"\"\" \n     predictions   =   []    # placeholder for N labels \n\n     # no. of classes = max label (labels starts from 0) \n     nof_classes   =   np . amax ( self . y_train )   +   1 \n\n     # loop over all test samples \n     for   x_test   in   x : \n       # array of distances between current test and all training samples \n       distances   =   np . sum ( self . distance ( self . x_train   -   x_test ),   axis = 1 ) \n\n       # placeholder for labels votes \n       votes   =   np . zeros ( nof_classes ,   dtype = np . int ) \n\n       # find k closet neighbors and vote \n       # argsort returns the indices that would sort an array \n       # so indices of nearest neighbors \n       # we take self.k first \n       for   neighbor_id   in   np . argsort ( distances )[: self . k ]: \n         # this is a label corresponding to one of the closest neighbor \n         neighbor_label   =   self . y_train [ neighbor_id ] \n         # which updates votes array \n         votes [ neighbor_label ]   +=   1 \n\n       # predicted label is the one with most votes \n       predictions . append ( np . argmax ( votes )) \n\n     return   predictions",
            "title": "Implementation"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#kanalysis",
            "text": "We also create  kAnalysis  based on  Analysis  for visualization of kNN results   class   kAnalysis ( Analysis ): \n   \"\"\"Apply kNearestNeighbor to generated (uniformly) test samples.\"\"\" \n\n   def   __init__ ( self ,   * x ,   k = 1 ,   distance = 1 ): \n     \"\"\"Generate labels and initilize classifier      x -- feature vectors arrays      k -- number of nearest neighbors      distance -- 0 for L1, 1 for L2          \"\"\" \n     # get number of classes \n     self . nof_classes   =   len ( x ) \n\n     # create lables array \n     y   =   [ i   *   np . ones ( _x . shape [ 0 ],   dtype = np . int )   for   i ,   _x   in   enumerate ( x )] \n     y   =   np . array ( y ) . ravel () \n\n     # save training samples to plot them later \n     self . x_train   =   x \n\n     # merge feature vector arrays for NearestNeighbor \n     x   =   np . concatenate ( x ,   axis = 0 ) \n\n     # train classifier (knn this time) \n     self . nn   =   kNearestNeighbors ( k ,   distance ) \n     self . nn . train ( x ,   y )",
            "title": "kAnalysis"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#sanity-check",
            "text": "k-Nearest Neighbor classifier with  k = 1  must give exactly the same results as Nearest Neighbor   # apply kNN with k=1 on the same set of training samples  knn   =   kAnalysis ( X1 ,   X2 ,   X3 ,   X4 ,   k = 1 ,   distance = 1 )  knn . prepare_test_samples ()  knn . analyse ()  knn . plot ()",
            "title": "Sanity check"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#k-test",
            "text": "For  k = 1  kNN is likely to overfit the problem    Although, it does not mean that higher  k  is better!    Now, let's see how different values of  k  affects the result    Later, we will learn how to find optimal value of  k  for given problem    # training size = 50  # let's check a few values between 1 and 50  for   k   in   ( 1 ,   5 ,   10 ,   50 ): \n   knn   =   kAnalysis ( X1 ,   X2 ,   X3 ,   X4 ,   k = k ,   distance = 1 ) \n   knn . prepare_test_samples () \n   knn . analyse () \n   knn . plot ( \"k = {}\" . format ( k ))",
            "title": "k-Test"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#hyperparameters",
            "text": "ML model may have some hyperparameters - parameters set before training    Please note, ML algorithm may have also parameters which are set during training    In the case of kNN there are two hyperparameters:    number of nearest neihgbors ( k )    the definition of distance      The choice of hyperparameters values highly depends on a problem    The wrong choice of hyperparameters may lead to underfitting or overfitting",
            "title": "Hyperparameters"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#over-under-fitting-example",
            "text": "# generate random data from x^2 function (with some noise)  data   =   np . array ([[ x ,   np . random . normal ( x ** 2 ,   0.1 )]  \\\n                  for   x   in   2 * np . random . random ( 10 )   -   1 ])  plot   =   init_plot ([ - 1 ,   1 ],   [ - 1 ,   1 ])  plot . plot ( * data . T ,   'o' );      Let's try to fit this data to a polynomial    The degree is a hyperparamter (which defines number of coefficients)    # loop over degrees of polynomial  # data is x^2, so let's try degrees 1, 2, 10  for   n   in   ( 1 ,   2 ,   10 ): \n   # polyfit returns an array with polynomial coefficients \n   # poly1d is a polynomial class \n   f   =   np . poly1d ( np . polyfit ( * data . T ,   n )) \n\n   # returns an array with 100 uniformly distributed numbers from -1 to 1 \n   x   =   np . linspace ( - 1 ,   1 ,   100 ) \n\n   plot   =   init_plot ([ - 1 ,   1 ],   [ - 1 ,   1 ]) \n   plot . set_title ( \"n = {}\" . format ( n )) \n   plot . plot ( * data . T ,   'o' ,   x ,   f ( x ))   /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RankWarning: Polyfit may be poorly conditioned\n  after removing the cwd from sys.path.       For  n = 1  we clearly underfit the data as we do not have enough parameters to describe the complexity of the problem    For  n = 2  we have appropriate capacity (as we actually generated data form  x^2 x^2  function)    For  n = 10  we overfit the data - training samples are described perfectly, but we clearly lost the generalization ability    Message 04: right choice of hyperparameters is crucial!",
            "title": "Over-, under-fitting example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#validation-dataset",
            "text": "One splits data into training and test samples    training samples are used to optimize model parameters    test samples are used to measure accuracy    there is no rule of thumb on how to split dataset      If a model has some hyperparameters the part of training set is used for valitation samples:    training samples - tuning model parameters    validation samples - tuning hyperparameters                        +---------------------+      +------------------------+\n+----------+      |                     |      |                        |\n|          |      | Measure accuracy on |      | Measure final accuracy |\n| Training | +--> |                     | +--> |                        |\n|          |      | validation samples  |      | on test samples        |\n+----------+      |                     |      |                        |\n     ^            +----------+----------+      +------------------------+\n     |                       |\n     |      Change           | \n     +-----------------------+\n         hyperparameters",
            "title": "Validation dataset"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#iris-dataset",
            "text": "The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.  src    Attribute Information:    sepal length in cm    sepal width in cm    petal length in cm    petal width in cm    class:     Iris Setosa    Iris Versicolour    Iris Virginica",
            "title": "Iris dataset"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#load-dataset",
            "text": "We use  pandas  for data manipulation - it is super handy and supports many formats   import   pandas   as   pd  # columns names - can be used to access columns later  columns   =   [ \"Sepal Length\" ,   \"Sepal Width\" , \n            \"Petal Length\" ,   \"Petal Width\" , \n            \"Class\" ]  # iris.data is a csv file  src   =   \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"  # load the file with pandas.read_csv   # it will name columns as defined in columns list  # so one can access a column through index or name  iris_data   =   pd . read_csv ( src ,   header = None ,   names = columns )   iris_data . head ()    # print a few first entries    \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       Sepal Length \n       Sepal Width \n       Petal Length \n       Petal Width \n       Class \n     \n   \n   \n     \n       0 \n       5.1 \n       3.5 \n       1.4 \n       0.2 \n       Iris-setosa \n     \n     \n       1 \n       4.9 \n       3.0 \n       1.4 \n       0.2 \n       Iris-setosa \n     \n     \n       2 \n       4.7 \n       3.2 \n       1.3 \n       0.2 \n       Iris-setosa \n     \n     \n       3 \n       4.6 \n       3.1 \n       1.5 \n       0.2 \n       Iris-setosa \n     \n     \n       4 \n       5.0 \n       3.6 \n       1.4 \n       0.2 \n       Iris-setosa",
            "title": "Load dataset"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#visualize-dataset",
            "text": "pandas  offers plotting through  matplotlib  integration    Let's visualize Iris data    Let's keep the code short - sorry if it is hard to follow    # to extract rows with class column == class_name  extract   =   lambda   class_name :   iris_data . loc [ iris_data [ 'Class' ]   ==   class_name ]  # axes settings - part = Sepal or Petal; x = Length, y = Width  set_ax   =   lambda   part :   { \"x\" :   part   +   \" Length\" , \n                        \"y\" :   part   +   \" Width\" , \n                        \"kind\" :   \"scatter\" }  # add iris type / sepal or petal / color to existing axis  plot   =   lambda   class_name ,   part ,   color ,   axis :  \\\n   extract ( class_name ) . plot ( ** set_ax ( part ), \n                            color = color , \n                            label = class_name , \n                            ax = axis )  # plot all Iris types (sepal or petal) on existing axis  plot_all   =   lambda   part ,   axis :  \\\n   [ plot ( iris ,   part ,   mpl_colors [ i ],   axis )  \\\n    for   i ,   iris   in   enumerate ( set ( iris_data [ 'Class' ]))]    # with pyplot.subplots we can create many plots on one figure  # here we create 2 plots - 1 row and 2 columns  # thus, subplots returns figure, axes of 1st plot, axes for 2nd plot  _ ,   ( ax1 ,   ax2 )   =   plt . subplots ( 1 ,   2 ,   figsize = ( 9 , 4 ))  # using messy lambda we can plot all Iris types at once  # Petal data on 1st plots and Sepal data on 2nd plot  plot_all ( \"Petal\" ,   ax1 )  plot_all ( \"Sepal\" ,   ax2 )  # tight_layout adjust subplots params so they fit into figure ares  plt . tight_layout ()",
            "title": "Visualize dataset"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#prepare-feature-vectors-and-labels",
            "text": "First step is to prepare data - we need feature vectors with corresponding labels    In this case every sample's feature vector is 4D (sepal length, sepal width, petal length, petal width) and is labeled with one of three classes (Iris Setosa, Iris Versicolour, Iris Virginica)    # every Iris has 4 features (forming our 4D feature vectors)  # pandaoc.DataFrame.iloc allows us access data through indices  # we create an array with feature vectors by taking all rows for first 4 columns  X   =   iris_data . iloc [:,   : 4 ]  # it is still pandoc.DataFrame object - pretty handy  X . head ()    \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       Sepal Length \n       Sepal Width \n       Petal Length \n       Petal Width \n     \n   \n   \n     \n       0 \n       5.1 \n       3.5 \n       1.4 \n       0.2 \n     \n     \n       1 \n       4.9 \n       3.0 \n       1.4 \n       0.2 \n     \n     \n       2 \n       4.7 \n       3.2 \n       1.3 \n       0.2 \n     \n     \n       3 \n       4.6 \n       3.1 \n       1.5 \n       0.2 \n     \n     \n       4 \n       5.0 \n       3.6 \n       1.4 \n       0.2 \n     \n       pandas.DataFrame  object are handy to manipulate data, but at the end of the day we want to perform algebra with  numpy   # create numpy array (matrix) for further processing  X   =   np . array ( X )  # print a few first entries  print ( X [: 5 ])   [[5.1 3.5 1.4 0.2]\n [4.9 3.  1.4 0.2]\n [4.7 3.2 1.3 0.2]\n [4.6 3.1 1.5 0.2]\n [5.  3.6 1.4 0.2]]   from the las column (\"Class\") we create our labels   # as mentioned before, we can access DataFrame object through column labels  Y   =   np . array ( iris_data [ \"Class\" ])  # print a few first entries  print ( Y [: 5 ])   ['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa']",
            "title": "Prepare feature vectors and labels"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#prepare-test-dataset",
            "text": "Let's use 80% for training and 20% for testing    We, obviously, can not just take last 20% of samples for testing because our data is ordered    But we can randomly select 20% of samples    Easy to do by hand, but let's start to use some ML frameworks    from   sklearn.model_selection   import   train_test_split   as   split  # train_test_split: Split arrays or matrices into random train and test subsets  X_train ,   X_test ,   Y_train ,   Y_test   =   split ( X ,   Y ,   test_size = 0.2 )  # let's use 20% of training samples for validation  X_train ,   X_valid ,   Y_train ,   Y_valid   =   split ( X_train ,   Y_train ,   test_size = 0.2 )  # check how many sample we have  print ( X_train . shape [ 0 ],   X_valid . shape [ 0 ],   X_test . shape [ 0 ])   96 24 30",
            "title": "Prepare test dataset"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#knn-from-scikit-learn",
            "text": "scikit-learn  has already implemented k-Nearest Neighbor algorithm (which is more flexible than the one implemented during this lecture)    Let's see how  complicated  is using one of ML frameworks with Python    from   sklearn.neighbors   import   KNeighborsClassifier  # create knn classifier with k = 48  knn   =   KNeighborsClassifier ( n_neighbors = 48 )  # train the model  knn . fit ( X_train ,   Y_train )  # predict labels for test samples  Y_pred   =   knn . predict ( X_valid )",
            "title": "kNN from scikit-learn"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#accuracy_1",
            "text": "First let's print true labels along with predicted ones   # use bold if true != predicted  for   true ,   pred   in   zip ( Y_valid ,   Y_pred ): \n   if   pred   ==   true : \n     print ( \"{} \\t  -> {}\" . format ( true ,   pred )) \n   else : \n     print ( \" \\033 [1m{} \\t  -> {} \\033 [0m\" . format ( true ,   pred ))   Iris-setosa  -> Iris-setosa\nIris-versicolor  -> Iris-versicolor\nIris-setosa  -> Iris-setosa\nIris-versicolor  -> Iris-versicolor\nIris-virginica   -> Iris-virginica\nIris-virginica   -> Iris-virginica\n\u001b[1mIris-versicolor  -> Iris-virginica\u001b[0m\nIris-virginica   -> Iris-virginica\nIris-versicolor  -> Iris-versicolor\nIris-setosa  -> Iris-setosa\nIris-virginica   -> Iris-virginica\nIris-versicolor  -> Iris-versicolor\nIris-virginica   -> Iris-virginica\n\u001b[1mIris-virginica   -> Iris-versicolor\u001b[0m\nIris-virginica   -> Iris-virginica\nIris-virginica   -> Iris-virginica\nIris-versicolor  -> Iris-versicolor\nIris-setosa  -> Iris-setosa\nIris-setosa  -> Iris-setosa\nIris-virginica   -> Iris-virginica\n\u001b[1mIris-virginica   -> Iris-versicolor\u001b[0m\nIris-setosa  -> Iris-setosa\nIris-versicolor  -> Iris-versicolor\n\u001b[1mIris-virginica   -> Iris-versicolor\u001b[0m   We can easily calculate accuracy by hand as it is just a number of correctly predicted labels divided by no. of samples   # Y_valid == Y_pred -> array of True/False (if two elements are equal or not)  # (Y_valid == Y_pred).sum() -> number of Trues  # Y_valid.shape[0] -> number of validation samples  accuracy   =   ( Y_valid   ==   Y_pred ) . sum ()   /   Y_valid . shape [ 0 ]  print ( accuracy )   0.8333333333333334   But we can also use  scikit-learn  function  accuracy_score   from   sklearn.metrics   import   accuracy_score  print ( accuracy_score ( Y_valid ,   Y_pred ))   0.8333333333333334",
            "title": "Accuracy"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#k-dependence-of-the-accuracy",
            "text": "Let's use validation set to determine the best hyperparameter  k    We will run kNN for various values of  k  and measure accuracy    This will allow us to find the optimal value of  k    And check the accuracy on the test dataset    scores   =   []    # placeholder for accuracy  max_k   =   85    # maximum number of voters  # loop over different values of k  for   k   in   range ( 1 ,   max_k ): \n   # create knn classifier with k = k \n   knn   =   KNeighborsClassifier ( n_neighbors = k ) \n\n   # train the model \n   knn . fit ( X_train ,   Y_train ) \n\n   # predict labels for test samples \n   Y_pred   =   knn . predict ( X_valid ) \n\n   # add accuracy to score table \n   scores . append ( accuracy_score ( Y_valid ,   Y_pred ))    Now, we can plot accuracy as a function of  k   def   k_accuracy_plot ( max_k = 85 ): \n   \"\"\"Just plot settings\"\"\" \n   plt . grid ( True ) \n   plt . xlabel ( \"k\" ) \n   plt . ylabel ( \"Accuracy\" ) \n   plt . xlim ([ 0 ,   max_k   +   5 ]) \n   plt . ylim ([ 0 ,   1 ]) \n   plt . xticks ( range ( 0 ,   max_k   +   5 ,   5 )) \n\n   return   plt  k_accuracy_plot () . plot ( range ( 1 ,   max_k ),   scores );     And check the accuracy measured on the test samples   knn   =   KNeighborsClassifier ( n_neighbors = 9 )  knn . fit ( X_train ,   Y_train )  Y_pred   =   knn . predict ( X_test )  print ( accuracy_score ( Y_test ,   Y_pred ))   0.9666666666666667    The accuracy plot is not smooth    It is common if one does not have enough validation samples    But there is another way to measure accuracy dependence on hyperparameters",
            "title": "k-dependence of the accuracy"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#cross-validation",
            "text": "Split training samples into N folds\n\n +-------+   +-------+   +-------+         +-------+\n |       |   |       |   |       |         |       |\n |   1   |   |   2   |   |   3   |   ...   |   N   |\n |       |   |       |   |       |         |       |\n +-------+   +-------+   +-------+         +-------+\n\nTake one fold as validation set and train on N-1 folds\n\n +-------+   +-------+   +-------+         +-------+\n |*******|   |       |   |       |         |       |\n |*******|   |   2   |   |   3   |   ...   |   N   |\n |*******|   |       |   |       |         |       |\n +-------+   +-------+   +-------+         +-------+\n\n         Take the next one as validation set\n\n +-------+   +-------+   +-------+         +-------+\n |       |   |*******|   |       |         |       |\n |   1   |   |*******|   |   3   |   ...   |   N   |\n |       |   |*******|   |       |         |       |\n +-------+   +-------+   +-------+         +-------+\n\n          Repeat the procedure for all folds\n\n +-------+   +-------+   +-------+         +-------+\n |       |   |       |   |       |         |*******|\n |   1   |   |   2   |   |   3   |   ...   |*******|\n |       |   |       |   |       |         |*******|\n +-------+   +-------+   +-------+         +-------+\n\n            And average out the accuracy   Once again  scikit-learn  has already implemented the procedure we need   from   sklearn.model_selection   import   cross_val_score  # this time we do not create dedicated validation set  X_train ,   X_test ,   Y_train ,   Y_test   =   split ( X ,   Y ,   test_size = 0.2 )  avg_scores   =   []    # average score for different k  nof_folds   =   10  # loop over different values of k  for   k   in   range ( 1 ,   max_k ): \n   # create knn classifier with k = k \n   knn   =   KNeighborsClassifier ( n_neighbors = k ) \n\n   # cross-validate knn on our training sample with nof_folds \n   scores   =   cross_val_score ( knn ,   X_train ,   Y_train , \n                            cv = nof_folds ,   scoring = 'accuracy' ) \n\n   # add avg accuracy to score table \n   avg_scores . append ( scores . mean ())   k_accuracy_plot () . plot ( range ( 1 ,   max_k ),   avg_scores );      In theory, k-fold cross-validation is the way to go (especially if a dataset is small)    In practice, people tend to use a single validation split as it is not that computational expensive",
            "title": "Cross-validation"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#data-normalization",
            "text": "Sometimes there is a need to preprocess data before training    Let's imagine Iris sepal data is in cm but petal data in mm    # original data - both in cm  print ( X [: 5 ])   [[5.1 3.5 1.4 0.2]\n [4.9 3.  1.4 0.2]\n [4.7 3.2 1.3 0.2]\n [4.6 3.1 1.5 0.2]\n [5.  3.6 1.4 0.2]]  # make a copy of X  Xmm   =   X . copy ()  # and multiply last two columns by 0.1  Xmm [:, 2 :]   *=   0.1  # and we have our fake Iris data with petal length/width in mm  print ( Xmm [: 5 ])   [[5.1  3.5  0.14 0.02]\n [4.9  3.   0.14 0.02]\n [4.7  3.2  0.13 0.02]\n [4.6  3.1  0.15 0.02]\n [5.   3.6  0.14 0.02]]   Let's compare result of the same classifier on both dataset   def   get_accuracy ( X ,   Y ,   k = 10 ): \n   \"\"\"Make training and test datasets and process through kNN\"\"\" \n\n   # prepare training / test samples \n   X_train ,   X_test ,   Y_train ,   Y_test   =   split ( X ,   Y ,   test_size = 0.2 ) \n\n   # create a kNN with k = k \n   knn   =   KNeighborsClassifier ( n_neighbors = k ) \n\n   # get prediction for original dataset \n   knn . fit ( X_train ,   Y_train ) \n   Y_pred   =   knn . predict ( X_test ) \n\n   return   accuracy_score ( Y_test ,   Y_pred )  cm   =   get_accuracy ( X ,   Y )  mm   =   get_accuracy ( Xmm ,   Y )  print ( \"Accuracy: \\n\\t both in cm: {} \\n\\t petal in mm: {}\" . format ( cm ,   mm ))   Accuracy:\n    both in cm: 1.0\n    petal in mm: 0.7    It is kind of obvious here - petal information will barely contribute to the distance    However, it is not always obvious if some features are not suppressed by the way data is normalized    Message 05: be aware of data normalization!",
            "title": "Data normalization"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#mnist",
            "text": "THE MNIST DATABASE of handwritten digits    The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.    It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.       To make it simpler (and faster) let's use digits toy dataset which comes with  scikit-learn   src    Each datapoint is a 8x8 image of a digit.    About 180 samples per class (digit)    Total number of samples 1797",
            "title": "MNIST"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#load-digits",
            "text": "from   sklearn.datasets   import   load_digits  digits   =   load_digits ()  print ( digits . data . shape )   (1797, 64)    digits.images  is a  numpy  array with 1797  numpy  arrays 8x8 (feature vectors) representing digits    digits.target  is a  numpy  array with 1797 integer numbers (class labels)    the code below allow us to visualize a random digits from the dataset    # set grayscale  plt . gray ()  # get some random index from 0 to dataset size  random_index   =   np . random . randint ( 1796 )  # draw random digit  plt . matshow ( digits . images [ random_index ])  # and print the matrix  plt . text ( 8 ,   5 ,   digits . images [ random_index ], \n          fontdict = { 'family' :   'monospace' ,   'size' :   16 })  # and the label  plt . text ( 10 ,   1 ,   \"This is: {}\" . format ( digits . target [ random_index ]), \n          fontdict = { 'family' :   'monospace' ,   'size' :   16 });   <matplotlib.figure.Figure at 0x7faccc90a048>",
            "title": "Load digits"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#distance-between-images",
            "text": "TEST      TRAIN    PIXEL-WISE\n| 4 2 0     2 5 8 |   |2 3 8|\n| 5 3 9  -  2 8 1 | = |3 5 8|  ->  38\n| 0 2 3     1 4 9 |   |1 2 6|",
            "title": "Distance between images"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#prepare-data",
            "text": "We need to split dataset to training and test samples    However, images are in 8x8 format and we have to flatten them first    # the original shape of an image  print ( digits . images . shape )   (1797, 8, 8)  # numpy.reshape is handy here  print ( digits . images . reshape (( 1797 ,   - 1 )) . shape )   (1797, 64)    Please note -1 in new shape    numpy.reshape  allows us to pass one  unknown  dimension which can be determined automatically    Thus, the above is equivalent to    print ( digits . images . reshape (( 1797 ,   64 )) . shape )   (1797, 64)  print ( digits . images . reshape (( - 1 ,   64 )) . shape )   (1797, 64)   As before, we can split our dataset using  sklearn.model_selection.train_test_split   data_train ,   data_test ,   label_train ,   label_test   =  \\\n   split ( digits . images . reshape (( 1797 ,   - 1 )),   digits . target ,   test_size = 0.2 )",
            "title": "Prepare data"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#cross-validation_1",
            "text": "We perform cross-validation on training samples to determine the best  k  (as for the Iris dataset)   avg_scores   =   []    # average score for different k  max_k   =   50  nof_folds   =   10  # loop over different values of k  for   k   in   range ( 1 ,   max_k ): \n   # create knn classifier with k = k \n   knn   =   KNeighborsClassifier ( n_neighbors = k ) \n\n   # cross-validate knn on our training sample with nof_folds \n   scores   =   cross_val_score ( knn ,   data_train ,   label_train , \n                            cv = nof_folds ,   scoring = 'accuracy' ) \n\n   # add avg accuracy to score table \n   avg_scores . append ( scores . mean ())   plt . grid ( True )  plt . xlabel ( \"k\" )  plt . ylabel ( \"Accuracy\" )  plt . xlim ([ 0 ,   max_k ])  plt . ylim ([ 0 ,   1 ])  plt . xticks ( range ( 0 ,   max_k ,   5 ))  plt . plot ( range ( 1 ,   max_k ),   avg_scores );      We used nearly the same procedure as for the Iris dataset    Note, that digits toy dataset prefer different  k    This is the idea of ML - the same algorithm can solve different problems if train on different data    Nowadays, in ML field  data is more important than algorithms  (we have good algorithms already)",
            "title": "Cross-validation"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#final-test",
            "text": "Let's take the bes  k  and check how the classifier works on test samples   from   sklearn.metrics   import   accuracy_score  knn   =   KNeighborsClassifier ( n_neighbors = 1 )  knn . fit ( data_train ,   label_train )  prediction   =   knn . predict ( data_test )  print ( accuracy_score ( label_test ,   prediction ))   0.9888888888888889   We can take a look at misclassified digits   for   i ,   ( true ,   predict )   in   enumerate ( zip ( label_test ,   prediction )): \n   if   true   !=   predict : \n     digit   =   data_test [ i ] . reshape (( 8 ,   8 ))    # reshape again to 8x8 \n     plt . matshow ( digit )                      # for matshow \n     plt . title ( \"{} predicted as {}\" . format ( true ,   predict ))",
            "title": "Final test"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#regression-with-knn",
            "text": "It is also possible to do regression using k-Nearest Neighbors    find  k  nearest neighbors from training samples    calculate the predicted value using inverse distance weighting method     y_{pred}(\\vec x) = \\frac{\\sum\\limits_i w_i(\\vec x) y_{train, i}}{\\sum\\limits_i w_i(\\vec x_i)}  y_{pred}(\\vec x) = \\frac{\\sum\\limits_i w_i(\\vec x) y_{train, i}}{\\sum\\limits_i w_i(\\vec x_i)}     where  w_i(\\vec x) = \\frac{1}{d(\\vec x, \\vec x_{train, i})} w_i(\\vec x) = \\frac{1}{d(\\vec x, \\vec x_{train, i})}    Note, that  y_{pred}(\\vec x) = y_{train, i} y_{pred}(\\vec x) = y_{train, i}  if  d(\\vec x, \\vec x_{train, i}) = 0 d(\\vec x, \\vec x_{train, i}) = 0",
            "title": "Regression with kNN"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#genearate-some-fake-data",
            "text": "Let's grab some random points from the sine function    And add some noise to make it more like real data    data_size   =   50  # generate and sort *data_size* numbers from 0 to 4pi   x_train   =   4   *   np . pi   *   np . sort ( np . random . rand ( data_size ,   1 ),   axis = 0 )  # let's fit to sine    y_train   =   np . sin ( x_train ) . ravel ()  # add some noise to the data  y_train   =   np . array ([ np . random . normal ( y ,   0.05 )   for   y   in   y_train ])  plt . plot ( x_train ,   y_train ,   'ro' );",
            "title": "Genearate some fake data"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#make-a-fit",
            "text": "In general, one should do cross-validation to determine the best  k    We will skip this part during the lecture (feel free to check this at home though!)    Let's just check how kNN fit works for a few different values of  k",
            "title": "Make a fit"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#comment-on-numpynewaxis",
            "text": "# let's create a 1D numpy array  D1   =   np . array ([ 1 ,   2 ,   3 ,   4 ])  print ( D1 )   [1 2 3 4]  # we can easily add another dimension using numpy.newaxis  D2   =   D1 [:,   np . newaxis ]  print ( D2 )   [[1]\n [2]\n [3]\n [4]]",
            "title": "Comment on numpy.newaxis"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#and-back-to-the-task",
            "text": "We use kNN regressor from  scikit-learn  (from intro:  What I really do... )   from   sklearn.neighbors   import   KNeighborsRegressor  # first we need test sample  x_test   =   np . linspace ( 0 ,   4 * np . pi ,   100 )[:,   np . newaxis ]  for   i ,   k   in   enumerate (( 1 ,   5 ,   10 ,   20 )): \n   # weights=distance - weight using distances \n   knn   =   KNeighborsRegressor ( k ,   weights = 'distance' ) \n\n   # calculate y_test for all points in x_test \n   y_test   =   knn . fit ( x_train ,   y_train ) . predict ( x_test ) \n\n   plt . subplot ( 2 ,   2 ,   i   +   1 ) \n\n   plt . title ( \"k = {}\" . format ( k )) \n\n   plt . plot ( x_train ,   y_train ,   'ro' ,   x_test ,   y_test ,   'g' );  plt . tight_layout ()",
            "title": "And back to the task"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/#summary",
            "text": "We have learned first ML algorithm - k-Nearest Neighbors    It has some pros:    easy to understand and implement    no time needed for training - may be used for initial analysis before one reaches for some  heavier  tool    solves nonlinear problems     limited number of hyperparameters    no parameters!    at the end of this lecture we will deal with tens of hyperparameters and thousands of parameters      Although cons make it hard to use in practice    training data must be kept for the whole time (so called  lazy training )    imagine having GB of training samples and you want to make mobile app    other algorithms allows to discard training samples once the model is trained ( eager learning ) - usually it means long training process but super fast classification (which is what we really want)      distance-comparing is not suitable for all data - a picture of a cat on a blue background (e.g. sky) can be close to a ship on a sea (because background pixels vote too)   e.g. for  CIFAR-10  (60k pictures, 10 classes, more about that later) vanilla kNN get less than 40% accuracy     still better than random guessing (10%), but convolutional neural networks get >95%      Still, we have learned from kNN a few important things:    Data is important (both size and quality)    Sometimes data requires preprocessing    Wrong choice of hyperparameters may lead to under- or over-fitting    Use validation samples to tune the model    And  DO NOT  touch test samples until you are done!",
            "title": "Summary"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/",
            "text": "Decision Trees\n\u00b6\n\n\nGraphviz\n\u00b6\n\n\nInstalling graphviz\n\u00b6\n\n\n!\napt\n \ninstall\n \n-\ny\n \ngraphviz\n\n\n!\npip\n \ninstall\n \ngraphviz\n\n\n\n\n\n\nA tree example\n\u00b6\n\n\nfrom\n \ngraphviz\n \nimport\n \nDigraph\n\n\n\nstyles\n \n=\n \n{\n\n    \n'top'\n:\n \n{\n'shape'\n:\n \n'ellipse'\n,\n \n'style'\n:\n \n'filled'\n,\n \n'color'\n:\n \n'lightblue'\n},\n\n    \n'no'\n:\n  \n{\n'shape'\n:\n \n'circle'\n,\n \n'style'\n:\n \n'filled'\n,\n \n'color'\n:\n \n'red'\n},\n\n    \n'yes'\n:\n \n{\n'shape'\n:\n \n'circle'\n,\n \n'style'\n:\n \n'filled'\n,\n \n'color'\n:\n \n'lightgreen'\n},\n\n    \n'qst'\n:\n \n{\n'shape'\n:\n \n'rect'\n}\n\n\n}\n\n\n\nexample_tree\n \n=\n \nDigraph\n()\n\n\n\nexample_tree\n.\nnode\n(\n'top'\n,\n \n'Should I attend the ML lecture?'\n,\n \nstyles\n[\n'top'\n])\n\n\nexample_tree\n.\nnode\n(\n'q1'\n,\n \n'Do I fulfill requirements?'\n,\n \nstyles\n[\n'qst'\n])\n\n\n\nexample_tree\n.\nnode\n(\n'q2'\n,\n \n'Do I like CS?'\n,\n \nstyles\n[\n'qst'\n])\n\n\nexample_tree\n.\nnode\n(\n'no1'\n,\n \n'No '\n,\n \nstyles\n[\n'no'\n])\n\n\n\nexample_tree\n.\nnode\n(\n'q3'\n,\n \n'Is the lecture early in the morning?'\n,\n \nstyles\n[\n'qst'\n])\n\n\nexample_tree\n.\nnode\n(\n'no2'\n,\n \n'No '\n,\n \nstyles\n[\n'no'\n])\n\n\n\nexample_tree\n.\nnode\n(\n'no3'\n,\n \n'No '\n,\n \nstyles\n[\n'no'\n])\n\n\nexample_tree\n.\nnode\n(\n'yes'\n,\n \n'Yes'\n,\n \nstyles\n[\n'yes'\n])\n\n\n\nexample_tree\n.\nedge\n(\n'top'\n,\n \n'q1'\n)\n\n\n\nexample_tree\n.\nedge\n(\n'q1'\n,\n \n'q2'\n,\n \n'Yes'\n)\n\n\nexample_tree\n.\nedge\n(\n'q1'\n,\n \n'no1'\n,\n \n'No'\n)\n\n\n\nexample_tree\n.\nedge\n(\n'q2'\n,\n \n'q3'\n,\n \n'Yes'\n)\n\n\nexample_tree\n.\nedge\n(\n'q2'\n,\n \n'no2'\n,\n \n'No'\n)\n\n\n\nexample_tree\n.\nedge\n(\n'q3'\n,\n \n'no3'\n,\n \n'Yes'\n)\n\n\nexample_tree\n.\nedge\n(\n'q3'\n,\n \n'yes'\n,\n \n'No'\n)\n\n\n\n\n\n\nexample_tree\n\n\n\n\n\n\n\n\nIntroduction\n\u00b6\n\n\nDecision trees\n\u00b6\n\n\n\n\n\n\nSupervised learning algorithm - training dataset with known labels\n\n\n\n\n\n\nEager learning - final model does not need training data to make prediction (all parameters are evaluated during learning step)\n\n\n\n\n\n\nIt can do both classification and regression\n\n\n\n\n\n\nA decision tree is built from:\n\n\n\n\ndecision nodes\n - correspond to features (attributes)\n\n\nleaf nodes\n - correspond to class labels\n\n\n\n\n\n\n\n\nThe \nroot\n of a tree is (should be) the best predictor (feature)\n\n\n\n\n\n\nExample\n\u00b6\n\n\nimport\n \nmatplotlib.pyplot\n \nas\n \nplt\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\n\n# just to overwrite default colab style\n\n\nplt\n.\nstyle\n.\nuse\n(\n'default'\n)\n\n\nplt\n.\nstyle\n.\nuse\n(\n'seaborn-talk'\n)\n\n\n\n\n\n\n# first define some points representing two classes\n\n\ngrid\n \n=\n \nnp\n.\nmgrid\n[\n0\n:\n10\n:\n2\n,\n \n0\n:\n10\n:\n2\n]\n\n\nset01\n \n=\n \nnp\n.\nvstack\n([\ngrid\n[\n0\n]\n.\nravel\n(),\n \ngrid\n[\n1\n]\n.\nravel\n()])\n.\nT\n\n\nset01\n \n=\n \nnp\n.\ndelete\n(\nset01\n,\n \n[\n17\n,\n \n18\n,\n \n19\n,\n \n22\n,\n \n24\n],\n \naxis\n=\n0\n)\n\n\n\ngrid\n \n=\n \nnp\n.\nmgrid\n[\n6\n:\n16\n:\n2\n,\n \n0\n:\n10\n:\n2\n]\n\n\nset02\n \n=\n \nnp\n.\nvstack\n([\ngrid\n[\n0\n]\n.\nravel\n(),\n \ngrid\n[\n1\n]\n.\nravel\n()])\n.\nT\n\n\nset02\n \n=\n \nnp\n.\ndelete\n(\nset02\n,\n \n[\n0\n,\n \n1\n,\n \n5\n,\n \n6\n,\n \n8\n],\n \naxis\n=\n0\n)\n\n\n\nplt\n.\nscatter\n(\n*\nset01\n.\nT\n)\n\n\nplt\n.\nscatter\n(\n*\nset02\n.\nT\n)\n\n\n\nplt\n.\ntext\n(\n15\n,\n \n4\n,\n \n\"There are two attributes: x and y\n\\n\\n\n\"\n\n                \n\"    * each decision node splits dataset based on one of the attributes\n\\n\\n\n\"\n\n                \n\"    * each leaf node defines a class label\"\n);\n\n\n\n\n\n\n\n\nplt\n.\nscatter\n(\n*\nset01\n.\nT\n)\n\n\nplt\n.\nscatter\n(\n*\nset02\n.\nT\n)\n\n\n\nplt\n.\nplot\n([\n5\n,\n \n5\n],\n \n[\n0\n,\n \n8\n],\n \n'r'\n)\n\n\nplt\n.\nplot\n([\n0\n,\n \n14\n],\n \n[\n3\n,\n \n3\n],\n \n'g'\n)\n\n\n\nplt\n.\ntext\n(\n15\n,\n \n3\n,\n \n\"We start with [20, 20] (blue, orange)\n\\n\\n\n\"\n\n                \n\"Red line splits dataset in [15, 0] (left) and [5, 20] (right)\n\\n\\n\n\"\n\n                \n\"Green line split dataset in [10, 6] (bottom) and [10, 14] (top)\n\\n\\n\n\"\n\n                \n\"Red line is a winner and should be the root of our tree\"\n);\n\n\n\n\n\n\n\n\ntree\n \n=\n \nDigraph\n()\n\n\n\ntree\n.\nedge\n(\n\"x > 5?\n\\n\n[20, 20]\"\n,\n \n\"blue\n\\n\n[15, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 5?\n\\n\n[20, 20]\"\n,\n \n\"y > 3?\n\\n\n[5, 20]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"y > 3?\n\\n\n[5, 20]\"\n,\n \n\"x > 9?\n\\n\n[4, 6]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"y > 3?\n\\n\n[5, 20]\"\n,\n \n\"almost orange\n\\n\n[1, 14]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"x > 9?\n\\n\n[4, 6]\"\n,\n \n\"blue\n\\n\n[4, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 9?\n\\n\n[4, 6]\"\n,\n \n\"orange\n\\n\n[0, 6]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"almost orange\n\\n\n[1, 14]\"\n,\n \n\"Should we continue?\n\\n\nOr would it be overfitting?\"\n)\n\n\n\ntree\n\n\n\n\n\n\n\n\n\n\n\n\nIt is important to start with good predictor\n\n\n\n\n\n\nOur choice of the root classifies 37.5% of points  in the first step\n\n\n\n\n\n\nNote, that we could also start with \nx > 9?\n\n\n\n\n\n\nHowever, if we started with \ny > 3\n we would never classify a point in the first step - does it mean that it is worse choice?\n\n\n\n\n\n\n\n\n\n\ntree\n \n=\n \nDigraph\n()\n\n\n\ntree\n.\nedge\n(\n\"y > 3?\n\\n\n[20, 20]\"\n,\n \n\"x > 9?\n\\n\n[10, 6]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"y > 3?\n\\n\n[20, 20]\"\n,\n \n\"x > 5?\n\\n\n[10, 14]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"x > 9?\n\\n\n[10, 6]\"\n,\n \n\"blue\n\\n\n[10, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 9?\n\\n\n[10, 6]\"\n,\n \n\"orange\n\\n\n[0, 6]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"x > 5?\n\\n\n[10, 14]\"\n,\n \n\"blue\n\\n\n[9, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 5?\n\\n\n[10, 14]\"\n,\n \n\"almost orange\n\\n\n[1, 14]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n\n\n\n\n\n\n\n\n\n\n\n\nIn this case we never have to make more than 2 checks\n\n\n\n\n\n\nThere are two open questions to answer:\n\n\n\n\n\n\nHow to automate the procees of chosing nodes?\n\n\n\n\n\n\nHow deep should we go?\n\n\n\n\n\n\n\n\n\n\nID3 and C4.5 algorithms\n\u00b6\n\n\n\n\n\n\nWe start with algorithms based on information theory\n\n\n\n\n\n\nID3 (Iterative Dichotomiser 3)\n\n\n\n\n\n\nC4.5 - extension of ID3 (why C4.5? C stands for programming language and 4.5 for version?)\n\n\n\n\n\n\nC5.0/See5 - improved C4.5 (commercial; single-threaded Linux version is available under GPL though)\n\n\n\n\n\n\n\n\n\n\nThe idea is to find nodes which maximize information gain\n\n\n\n\n\n\nInformation gain\n\u00b6\n\n\nSelf-information\n\u00b6\n\n\n\n\n\n\nLet \nX = (x_1, x_2, ..., x_n)\nX = (x_1, x_2, ..., x_n)\n be our \ninformation source\n (feature), e.g. weather condition: \nx_1\nx_1\n = sunny, \nx_2\nx_2\n = overcast, \nx_3\nx_3\n = rainy\n\n\n\n\n\n\nAnd let \nP = (p_1, p_2, ..., p_n)\nP = (p_1, p_2, ..., p_n)\n be corresponding probrability distribution (or more precisely - probability mass function)\n\n\n\n\n\n\nWe want some measure of information \nI\nI\n provided by an event. It should satisfy the following properties:\n\n\n\n\n\n\nI\nI\n depends only on the probability of \nx_i\nx_i\n, thus \nI \\equiv I(p_i)\nI \\equiv I(p_i)\n\n\n\n\n\n\nI\nI\n is continuous and deacreasing function of \np_i\np_i\n\n\n\n\n\n\nI\nI\n is non-negative and \nI(1) = 0\nI(1) = 0\n\n\n\n\n\n\nif \np_i = p_{i, 1} \\cdot p_{i, 2}\np_i = p_{i, 1} \\cdot p_{i, 2}\n (independent events) then \nI(p_i) = I(p_{i, 1}) + I(p_{i, 2})\nI(p_i) = I(p_{i, 1}) + I(p_{i, 2})\n\n\n\n\n\n\n\n\n\n\nLogarithmic function satisfies all above condition, so we define self-information as: \nI(p) = -\\log(p)\nI(p) = -\\log(p)\n\n\n\n\n\n\nThe most common log base is \n2\n and then information is in \nshannons (Sh)\n, also known as \nbits\n\n\n\n\n\n\nIn the case of \nnatural logarithm\n the unit is \nnat\n (natural unit of information)\n\n\n\n\n\n\nIn the case of base \n10\n the unit is \nhartley (Hart)\n, also known as \ndit\n\n\n\n\n\n\n\n\n\n\nx\n \n=\n \nnp\n.\narange\n(\n0.01\n,\n \n1.01\n,\n \n0.01\n)\n\n\n\nplt\n.\nxlabel\n(\n\"p\"\n)\n\n\nplt\n.\nylabel\n(\n\"I(p)\"\n)\n\n\n\nplt\n.\nplot\n(\nx\n,\n \n-\nnp\n.\nlog2\n(\nx\n),\n \nlabel\n=\n\"bit\"\n)\n\n\nplt\n.\nplot\n(\nx\n,\n \n-\nnp\n.\nlog\n(\nx\n),\n \nlabel\n=\n\"nat\"\n)\n\n\nplt\n.\nplot\n(\nx\n,\n \n-\nnp\n.\nlog10\n(\nx\n),\n \nlabel\n=\n\"dit\"\n)\n\n\n\nplt\n.\nlegend\n();\n\n\n\n\n\n\n\n\n\n\n\n\nLets X = (head, tail) with P = (0.5, 0.5)\n\n\n\n\nWe get 1 Sh of information\n\n\n\n\n\n\n\n\nLets X = (sunny, overcast, rainy) with P = (0.25, 0.75, 0.25)\n\n\n\n\n\n\nIf it is overcast, we get 0.415 Sh of information\n\n\n\n\n\n\nOtherwise, we get 2 Sh of information\n\n\n\n\n\n\n\n\n\n\nIf an event is more likely we learn less\n\n\n\n\n\n\nInformation entropy\n\u00b6\n\n\n\n\n\n\nAlso called Shannon entropy (after the father of intromation theory)\n\n\n\n\n\n\nUsually information entropy is denoted as \nH\nH\n\n\n\n\n\n\nH\nH\n is defined as the weighted average of the self-information of all possible outcomes \nH(X) = \\sum\\limits_{i=1}^N p_i \\cdot I(p_i) = -\\sum\\limits_{i=1}^N p_i\\cdot\\log(p_i)\nH(X) = \\sum\\limits_{i=1}^N p_i \\cdot I(p_i) = -\\sum\\limits_{i=1}^N p_i\\cdot\\log(p_i)\n\n\n\n\n\n\nLets consider two case scenario with \nP = (p, 1 - p)\nP = (p, 1 - p)\n, so entropy is given by \nH = -p \\log(p) - (1 - p) \\log(1 - p)\nH = -p \\log(p) - (1 - p) \\log(1 - p)\n\n\n\n\n\n\np\n \n=\n \nnp\n.\narange\n(\n0.01\n,\n \n1.0\n,\n \n0.01\n)\n\n\n\nplt\n.\nxlabel\n(\n\"p\"\n)\n\n\nplt\n.\nylabel\n(\n\"H\"\n)\n\n\n\nplt\n.\nannotate\n(\n'we are surprised'\n,\n \nxy\n=\n(\n0.5\n,\n \n1\n),\n \nxytext\n=\n(\n0.5\n,\n \n0.75\n),\n\n             \narrowprops\n=\ndict\n(\nfacecolor\n=\n'black'\n,\n \nshrink\n=\n0.1\n))\n\n\n\nplt\n.\nannotate\n(\n'we are not that surprised'\n,\n \nxy\n=\n(\n1\n,\n \n0.1\n),\n \nxytext\n=\n(\n0.5\n,\n \n0.25\n),\n\n             \narrowprops\n=\ndict\n(\nfacecolor\n=\n'black'\n,\n \nshrink\n=\n0.1\n))\n\n\n\nplt\n.\nplot\n(\np\n,\n \n-\np\n \n*\n \nnp\n.\nlog2\n(\np\n)\n \n-\n \n(\n1\n \n-\n \np\n)\n \n*\n \nnp\n.\nlog2\n(\n1\n \n-\n \np\n));\n\n\n\n\n\n\n\n\n\n\nLets consider three case scenario with \nP = (p, q, 1 - p - q)\nP = (p, q, 1 - p - q)\n, so entropy is given by \nH = -p \\log(p) - q\\log(q) - (1 - p - q) \\log(1 - p - q)\nH = -p \\log(p) - q\\log(q) - (1 - p - q) \\log(1 - p - q)\n\n\n\n\nfrom\n \nmpl_toolkits\n \nimport\n \nmplot3d\n\n\n\n# grid of p, q probabilities\n\n\np\n,\n \nq\n \n=\n \nnp\n.\nmeshgrid\n(\nnp\n.\narange\n(\n0.01\n,\n \n1.0\n,\n \n0.01\n),\n \nnp\n.\narange\n(\n0.01\n,\n \n1.0\n,\n \n0.01\n))\n\n\n\n# remove (set to 0) points which do not fulfill P <= 1\n\n\nidx\n \n=\n \np\n \n+\n \nq\n \n>\n \n1\n\n\np\n[\nidx\n]\n \n=\n \n0\n\n\nq\n[\nidx\n]\n \n=\n \n0\n\n\n\n# calculate entropy (disable warnings - we are aware of log(0))\n\n\nnp\n.\nwarnings\n.\nfilterwarnings\n(\n'ignore'\n)\n\n\nh\n \n=\n \n-\np\n \n*\n \nnp\n.\nlog2\n(\np\n)\n \n-\n \nq\n \n*\n \nnp\n.\nlog2\n(\nq\n)\n \n-\n \n(\n1\n \n-\n \np\n \n-\n \nq\n)\n \n*\n \nnp\n.\nlog2\n(\n1\n \n-\n \np\n \n-\n \nq\n)\n\n\n\n# make a plot\n\n\nplt\n.\naxes\n(\nprojection\n=\n'3d'\n)\n.\nplot_surface\n(\np\n,\n \nq\n,\n \nh\n);\n\n\n\n\n\n\n\n\nInformation gain\n\u00b6\n\n\n\n\n\n\nLet \nT\nT\n be the set of training samples with \nn\nn\n possible outcomes, thus \nT = \\{T_1, T_2, ..., T_n\\}\nT = \\{T_1, T_2, ..., T_n\\}\n\n\n\n\n\n\nThe entropy is given by \nH(T) = -\\sum\\limits_{i=1}^N p_i\\cdot\\log(p_i) = -\\sum\\limits_{i=1}^N \\frac{|T_i|}{|T|}\\cdot\\log(\\frac{|T_i|}{|T|})\nH(T) = -\\sum\\limits_{i=1}^N p_i\\cdot\\log(p_i) = -\\sum\\limits_{i=1}^N \\frac{|T_i|}{|T|}\\cdot\\log(\\frac{|T_i|}{|T|})\n\n\n\n\n\n\nWe can also calulate the entropy after \nT\nT\n was partitioned in \nT_i\nT_i\n with respect to some feature \nX\nX\n \nH(T, X) = \\sum\\limits_{i=1}^N p_i\\cdot H(T_i)\nH(T, X) = \\sum\\limits_{i=1}^N p_i\\cdot H(T_i)\n\n\n\n\n\n\nAnd the information gain is defined as \nG(X) = H(T) - H(T, X)\nG(X) = H(T) - H(T, X)\n\n\n\n\n\n\nExample\n\u00b6\n\n\n\n\n\n\nLets calculate some example step by step\n\n\n\n\n\n\nLets consider a fake dataset\n\n\n\n\n\n\ntwo classes: C01, C02\n\n\n\n\n\n\nthree features: X1, X2, X3\n\n\n\n\n\n\n\n\n\n\n   X1  ||  A  |  A  |  A  |  B  |  B  |  C  |  C  |  C  |  C  |\n---------------------------------------------------------------\n   X2  ||  0  |  0  |  1  |  1  |  0  |  1  |  1  |  1  |  0  |\n---------------------------------------------------------------\n   X3  || RED | GRN | GRN | BLU | RED | GRN | BLU | RED | GRN |\n===============================================================\n Class || C01 | C01 | C02 | C02 | C02 | C02 | C01 | C01 | C02 |\n\n\n\n\n\nfrom\n \nmath\n \nimport\n \nlog\n\n\n\ndef\n \nentropy\n(\n*\nprobs\n):\n\n  \n\"\"\"Calculate information entropy\"\"\"\n\n  \ntry\n:\n\n    \ntotal\n \n=\n \nsum\n(\nprobs\n)\n\n    \nreturn\n \nsum\n([\n-\np\n \n/\n \ntotal\n \n*\n \nlog\n(\np\n \n/\n \ntotal\n,\n \n2\n)\n \nfor\n \np\n \nin\n \nprobs\n])\n\n  \nexcept\n:\n\n    \nreturn\n \n0\n\n\n\nprint\n(\nentropy\n(\n4\n,\n \n5\n),\n \nentropy\n(\n2\n,\n \n1\n),\n \nentropy\n(\n2\n,\n \n2\n))\n\n\n\n\n\n\n0.9910760598382222 0.9182958340544896 1.0\n\n\n\n\n\n\n\n\n\nThe \nroot\n entropy\n\n\n\n\nWe have 9 samples: 4 belong to class C01 and 5 to C02 \nH(T) = -\\frac{4}{9}\\log(\\frac{4}{9}) - \\frac{5}{9}\\log(\\frac{5}{9}) = 0.99\nH(T) = -\\frac{4}{9}\\log(\\frac{4}{9}) - \\frac{5}{9}\\log(\\frac{5}{9}) = 0.99\n\n\n\n\n\n\n\n\nNow lets consider feature X1, which splits data into subsets \nT_1\nT_1\n, \nT_2\nT_2\n, and \nT_3\nT_3\n (with X1 value A, B, and C, respectively)\n\n\n\n\n\n\nWithin \nT_1\nT_1\n there are 3 samples: 2 from C01 and 1 from C02 \nH(T_1) = -\\frac{2}{3}\\log(\\frac{2}{3}) - \\frac{1}{3}\\log(\\frac{1}{3}) = 0.92\nH(T_1) = -\\frac{2}{3}\\log(\\frac{2}{3}) - \\frac{1}{3}\\log(\\frac{1}{3}) = 0.92\n\n\n\n\n\n\nWithin \nT_2\nT_2\n there are 2 samples: 0 from C01 and 2 from C02 \nH(T_2) = -\\frac{2}{2}\\log(\\frac{2}{2}) - \\frac{0}{2}\\log(\\frac{0}{2}) = 0.00\nH(T_2) = -\\frac{2}{2}\\log(\\frac{2}{2}) - \\frac{0}{2}\\log(\\frac{0}{2}) = 0.00\n\n\n\n\n\n\nWithin \nT_3\nT_3\n there are 4 samples: 2 from C01 and 2 from C02 \nH(T_3) = -\\frac{2}{4}\\log(\\frac{2}{4}) - \\frac{2}{4}\\log(\\frac{2}{4}) = 1.00\nH(T_3) = -\\frac{2}{4}\\log(\\frac{2}{4}) - \\frac{2}{4}\\log(\\frac{2}{4}) = 1.00\n\n\n\n\n\n\nThe resulting entropy is \nH(T, X1) = \\frac{3}{9}\\cdot H(T_1) + \\frac{2}{9}\\cdot H(T_2) + \\frac{4}{9}\\cdot H(T_3) = 0.75\nH(T, X1) = \\frac{3}{9}\\cdot H(T_1) + \\frac{2}{9}\\cdot H(T_2) + \\frac{4}{9}\\cdot H(T_3) = 0.75\n\n\n\n\n\n\nThus, infromation gain if the set is split according to X1 \nG(X1) = H(T) - H(T, X1) = 0.99 - 0.75 = 0.24 \\mbox{ Sh }\nG(X1) = H(T) - H(T, X1) = 0.99 - 0.75 = 0.24 \\mbox{ Sh }\n\n\n\n\n\n\n\n\n\n\nID3 algorithm\n\u00b6\n\n\n\n\n\n\nFor every attribute (feature) calculate the entropy\n\n\n\n\n\n\nSplit the training set using the one for which information gain is maximum\n\n\n\n\n\n\nContinue recursively on subsets using remaining features\n\n\n\n\n\n\nPlay Golf dataset\n\u00b6\n\n\n\n\n\n\nPopular dataset to explain decision trees\n\n\n\n\n\n\n4 features:\n\n\n\n\n\n\noutlook\n: \nrainy, overcast, sunny\n\n\n\n\n\n\ntemperature\n: \ncool, mild, hot\n\n\n\n\n\n\nhumidity\n: \nnormal, high\n\n\n\n\n\n\nwindy\n: \nfalse, true\n\n\n\n\n\n\n\n\n\n\nPossible outcomes (play golf?):\n\n\n\n\n\n\nfalse\n\n\n\n\n\n\ntrue\n\n\n\n\n\n\n\n\n\n\nimport\n \npandas\n \nas\n \npd\n\n\n\n# first row = headers\n\n\nsrc\n \n=\n \n\"http://chem-eng.utoronto.ca/~datamining/dmc/datasets/weather_nominal.csv\"\n\n\n\ngolf_data\n \n=\n \npd\n.\nread_csv\n(\nsrc\n)\n\n\n\n\n\n\ngolf_data\n\n\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nOutlook\n\n      \nTemperature\n\n      \nHumidity\n\n      \nWindy\n\n      \nPlay golf\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \nRainy\n\n      \nHot\n\n      \nHigh\n\n      \nFalse\n\n      \nNo\n\n    \n\n    \n\n      \n1\n\n      \nRainy\n\n      \nHot\n\n      \nHigh\n\n      \nTrue\n\n      \nNo\n\n    \n\n    \n\n      \n2\n\n      \nOvercast\n\n      \nHot\n\n      \nHigh\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n3\n\n      \nSunny\n\n      \nMild\n\n      \nHigh\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n4\n\n      \nSunny\n\n      \nCool\n\n      \nNormal\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n5\n\n      \nSunny\n\n      \nCool\n\n      \nNormal\n\n      \nTrue\n\n      \nNo\n\n    \n\n    \n\n      \n6\n\n      \nOvercast\n\n      \nCool\n\n      \nNormal\n\n      \nTrue\n\n      \nYes\n\n    \n\n    \n\n      \n7\n\n      \nRainy\n\n      \nMild\n\n      \nHigh\n\n      \nFalse\n\n      \nNo\n\n    \n\n    \n\n      \n8\n\n      \nRainy\n\n      \nCool\n\n      \nNormal\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n9\n\n      \nSunny\n\n      \nMild\n\n      \nNormal\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n10\n\n      \nRainy\n\n      \nMild\n\n      \nNormal\n\n      \nTrue\n\n      \nYes\n\n    \n\n    \n\n      \n11\n\n      \nOvercast\n\n      \nMild\n\n      \nHigh\n\n      \nTrue\n\n      \nYes\n\n    \n\n    \n\n      \n12\n\n      \nOvercast\n\n      \nHot\n\n      \nNormal\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n13\n\n      \nSunny\n\n      \nMild\n\n      \nHigh\n\n      \nTrue\n\n      \nNo\n\n    \n\n  \n\n\n\n\n\n\n\nPlay golf entropy\n\u00b6\n\n\nentropy\n(\n9\n,\n \n5\n)\n\n\n\n\n\n\n0.9402859586706309\n\n\n\n\n\n| Play golf |\n=============\n| yes | no  |  -> H(T) = 0.94\n-------------\n|  9  |  5  |\n\n\n\n\n\nPlay golf vs outlook\n\u00b6\n\n\n                   | Play golf |\n                   =============\n                   | yes | no  |\n        ------------------------\n        | sunny    |  3  |  2  |  5\noutlook | overcast |  4  |  0  |  4\n        | rainy    |  2  |  3  |  5\n        ------------------------\n                      9     5\n\n\n\n\n\nentropy\n(\n3\n,\n \n2\n),\n \n0\n,\n \nentropy\n(\n2\n,\n \n3\n)\n\n\n\n\n\n\n(0.9709505944546686, 0, 0.9709505944546686)\n\n\n\n\n\n\n\n\n\\begin{eqnarray}\n   H(\\mbox{sunny}) & = & 0.97 \\\\\n   H(\\mbox{rainy}) & = & 0.97 \\\\\nH(\\mbox{overcast}) & = & 0\n\\end{eqnarray}\n\n\n\n\n\n\n\n\n\n\n\n\\begin{eqnarray}\nH(T, \\mbox{outlook}) & = & P(\\mbox{sunny})\\cdot H(\\mbox{sunny}) + P(\\mbox{overcast})\\cdot H(\\mbox{overcast}) + P(\\mbox{rainy})\\cdot H(\\mbox{rainy}) \\\\\n                     & = & \\frac{5}{14}\\cdot 0.97 + \\frac{4}{14} \\cdot 0 + \\frac{5}{14}\\cdot 0.97 = 0.69\n\\end{eqnarray}\n\n\n\n\n\n\n\n\n\n\n\n\\begin{eqnarray}\nG(\\mbox{outlook}) & = & H(T) - H(T, \\mbox{outlook}) = 0.94 - 0.69 = 0.25\n\\end{eqnarray}\n\n\n\n\n\n\nResults for all features\n\u00b6\n\n\n                    | Play golf |                         | Play golf |\n                    =============                         =============\n                    | yes | no  |                         | yes | no  |\n         ------------------------                 --------------------\n         | sunny    |  3  |  2  |                 | hot   |  2  |  2  |\n outlook | overcast |  4  |  0  |     temperature | mild  |  4  |  2  |\n         | rainy    |  2  |  3  |                 | cool  |  3  |  1  |\n         ------------------------                 --------------------\n            Info. gain = 0.25                       Info gain = 0.03\n\n\n                    | Play golf |                         | Play golf |\n                    =============                         =============\n                    | yes | no  |                         | yes | no  |\n         ------------------------                 --------------------\n         | high     |  3  |  4  |                 | false |  6  |  2  |\nhumidity | normal   |  6  |  1  |           windy | true  |  3  |  3  |\n         ------------------------                 --------------------\n            Info. gain = 0.15                       Info gain = 0.05\n\n\n\n\n\nRoot of the tree\n\u00b6\n\n\n\n\n\n\nStart building a tree with the feature with the largest information gain: \noutlook\n\n\n\n\n\n\nA branch with \nentropy 0\n is a leaf node: \novercast\n\n\n\n\n\n\nOther branches must be spliited using other features \n\n\n\n\n\n\ntree\n \n=\n \nDigraph\n()\n\n\n\ntree\n.\nedge\n(\n\"outlook\"\n,\n \n\"sunny\"\n)\n\n\ntree\n.\nedge\n(\n\"outlook\"\n,\n \n\"overcast\"\n)\n\n\ntree\n.\nedge\n(\n\"outlook\"\n,\n \n\"rainy\"\n)\n\n\n\ntree\n.\nedge\n(\n\"overcast\"\n,\n \n\"yes\"\n)\n\n\n\ntree\n\n\n\n\n\n\n\n\nNext branch\n\u00b6\n\n\ngolf_data\n.\nloc\n[\ngolf_data\n[\n'Outlook'\n]\n \n==\n \n\"Sunny\"\n]\n\n\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nOutlook\n\n      \nTemperature\n\n      \nHumidity\n\n      \nWindy\n\n      \nPlay golf\n\n    \n\n  \n\n  \n\n    \n\n      \n3\n\n      \nSunny\n\n      \nMild\n\n      \nHigh\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n4\n\n      \nSunny\n\n      \nCool\n\n      \nNormal\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n5\n\n      \nSunny\n\n      \nCool\n\n      \nNormal\n\n      \nTrue\n\n      \nNo\n\n    \n\n    \n\n      \n9\n\n      \nSunny\n\n      \nMild\n\n      \nNormal\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n13\n\n      \nSunny\n\n      \nMild\n\n      \nHigh\n\n      \nTrue\n\n      \nNo\n\n    \n\n  \n\n\n\n\n\n\n\n\n\n\n\nIn general, one should calculate information gain for each feature for this subset\n\n\n\n\n\n\nIn this case it is clear that we can take \nwindy\n \n\n\n\n\n\n\ntree\n.\nedge\n(\n\"sunny\"\n,\n \n\"windy\"\n)\n\n\n\ntree\n.\nedge\n(\n\"windy\"\n,\n \n\"false\"\n)\n\n\ntree\n.\nedge\n(\n\"windy\"\n,\n \n\"true\"\n)\n\n\n\ntree\n.\nedge\n(\n\"false\"\n,\n \n\"yes\"\n)\n\n\ntree\n.\nedge\n(\n\"true\"\n,\n \n\"no\"\n)\n\n\n\ntree\n\n\n\n\n\n\n\n\nLast branch\n\u00b6\n\n\ngolf_data\n.\nloc\n[\ngolf_data\n[\n'Outlook'\n]\n \n==\n \n\"Rainy\"\n]\n\n\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nOutlook\n\n      \nTemperature\n\n      \nHumidity\n\n      \nWindy\n\n      \nPlay golf\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \nRainy\n\n      \nHot\n\n      \nHigh\n\n      \nFalse\n\n      \nNo\n\n    \n\n    \n\n      \n1\n\n      \nRainy\n\n      \nHot\n\n      \nHigh\n\n      \nTrue\n\n      \nNo\n\n    \n\n    \n\n      \n7\n\n      \nRainy\n\n      \nMild\n\n      \nHigh\n\n      \nFalse\n\n      \nNo\n\n    \n\n    \n\n      \n8\n\n      \nRainy\n\n      \nCool\n\n      \nNormal\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n10\n\n      \nRainy\n\n      \nMild\n\n      \nNormal\n\n      \nTrue\n\n      \nYes\n\n    \n\n  \n\n\n\n\n\n\n\ntree\n.\nedge\n(\n\"rainy\"\n,\n \n\"humidity\"\n)\n\n\n\ntree\n.\nedge\n(\n\"humidity\"\n,\n \n\"high\"\n)\n\n\ntree\n.\nedge\n(\n\"humidity\"\n,\n \n\"normal\"\n)\n\n\n\ntree\n.\nedge\n(\n\"normal\"\n,\n \n\"yes\"\n)\n\n\ntree\n.\nedge\n(\n\"high\"\n,\n \n\"no \"\n)\n\n\n\ntree\n\n\n\n\n\n\n\n\nSummary\n\u00b6\n\n\n\n\n\n\nWe got the final tree for Play Golf dataset using ID3 algorithm\n\n\n\n\n\n\nWe do not even use temperature attribute (for which information gain was 0.03)\n\n\n\n\n\n\nThe main problem is that the algorithm may overfit easily (tree does not stop growing until the whole training set is classified)\n\n\n\n\n\n\nImagine some crazy guys went playing on a \nrainy\n, \nwindy\n day with \nhigh humidity\n, beacaue it was still \nhot\n\n\n\n\n\n\nWith this extra data point we would have to create more branches\n\n\n\n\n\n\nIs one unique data sample worth to extend the whole tree?\n\n\n\n\n\n\n\n\n\n\nAnd there is more disadvantages:\n\n\n\n\n\n\nIt handles only discrete attributes\n\n\n\n\n\n\nThere is a strong bias for features with many possible outcomes\n\n\n\n\n\n\nAnd finally, it does not handle missing values\n\n\n\n\n\n\n\n\n\n\nC4.5 algorithm\n\u00b6\n\n\n\n\n\n\nC4.5 introduces some improvements to ID3:\n\n\n\n\n\n\ncontinuous values using threshold\n\n\n\n\n\n\ntree pruning to avoid overfitting\n\n\n\n\n\n\nnormalized information gain\n\n\n\n\n\n\nmissing values\n\n\n\n\n\n\n\n\n\n\nInformation gain ratio\n\u00b6\n\n\n\n\n\n\nTo avoid a bias in favor of features with a lot of different values C4.5 uses information gain ratio instead of information gain\n\n\n\n\n\n\nLets define intrinsic value \nV\nV\n of an attribute \nX\nX\n as \nV(X) = -\\sum\\limits_{i=1}^N \\frac{|T_i|}{|T|}\\cdot\\log(\\frac{|T_i|}{|T|})\nV(X) = -\\sum\\limits_{i=1}^N \\frac{|T_i|}{|T|}\\cdot\\log(\\frac{|T_i|}{|T|})\n\n\n\n\n\n\nwhere \nT_i\nT_i\n are samples corresponding to \ni\ni\n-th possible value of \nX\nX\n feature\n\n\n\n\n\n\nInformation gain ratio \nR(X)\nR(X)\n is defined as \nR(X) = \\frac{G(X)}{V(X)}\nR(X) = \\frac{G(X)}{V(X)}\n\n\n\n\n\n\nExample\n\u00b6\n\n\n\n\n\n\nLets consider a fake data set\n\n\n\n\n\n\nThe goal is to determine if someone plays or not video games\n\n\n\n\n\n\nWe have three features:\n\n\n\n\n\n\nname - mostly unique\n\n\n\n\n\n\nsex - 50% females and 50% males \n\n\n\n\n\n\nage - just old or young\n\n\n\n\n\n\n\n\n\n\nLooking at data we can say that\n\n\n\n\n\n\nmost young people play video games, why old people don't\n\n\n\n\n\n\nsex does not matter\n\n\n\n\n\n\nnames are almost distinct\n\n\n\n\n\n\n\n\n\n\n     name   ||  John  |  Mark  |  Anne  |  Adam  |  John  |  Alex  |  Alex  |  Xena  |  Tina  |  Lucy  |\n--------------------------------------------------------------------------------------------------------\n     sex    ||   M    |   M    |   F    |   M    |   M    |   F    |   M    |   F    |   F    |   F    |\n--------------------------------------------------------------------------------------------------------\n     age    ||  old   | young  |  old   | young  | young  | young  |  old   |  old   | young  | young  |\n========================================================================================================\n play games ||   N    |   Y    |   Y    |   Y    |   Y    |   N    |   N    |   N    |   Y    |   Y    |\n\n\n\n\n\n\n\nInformation gain for \nname\n\n\n\n\nh\n \n=\n \nentropy\n(\n4\n,\n \n6\n)\n  \n# dataset entropy H(T)\n\n\n\n# one John plays and the other one doesn't\n\n\n# in other cases entropy = 0\n\n\ng_name\n \n=\n \nh\n \n-\n \n2\n/\n10\n \n*\n \nentropy\n(\n1\n,\n \n1\n)\n\n\n\nprint\n(\ng_name\n)\n\n\n\n\n\n\n0.7709505944546686\n\n\n\n\n\n\n\nInformation gain for \nsex\n\n\n\n\n# 5 men - 3 play\n\n\n# 5 women - 3 play\n\n\ng_sex\n \n=\n \nh\n \n-\n \n5\n/\n10\n \n*\n \nentropy\n(\n2\n,\n \n3\n)\n \n-\n \n5\n/\n10\n \n*\n \nentropy\n(\n2\n,\n \n3\n)\n\n\n\nprint\n(\ng_sex\n)\n\n\n\n\n\n\n0.0\n\n\n\n\n\n\n\nInformation gain for \nage\n\n\n\n\n# 4 old people - 1 plays\n\n\n# 6 young people - 5 play\n\n\ng_age\n \n=\n \nh\n \n-\n \n4\n/\n10\n \n*\n \nentropy\n(\n1\n,\n \n3\n)\n \n-\n \n6\n/\n10\n \n*\n \nentropy\n(\n1\n,\n \n5\n)\n\n\n\nprint\n(\ng_age\n)\n\n\n\n\n\n\n0.256425891682003\n\n\n\n\n\n\n\n\n\nIn ID3 a feature with entropy = 0 is always a winner\n\n\n\n\nImagine having all distinct values (e.g. credit card numbers)\n\n\n\n\n\n\n\n\nIn this case we would choose \nname\n as the best predictor\n\n\n\n\n\n\nCreating a tree with 8 branches (from 10 samples)\n\n\n\n\n\n\nTraining data would be perfectly classify\n\n\n\n\n\n\nBut it is unlikely that the algorithm would be able to generalize for unseen data\n\n\n\n\n\n\n\n\n\n\nLets calculate information gain ratio and see how it changes the choice of the best feature\n\n\n\n\n\n\nInformation gain ratio for \nname\n\n\n\n\n\n\n# 2x John, 2x Alex, 6x unique name \n\n\ng_name\n \n/\n \nentropy\n(\n2\n,\n \n2\n,\n \n*\n[\n1\n]\n*\n6\n)\n\n\n\n\n\n\n0.26384995435159336\n\n\n\n\n\n\n\nInformation gain ratio for \nsex\n\n\n\n\n# 5 males and 5 females - zero stays zero though\n\n\ng_sex\n \n/\n \nentropy\n(\n5\n,\n \n5\n)\n\n\n\n\n\n\n0.0\n\n\n\n\n\n\n\nInformation gain ratio for \nage\n\n\n\n\n# 4x old and 6x young\n\n\ng_age\n \n/\n \nentropy\n(\n4\n,\n \n6\n)\n\n\n\n\n\n\n0.26409777505314147\n\n\n\n\n\n\n\n\n\nBased on information gain ratio we choose \nage\n as the best predictor\n\n\n\n\n\n\nBecause the denominator in a ratio penalizes features with many values\n\n\n\n\n\n\nprint\n(\n\"Two possible values:\n\\n\n\"\n)\n\n\n\nfor\n \ni\n \nin\n \nrange\n(\n0\n,\n \n11\n):\n\n  \nprint\n(\n\"\n\\t\n({}, {}) split -> entropy = {}\"\n.\nformat\n(\ni\n,\n \n10\n-\ni\n,\n \nentropy\n(\ni\n,\n \n10\n-\ni\n)))\n\n\n\nprint\n(\n\"\n\\n\n10 possible values:\"\n,\n \nentropy\n(\n*\n[\n1\n]\n*\n10\n))\n\n\n\n\n\n\nTwo possible values:\n\n    (0, 10) split -> entropy = 0\n    (1, 9) split -> entropy = 0.4689955935892812\n    (2, 8) split -> entropy = 0.7219280948873623\n    (3, 7) split -> entropy = 0.8812908992306927\n    (4, 6) split -> entropy = 0.9709505944546686\n    (5, 5) split -> entropy = 1.0\n    (6, 4) split -> entropy = 0.9709505944546686\n    (7, 3) split -> entropy = 0.8812908992306927\n    (8, 2) split -> entropy = 0.7219280948873623\n    (9, 1) split -> entropy = 0.4689955935892812\n    (10, 0) split -> entropy = 0\n\n10 possible values: 3.321928094887362\n\n\n\n\n\n\n\nThis datset was handcrafted to make a point, but I hope the message is still clear\n\n\n\n\nContinuous values\n\u00b6\n\n\n\n\n\n\nAttributes with continuous values must be first discretize\n\n\n\n\n\n\nThe best way is to find an optimal threshold which splits the set\n\n\n\n\n\n\nThe optimal threshold is the one which maximize the infromation gain\n\n\n\n\n\n\nExample\n\u00b6\n\n\n\n\n\n\nLets consider the same example as before\n\n\n\n\n\n\nBut this time age has numerical values\n\n\n\n\n\n\n     name   ||  John  |  Mark  |  Anne  |  Adam  |  John  |  Alex  |  Alex  |  Xena  |  Tina  |  Lucy  |\n--------------------------------------------------------------------------------------------------------\n     sex    ||   M    |   M    |   F    |   M    |   M    |   F    |   M    |   F    |   F    |   F    |\n--------------------------------------------------------------------------------------------------------\n     age    ||   50   |   18   |   65   |   24   |   31   |   18   |   50   |   50   |   24   |   31   |\n========================================================================================================\n play games ||   N    |   Y    |   Y    |   Y    |   Y    |   N    |   N    |   N    |   Y    |   Y    |\n\n\n\n\n\n\n\nThe possible thesholds are therefore \n\\{18, 24, 31, 50\\}\n\\{18, 24, 31, 50\\}\n\n\n\n\n# calculate entropy for all possible thresholds\n\n\ne18\n \n=\n \n2\n/\n10\n \n*\n \nentropy\n(\n1\n,\n \n1\n)\n \n+\n \n8\n/\n10\n \n*\n \nentropy\n(\n3\n,\n \n5\n)\n\n\ne24\n \n=\n \n4\n/\n10\n \n*\n \nentropy\n(\n1\n,\n \n3\n)\n \n+\n \n6\n/\n10\n \n*\n \nentropy\n(\n3\n,\n \n3\n)\n\n\ne31\n \n=\n \n6\n/\n10\n \n*\n \nentropy\n(\n1\n,\n \n5\n)\n \n+\n \n4\n/\n10\n \n*\n \nentropy\n(\n3\n,\n \n1\n)\n\n\ne50\n \n=\n \n9\n/\n10\n \n*\n \nentropy\n(\n4\n,\n \n5\n)\n \n+\n \n1\n/\n10\n \n*\n \nentropy\n(\n0\n,\n \n1\n)\n\n\n\nprint\n(\n\"With threshold = {}, entropy = {}\"\n.\nformat\n(\n18\n,\n \ne18\n))\n\n\nprint\n(\n\"With threshold = {}, entropy = {}\"\n.\nformat\n(\n24\n,\n \ne24\n))\n\n\nprint\n(\n\"With threshold = {}, entropy = {}\"\n.\nformat\n(\n31\n,\n \ne31\n))\n\n\nprint\n(\n\"With threshold = {}, entropy = {}\"\n.\nformat\n(\n50\n,\n \ne50\n))\n\n\n\n\n\n\nWith threshold = 18, entropy = 0.963547202339972\nWith threshold = 24, entropy = 0.9245112497836532\nWith threshold = 31, entropy = 0.7145247027726656\nWith threshold = 50, entropy = 0.8919684538544\n\n\n\n\n\n\n\n\n\nThe best test is \nif age > 31\n\n\n\n\nit splits the dataset to 6 samples (with 5 players) and 4 samples (with 3 non-players)\n\n\n\n\n\n\n\n\nPlease note, that the best threshold may change once a node is created\n\n\n\n\n\n\nUnknown parameters\n\u00b6\n\n\n\n\n\n\nIn the case some samples are incomplete one needs to correct the information gain\n\n\n\n\n\n\nThe information gain is calculated as before for samples with known attributes\n\n\n\n\n\n\nBut then it is normalized with respect to the probability that the given attribute has known values\n\n\n\n\n\n\nLets define the factor \nF\nF\n as the ratio of the number of samples with known value for a given feature to the number of all samples in a dataset\n\n\n\n\n\n\nThen information gain is defines as \nG(X) = F\\cdot (H(T) - H(T, X))\nG(X) = F\\cdot (H(T) - H(T, X))\n\n\n\n\n\n\nPlease note, that \nF = 1\nF = 1\n if all values are known\n\n\n\n\n\n\nOtherwise, information gain is scaled accordingly\n\n\n\n\n\n\nPruning\n\u00b6\n\n\n\n\n\n\nThe algorithm creates as many nodes as needed to classify all test samples\n\n\n\n\n\n\nIt may lead to overfitting and the resulting tree would fail to classify correctly unseen samples\n\n\n\n\n\n\nTo avoid this one can prune a tree\n\n\n\n\n\n\npre-pruning (early stopping)\n\n\n\n\n\n\nstop building a tree before leaves with few samples are produced\n\n\n\n\n\n\nhow to decide when it is good time to stop? e.g. using cross-validation on validation set (stop if the error does not increase significantly)\n\n\n\n\n\n\nunderfitting if stop to early\n\n\n\n\n\n\n\n\n\n\npost-pruning\n\n\n\n\n\n\nlet a tree grow completely\n\n\n\n\n\n\nthen go from bottom to top and try to replace a node with a leaf\n\n\n\n\n\n\nif there is improvement in accuracy - cut a tree\n\n\n\n\n\n\nif the accuracy stays the same - cut a tree (Occam's razor)\n\n\n\n\n\n\notherwise leave a node   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst example - step by step\n\u00b6\n\n\n\n\n\n\nLets consider the problem from the beginning of the lecture\n\n\n\n\n\n\nOur dataset has 20 blue points and 20 orange points\n\n\n\n\n\n\nEach point has two features (both are numerical)\n\n\n\n\n\n\nWe expect overfitting if pruning is not applied\n\n\n\n\n\n\nWe will calculate everything step by step (it is boring, but demonstrates how the algorithm works)\n\n\n\n\n\n\n# first define some points representing two classes\n\n\ngrid\n \n=\n \nnp\n.\nmgrid\n[\n0\n:\n10\n:\n2\n,\n \n0\n:\n10\n:\n2\n]\n\n\nset01\n \n=\n \nnp\n.\nvstack\n([\ngrid\n[\n0\n]\n.\nravel\n(),\n \ngrid\n[\n1\n]\n.\nravel\n()])\n.\nT\n\n\nset01\n \n=\n \nnp\n.\ndelete\n(\nset01\n,\n \n[\n17\n,\n \n18\n,\n \n19\n,\n \n22\n,\n \n24\n],\n \naxis\n=\n0\n)\n\n\n\ngrid\n \n=\n \nnp\n.\nmgrid\n[\n6\n:\n16\n:\n2\n,\n \n0\n:\n10\n:\n2\n]\n\n\nset02\n \n=\n \nnp\n.\nvstack\n([\ngrid\n[\n0\n]\n.\nravel\n(),\n \ngrid\n[\n1\n]\n.\nravel\n()])\n.\nT\n\n\nset02\n \n=\n \nnp\n.\ndelete\n(\nset02\n,\n \n[\n0\n,\n \n1\n,\n \n5\n,\n \n6\n,\n \n8\n],\n \naxis\n=\n0\n)\n\n\n\nplt\n.\nscatter\n(\n*\nset01\n.\nT\n)\n\n\nplt\n.\nscatter\n(\n*\nset02\n.\nT\n);\n\n\n\n\n\n\n\n\nValidation set\n\u00b6\n\n\n\n\n\n\nWe will use 10 points from the dataset for validation\n\n\n\n\n\n\nThis time selected manually to perform by hand calculations\n\n\n\n\n\n\nOn the plot below X denotes validation samples\n\n\n\n\n\n\n# split dataset to training and validation set\n\n\n# note, we should splt them randomly\n\n\n# but here we do this by hand\n\n\nvalid_idx\n \n=\n \n[\n3\n,\n \n7\n,\n \n10\n,\n \n14\n,\n \n18\n]\n\n\n\nblue_valid\n \n=\n \nset01\n[\nvalid_idx\n]\n\n\nblue_train\n \n=\n \nnp\n.\ndelete\n(\nset01\n,\n \nvalid_idx\n,\n \naxis\n=\n0\n)\n\n\n\norange_valid\n \n=\n \nset02\n[\nvalid_idx\n]\n\n\norange_train\n \n=\n \nnp\n.\ndelete\n(\nset02\n,\n \nvalid_idx\n,\n \naxis\n=\n0\n)\n\n\n\n# circles - training set\n\n\n# x - validation set\n\n\nplt\n.\nscatter\n(\n*\nblue_train\n.\nT\n)\n\n\nplt\n.\nscatter\n(\n*\nblue_valid\n.\nT\n,\n \ncolor\n=\n'C0'\n,\n \nmarker\n=\n'x'\n)\n\n\nplt\n.\nscatter\n(\n*\norange_train\n.\nT\n)\n\n\nplt\n.\nscatter\n(\n*\norange_valid\n.\nT\n,\n \ncolor\n=\n'C1'\n,\n \nmarker\n=\n'x'\n);\n\n\n\n\n\n\n\n\nThresholds finder\n\u00b6\n\n\n\n\n\n\nWhen building a tree we need to calculate information gain for every threshold in current subset\n\n\n\n\n\n\nEvery subset \nS\nS\n has \nN_b\nN_b\n blue samples and \nN_o\nN_o\n orange samples\n\n\n\n\n\n\nAfter split into accoring to some threshold we get two subsets\n\n\n\n\n\n\nn_b\nn_b\n of blue points and \nn_o\nn_o\n of orange points (\nS_1\nS_1\n)\n\n\n\n\n\n\nN_b - n_b\nN_b - n_b\n of blue points and \nN_o - n_o\nN_o - n_o\n of orange points (\nS_2\nS_2\n)\n\n\n\n\n\n\n\n\n\n\ndef\n \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \nnb\n,\n \nno\n):\n\n  \n\"\"\"Calculate information gain for given split\"\"\"\n\n  \nh\n \n=\n \nentropy\n(\nNb\n,\n \nNo\n)\n \n# H(S)\n\n  \ntotal\n \n=\n \nNb\n \n+\n \nNo\n     \n# total number of samples\n\n  \nsubtotal\n \n=\n \nnb\n \n+\n \nno\n  \n# number of samples in subset\n\n\n  \nreturn\n \nh\n \n-\n \nsubtotal\n \n/\n \ntotal\n \n*\n \nentropy\n(\nnb\n,\n \nno\n)\n \\\n           \n-\n \n(\ntotal\n \n-\n \nsubtotal\n)\n \n/\n \ntotal\n \n*\n \nentropy\n(\nNb\n \n-\n \nnb\n,\n \nNo\n \n-\n \nno\n)\n\n\n\n\n\n\nFeature X\n\u00b6\n\n\n\n\n\n\nWe need to calculate information gain ratio for the best threshold (the one that maximize information gain)\n\n\n\n\n\n\nPossible thresholds \n\\{0, 2, 4, 6, 8, 10, 12\\}\n\\{0, 2, 4, 6, 8, 10, 12\\}\n\n\n\n\n\n\nNb\n \n=\n \n15\n\n\nNo\n \n=\n \n15\n\n\n\nsplits\n \n=\n \n{\n\"0\"\n:\n \n(\n4\n,\n \n0\n),\n \n\"2 \"\n:\n \n(\n8\n,\n \n0\n),\n \n\"4\"\n:\n \n(\n11\n,\n \n0\n),\n \n\"6\"\n:\n \n(\n13\n,\n \n3\n),\n\n          \n\"8\"\n:\n \n(\n15\n,\n \n4\n),\n \n\"10\"\n:\n \n(\n15\n,\n \n8\n),\n \n\"12\"\n:\n \n(\n15\n,\n \n11\n)}\n\n\n\nfor\n \nthreshold\n,\n \n(\nno\n,\n \nnb\n)\n \nin\n \nsplits\n.\nitems\n():\n\n  \nprint\n(\n\"Threshold = {}\n\\t\n -> {}\"\n.\nformat\n(\nthreshold\n,\n \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \nno\n,\n \nnb\n)))\n\n\n\n\n\n\nThreshold = 0    -> 0.14818913558232172\nThreshold = 2    -> 0.33824492595034883\nThreshold = 4    -> 0.5297578726233217\nThreshold = 6    -> 0.3525728312615027\nThreshold = 8    -> 0.5297578726233217\nThreshold = 10   -> 0.28538113149388267\nThreshold = 12   -> 0.14818913558232172\n\n\n\n\n\n\n\n\n\nWe got the same cuts as predicted at the beginning of the lecture: \nx > 4\nx > 4\n or \nx > 8\nx > 8\n\n\n\n\n\n\nLets choose \nx > 4\nx > 4\n and calculate information gain ratio\n\n\n\n\n\n\n# 4 samples with x = 0, 4 samples with x = 2 etc\n\n\ninfo_gain\n(\nNb\n,\n \nNo\n,\n \n*\nsplits\n[\n\"4\"\n])\n \n/\n \nentropy\n(\n4\n,\n \n4\n,\n \n3\n,\n \n5\n,\n \n3\n,\n \n4\n,\n \n3\n,\n \n4\n)\n\n\n\n\n\n\n0.1779055922617179\n\n\n\n\n\nFeature Y\n\u00b6\n\n\n\n\n\n\nRepeat the procedure\n\n\n\n\n\n\nThis time possible thresholds = \n\\{0, 2, 4, 6\\}\n\\{0, 2, 4, 6\\}\n\n\n\n\n\n\nNb\n \n=\n \n15\n\n\nNo\n \n=\n \n15\n\n\n\nsplits\n \n=\n \n{\n\"0\"\n:\n \n(\n4\n,\n \n2\n),\n \n\"2\"\n:\n \n(\n8\n,\n \n5\n),\n \n\"4\"\n:\n \n(\n10\n,\n \n8\n),\n \n\"6\"\n:\n \n(\n13\n,\n \n11\n)}\n\n\n\nfor\n \nthreshold\n,\n \n(\nno\n,\n \nnb\n)\n \nin\n \nsplits\n.\nitems\n():\n\n  \nprint\n(\n\"Threshold = {}\n\\t\n -> {}\"\n.\nformat\n(\nthreshold\n,\n \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \nno\n,\n \nnb\n)))\n\n\n\n\n\n\nThreshold = 0    -> 0.02035297064032593\nThreshold = 2    -> 0.029594041354123246\nThreshold = 4    -> 0.013406861436605633\nThreshold = 6    -> 0.0203529706403259\n\n\n\n\n\n\n\n\n\nThe best cut is \ny > 2\ny > 2\n (as predicted before)\n\n\n\n\n\n\nLets calculate information gain ratio\n\n\n\n\n\n\ninfo_gain\n(\nNb\n,\n \nNo\n,\n \n*\nsplits\n[\n\"2\"\n])\n \n/\n \nentropy\n(\n6\n,\n \n7\n,\n \n5\n,\n \n6\n,\n \n6\n)\n\n\n\n\n\n\n0.01278981522839263\n\n\n\n\n\nThe root\n\u00b6\n\n\n\n\n\n\nAt the beginning we discussed the choice of \ny\ny\n as a root predictor\n\n\n\n\n\n\nID3 and C4.5 are greedy algorithms and select optimal solution at given stage\n\n\n\n\n\n\nWe can start to build the tree with the first best predictor\n\n\n\n\n\n\ntree\n \n=\n \nDigraph\n()\n\n\n\ntree\n.\nedge\n(\n\"x > 4?\n\\n\n[15, 15]\"\n,\n \n\"blue\n\\n\n[11, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 4?\n\\n\n[15, 15]\"\n,\n \n\"[4, 15]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n\n\n\n\n\n\n\n\nBranch x > 4\n\u00b6\n\n\n\n\n\n\nNow we have to repeat the procedure for the branch \n[4, 15]\n[4, 15]\n\n\n\n\n\n\nLets take a look what points are left\n\n\n\n\n\n\nplt\n.\nxlim\n([\n5.5\n,\n \n14.5\n])\n\n\n\nplt\n.\nscatter\n(\n*\nblue_train\n.\nT\n)\n\n\nplt\n.\nscatter\n(\n*\norange_train\n.\nT\n);\n\n\n\n\n\n\n\n\n\n\nCheck \nx\nx\n maximum information gain ratio\n\n\n\n\nNb\n \n=\n \n4\n\n\nNo\n \n=\n \n15\n\n\n\nsplits\n \n=\n \n{\n\"6\"\n:\n \n(\n2\n,\n \n3\n),\n \n\"8\"\n:\n \n(\n4\n,\n \n4\n),\n \n\"10\"\n:\n \n(\n4\n,\n \n8\n),\n \n\"12\"\n:\n \n(\n4\n,\n \n11\n)}\n\n\n\nfor\n \nthreshold\n,\n \n(\nno\n,\n \nnb\n)\n \nin\n \nsplits\n.\nitems\n():\n\n  \nprint\n(\n\"Threshold = {}\n\\t\n -> {}\"\n.\nformat\n(\nthreshold\n,\n \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \nno\n,\n \nnb\n)))\n\n\n\n\n\n\nThreshold = 6    -> 0.051004839414443226\nThreshold = 8    -> 0.32143493796317624\nThreshold = 10   -> 0.16251125329718286\nThreshold = 12   -> 0.08198172064120202\n\n\n\n\n\nprint\n(\n\"Information gain ratio with x > 8:\"\n,\n\n      \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \n*\nsplits\n[\n\"8\"\n])\n \n/\n \nentropy\n(\n5\n,\n \n3\n,\n \n4\n,\n \n3\n,\n \n4\n))\n\n\n\n\n\n\nInformation gain ratio with x > 8: 0.14010311259651076\n\n\n\n\n\n\n\nCheck \ny\ny\n maximum information gain ratio\n\n\n\n\nNb\n \n=\n \n4\n\n\nNo\n \n=\n \n15\n\n\n\nsplits\n \n=\n \n{\n\"0\"\n:\n \n(\n2\n,\n \n2\n),\n \n\"2\"\n:\n \n(\n3\n,\n \n5\n),\n \n\"4\"\n:\n \n(\n3\n,\n \n6\n),\n \n\"6\"\n:\n \n(\n4\n,\n \n9\n)}\n\n\n\nfor\n \nthreshold\n,\n \n(\nno\n,\n \nnb\n)\n \nin\n \nsplits\n.\nitems\n():\n\n  \nprint\n(\n\"Threshold = {}\n\\t\n -> {}\"\n.\nformat\n(\nthreshold\n,\n \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \nno\n,\n \nnb\n)))\n\n\n\n\n\n\nThreshold = 0    -> 0.08471690647404045\nThreshold = 2    -> 0.08617499693494635\nThreshold = 4    -> 0.06066554625879636\nThreshold = 6    -> 0.13320381570773476\n\n\n\n\n\nprint\n(\n\"Information gain ratio with y > 6:\"\n,\n\n      \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \n*\nsplits\n[\n\"6\"\n])\n \n/\n \nentropy\n(\n4\n,\n \n4\n,\n \n3\n,\n \n4\n,\n \n4\n))\n\n\n\n\n\n\nInformation gain ratio with y > 6: 0.05757775370755489\n\n\n\n\n\n\n\n\n\nOnce again \nx\nx\n is a winner\n\n\n\n\n\n\nAnd we have a new node\n\n\n\n\n\n\ntree\n \n=\n \nDigraph\n()\n\n\n\ntree\n.\nedge\n(\n\"x > 4?\n\\n\n[15, 15]\"\n,\n \n\"blue\n\\n\n[11, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 4?\n\\n\n[15, 15]\"\n,\n \n\"x > 8?\n\\n\n[4, 15]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"x > 8?\n\\n\n[4, 15]\"\n,\n \n\"[4, 4]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 8?\n\\n\n[4, 15]\"\n,\n \n\"orange\n\\n\n[0, 11]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n\n\n\n\n\n\n\n\nBranch x<= 8\n\u00b6\n\n\n\n\nWe will continue until the tree is fully grown\n\n\n\n\nplt\n.\nxlim\n([\n5.5\n,\n \n8.5\n])\n\n\n\nplt\n.\nscatter\n(\n*\nblue_train\n.\nT\n)\n\n\nplt\n.\nscatter\n(\n*\norange_train\n.\nT\n);\n\n\n\n\n\n\n\n\n\n\nAgain, the best cut may be pretty obvious, but lets check the math\n\n\nWe have one possible cut in \nx\nx\n\n\n\n\nNb\n \n=\n \n4\n\n\nNo\n \n=\n \n4\n\n\n\nprint\n(\n\"Information gain ratio with x > 6:\"\n,\n\n      \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \n2\n,\n \n3\n)\n \n/\n \nentropy\n(\n5\n,\n \n3\n))\n\n\n\n\n\n\nInformation gain ratio with x > 6: 0.05112447853477686\n\n\n\n\n\n\n\nAnd usual threshold candidates in \ny\ny\n\n\n\n\nsplits\n \n=\n \n{\n\"0\"\n:\n \n(\n2\n,\n \n0\n),\n \n\"2\"\n:\n \n(\n3\n,\n \n0\n),\n \n\"4\"\n:\n \n(\n3\n,\n \n1\n),\n \n\"6\"\n:\n \n(\n4\n,\n \n2\n)}\n\n\n\nfor\n \nthreshold\n,\n \n(\nno\n,\n \nnb\n)\n \nin\n \nsplits\n.\nitems\n():\n\n  \nprint\n(\n\"Threshold = {}\n\\t\n -> {}\"\n.\nformat\n(\nthreshold\n,\n \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \nno\n,\n \nnb\n)))\n\n\n\n\n\n\nThreshold = 0    -> 0.31127812445913283\nThreshold = 2    -> 0.5487949406953986\nThreshold = 4    -> 0.1887218755408671\nThreshold = 6    -> 0.31127812445913283\n\n\n\n\n\nprint\n(\n\"Information gain ratio with y > 2:\"\n,\n\n      \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \n*\nsplits\n[\n\"2\"\n])\n \n/\n \nentropy\n(\n2\n,\n \n1\n,\n \n1\n,\n \n2\n,\n \n2\n))\n\n\n\n\n\n\nInformation gain ratio with y > 2: 0.24390886253128827\n\n\n\n\n\n\n\nAnd the tree is growing\n\n\n\n\ntree\n \n=\n \nDigraph\n()\n\n\n\ntree\n.\nedge\n(\n\"x > 4?\n\\n\n[15, 15]\"\n,\n \n\"blue\n\\n\n[11, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 4?\n\\n\n[15, 15]\"\n,\n \n\"x > 8?\n\\n\n[4, 15]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"x > 8?\n\\n\n[4, 15]\"\n,\n \n\"y > 2?\n\\n\n[4, 4]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 8?\n\\n\n[4, 15]\"\n,\n \n\"orange\n\\n\n[0, 11]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"y > 2?\n\\n\n[4, 4]\"\n,\n \n\"blue\n\\n\n[3, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"y > 2?\n\\n\n[4, 4]\"\n,\n \n\"[1, 4]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n\n\n\n\n\n\n\n\nBranch y > 2\n\u00b6\n\n\nplt\n.\nxlim\n([\n5.5\n,\n \n8.5\n])\n\n\nplt\n.\nylim\n([\n3.5\n,\n \n8.5\n])\n\n\n\nplt\n.\nscatter\n(\n*\nblue_train\n.\nT\n)\n\n\nplt\n.\nscatter\n(\n*\norange_train\n.\nT\n);\n\n\n\n\n\n\n\n\nNb\n \n=\n \n1\n\n\nNo\n \n=\n \n4\n\n\n\nprint\n(\n\"Information gain ratio with x > 6:\"\n,\n\n      \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \n0\n,\n \n3\n)\n \n/\n \nentropy\n(\n3\n,\n \n2\n))\n\n\n\n\n\n\nInformation gain ratio with x > 6: 0.33155970728682876\n\n\n\n\n\nprint\n(\n\"Information gain ratio with y > 4:\"\n,\n\n      \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \n0\n,\n \n1\n)\n \n/\n \nentropy\n(\n1\n,\n \n2\n,\n \n2\n))\n\n\n\nprint\n(\n\"Information gain ratio with y > 6:\"\n,\n\n      \ninfo_gain\n(\nNb\n,\n \nNo\n,\n \n1\n,\n \n2\n)\n \n/\n \nentropy\n(\n1\n,\n \n2\n,\n \n2\n))\n\n\n\n\n\n\nInformation gain ratio with y > 4: 0.047903442721748145\nInformation gain ratio with y > 6: 0.11232501392736344\n\n\n\n\n\nThe final tree\n\u00b6\n\n\ntree\n \n=\n \nDigraph\n()\n\n\n\ntree\n.\nedge\n(\n\"x > 4?\n\\n\n[15, 15]\"\n,\n \n\"blue\n\\n\n[11, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 4?\n\\n\n[15, 15]\"\n,\n \n\"x > 8?\n\\n\n[4, 15]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"x > 8?\n\\n\n[4, 15]\"\n,\n \n\"y > 2?\n\\n\n[4, 4]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 8?\n\\n\n[4, 15]\"\n,\n \n\"orange\n\\n\n[0, 11]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"y > 2?\n\\n\n[4, 4]\"\n,\n \n\"blue\n\\n\n[3, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"y > 2?\n\\n\n[4, 4]\"\n,\n \n\"x > 6?\n\\n\n[1, 4]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"x > 6?\n\\n\n[1, 4]\"\n,\n \n\"orange\n\\n\n[0, 3]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 6?\n\\n\n[1, 4]\"\n,\n \n\"y > 6?\n\\n\n[1, 1]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"y > 6?\n\\n\n[1, 1]\"\n,\n \n\"blue\n\\n\n[1, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"y > 6?\n\\n\n[1, 1]\"\n,\n \n\"orange\n\\n\n[0, 1]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n\n\n\n\n\n\n\n\n\n\n\n\nIt is likely that this tree is overfitted\n\n\n\n\n\n\nWe will proceed with pruning as it was explained\n\n\n\n\n\n\nBut first lets implement decision rules to measure accuracy\n\n\n\n\n\n\ndef\n \ntree_nominal\n(\nx\n,\n \ny\n):\n\n  \n\"\"\"Implementation of above tree\"\"\"\n\n  \nif\n \nx\n \n<=\n \n4\n:\n\n    \nreturn\n \n\"blue\"\n\n  \nelif\n \nx\n \n>\n \n8\n:\n\n    \nreturn\n \n\"orange\"\n\n  \nelif\n \ny\n \n<=\n \n2\n:\n\n    \nreturn\n \n\"blue\"\n\n  \nelif\n \nx\n \n<=\n \n6\n:\n\n    \nreturn\n \n\"orange\"\n\n  \nelse\n:\n\n    \nreturn\n \n\"orange\"\n \nif\n \ny\n \n>\n \n6\n \nelse\n \n\"blue\"\n\n\n\n\n\n\nSanity check\n\u00b6\n\n\n\n\nIf the tree is built \ncorrectly\n we expect 100% accuracy on training set\n\n\n\n\nfor\n \nx\n,\n \ny\n \nin\n \nblue_train\n:\n\n  \nprint\n(\ntree_nominal\n(\nx\n,\n \ny\n),\n \nend\n=\n' '\n)\n\n\n\n\n\n\nblue blue blue blue blue blue blue blue blue blue blue blue blue blue blue\n\n\n\n\n\nfor\n \nx\n,\n \ny\n \nin\n \norange_train\n:\n\n  \nprint\n(\ntree_nominal\n(\nx\n,\n \ny\n),\n \nend\n=\n' '\n)\n \n\n\n\n\n\norange orange orange orange orange orange orange orange orange orange orange orange orange orange orange\n\n\n\n\n\nAccuracy before pruning\n\u00b6\n\n\ndef\n \naccuracy\n(\nsamples\n,\n \ntree\n):\n\n  \n\"\"\"Just print the result of classification\"\"\"\n\n  \nfor\n \nx\n,\n \ny\n \nin\n \nsamples\n:\n\n    \nprint\n(\n\"({}, {}) -> {}\"\n.\nformat\n(\nx\n,\n \ny\n,\n \ntree\n(\nx\n,\n \ny\n)))\n\n\n\n\n\n\naccuracy\n(\nblue_valid\n,\n \ntree_nominal\n)\n\n\n\n\n\n\n(0, 6) -> blue\n(2, 4) -> blue\n(4, 0) -> blue\n(4, 8) -> blue\n(8, 2) -> blue\n\n\n\n\n\naccuracy\n(\norange_valid\n,\n \ntree\n)\n\n\n\n\n\n\n(8, 4) -> blue\n(10, 4) -> orange\n(12, 0) -> orange\n(12, 8) -> orange\n(14, 6) -> orange\n\n\n\n\n\nPruning I\n\u00b6\n\n\n\n\n\n\nWe want to prune last decision node \ny > 6\ny > 6\n\n\n\n\n\n\nIn general, majority decides about the leaf node class\n\n\n\n\n\n\nAs it is a tie here, lets check both\n\n\n\n\n\n\ndef\n \ntree_prune01a\n(\nx\n,\n \ny\n):\n\n  \n\"\"\"Implementation of above tree\"\"\"\n\n  \nif\n \nx\n \n<=\n \n4\n:\n\n    \nreturn\n \n\"blue\"\n\n  \nelif\n \nx\n \n>\n \n8\n:\n\n    \nreturn\n \n\"orange\"\n\n  \nelif\n \ny\n \n<=\n \n2\n:\n\n    \nreturn\n \n\"blue\"\n\n  \nelif\n \nx\n \n<=\n \n6\n:\n\n    \nreturn\n \n\"orange\"\n\n  \nelse\n:\n\n    \nreturn\n \n\"blue\"\n\n\n\ndef\n \ntree_prune01b\n(\nx\n,\n \ny\n):\n\n  \n\"\"\"Implementation of above tree\"\"\"\n\n  \nif\n \nx\n \n<=\n \n4\n:\n\n    \nreturn\n \n\"blue\"\n\n  \nelif\n \nx\n \n>\n \n8\n:\n\n    \nreturn\n \n\"orange\"\n\n  \nelif\n \ny\n \n<=\n \n2\n:\n\n    \nreturn\n \n\"blue\"\n\n  \nelse\n:\n\n    \nreturn\n \n\"orange\"\n\n\n\n\n\n\naccuracy\n(\nblue_valid\n,\n \ntree_prune01a\n)\n\n\n\n\n\n\n(0, 6) -> blue\n(2, 4) -> blue\n(4, 0) -> blue\n(4, 8) -> blue\n(8, 2) -> blue\n\n\n\n\n\naccuracy\n(\norange_valid\n,\n \ntree_prune01a\n)\n\n\n\n\n\n\n(8, 4) -> blue\n(10, 4) -> orange\n(12, 0) -> orange\n(12, 8) -> orange\n(14, 6) -> orange\n\n\n\n\n\n\n\n\n\nPruning does not change the accuracy\n\n\n\n\n\n\nWe always use Occam's razor and \nprune01a\n is preferred over nominal tree\n\n\n\n\n\n\nBut lets see how \nprune01b\n works\n\n\n\n\n\n\naccuracy\n(\nblue_valid\n,\n \ntree_prune01b\n)\n\n\n\n\n\n\n(0, 6) -> blue\n(2, 4) -> blue\n(4, 0) -> blue\n(4, 8) -> blue\n(8, 2) -> blue\n\n\n\n\n\naccuracy\n(\norange_valid\n,\n \ntree_prune01b\n)\n\n\n\n\n\n\n(8, 4) -> orange\n(10, 4) -> orange\n(12, 0) -> orange\n(12, 8) -> orange\n(14, 6) -> orange\n\n\n\n\n\n\n\n\n\nIn this case we even get the increase of the accuracy\n\n\n\n\n\n\nWe decide to prune a tree by replacing \ny > 6\ny > 6\n decision node with \"orange\" leaf node\n\n\n\n\n\n\nWhich automatically removes \nx > 6\nx > 6\n decision node\n\n\n\n\n\n\ntree\n \n=\n \nDigraph\n()\n\n\n\ntree\n.\nedge\n(\n\"x > 4?\n\\n\n[15, 15]\"\n,\n \n\"blue\n\\n\n[11, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 4?\n\\n\n[15, 15]\"\n,\n \n\"x > 8?\n\\n\n[4, 15]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"x > 8?\n\\n\n[4, 15]\"\n,\n \n\"y > 2?\n\\n\n[4, 4]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x > 8?\n\\n\n[4, 15]\"\n,\n \n\"orange\n\\n\n[0, 11]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"y > 2?\n\\n\n[4, 4]\"\n,\n \n\"blue\n\\n\n[3, 0]\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"y > 2?\n\\n\n[4, 4]\"\n,\n \n\"orange\n\\n\n[1, 4]\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n\n\n\n\n\n\n\n\nPruning II\n\u00b6\n\n\n\n\n\n\nNow, lets see the accuracy after removing \ny > 2\ny > 2\n node\n\n\n\n\n\n\nIt is once again a tie, so lets check both scenarios\n\n\n\n\n\n\ndef\n \ntree_prune02a\n(\nx\n,\n \ny\n):\n\n  \n\"\"\"Implementation of above tree\"\"\"\n\n  \nif\n \nx\n \n<=\n \n4\n:\n\n    \nreturn\n \n\"blue\"\n\n  \nelse\n:\n\n    \nreturn\n \n\"orange\"\n\n\n\ndef\n \ntree_prune02b\n(\nx\n,\n \ny\n):\n\n  \n\"\"\"Implementation of above tree\"\"\"\n\n  \nif\n \nx\n \n<=\n \n4\n:\n\n    \nreturn\n \n\"blue\"\n\n  \nelif\n \nx\n \n>\n \n8\n:\n\n    \nreturn\n \n\"orange\"\n\n  \nelse\n:\n\n    \nreturn\n \n\"blue\"\n\n\n\n\n\n\naccuracy\n(\nblue_valid\n,\n \ntree_prune02a\n)\n\n\n\n\n\n\n(0, 6) -> blue\n(2, 4) -> blue\n(4, 0) -> blue\n(4, 8) -> blue\n(8, 2) -> orange\n\n\n\n\n\naccuracy\n(\norange_valid\n,\n \ntree_prune02a\n)\n\n\n\n\n\n\n(8, 4) -> orange\n(10, 4) -> orange\n(12, 0) -> orange\n(12, 8) -> orange\n(14, 6) -> orange\n\n\n\n\n\naccuracy\n(\nblue_valid\n,\n \ntree_prune02b\n)\n\n\n\n\n\n\n(0, 6) -> blue\n(2, 4) -> blue\n(4, 0) -> blue\n(4, 8) -> blue\n(8, 2) -> blue\n\n\n\n\n\naccuracy\n(\norange_valid\n,\n \ntree_prune02b\n)\n\n\n\n\n\n\n(8, 4) -> blue\n(10, 4) -> orange\n(12, 0) -> orange\n(12, 8) -> orange\n(14, 6) -> orange\n\n\n\n\n\n\n\n\n\nIn both cases the error increased\n\n\n\n\n\n\nWe stop pruning and leave the tree as it is in \nprune01b\n version \n\n\n\n\n\n\nSummary\n\u00b6\n\n\n\n\n\n\nC4.5 algorithm gives the full and clear prescription for building decision trees\n\n\n\n\n\n\nIt may look as a long procedure, but it is only because I wanted to show everything step by step and avoid \n\"after a few trivial steps...\"\n\n\n\n\n\n\nID3/C4.5/C5.0 are based on information theory\n\n\n\n\n\n\nThere is alternative procedure based on \ngini impurity\n, which is used by CART\n\n\n\n\n\n\nCART\n\u00b6\n\n\n\n\n\n\nCART stands for Classification and Regression Tree\n\n\n\n\n\n\nIt was created independently from ID3 (more or less at the same time)\n\n\n\n\n\n\nThe main differences:\n\n\n\n\n\n\nit creates binary trees (each decision node has two branches)\n\n\n\n\n\n\nit uses gini impurity instead of information gain\n\n\n\n\n\n\nit supports numerical target variables (regression)\n\n\n\n\n\n\n\n\n\n\nGini impurity\n\u00b6\n\n\n\n\n\n\nLet \nT = \\{T_1, T_2, ..., T_n\\}\nT = \\{T_1, T_2, ..., T_n\\}\n be the set of \nn\nn\n classes\n\n\n\n\n\n\nand \nP = \\{p_1, p_2, ..., p_n\\}\nP = \\{p_1, p_2, ..., p_n\\}\n be the probability distribution\n\n\n\n\n\n\nwhere \np_i\np_i\n is the probability that a sample belongs to class \nT_i\nT_i\n\n\n\n\n\n\nand \n1 - p_i\n1 - p_i\n is the probability that it belongs to another class\n\n\n\n\n\n\nGini impurity is defines as \nI(P) = \\sum\\limits_{i=1}^n p_i\\cdot (1 - p_i) = \\sum\\limits_{i=1}^n p_i - \\sum\\limits_{i=1}^n p_i^2 = 1 - \\sum\\limits_{i=1}^n p_i^2\nI(P) = \\sum\\limits_{i=1}^n p_i\\cdot (1 - p_i) = \\sum\\limits_{i=1}^n p_i - \\sum\\limits_{i=1}^n p_i^2 = 1 - \\sum\\limits_{i=1}^n p_i^2\n\n\n\n\n\n\nAs before (for entropy), lets consider two case scenario with \nP = (p, 1 - p)\nP = (p, 1 - p)\n, so gini impurity is given by \nI = 1 - p^2 - (1 - p)^2 = -2p(p - 1)\nI = 1 - p^2 - (1 - p)^2 = -2p(p - 1)\n\n\n\n\n\n\np\n \n=\n \nnp\n.\narange\n(\n0.01\n,\n \n1.0\n,\n \n0.01\n)\n\n\n\nplt\n.\nxlabel\n(\n\"p\"\n)\n\n\nplt\n.\nylabel\n(\n\"surprise factor\"\n)\n\n\n\nplt\n.\nplot\n(\np\n,\n \n-\np\n \n*\n \nnp\n.\nlog2\n(\np\n)\n \n-\n \n(\n1\n \n-\n \np\n)\n \n*\n \nnp\n.\nlog2\n(\n1\n \n-\n \np\n),\n \nlabel\n=\n\"Entropy\"\n);\n\n\nplt\n.\nplot\n(\np\n,\n \n-\n2\n*\np\n*\n(\np\n \n-\n \n1\n),\n \nlabel\n=\n\"Gini impurity\"\n)\n\n\n\nplt\n.\nlegend\n();\n\n\n\n\n\n\n\n\nPlay Golf\n\u00b6\n\n\n\n\nLets consider once again Play Golf dataset\n\n\n\n\nimport\n \npandas\n \nas\n \npd\n\n\n\n# first row = headers\n\n\nsrc\n \n=\n \n\"http://chem-eng.utoronto.ca/~datamining/dmc/datasets/weather_nominal.csv\"\n\n\n\ngolf_data\n \n=\n \npd\n.\nread_csv\n(\nsrc\n)\n\n\n\n\n\n\ngolf_data\n\n\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nOutlook\n\n      \nTemperature\n\n      \nHumidity\n\n      \nWindy\n\n      \nPlay golf\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \nRainy\n\n      \nHot\n\n      \nHigh\n\n      \nFalse\n\n      \nNo\n\n    \n\n    \n\n      \n1\n\n      \nRainy\n\n      \nHot\n\n      \nHigh\n\n      \nTrue\n\n      \nNo\n\n    \n\n    \n\n      \n2\n\n      \nOvercast\n\n      \nHot\n\n      \nHigh\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n3\n\n      \nSunny\n\n      \nMild\n\n      \nHigh\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n4\n\n      \nSunny\n\n      \nCool\n\n      \nNormal\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n5\n\n      \nSunny\n\n      \nCool\n\n      \nNormal\n\n      \nTrue\n\n      \nNo\n\n    \n\n    \n\n      \n6\n\n      \nOvercast\n\n      \nCool\n\n      \nNormal\n\n      \nTrue\n\n      \nYes\n\n    \n\n    \n\n      \n7\n\n      \nRainy\n\n      \nMild\n\n      \nHigh\n\n      \nFalse\n\n      \nNo\n\n    \n\n    \n\n      \n8\n\n      \nRainy\n\n      \nCool\n\n      \nNormal\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n9\n\n      \nSunny\n\n      \nMild\n\n      \nNormal\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n10\n\n      \nRainy\n\n      \nMild\n\n      \nNormal\n\n      \nTrue\n\n      \nYes\n\n    \n\n    \n\n      \n11\n\n      \nOvercast\n\n      \nMild\n\n      \nHigh\n\n      \nTrue\n\n      \nYes\n\n    \n\n    \n\n      \n12\n\n      \nOvercast\n\n      \nHot\n\n      \nNormal\n\n      \nFalse\n\n      \nYes\n\n    \n\n    \n\n      \n13\n\n      \nSunny\n\n      \nMild\n\n      \nHigh\n\n      \nTrue\n\n      \nNo\n\n    \n\n  \n\n\n\n\n\n\n\nGini impurity\n\u00b6\n\n\n\n\n\n\nWe treat all values as they are continues\n\n\n\n\n\n\nAnd consider all possible split\n\n\n\n\n\n\nEvery split leads to two subsets \nS_1\nS_1\n and \nS_2\nS_2\n\n\n\n\n\n\nAnd gini impurity for a set \nS\nS\n for given split is given by: \nI(S) = \\frac{|S_1|}{|S|}\\cdot I(S_1) + \\frac{|S_2|}{|S|}\\cdot I(S_2)\nI(S) = \\frac{|S_1|}{|S|}\\cdot I(S_1) + \\frac{|S_2|}{|S|}\\cdot I(S_2)\n\n\n\n\n\n\ndef\n \ngini\n(\n*\ndistribution\n):\n\n  \n\"\"\"Calculate gini impurity for given ditribution of samples\"\"\"\n\n  \nsum2\n \n=\n \nsum\n(\ndistribution\n)\n**\n2\n  \n# normalization factor\n\n\n  \nreturn\n \n1\n \n-\n \nsum\n([\np\n**\n2\n \nfor\n \np\n \nin\n \ndistribution\n])\n/\nsum2\n\n\n\n\n\n\ndef\n \ngini_split\n(\ns1\n,\n \ns2\n,\n \ng1\n,\n \ng2\n):\n\n  \n\"\"\"Calcualte impurity for given split\n\n\n\n  s1 -- the size of S1 subset\n\n\n  s1 -- the size of S2 subset\n\n\n  g1 -- I(S1)\n\n\n  g2 -- I(S2)\n\n\n  \"\"\"\n\n  \ns\n \n=\n \ns1\n \n+\n \ns2\n  \n# the total set size\n\n\n  \nreturn\n \ns1\n/\ns\n \n*\n \ng1\n \n+\n \ns2\n/\ns\n \n*\n \ng2\n\n\n\n\n\n\n            | Play golf |\n            =============\n            | yes | no  |\n      -------------------\n      | yes |  2  |  3  | 5\nrainy | no  |  7  |  2  | 9\n      -------------------\n               9     5\n\n\n\n\n\ngini_split\n(\n5\n,\n \n9\n,\n \ngini\n(\n2\n,\n \n3\n),\n \ngini\n(\n7\n,\n \n2\n))\n\n\n\n\n\n\n0.3936507936507937\n\n\n\n\n\n            | Play golf |\n            =============\n            | yes | no  |\n      -------------------\n      | yes |  3  |  2  | 5\nsunny | no  |  6  |  3  | 9\n      -------------------\n               9     5\n\n\n\n\n\ngini_split\n(\n5\n,\n \n9\n,\n \ngini\n(\n3\n,\n \n2\n),\n \ngini\n(\n6\n,\n \n3\n))\n\n\n\n\n\n\n0.45714285714285713\n\n\n\n\n\n               | Play golf |\n               =============\n               | yes | no  |\n         -------------------\n         | yes |  4  |  0  | 4\novercast | no  |  5  |  5  | 10\n         -------------------\n                  9     5\n\n\n\n\n\ngini_split\n(\n4\n,\n \n10\n,\n \ngini\n(\n4\n,\n \n0\n),\n \ngini\n(\n5\n,\n \n5\n))\n\n\n\n\n\n\n0.35714285714285715\n\n\n\n\n\n\n\n\n\nFrom \nOutlook\n feature the best choice is \nOvercast\n as it minimizes impurity\n\n\n\n\n\n\nHowever, we would have to check other features and choose the best predictor from all possibilities\n\n\n\n\n\n\nWe have one step by step example done though\n\n\n\n\n\n\nSo lets use some tool\n\n\n\n\n\n\nScikit learn\n\u00b6\n\n\n\n\n\n\nOne step by step example is behind us, so now lets use some tool\n\n\n\n\n\n\nCART is implemented in \nscikit-learn\n\n\n\n\n\n\nHowever, their implementation takes only numerical values\n\n\n\n\n\n\nSo we will use \nLabelDecoder\n to convert all values to numbers\n\n\n\n\n\n\nfrom\n \nsklearn.preprocessing\n \nimport\n \nLabelEncoder\n\n\n\n# pandas.DataFrame.apply applies a function to given axis (0 by default)\n\n\n# LabelEncoder encodes class labels with values between 0 and n-1\n\n\ngolf_data_num\n \n=\n \ngolf_data\n.\napply\n(\nLabelEncoder\n()\n.\nfit_transform\n)\n\n\n\n\n\n\ngolf_data_num\n\n\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nOutlook\n\n      \nTemperature\n\n      \nHumidity\n\n      \nWindy\n\n      \nPlay golf\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n1\n\n      \n1\n\n      \n0\n\n      \n0\n\n      \n0\n\n    \n\n    \n\n      \n1\n\n      \n1\n\n      \n1\n\n      \n0\n\n      \n1\n\n      \n0\n\n    \n\n    \n\n      \n2\n\n      \n0\n\n      \n1\n\n      \n0\n\n      \n0\n\n      \n1\n\n    \n\n    \n\n      \n3\n\n      \n2\n\n      \n2\n\n      \n0\n\n      \n0\n\n      \n1\n\n    \n\n    \n\n      \n4\n\n      \n2\n\n      \n0\n\n      \n1\n\n      \n0\n\n      \n1\n\n    \n\n    \n\n      \n5\n\n      \n2\n\n      \n0\n\n      \n1\n\n      \n1\n\n      \n0\n\n    \n\n    \n\n      \n6\n\n      \n0\n\n      \n0\n\n      \n1\n\n      \n1\n\n      \n1\n\n    \n\n    \n\n      \n7\n\n      \n1\n\n      \n2\n\n      \n0\n\n      \n0\n\n      \n0\n\n    \n\n    \n\n      \n8\n\n      \n1\n\n      \n0\n\n      \n1\n\n      \n0\n\n      \n1\n\n    \n\n    \n\n      \n9\n\n      \n2\n\n      \n2\n\n      \n1\n\n      \n0\n\n      \n1\n\n    \n\n    \n\n      \n10\n\n      \n1\n\n      \n2\n\n      \n1\n\n      \n1\n\n      \n1\n\n    \n\n    \n\n      \n11\n\n      \n0\n\n      \n2\n\n      \n0\n\n      \n1\n\n      \n1\n\n    \n\n    \n\n      \n12\n\n      \n0\n\n      \n1\n\n      \n1\n\n      \n0\n\n      \n1\n\n    \n\n    \n\n      \n13\n\n      \n2\n\n      \n2\n\n      \n0\n\n      \n1\n\n      \n0\n\n    \n\n  \n\n\n\n\n\n\n\n\n\nNow, lets splits our dataset to features and labels\n\n\n\n\n# DataFrame.iloc makes an access thourgh indices\n\n\n# we want all rows and first 4 columns for features\n\n\n# and the last column for labels\n\n\ndata\n \n=\n \nnp\n.\narray\n(\ngolf_data_num\n.\niloc\n[:,\n \n:\n4\n])\n\n\ntarget\n \n=\n \nnp\n.\narray\n(\ngolf_data_num\n.\niloc\n[:,\n \n4\n])\n\n\n\n\n\n\n\n\nOnce data is prepared, creating a tree is as easy as 2 + 2 -1\n\n\n\n\nfrom\n \nsklearn\n \nimport\n \ntree\n\n\n\ngolf_tree\n \n=\n \ntree\n.\nDecisionTreeClassifier\n()\n\n\n\ngolf_tree\n.\nfit\n(\ndata\n,\n \ntarget\n);\n\n\n\n\n\n\n\n\nsklearn.tree\n supports drawing a tree using \ngraphviz\n\n\n\n\nimport\n \ngraphviz\n\n\n\n# dot is a graph description language\n\n\ndot\n \n=\n \ntree\n.\nexport_graphviz\n(\ngolf_tree\n,\n \nout_file\n=\nNone\n,\n \n                           \nfeature_names\n=\ngolf_data\n.\ncolumns\n.\nvalues\n[:\n4\n],\n  \n                           \nclass_names\n=\n[\n\"no\"\n,\n \n\"yes\"\n],\n  \n                           \nfilled\n=\nTrue\n,\n \nrounded\n=\nTrue\n,\n  \n                           \nspecial_characters\n=\nTrue\n)\n \n\n\n# we create a graph from dot source using graphviz.Source\n\n\ngraph\n \n=\n \ngraphviz\n.\nSource\n(\ndot\n)\n \n\ngraph\n\n\n\n\n\n\n\n\n\n\nPlease note, that in the case of a real problem we would want to have a validation set and perform a pruning (\nscikit-learn\n does not support it though)\n\n\n\n\nRegression\n\u00b6\n\n\n\n\n\n\nThe difference now is that targets are numerical values (instead of categorical), e.g. in golf data - number of hours played instead of \"yes / no\"\n\n\n\n\n\n\nFeatures may be either discrete or continuous\n\n\n\n\n\n\nThe idea is the same though - we want to create a binary tree and minimize the error on in each leaf\n\n\n\n\n\n\nHowever, having continuous values as targets we can not simply use entropy or gini\n\n\n\n\n\n\nWe need to use different measurement - variance \nV(X) = \\frac{1}{n}\\sum\\limits_{i=1}^n (x_i - \\bar x)^2\nV(X) = \\frac{1}{n}\\sum\\limits_{i=1}^n (x_i - \\bar x)^2\n\n\n\n\n\n\nwhere \nX = \\{x_1, ..., x_n\\}\nX = \\{x_1, ..., x_n\\}\n and \n\\bar x\n\\bar x\n is the average value\n\n\n\n\n\n\nNote, that here \nx_i\nx_i\n are equally likely\n\n\n\n\n\n\nSimple example\n\u00b6\n\n\n\n\n\n\nBefore we learn how to grow a regression tree, lets take a look how it works on a simple example\n\n\n\n\n\n\nLets consider data distributed according to \nx^2\nx^2\n (with some noise, obviously)\n\n\n\n\n\n\nIt means with have continuous features (\nx\nx\n) and targets (\ny\ny\n)\n\n\n\n\n\n\nWe will split by hand the domain in \n0.3\n0.3\n and \n0.6\n0.6\n\n\n\n\n\n\nX\n \n=\n \nnp\n.\nrandom\n.\nsample\n(\n50\n)\n\n\nY\n \n=\n \nnp\n.\narray\n([\nx\n**\n2\n \n+\n \nnp\n.\nrandom\n.\nnormal\n(\n0\n,\n \n0.05\n)\n \nfor\n \nx\n \nin\n \nX\n])\n\n\n\nplt\n.\nxlabel\n(\n\"x\"\n)\n\n\nplt\n.\nylabel\n(\n\"y\"\n)\n\n\n\nplt\n.\nscatter\n(\nX\n,\n \nY\n,\n \ncolor\n=\n'b'\n)\n\n\nplt\n.\nplot\n([\n0.3\n,\n \n0.3\n],\n \n[\n-\n0.2\n,\n \n1.2\n],\n \n'g--'\n)\n\n\nplt\n.\nplot\n([\n0.6\n,\n \n0.6\n],\n \n[\n-\n0.2\n,\n \n1.2\n],\n \n'g--'\n);\n\n\n\n\n\n\n\n\n\n\nThe corresponding tree would look like this\n\n\n\n\ntree\n \n=\n \nDigraph\n()\n\n\n\ntree\n.\nedge\n(\n\"x < 0.3?\"\n,\n \n\"?\"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x < 0.3?\"\n,\n \n\"x < 0.6?\"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n.\nedge\n(\n\"x < 0.6?\"\n,\n \n\"? \"\n,\n \n\"No\"\n)\n\n\ntree\n.\nedge\n(\n\"x < 0.6?\"\n,\n \n\"?  \"\n,\n \n\"Yes\"\n)\n\n\n\ntree\n\n\n\n\n\n\n\n\n\n\nFor each split lets find a value \n\\bar y\n\\bar y\n\n\n\n\ndef\n \navg\n(\nX\n,\n \nY\n,\n \nx_min\n,\n \nx_max\n):\n\n  \n\"\"\"Return the average value in (x_min, x_max) range\"\"\"\n\n  \nn\n \n=\n \n0\n    \n# number of samples in given split \n\n  \navg\n \n=\n \n0\n  \n# average value\n\n\n  \nfor\n \nx\n,\n \ny\n \nin\n \nzip\n(\nX\n,\n \nY\n):\n\n    \nif\n \nx\n \n>=\n \nx_min\n \nand\n \nx\n \n<\n \nx_max\n:\n\n      \nn\n \n+=\n \n1\n\n      \navg\n \n+=\n \ny\n\n\n  \nreturn\n \navg\n \n/\n \nn\n\n\n\n\n\n\nplt\n.\nscatter\n(\nX\n,\n \nY\n,\n \ncolor\n=\n'b'\n)\n\n\n\nplt\n.\nplot\n([\n0.3\n,\n \n0.3\n],\n \n[\n-\n0.2\n,\n \n1.2\n],\n \n'g--'\n)\n\n\nplt\n.\nplot\n([\n0.6\n,\n \n0.6\n],\n \n[\n-\n0.2\n,\n \n1.2\n],\n \n'g--'\n)\n\n\n\ny\n \n=\n \navg\n(\nX\n,\n \nY\n,\n \n0\n,\n \n0.3\n)\n\n\nplt\n.\nplot\n([\n0.0\n,\n \n0.3\n],\n \n[\ny\n,\n \ny\n],\n \n'r'\n)\n\n\n\ny\n \n=\n \navg\n(\nX\n,\n \nY\n,\n \n0.3\n,\n \n0.6\n)\n\n\nplt\n.\nplot\n([\n0.3\n,\n \n0.6\n],\n \n[\ny\n,\n \ny\n],\n \n'r'\n)\n\n\n\ny\n \n=\n \navg\n(\nX\n,\n \nY\n,\n \n0.6\n,\n \n1\n)\n\n\nplt\n.\nplot\n([\n0.6\n,\n \n1.0\n],\n \n[\ny\n,\n \ny\n],\n \n'r'\n);\n\n\n\n\n\n\n\n\n\n\nAlternatively, one could do linear regression for split\n\n\n\n\nGrowing a tree\n\u00b6\n\n\n\n\n\n\nThe idea is similar as for numerical values in classification problems\n\n\n\n\n\n\nFor each feature we check all possible splits and calculate variance\n\n\n\n\n\n\nWe choose a binary split which minimzes variance\n\n\n\n\n\n\nfrom\n \nsklearn.tree\n \nimport\n \nDecisionTreeRegressor\n\n\n\n# create a decision tree regressor\n\n\nfit\n \n=\n \nDecisionTreeRegressor\n()\n\n\n\n# and grow it (note that X must be reshaped)\n\n\nfit\n.\nfit\n(\nnp\n.\nreshape\n(\nX\n,\n \n(\n-\n1\n,\n \n1\n)),\n \nY\n);\n\n\n\n\n\n\n# prepare test sample with \"newaxis\" trick\n\n\nX_test\n \n=\n \nnp\n.\narange\n(\n0.0\n,\n \n1.0\n,\n \n0.01\n)[:,\n \nnp\n.\nnewaxis\n]\n\n\nY_test\n \n=\n \nfit\n.\npredict\n(\nX_test\n)\n\n\n\n\n\n\nplt\n.\nscatter\n(\nX\n,\n \nY\n,\n \ncolor\n=\n'b'\n)\n\n\nplt\n.\nplot\n(\nX_test\n,\n \nY_test\n);\n\n\n\n\n\n\n\n\n\n\n\n\nAnd this is a perfect example of \noverfitting\n\n\n\n\n\n\nEach point was \nclassified\n as a separate target\n\n\n\n\n\n\nBeacause without any stopping criterion the tree is growing until there is a single point in a leaf\n\n\n\n\n\n\nThere are several strategies to pre-prune a tree:\n\n\n\n\n\n\ndefine a max depth of a tree\n\n\n\n\n\n\ndefine a minimum number of samples in a leaf\n\n\n\n\n\n\ndefine a minimum impurity\n\n\n\n\n\n\ndefine a minimum impurity decrease\n\n\n\n\n\n\n\n\n\n\nWhatever method is chosen you get a hyperparameter\n\n\n\n\n\n\nAnd we already know how to find an optimal hyperparameter: cross-validation\n\n\n\n\n\n\nTree: cross-validation\n\u00b6\n\n\n\n\n\n\nTo make it easier to check all possible methods lets create a simple class to do that for us\n\n\n\n\n\n\nIt takes training data and hyperparameter name (as named in \nscikit-learn\n)\n\n\n\n\n\n\nIt can change hyperparameter\n\n\n\n\n\n\nIt can perform a cross-validation for a set of hyperparameter values\n\n\n\n\n\n\nIt can make accuracy and best fit plots\n\n\n\n\n\n\nfrom\n \nsklearn.model_selection\n \nimport\n \ncross_val_score\n\n\nfrom\n \nsklearn.tree\n \nimport\n \nDecisionTreeRegressor\n\n\n\nclass\n \nTreeCV\n:\n\n  \n\"\"\"Perform a cross-validation for chosen hyperparameter\"\"\"\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \nX\n,\n \nY\n,\n \nhp\n=\n\"max_depth\"\n):\n\n    \n\"\"\"Save training data\"\"\"\n\n    \nself\n.\nX\n \n=\n \nX\n    \n# features\n\n    \nself\n.\nY\n \n=\n \nY\n    \n# targets\n\n    \nself\n.\nhp\n \n=\n \nhp\n  \n# hyperparameter\n\n\n\n  \ndef\n \nset_method\n(\nself\n,\n \nhp\n):\n\n    \n\"\"\"Set hyperparameter to use\"\"\"\n\n    \nself\n.\nhp\n \n=\n \nhp\n\n\n\n  \ndef\n \ncross_me\n(\nself\n,\n \n*\nhp_vals\n):\n\n    \n\"\"\"Perform cross validation for given hyperparameter values\"\"\"\n\n    \nself\n.\nscores\n \n=\n \n[]\n  \n# the accuracy table\n\n    \nself\n.\nbest\n \n=\n \nNone\n  \n# the best fit\n\n\n    \nbest_score\n \n=\n \n0\n\n\n    \nfor\n \nhp\n \nin\n \nhp_vals\n:\n\n      \n# create a tree with given hyperparameter cut\n\n      \nfit\n \n=\n \nDecisionTreeRegressor\n(\n**\n{\nself\n.\nhp\n:\n \nhp\n})\n\n\n      \n# calculate a cross validation scores and a mean value\n\n      \nscore\n \n=\n \ncross_val_score\n(\nfit\n,\n \nnp\n.\nreshape\n(\nX\n,\n \n(\n-\n1\n,\n \n1\n)),\n \nY\n)\n.\nmean\n()\n\n\n      \n# update best fit if necessary\n\n      \nif\n \nscore\n \n>\n \nbest_score\n:\n\n        \nself\n.\nbest\n \n=\n \nfit\n\n        \nbest_score\n \n=\n \nscore\n\n\n      \nself\n.\nscores\n.\nappend\n([\nhp\n,\n \nscore\n])\n\n\n    \n# train the best fit\n\n    \nself\n.\nbest\n.\nfit\n(\nnp\n.\nreshape\n(\nX\n,\n \n(\n-\n1\n,\n \n1\n)),\n \nY\n)\n\n\n\n  \ndef\n \nplot\n(\nself\n):\n\n    \n\"\"\"Plot accuracy as a function of hyperparameter values and best fit\"\"\"\n\n    \nplt\n.\nfigure\n(\nfigsize\n=\n(\n15\n,\n \n5\n))\n\n\n    \nplt\n.\nsubplot\n(\n1\n,\n \n2\n,\n \n1\n)\n\n\n    \nplt\n.\nxlabel\n(\nself\n.\nhp\n)\n\n    \nplt\n.\nylabel\n(\n\"accuracy\"\n)\n\n\n    \nplt\n.\nplot\n(\n*\nzip\n(\n*\nself\n.\nscores\n))\n\n\n    \nplt\n.\nsubplot\n(\n1\n,\n \n2\n,\n \n2\n)\n\n\n    \nX_test\n \n=\n \nnp\n.\narange\n(\n0.0\n,\n \n1.0\n,\n \n0.01\n)[:,\n \nnp\n.\nnewaxis\n]\n\n    \nY_test\n \n=\n \nself\n.\nbest\n.\npredict\n(\nX_test\n)\n\n\n    \nplt\n.\nscatter\n(\nself\n.\nX\n,\n \nself\n.\nY\n,\n \ncolor\n=\n'b'\n,\n \nmarker\n=\n'.'\n,\n \nlabel\n=\n\"Training data\"\n)\n\n    \nplt\n.\nplot\n(\nX_test\n,\n \nX_test\n \n*\n \nX_test\n,\n \n'g'\n,\n \nlabel\n=\n\"True distribution\"\n)\n    \n    \nplt\n.\nplot\n(\nX_test\n,\n \nY_test\n,\n \n'r'\n,\n \nlabel\n=\n\"Decision tree\"\n)\n\n\n    \nplt\n.\nlegend\n()\n\n\n\n\n\n\nTraning dataset\n\u00b6\n\n\nX\n \n=\n \nnp\n.\nrandom\n.\nsample\n(\n200\n)\n\n\nY\n \n=\n \nnp\n.\narray\n([\nx\n**\n2\n \n+\n \nnp\n.\nrandom\n.\nnormal\n(\n0\n,\n \n0.05\n)\n \nfor\n \nx\n \nin\n \nX\n])\n\n\n\n\n\n\nmax_depth\n\u00b6\n\n\ntree_handler\n \n=\n \nTreeCV\n(\nX\n,\n \nY\n)\n\n\ntree_handler\n.\ncross_me\n(\n*\nrange\n(\n1\n,\n \n10\n))\n\n\ntree_handler\n.\nplot\n()\n\n\n\n\n\n\n\n\nmin_samples_leaf\n\u00b6\n\n\ntree_handler\n.\nset_method\n(\n\"min_samples_leaf\"\n)\n\n\ntree_handler\n.\ncross_me\n(\n*\nrange\n(\n1\n,\n \n10\n))\n\n\ntree_handler\n.\nplot\n()\n\n\n\n\n\n\n\n\nmin_impurity_split\n\u00b6\n\n\n# min_impurity_split is depracated so lets disable warnings\n\n\nimport\n \nwarnings\n\n\nwarnings\n.\nfilterwarnings\n(\n\"ignore\"\n,\n \ncategory\n=\nDeprecationWarning\n)\n\n\n\n\n\n\ntree_handler\n.\nset_method\n(\n\"min_impurity_split\"\n)\n\n\ntree_handler\n.\ncross_me\n(\n*\nnp\n.\narange\n(\n0.0\n,\n \n5e-3\n,\n \n1e-4\n))\n\n\ntree_handler\n.\nplot\n()\n\n\n\n\n\n\n\n\nmin_impurity_decrease\n\u00b6\n\n\ntree_handler\n.\nset_method\n(\n\"min_impurity_decrease\"\n)\n\n\ntree_handler\n.\ncross_me\n(\n*\nnp\n.\narange\n(\n0.0\n,\n \n5e-4\n,\n \n1e-5\n))\n\n\ntree_handler\n.\nplot\n()\n\n\n\n\n\n\n\n\nBias-Variance trade-off\n\u00b6\n\n\n        +================+================+\n       /\\                |                /\\\n      /  \\               |               /  \\\n     /    \\              |              /    \\\n    /      \\             |             /      \\\n   /        \\            |            /        \\\n  / variance \\           |           /   bias   \\\n  ^^^^^^^^^^^^           |           ^^^^^^^^^^^^\n                         |\n                         |\n                         |\noverfitting   <----------+--------->   underfitting\n\n\n\n\n\n\n\n\n\nBias is an error coming from wrong model assumptions, which do not allow an algorithm to learn all patterns from training data.\n\n\n\n\n\n\nVariance is an error coming from sensivity to features specific for training data.\n\n\n\n\n\n\nHigh bias leads to underfitting and high variance to overfitting.\n\n\n\n\n\n\nTotal error also depends on irreducible error (\nnoise\n that can not be reduced by algorithm)\n\n\n\n\n\n\nUltmiate goal is to minimize the total error\n\n\n\n\n\n\n# fake bias, variance and noise\n\n\ncomplexity\n \n=\n \nnp\n.\narange\n(\n1\n,\n \n2\n,\n \n0.1\n)\n\n\nvariance\n \n=\n \nnp\n.\npower\n(\ncomplexity\n,\n \n5\n)\n\n\nbias2\n \n=\n \nvariance\n[::\n-\n1\n]\n\n\nirreducible\n \n=\n \n[\n10\n*\nnp\n.\nrandom\n.\nnormal\n(\nabs\n(\nx\n \n-\n \n1.5\n),\n \n0.01\n)\n \nfor\n \nx\n \nin\n \ncomplexity\n]\n\n\n\n# total error = variance + bias^2 + irreducible\n\n\ntotal\n \n=\n \nvariance\n \n+\n \nbias2\n \n+\n \nirreducible\n\n\n\nplt\n.\nxticks\n([])\n\n\nplt\n.\nyticks\n([])\n\n\n\nplt\n.\nxlabel\n(\n\"Algorithm complexity\"\n)\n\n\nplt\n.\nylabel\n(\n\"Error\"\n)\n\n\n\nplt\n.\nplot\n(\ncomplexity\n,\n \nvariance\n,\n \n'C0o-'\n,\n \nlabel\n=\n'Variance'\n)\n\n\nplt\n.\nplot\n(\ncomplexity\n,\n \nbias2\n,\n \n'C1o-'\n,\n \nlabel\n=\n\"Bias^2\"\n)\n\n\nplt\n.\nplot\n(\ncomplexity\n,\n \ntotal\n,\n \n'C2o-'\n,\n \nlabel\n=\n\"Total = Bias^2 + Variance + Irreducible error\"\n)\n\n\n\nplt\n.\nplot\n([\n1.5\n,\n \n1.5\n],\n \n[\n0\n,\n \n25\n],\n \n'C3--'\n)\n\n\n\nplt\n.\ntext\n(\n1.0\n,\n \n7\n,\n \n\"$\\longleftarrow$ better chance of generalizing\"\n,\n \ncolor\n=\n'C0'\n)\n\n\nplt\n.\ntext\n(\n1.6\n,\n \n7\n,\n \n\"better chance of approximating $\\longrightarrow$\"\n,\n \ncolor\n=\n'C1'\n)\n\n\n\nplt\n.\nlegend\n();\n\n\n\n\n\n\n\n\n\n\n\n\nDecision trees are sensitive to splits - small changes in training data may change a tree structure\n\n\n\n\n\n\ndeep trees tend to have high variance and low bias\n\n\n\n\n\n\nshallow trees tend to have low variance and high bias\n\n\n\n\n\n\n\n\n\n\nQuick math\n\u00b6\n\n\nBasic\n\u00b6\n\n\n\n\n\n\nThe general goal of regression is to find how some dependent variable (target, \ny\ny\n) is changing when independent variable (feature, \nx\nx\n) varies\n\n\n\n\n\n\nLets assume there is some \ntrue\n relationship describing this dependence \ny = f(x)\ny = f(x)\n\n\n\n\n\n\nWe want to find \nf(x)\nf(x)\n from observations of \n(x, y)\n(x, y)\n pairs\n\n\n\n\n\n\nAlthough, in real life we get some noisy observation, so \ny = f(x) + \\epsilon\ny = f(x) + \\epsilon\n\n\n\n\n\n\nAs we do not know function \nf(x)\nf(x)\n we want to approximate it with some other function \ng(x)\ng(x)\n (estimator)\n\n\n\n\n\n\nIn general, \ng(x)\ng(x)\n is a parametrized model which can take many possible functional form\n\n\n\n\ne.g. \ng(x) = a\\cdot x^2 + b\\cdot x + c\ng(x) = a\\cdot x^2 + b\\cdot x + c\n can take different coefficients (based on a training dataset)\n\n\n\n\n\n\n\n\nBias and variance\n\u00b6\n\n\n\n\n\n\nLets imagine there are \nN\nN\n possible training datasets \n\\{D_1, D_2, ..., D_N\\}\n\\{D_1, D_2, ..., D_N\\}\n\n\n\n\n\n\nFor a given dataset one gets an estimators \ng^{(D)}(x)\ng^{(D)}(x)\n\n\n\n\n\n\nLets denote the expected estimator by \n\\mathbf{E}_{D}\\left[g^{(D)}(x)\\right] \\equiv \\bar g(x)\n\\mathbf{E}_{D}\\left[g^{(D)}(x)\\right] \\equiv \\bar g(x)\n\n\n\n\n\n\nIf \nN\nN\n is large we can approximate it by an average over all datasets (law of large numbers) \n\\bar g(x) \\approx \\frac{1}{N}\\sum\\limits_{i=1}^N g^{(D_i)}(x)\n\\bar g(x) \\approx \\frac{1}{N}\\sum\\limits_{i=1}^N g^{(D_i)}(x)\n\n\n\n\n\n\nThe \nvariance\n of an estimator tells us how far particular predictions are from the mean value \nvar = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\bar g(x)\\right)^2\\right]\nvar = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\bar g(x)\\right)^2\\right]\n\n\n\n\n\n\nThus, if the training does not depend on the choice of a dataset the variance is low\n\n\n\n\n\n\nThe \nbias\n of an estimator tells us how far the mean value is from the true value \nbias = \\bar g(x) - f(x)\nbias = \\bar g(x) - f(x)\n\n\n\n\n\n\nThus, if the model decribes data accurately the bias is low\n\n\n\n\n\n\nPlease note the hidden assumption that all possible values of \nx\nx\n are equally likely\n\n\n\n\n\n\nGoodness of a model\n\u00b6\n\n\n\n\n\n\nThe common practice to determine the goodness of a model fit is to calculate mean squared error\n\n\n\n\n\n\nThe mean squared error is, well, the mean value of error squared: \nmse = \\mathbf{E}_{x}\\left[\\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - y\\right)^2\\right]\\right]\nmse = \\mathbf{E}_{x}\\left[\\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - y\\right)^2\\right]\\right]\n\n\n\n\n\n\nLets consider MSE for a particlar point \nx\nx\n, \ny = f(x) + \\epsilon\ny = f(x) + \\epsilon\n, so \nmse = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - y\\right)^2\\right] = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x)\\right)^2\\right]- 2\\cdot\\mathbf{E}_{D}\\left[g^{(D)}(x)\\cdot y\\right] + \\mathbf{E}_{D}\\left[y^2\\right]\nmse = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - y\\right)^2\\right] = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x)\\right)^2\\right]- 2\\cdot\\mathbf{E}_{D}\\left[g^{(D)}(x)\\cdot y\\right] + \\mathbf{E}_{D}\\left[y^2\\right]\n\n\n\n\n\n\nHere, we used the linearity of the expected value operator. Lets use another common property: \n\\mathbf{E}\\left[X^2\\right] = \\mathbf{E}\\left[\\left(X - \\mathbf{E}\\left[X\\right]\\right)^2\\right] + \\mathbf{E}\\left[X\\right]^2\n\\mathbf{E}\\left[X^2\\right] = \\mathbf{E}\\left[\\left(X - \\mathbf{E}\\left[X\\right]\\right)^2\\right] + \\mathbf{E}\\left[X\\right]^2\n \n\n\n\n\n\n\nSo the first term can be rewritten in the form \n\\mathbf{E}_{D}\\left[\\left(g^{(D)}(x)\\right)^2\\right] = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\mathbf{E}_{D}\\left[g^{(D)}(x)\\right]\\right)^2\\right] + \\mathbf{E}_{D}\\left[g^{(D)}(x)\\right]^2 = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\bar g(x)\\right)^2\\right] + \\left(\\bar g(x)\\right)^2\n\\mathbf{E}_{D}\\left[\\left(g^{(D)}(x)\\right)^2\\right] = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\mathbf{E}_{D}\\left[g^{(D)}(x)\\right]\\right)^2\\right] + \\mathbf{E}_{D}\\left[g^{(D)}(x)\\right]^2 = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\bar g(x)\\right)^2\\right] + \\left(\\bar g(x)\\right)^2\n\n\n\n\n\n\nAnd the last term \n\\mathbf{E}_{D}\\left[y^2\\right] = \\mathbf{E}_{D}\\left[\\left(y - \\mathbf{E}_{D}\\left[y\\right]\\right)^2\\right] + \\mathbf{E}_{D}\\left[y\\right]^2 = \\mathbf{E}_{D}\\left[\\left(y - f(x)\\right)^2\\right] + \\left(f(x)\\right)^2\n\\mathbf{E}_{D}\\left[y^2\\right] = \\mathbf{E}_{D}\\left[\\left(y - \\mathbf{E}_{D}\\left[y\\right]\\right)^2\\right] + \\mathbf{E}_{D}\\left[y\\right]^2 = \\mathbf{E}_{D}\\left[\\left(y - f(x)\\right)^2\\right] + \\left(f(x)\\right)^2\n\n\n\n\n\n\nHere, we used the fact that \n\\mathbf{E}_{D}\\left[y\\right] = f(x)\n\\mathbf{E}_{D}\\left[y\\right] = f(x)\n (noise would average out when averaging over \ninfinite\n number of datasets)\n\n\n\n\n\n\nFor the middle term we use the fact that for independent \nX\nX\n and \nY\nY\n: \n\\mathbf{E}\\left[XY\\right] = \\mathbf{E}\\left[X\\right]\\cdot\\mathbf{E}\\left[Y\\right]\n\\mathbf{E}\\left[XY\\right] = \\mathbf{E}\\left[X\\right]\\cdot\\mathbf{E}\\left[Y\\right]\n, so \n\\mathbf{E}_{D}\\left[g^{(D)}(x)\\cdot y\\right] = \\bar g(x)\\cdot f(x)\n\\mathbf{E}_{D}\\left[g^{(D)}(x)\\cdot y\\right] = \\bar g(x)\\cdot f(x)\n\n\n\n\n\n\nTaking all together we get \nmse = \\underbrace{\\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\bar g(x)\\right)^2\\right]}_{variance} + \\underbrace{\\left(\\bar g(x) - f(x)\\right)^2}_{bias^2} + \\underbrace{\\mathbf{E}_{D}\\left[\\left(y - f(x)\\right)^2\\right]}_{noise}\nmse = \\underbrace{\\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\bar g(x)\\right)^2\\right]}_{variance} + \\underbrace{\\left(\\bar g(x) - f(x)\\right)^2}_{bias^2} + \\underbrace{\\mathbf{E}_{D}\\left[\\left(y - f(x)\\right)^2\\right]}_{noise}\n\n\n\n\n\n\nExample\n\u00b6\n\n\n\n\n\n\nLets consider \nf(x) = \\sin(\\pi x)\nf(x) = \\sin(\\pi x)\n\n\n\n\n\n\nWith a noise given by a zero-mean Gaussian with a variance \n\\sigma^2\n\\sigma^2\n\n\n\n\n\n\nSo the observation \ny = f(x) + \\mathcal{N}(0, \\sigma^2)\ny = f(x) + \\mathcal{N}(0, \\sigma^2)\n\n\n\n\n\n\nfrom\n \nmath\n \nimport\n \nsin\n,\n \ncos\n,\n \npi\n,\n \nexp\n\n\n\ndef\n \nget_dataset\n(\nN\n=\n20\n,\n \nsigma\n=\n0.1\n):\n\n  \n\"\"\"Generate N training samples\"\"\"\n\n  \n# X is a set of random points from [-1, 1]\n\n  \nX\n \n=\n \n2\n \n*\n \nnp\n.\nrandom\n.\nsample\n(\nN\n)\n \n-\n \n1\n\n  \n# Y are corresponding target values (with noise included)\n\n  \nY\n \n=\n \nnp\n.\narray\n([\nsin\n(\npi\n*\nx\n)\n \n+\n \nnp\n.\nrandom\n.\nnormal\n(\n0\n,\n \nsigma\n)\n \nfor\n \nx\n \nin\n \nX\n])\n\n\n  \nreturn\n \nX\n,\n \nY\n\n\n\n# plot a sample\n\n\nX\n,\n \nY\n \n=\n \nget_dataset\n()\n\n\n\nx_\n \n=\n \nnp\n.\narange\n(\n-\n1\n,\n \n1\n,\n \n0.01\n)\n\n\n\nplt\n.\nscatter\n(\nX\n,\n \nY\n,\n \ncolor\n=\n'C1'\n)\n\n\nplt\n.\nplot\n(\nx_\n,\n \nnp\n.\nsin\n(\nnp\n.\npi\n \n*\n \nx_\n),\n \n'C0--'\n);\n\n\n\n\n\n\n\n\n\n\n\n\nWe do not know \nf(x)\nf(x)\n\n\n\n\n\n\nWe assume it is a polynomial\n\n\n\n\n\n\nLets consider polynomials of orders: \n1 - 9\n1 - 9\n\n\n\n\n\n\ng_1(x) = a_1\\cdot x + a_0\ng_1(x) = a_1\\cdot x + a_0\n\n\n\n\n\n\ng_2(x) = a_2\\cdot x^2 + \\cdots + a_0\ng_2(x) = a_2\\cdot x^2 + \\cdots + a_0\n\n\n\n\n\n\ng_3(x) = a_3\\cdot x^3 + \\cdots + a_0\ng_3(x) = a_3\\cdot x^3 + \\cdots + a_0\n\n\n\n\n\n\n...\n\n\n\n\n\n\n\n\n\n\nLets assume we have 100 independent dataset\n\n\n\n\n\n\nEach one has 20 points \n(x, y = \\sin(\\pi x) + \\mathcal{N}(0, \\sigma^2))\n(x, y = \\sin(\\pi x) + \\mathcal{N}(0, \\sigma^2))\n\n\n\n\n\n\n# generate 100 datasets with default settings\n\n\ndatasets\n \n=\n \n[\nget_dataset\n()\n \nfor\n \ni\n \nin\n \nrange\n(\n100\n)]\n\n\n\n# and plot them all together with true signal\n\n\nfor\n \ni\n \nin\n \nrange\n(\n100\n):\n\n  \nplt\n.\nscatter\n(\ndatasets\n[\ni\n][\n0\n],\n \ndatasets\n[\ni\n][\n1\n],\n \nmarker\n=\n'.'\n)\n\n\n\nplt\n.\nplot\n(\nx_\n,\n \nnp\n.\nsin\n(\nnp\n.\npi\n \n*\n \nx_\n),\n \n'C0--'\n);\n\n\n\n\n\n\n\n\n\n\nNow we need to fit each polynomial to each dataset separately\n\n\n\n\ndef\n \nget_fit\n(\nN\n,\n \ndata\n):\n\n  \n\"\"\"Find a fit of polynomial of order N to data = (X, Y)\"\"\"\n\n  \nreturn\n \nnp\n.\npoly1d\n(\nnp\n.\npolyfit\n(\ndata\n[\n0\n],\n \ndata\n[\n1\n],\n \nN\n))\n\n\n\n# for the whole range of possible polynomials orders\n\n\n# create a list of fits to different datasets\n\n\nfits\n \n=\n \n[[\nget_fit\n(\norder\n,\n \ndata\n)\n \nfor\n \ndata\n \nin\n \ndatasets\n]\n \nfor\n \norder\n \nin\n \nrange\n(\n1\n,\n \n10\n)]\n\n\n\n\n\n\nplt\n.\nfigure\n(\nfigsize\n=\n(\n13\n,\n \n10\n))\n\n\n\nfor\n \norder\n \nin\n \nrange\n(\n1\n,\n \n10\n):\n\n  \nplt\n.\nsubplot\n(\n3\n,\n \n3\n,\n \norder\n)\n\n  \nplt\n.\nylim\n([\n-\n1.5\n,\n1.5\n])\n\n\n  \nfor\n \ng\n \nin\n \nfits\n[\norder\n \n-\n \n1\n]:\n\n    \nplt\n.\nplot\n(\nx_\n,\n \ng\n(\nx_\n),\n \n'C1-'\n,\n \nlinewidth\n=\n0.1\n)\n\n\n  \nplt\n.\nplot\n(\nx_\n,\n \nnp\n.\nsin\n(\nnp\n.\npi\n \n*\n \nx_\n),\n \n'C0--'\n)\n\n\n  \nplt\n.\ntitle\n(\n\"Polynomial of order {}\"\n.\nformat\n(\norder\n));\n\n\n\nplt\n.\ntight_layout\n();\n\n\n\n\n\n\n\n\nTraining and test errors\n\u00b6\n\n\n\n\n\n\nIn real life is impossible (unless one creates data by hand) to calculate true variance and bias\n\n\n\n\n\n\nOne would need all possible datasets \nD\nD\n and all possible input values \nx\nx\n\n\n\n\n\n\nThus, usually one looks at training and test errors\n\n\n\n\n\n\nTraining error is measured on the data used to make a fit\n\n\n\n\n\n\nTest/validation error is measured on unseen data\n\n\n\n\n\n\n\n\n\n\n# fake error\n\n\ncomplexity\n \n=\n \nnp\n.\narange\n(\n0.1\n,\n \n2\n,\n \n0.1\n)\n\n\ntrain_error\n \n=\n \n-\nnp\n.\nlog\n(\ncomplexity\n)\n\n\ntest_error\n \n=\n \n-\nnp\n.\nlog\n(\ncomplexity\n)\n \n+\n \nnp\n.\npower\n(\ncomplexity\n,\n \n1\n)\n\n\n\nplt\n.\nxticks\n([])\n\n\nplt\n.\nyticks\n([])\n\n\n\nplt\n.\nxlabel\n(\n\"Algorithm complexity\"\n)\n\n\nplt\n.\nylabel\n(\n\"Error\"\n)\n\n\n\nplt\n.\nplot\n(\ncomplexity\n,\n \ntrain_error\n,\n \n'C0o-'\n,\n \nlabel\n=\n'Training error'\n)\n\n\nplt\n.\nplot\n(\ncomplexity\n,\n \ntest_error\n,\n \n'C1o-'\n,\n \nlabel\n=\n\"Test error\"\n)\n\n\n\nplt\n.\ntext\n(\n0.1\n,\n \n0.25\n,\n \n\"$\\longleftarrow$ high bias\"\n,\n \ncolor\n=\n'C0'\n)\n\n\nplt\n.\ntext\n(\n1.5\n,\n \n0.25\n,\n \n\"high variance $\\longrightarrow$\"\n,\n \ncolor\n=\n'C1'\n)\n\n\n\nplt\n.\nlegend\n();\n\n\n\n\n\n\n\n\n\n\n\n\nHigh training error indicates high bias (which means underfitting)\n\n\n\n\n\n\nTraining error must decrease with model complexity\n\n\n\n\n\n\nIf the training error is high:\n\n\n\n\n\n\nUse more complex model (or new model architecture)\n\n\n\n\n\n\nUse more features - maybe there is just not enough information to make a good prediction\n\n\n\n\n\n\nTrain longer (if the algorithm is an iterative optimization problem)\n\n\n\n\n\n\nDecrease regularization (next lecture)\n\n\n\n\n\n\n\n\n\n\nTest error deacreses with model complexity up to a point when algorithm is to sensitive to features seen in training data\n\n\n\n\n\n\nIf test error starts to increase it indicates high variance (which means overfitting)\n\n\n\n\n\n\nIf test error is high:\n\n\n\n\n\n\nUse more data - easy to say hard to do...\n\n\n\n\n\n\nUse less features\n\n\n\n\n\n\nIncrease regularization \n\n\n\n\n\n\nUse different model\n\n\n\n\n\n\n\n\n\n\nEnsemble learning\n\u00b6\n\n\n\n\n\n\nLets first define a \nweak learner\n as a classifier which is just slighlty better than random guessing\n\n\n\n\n\n\nThe idea behind ensemble learning is to create a \nstrong learner\n as a combination of many \nweak learners\n\n\n\n\n\n\nWe will discuss two popular ensemble methods:\n\n\n\n\n\n\nBagging (\nb\nootstrap \nagg\nregat\ning\n), e.g. random forest\n\n\n\n\n\n\nBoosting, e.g. boosted decision tress\n\n\n\n\n\n\n\n\n\n\nRandom forest\n\u00b6\n\n\n\n\n\n\nOnce we know a way to produce a tree we can create a forest\n\n\n\n\n\n\nAnd each tree contributes to a final prediction\n\n\n\n\n\n\nIt is a \nrandom\n forest, because each tree is randomly \nincomplete\n - trained only on a random subsets of samples and features (\nfeatures bagging\n)\n\n\n\n\n\n\nThe final prediction of a random forest is an avearge predictions (for regression) or a majority vote (classification)\n\n\n\n\n\n\nIntuitive / naive example\n\u00b6\n\n\n\n\n\n\nImagine you want to go to a cinema and need to choose a movie to watch\n\n\n\n\n\n\nYou can ask a friend about the recommendation\n\n\n\n\n\n\nShe/he would ask you about movies you watched in the past\n\n\n\n\n\n\nand (based on your answers) create a set of rules (a decision tree)\n\n\n\n\n\n\nto finally recommend you a movie (make a prediction)\n\n\n\n\n\n\n\n\n\n\nAlternatively, you can ask many friends for an advice\n\n\n\n\n\n\nEach friend would ask you random questions to give an answer\n\n\n\n\n\n\nAt the end, you choose a movie with most votes\n\n\n\n\n\n\n\n\n\n\nThe algorithm\n\u00b6\n\n\n\n\n\n\nLets imagine we have \nN\nN\n samples in our dataset (e.g. \nN\nN\n movies you watched)\n\n\n\n\n\n\nEach sample has \nM\nM\n features (e.g. do you like a movie? do you like the leading actor / actress or director?)\n\n\n\n\n\n\nTo create a tree take \nn\nn\n random samples from the dataset and \nat each node\n select \nm << M\nm << M\n features (\nm \\sim \\sqrt M\nm \\sim \\sqrt M\n) to find the best predictor\n\n\n\n\n\n\nRepeat the procedure for next trees until you reach desired size of a forest\n\n\n\n\n\n\n                    +------------+\n                    |            |\n                    |   Dataset  |\n         +----------+            +----------+\n         |          | N features |          |\n         |          |            |          |\n         v          +------------+          v                    Hyperparameters:\n\n+-----------------+                 +-----------------+\n|                 |                 |                 |\n| Random subset 1 |      . . .      | Random subset T |\n|                 |                 |                 |              - the number of trees\n|   N features    |                 |   N features    |              - the size of subsets\n|                 |                 |                 |\n+--------+--------+                 +--------+--------+\n         |                                   |\n         v                                   v\n\n+-----------------+                 +-----------------+\n|                 |                 |                 |\n|     Tree 1      |      . . .      |     Tree T      |\n|                 |                 |                 |\n+--------+--------+                 +-----------------+\n         |\n         |          +-------------+          +--------+\n         |          |             |          |        |\n         +--------> | M1 features +--------> | Node 1 |              - the number of random features\n         |          |             |          |        |              - the size of a single tree\n         |          +-------------+          +--------+\n         |\n         |          +-------------+          +--------+\n         |          |             |          |        |\n         +--------> | M2 features +--------> | Node 2 |\n         |          |             |          |        |\n         |          +-------------+          +--------+\n\n                           .\n                           .\n                           .\n\n\n\n\n\nBoosted trees\n\u00b6\n\n\n\n\n\n\nThe idea is similar to bagging\n\n\n\n\n\n\nThe key differences are:\n\n\n\n\n\n\nData is reweighted every time a \nweak learner\n is added (so future learners focus on misclassified samples)\n\n\n\n\n\n\nThe final prediction is weighted average (better classifiers have higher weights)\n\n\n\n\n\n\n\n\n\n\n                            Bagging (parallel)\n\n       +--------------------+                +----------------+\n       |                    |                |                |\n+----> |      Dataset       +--------------> | Weak learner 1 |\n|      |                    |                |                |\n|      +--------------------+                +----------------+\n|\n|\n|      +--------------------+                +----------------+\n|      |                    |                |                |\n+----> |      Dataset       +------------->  | Weak learner 2 |\n|      |                    |                |                |\n|      +--------------------+       .        +----------------+\n|                                   .\n|                                   .\n|      +--------------------+                +----------------+\n|      |                    |                |                |\n+----> |      Dataset       +------------->  | Weak learner N |\n       |                    |                |                |\n       +--------------------+                +----------------+\n\n\n\n\n\n\\hspace{140pt}\\text{output} = \\frac{1}{N}\\sum \\text{output}_i\n\\hspace{140pt}\\text{output} = \\frac{1}{N}\\sum \\text{output}_i\n \n\n\n\n\n\n                          Boosting (sequential)\n\n      +--------------------+                +----------------+\n      |                    |                |                |\n      |      Dataset       +--------------> | Weak learner 1 |\n      |                    |                |                |\n      +--------------------+                +--------+-------+\n                                                     |\n                +------------------------------------+\n                |\n                v\n\n      +--------------------+                +----------------+\n      |                    |                |                |\n      | Reweighted dataset +------------->  | Weak learner 2 |\n      |                    |                |                |\n      +--------------------+                +--------+-------+\n                                                     |\n                +------------     . . .      --------+\n                |\n                v\n\n      +--------------------+                +----------------+\n      |                    |                |                |\n      | Reweighted dataset +------------->  | Weak learner N |\n      |                    |                |                |\n      +--------------------+                +----------------+\n\n\n\n\n\n\\hspace{130pt}\\text{output} = \\sum w_i\\cdot \\text{output}_i\n\\hspace{130pt}\\text{output} = \\sum w_i\\cdot \\text{output}_i\n \n\n\nAdaBoost\n\u00b6\n\n\n\n\n\n\nAdaBoost is on of the most famous algorithms in machine learning\n\n\n\n\n\n\nY. Freund and R. Schapire got a G\u00f6del Prize for this\n\n\n\n\n\n\nLets consider \nN\nN\n labled training examples \n(x_1, y_1), \\cdots, (x_N, y_N)\n(x_1, y_1), \\cdots, (x_N, y_N)\n, where \nx_i \\in X\nx_i \\in X\n and \ny_i = \\left\\{-1, 1\\right\\}\ny_i = \\left\\{-1, 1\\right\\}\n\n\n\n\n\n\nThe initial distribution is initizlized with \nD_1(i) = \\frac{1}{N}\nD_1(i) = \\frac{1}{N}\n, where \ni = 1, \\cdots, N\ni = 1, \\cdots, N\n (so every sample is equaly likely)\n\n\n\n\n\n\nFor \nt = 1, \\cdots, T\nt = 1, \\cdots, T\n:\n\n\n\n\n\n\ntrain a weak learner using \nD_t\nD_t\n, \nh_t: X \\rightarrow \\left\\{-1, 1\\right\\}\nh_t: X \\rightarrow \\left\\{-1, 1\\right\\}\n\n\n\n\n\n\nchoose the one which minimizes the weighted error \n\\epsilon_t = \\sum\\limits_{i=1}^{N}D_t(i)\\delta\\left(h_t(x_i)\\neq y_i\\right)\n\\epsilon_t = \\sum\\limits_{i=1}^{N}D_t(i)\\delta\\left(h_t(x_i)\\neq y_i\\right)\n\n\n\n\n\n\ncalculate \n\\alpha_t\n\\alpha_t\n \n\\alpha_t = \\frac{1}{2}\\ln\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)\n\\alpha_t = \\frac{1}{2}\\ln\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)\n\n\n\n\n\n\nFor \ni = 1, \\cdots, N\ni = 1, \\cdots, N\n update weights according to \nD_{t+1} = \\frac{D_t(i)}{Z_t}\\begin{cases}e^{-\\alpha_t} & h_t(x_i) = y_i \\\\ e^{\\alpha_t} & h_t(x_i) \\neq y_i \\end{cases} = \\frac{D_t(i)}{Z_t}e^{-\\alpha_ty_ih_t(x_i)}\nD_{t+1} = \\frac{D_t(i)}{Z_t}\\begin{cases}e^{-\\alpha_t} & h_t(x_i) = y_i \\\\ e^{\\alpha_t} & h_t(x_i) \\neq y_i \\end{cases} = \\frac{D_t(i)}{Z_t}e^{-\\alpha_ty_ih_t(x_i)}\n\n\n\n\n\n\nZ_t\nZ_t\n is a normilization factor so \nD_{t+1}\nD_{t+1}\n is a distribution \nZ_t = \\sum\\limits_{i=1}^ND_t(i)e^{-\\alpha_ty_ih_t(x_i)}\nZ_t = \\sum\\limits_{i=1}^ND_t(i)e^{-\\alpha_ty_ih_t(x_i)}\n\n\n\n\n\n\n\n\n\n\nThe final hyptohesis \nH\nH\n is computes the sign of a weighted combination of weak hypotheses \nH(x) = \\text{sign}\\left(\\sum\\limits_{t=1}^T\\alpha_th_t(x)\\right)\nH(x) = \\text{sign}\\left(\\sum\\limits_{t=1}^T\\alpha_th_t(x)\\right)\n \n\n\n\n\n\n\nOut-of-bag error\n\u00b6\n\n\n\n\n\n\nOut-of-bag (OOB) error may be used for machine learning models using bootstrap aggregation (like random forest and boosted trees) instead of cross-validation\n\n\n\n\n\n\nBagging involves random sampling with replacement\n\n\n\n\n\n\nSome samples are not used in the training process (out-of-bag samples) and therefore can be used to calculate test error\n\n\n\n\n\n\nOOB error is the average error over all training samples calculated using predictions from weak classifiers which do not contain particular sample in their bootstrap samples\n\n\n\n\n\n\nExample\n\u00b6\n\n\n\n\n\n\nLets consider some fake data generated with \nmake_blobs\n from \nscikit-learn\n\n\n\n\n\n\nand then apply decision trees with different maximum depths\n\n\n\n\n\n\nand random forests with different maximum depths\n\n\n\n\n\n\nDataset\n\u00b6\n\n\n\n\n\n\nsklearn.datasets.make_blobs\n allows to generate random Gaussian blobs\n\n\n\n\n\n\nWe generate 8 blobs with fixed random generator (just to make sure we get the same set every time)\n\n\n\n\n\n\nfrom\n \nsklearn.datasets\n \nimport\n \nmake_blobs\n\n\n\n# generate 5 blobs with fixed random generator\n\n\nX\n,\n \nY\n \n=\n \nmake_blobs\n(\nn_samples\n=\n500\n,\n \ncenters\n=\n8\n,\n \nrandom_state\n=\n300\n)\n\n\n\nplt\n.\nscatter\n(\n*\nX\n.\nT\n,\n \nc\n=\nY\n,\n \nmarker\n=\n'.'\n,\n \ncmap\n=\n'Dark2'\n);\n\n\n\n\n\n\n\n\nTrain and visualize\n\u00b6\n\n\n\n\n\n\nTo make our life easier we create a function to\n\n\n\n\n\n\nplot training data on existing axes or new one (if not provided)\n\n\n\n\n\n\ntrain given classifier on given dataset\n\n\n\n\n\n\ncreate countours representing predictions of the classifier\n\n\n\n\n\n\n\n\n\n\ndef\n \ntrain_and_look\n(\nclassifier\n,\n \nX\n,\n \nY\n,\n \nax\n=\nNone\n,\n \ntitle\n=\n''\n,\n \ncmap\n=\n'Dark2'\n):\n\n  \n\"\"\"Train classifier on (X,Y). Plot data and prediction.\"\"\"\n\n  \n# create new axis if not provided\n\n  \nax\n \n=\n \nax\n \nor\n \nplt\n.\ngca\n();\n\n\n  \nax\n.\nset_title\n(\ntitle\n)\n\n\n  \n# plot training data\n\n  \nax\n.\nscatter\n(\n*\nX\n.\nT\n,\n \nc\n=\nY\n,\n \nmarker\n=\n'.'\n,\n \ncmap\n=\ncmap\n)\n\n\n  \n# train a cliassifier\n\n  \nclassifier\n.\nfit\n(\nX\n,\n \nY\n)\n\n\n  \n# create a grid of testing points\n\n  \nx_\n,\n \ny_\n \n=\n \nnp\n.\nmeshgrid\n(\nnp\n.\nlinspace\n(\n*\nax\n.\nget_xlim\n(),\n \nnum\n=\n200\n),\n\n                       \nnp\n.\nlinspace\n(\n*\nax\n.\nget_ylim\n(),\n \nnum\n=\n200\n))\n\n\n  \n# convert to an array of 2D points\n\n  \ntest_data\n \n=\n \nnp\n.\nvstack\n([\nx_\n.\nravel\n(),\n \ny_\n.\nravel\n()])\n.\nT\n\n\n  \n# make a prediction and reshape to grid structure \n\n  \nz_\n \n=\n \nclassifier\n.\npredict\n(\ntest_data\n)\n.\nreshape\n(\nx_\n.\nshape\n)\n\n\n  \n# arange z bins so class labels are in the middle\n\n  \nz_levels\n \n=\n \nnp\n.\narange\n(\nlen\n(\nnp\n.\nunique\n(\nY\n))\n \n+\n \n1\n)\n \n-\n \n0.5\n\n\n  \n# plot contours corresponding to classifier prediction\n\n  \nax\n.\ncontourf\n(\nx_\n,\n \ny_\n,\n \nz_\n,\n \nalpha\n=\n0.25\n,\n \ncmap\n=\ncmap\n,\n \nlevels\n=\nz_levels\n)\n\n\n\n\n\n\n\n\nLet check how it works on a decision tree classifier with default \nsklearn\n setting\n\n\n\n\nfrom\n \nsklearn.tree\n \nimport\n \nDecisionTreeClassifier\n \nas\n \nDT\n\n\n\ntrain_and_look\n(\nDT\n(),\n \nX\n,\n \nY\n)\n\n\n\n\n\n\n\n\nDecision tree\n\u00b6\n\n\n\n\nWe consider decision trees with fixed maximum depths from 1 to 9\n\n\n\n\n# create a figure with 9 axes 3x3\n\n\nfig\n,\n \nax\n \n=\n \nplt\n.\nsubplots\n(\n3\n,\n \n3\n,\n \nfigsize\n=\n(\n15\n,\n15\n))\n\n\n\n# train and look at decision trees with different max depth\n\n\nfor\n \nmax_depth\n \nin\n \nrange\n(\n0\n,\n \n9\n):\n\n  \ntrain_and_look\n(\nDT\n(\nmax_depth\n=\nmax_depth\n \n+\n \n1\n),\n \nX\n,\n \nY\n,\n\n                 \nax\n=\nax\n[\nmax_depth\n \n//\n \n3\n][\nmax_depth\n \n%\n \n3\n],\n\n                 \ntitle\n=\n\"Max depth = {}\"\n.\nformat\n(\nmax_depth\n \n+\n \n1\n))\n\n\n\n\n\n\n\n\n\n\nmax_depth\n <= 3 - undefitting\n\n\nmax_depth\n <= 6 - quite good\n\n\nmax_depth\n  > 6 - overfitting\n\n\n\n\nRandom forest\n\u00b6\n\n\n\n\nLets do the same with random forests (100 trees in each forest)\n\n\n\n\nfrom\n \nsklearn.ensemble\n \nimport\n \nRandomForestClassifier\n \nas\n \nRF\n\n\n\n# create a figure with 9 axes 3x3\n\n\nfig\n,\n \nax\n \n=\n \nplt\n.\nsubplots\n(\n3\n,\n \n3\n,\n \nfigsize\n=\n(\n15\n,\n15\n))\n\n\n\n# train and look at decision trees with different max depth\n\n\nfor\n \nmax_depth\n \nin\n \nrange\n(\n0\n,\n \n9\n):\n\n  \ntrain_and_look\n(\nRF\n(\nn_estimators\n=\n100\n,\n \nmax_depth\n=\nmax_depth\n \n+\n \n1\n),\n \nX\n,\n \nY\n,\n\n                 \nax\n=\nax\n[\nmax_depth\n \n//\n \n3\n][\nmax_depth\n \n%\n \n3\n],\n\n                 \ntitle\n=\n\"Max depth = {}\"\n.\nformat\n(\nmax_depth\n \n+\n \n1\n))\n\n\n\n\n\n\n\n\n\n\n\n\nThe combination of shallow trees (weak learners) does a good job\n\n\n\n\n\n\nOverfitting is somehow prevented \n\n\n\n\n\n\nSummary\n\u00b6\n\n\n\n\n\n\nThe most important lesson today: \nbias-variance trade-off\n\n\n\n\n\n\nFor the lecture easy examples are chosen so they can be visualize\n\n\n\n\n\n\nIn real life problems, it is hard / impossible to determine using \"bye eye\" method if the model is underfitted or overfitted\n\n\n\n\n\n\nNote, that actually you should never used this method even if you think \"your eye\" is right - you would be surprised how it is not\n\n\n\n\n\n\nOne needs a way to measure the goodnes of a model - usually mean squared error\n\n\n\n\n\n\nIn practice, most people use cross-calidation technique \n\n\n\n\n\n\n\n\n\n\nThe biggest advantages of decision trees algoritms is that they are east to interpret (it is easy to explain even to non-experts how they work, which is not the case with e.g. deep neural networks)\n\n\n\n\n\n\nUsually, decision trees are used as weak learners in ensemble learning\n\n\n\n\n\n\nThe most famous boosting algorithm is AdaBoost, because it is a good one and the first one. Although, there are many other boosting methods on market right now with XGBoost being one of the most popular one\n\n\n\n\n\n\nAs for today, deep learning has better publicity, but boosted trees are still one of the most common algorithms used in \nKaggle competitions\n\n\n\n\n\n\nBoosted trees are also popular among physicist and used by them as an alternative to neural networks for experimental elementary particle physics in event reconstruction procedures",
            "title": "Decision Trees"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#decision-trees",
            "text": "",
            "title": "Decision Trees"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#graphviz",
            "text": "",
            "title": "Graphviz"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#installing-graphviz",
            "text": "! apt   install   - y   graphviz  ! pip   install   graphviz",
            "title": "Installing graphviz"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#a-tree-example",
            "text": "from   graphviz   import   Digraph  styles   =   { \n     'top' :   { 'shape' :   'ellipse' ,   'style' :   'filled' ,   'color' :   'lightblue' }, \n     'no' :    { 'shape' :   'circle' ,   'style' :   'filled' ,   'color' :   'red' }, \n     'yes' :   { 'shape' :   'circle' ,   'style' :   'filled' ,   'color' :   'lightgreen' }, \n     'qst' :   { 'shape' :   'rect' }  }  example_tree   =   Digraph ()  example_tree . node ( 'top' ,   'Should I attend the ML lecture?' ,   styles [ 'top' ])  example_tree . node ( 'q1' ,   'Do I fulfill requirements?' ,   styles [ 'qst' ])  example_tree . node ( 'q2' ,   'Do I like CS?' ,   styles [ 'qst' ])  example_tree . node ( 'no1' ,   'No ' ,   styles [ 'no' ])  example_tree . node ( 'q3' ,   'Is the lecture early in the morning?' ,   styles [ 'qst' ])  example_tree . node ( 'no2' ,   'No ' ,   styles [ 'no' ])  example_tree . node ( 'no3' ,   'No ' ,   styles [ 'no' ])  example_tree . node ( 'yes' ,   'Yes' ,   styles [ 'yes' ])  example_tree . edge ( 'top' ,   'q1' )  example_tree . edge ( 'q1' ,   'q2' ,   'Yes' )  example_tree . edge ( 'q1' ,   'no1' ,   'No' )  example_tree . edge ( 'q2' ,   'q3' ,   'Yes' )  example_tree . edge ( 'q2' ,   'no2' ,   'No' )  example_tree . edge ( 'q3' ,   'no3' ,   'Yes' )  example_tree . edge ( 'q3' ,   'yes' ,   'No' )   example_tree",
            "title": "A tree example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#introduction",
            "text": "",
            "title": "Introduction"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#decision-trees_1",
            "text": "Supervised learning algorithm - training dataset with known labels    Eager learning - final model does not need training data to make prediction (all parameters are evaluated during learning step)    It can do both classification and regression    A decision tree is built from:   decision nodes  - correspond to features (attributes)  leaf nodes  - correspond to class labels     The  root  of a tree is (should be) the best predictor (feature)",
            "title": "Decision trees"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#example",
            "text": "import   matplotlib.pyplot   as   plt  import   numpy   as   np  # just to overwrite default colab style  plt . style . use ( 'default' )  plt . style . use ( 'seaborn-talk' )   # first define some points representing two classes  grid   =   np . mgrid [ 0 : 10 : 2 ,   0 : 10 : 2 ]  set01   =   np . vstack ([ grid [ 0 ] . ravel (),   grid [ 1 ] . ravel ()]) . T  set01   =   np . delete ( set01 ,   [ 17 ,   18 ,   19 ,   22 ,   24 ],   axis = 0 )  grid   =   np . mgrid [ 6 : 16 : 2 ,   0 : 10 : 2 ]  set02   =   np . vstack ([ grid [ 0 ] . ravel (),   grid [ 1 ] . ravel ()]) . T  set02   =   np . delete ( set02 ,   [ 0 ,   1 ,   5 ,   6 ,   8 ],   axis = 0 )  plt . scatter ( * set01 . T )  plt . scatter ( * set02 . T )  plt . text ( 15 ,   4 ,   \"There are two attributes: x and y \\n\\n \" \n                 \"    * each decision node splits dataset based on one of the attributes \\n\\n \" \n                 \"    * each leaf node defines a class label\" );    plt . scatter ( * set01 . T )  plt . scatter ( * set02 . T )  plt . plot ([ 5 ,   5 ],   [ 0 ,   8 ],   'r' )  plt . plot ([ 0 ,   14 ],   [ 3 ,   3 ],   'g' )  plt . text ( 15 ,   3 ,   \"We start with [20, 20] (blue, orange) \\n\\n \" \n                 \"Red line splits dataset in [15, 0] (left) and [5, 20] (right) \\n\\n \" \n                 \"Green line split dataset in [10, 6] (bottom) and [10, 14] (top) \\n\\n \" \n                 \"Red line is a winner and should be the root of our tree\" );    tree   =   Digraph ()  tree . edge ( \"x > 5? \\n [20, 20]\" ,   \"blue \\n [15, 0]\" ,   \"No\" )  tree . edge ( \"x > 5? \\n [20, 20]\" ,   \"y > 3? \\n [5, 20]\" ,   \"Yes\" )  tree . edge ( \"y > 3? \\n [5, 20]\" ,   \"x > 9? \\n [4, 6]\" ,   \"No\" )  tree . edge ( \"y > 3? \\n [5, 20]\" ,   \"almost orange \\n [1, 14]\" ,   \"Yes\" )  tree . edge ( \"x > 9? \\n [4, 6]\" ,   \"blue \\n [4, 0]\" ,   \"No\" )  tree . edge ( \"x > 9? \\n [4, 6]\" ,   \"orange \\n [0, 6]\" ,   \"Yes\" )  tree . edge ( \"almost orange \\n [1, 14]\" ,   \"Should we continue? \\n Or would it be overfitting?\" )  tree      It is important to start with good predictor    Our choice of the root classifies 37.5% of points  in the first step    Note, that we could also start with  x > 9?    However, if we started with  y > 3  we would never classify a point in the first step - does it mean that it is worse choice?      tree   =   Digraph ()  tree . edge ( \"y > 3? \\n [20, 20]\" ,   \"x > 9? \\n [10, 6]\" ,   \"No\" )  tree . edge ( \"y > 3? \\n [20, 20]\" ,   \"x > 5? \\n [10, 14]\" ,   \"Yes\" )  tree . edge ( \"x > 9? \\n [10, 6]\" ,   \"blue \\n [10, 0]\" ,   \"No\" )  tree . edge ( \"x > 9? \\n [10, 6]\" ,   \"orange \\n [0, 6]\" ,   \"Yes\" )  tree . edge ( \"x > 5? \\n [10, 14]\" ,   \"blue \\n [9, 0]\" ,   \"No\" )  tree . edge ( \"x > 5? \\n [10, 14]\" ,   \"almost orange \\n [1, 14]\" ,   \"Yes\" )  tree      In this case we never have to make more than 2 checks    There are two open questions to answer:    How to automate the procees of chosing nodes?    How deep should we go?",
            "title": "Example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#id3-and-c45-algorithms",
            "text": "We start with algorithms based on information theory    ID3 (Iterative Dichotomiser 3)    C4.5 - extension of ID3 (why C4.5? C stands for programming language and 4.5 for version?)    C5.0/See5 - improved C4.5 (commercial; single-threaded Linux version is available under GPL though)      The idea is to find nodes which maximize information gain",
            "title": "ID3 and C4.5 algorithms"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#information-gain",
            "text": "",
            "title": "Information gain"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#self-information",
            "text": "Let  X = (x_1, x_2, ..., x_n) X = (x_1, x_2, ..., x_n)  be our  information source  (feature), e.g. weather condition:  x_1 x_1  = sunny,  x_2 x_2  = overcast,  x_3 x_3  = rainy    And let  P = (p_1, p_2, ..., p_n) P = (p_1, p_2, ..., p_n)  be corresponding probrability distribution (or more precisely - probability mass function)    We want some measure of information  I I  provided by an event. It should satisfy the following properties:    I I  depends only on the probability of  x_i x_i , thus  I \\equiv I(p_i) I \\equiv I(p_i)    I I  is continuous and deacreasing function of  p_i p_i    I I  is non-negative and  I(1) = 0 I(1) = 0    if  p_i = p_{i, 1} \\cdot p_{i, 2} p_i = p_{i, 1} \\cdot p_{i, 2}  (independent events) then  I(p_i) = I(p_{i, 1}) + I(p_{i, 2}) I(p_i) = I(p_{i, 1}) + I(p_{i, 2})      Logarithmic function satisfies all above condition, so we define self-information as:  I(p) = -\\log(p) I(p) = -\\log(p)    The most common log base is  2  and then information is in  shannons (Sh) , also known as  bits    In the case of  natural logarithm  the unit is  nat  (natural unit of information)    In the case of base  10  the unit is  hartley (Hart) , also known as  dit      x   =   np . arange ( 0.01 ,   1.01 ,   0.01 )  plt . xlabel ( \"p\" )  plt . ylabel ( \"I(p)\" )  plt . plot ( x ,   - np . log2 ( x ),   label = \"bit\" )  plt . plot ( x ,   - np . log ( x ),   label = \"nat\" )  plt . plot ( x ,   - np . log10 ( x ),   label = \"dit\" )  plt . legend ();      Lets X = (head, tail) with P = (0.5, 0.5)   We get 1 Sh of information     Lets X = (sunny, overcast, rainy) with P = (0.25, 0.75, 0.25)    If it is overcast, we get 0.415 Sh of information    Otherwise, we get 2 Sh of information      If an event is more likely we learn less",
            "title": "Self-information"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#information-entropy",
            "text": "Also called Shannon entropy (after the father of intromation theory)    Usually information entropy is denoted as  H H    H H  is defined as the weighted average of the self-information of all possible outcomes  H(X) = \\sum\\limits_{i=1}^N p_i \\cdot I(p_i) = -\\sum\\limits_{i=1}^N p_i\\cdot\\log(p_i) H(X) = \\sum\\limits_{i=1}^N p_i \\cdot I(p_i) = -\\sum\\limits_{i=1}^N p_i\\cdot\\log(p_i)    Lets consider two case scenario with  P = (p, 1 - p) P = (p, 1 - p) , so entropy is given by  H = -p \\log(p) - (1 - p) \\log(1 - p) H = -p \\log(p) - (1 - p) \\log(1 - p)    p   =   np . arange ( 0.01 ,   1.0 ,   0.01 )  plt . xlabel ( \"p\" )  plt . ylabel ( \"H\" )  plt . annotate ( 'we are surprised' ,   xy = ( 0.5 ,   1 ),   xytext = ( 0.5 ,   0.75 ), \n              arrowprops = dict ( facecolor = 'black' ,   shrink = 0.1 ))  plt . annotate ( 'we are not that surprised' ,   xy = ( 1 ,   0.1 ),   xytext = ( 0.5 ,   0.25 ), \n              arrowprops = dict ( facecolor = 'black' ,   shrink = 0.1 ))  plt . plot ( p ,   - p   *   np . log2 ( p )   -   ( 1   -   p )   *   np . log2 ( 1   -   p ));     Lets consider three case scenario with  P = (p, q, 1 - p - q) P = (p, q, 1 - p - q) , so entropy is given by  H = -p \\log(p) - q\\log(q) - (1 - p - q) \\log(1 - p - q) H = -p \\log(p) - q\\log(q) - (1 - p - q) \\log(1 - p - q)   from   mpl_toolkits   import   mplot3d  # grid of p, q probabilities  p ,   q   =   np . meshgrid ( np . arange ( 0.01 ,   1.0 ,   0.01 ),   np . arange ( 0.01 ,   1.0 ,   0.01 ))  # remove (set to 0) points which do not fulfill P <= 1  idx   =   p   +   q   >   1  p [ idx ]   =   0  q [ idx ]   =   0  # calculate entropy (disable warnings - we are aware of log(0))  np . warnings . filterwarnings ( 'ignore' )  h   =   - p   *   np . log2 ( p )   -   q   *   np . log2 ( q )   -   ( 1   -   p   -   q )   *   np . log2 ( 1   -   p   -   q )  # make a plot  plt . axes ( projection = '3d' ) . plot_surface ( p ,   q ,   h );",
            "title": "Information entropy"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#information-gain_1",
            "text": "Let  T T  be the set of training samples with  n n  possible outcomes, thus  T = \\{T_1, T_2, ..., T_n\\} T = \\{T_1, T_2, ..., T_n\\}    The entropy is given by  H(T) = -\\sum\\limits_{i=1}^N p_i\\cdot\\log(p_i) = -\\sum\\limits_{i=1}^N \\frac{|T_i|}{|T|}\\cdot\\log(\\frac{|T_i|}{|T|}) H(T) = -\\sum\\limits_{i=1}^N p_i\\cdot\\log(p_i) = -\\sum\\limits_{i=1}^N \\frac{|T_i|}{|T|}\\cdot\\log(\\frac{|T_i|}{|T|})    We can also calulate the entropy after  T T  was partitioned in  T_i T_i  with respect to some feature  X X   H(T, X) = \\sum\\limits_{i=1}^N p_i\\cdot H(T_i) H(T, X) = \\sum\\limits_{i=1}^N p_i\\cdot H(T_i)    And the information gain is defined as  G(X) = H(T) - H(T, X) G(X) = H(T) - H(T, X)",
            "title": "Information gain"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#example_1",
            "text": "Lets calculate some example step by step    Lets consider a fake dataset    two classes: C01, C02    three features: X1, X2, X3         X1  ||  A  |  A  |  A  |  B  |  B  |  C  |  C  |  C  |  C  |\n---------------------------------------------------------------\n   X2  ||  0  |  0  |  1  |  1  |  0  |  1  |  1  |  1  |  0  |\n---------------------------------------------------------------\n   X3  || RED | GRN | GRN | BLU | RED | GRN | BLU | RED | GRN |\n===============================================================\n Class || C01 | C01 | C02 | C02 | C02 | C02 | C01 | C01 | C02 |  from   math   import   log  def   entropy ( * probs ): \n   \"\"\"Calculate information entropy\"\"\" \n   try : \n     total   =   sum ( probs ) \n     return   sum ([ - p   /   total   *   log ( p   /   total ,   2 )   for   p   in   probs ]) \n   except : \n     return   0  print ( entropy ( 4 ,   5 ),   entropy ( 2 ,   1 ),   entropy ( 2 ,   2 ))   0.9910760598382222 0.9182958340544896 1.0    The  root  entropy   We have 9 samples: 4 belong to class C01 and 5 to C02  H(T) = -\\frac{4}{9}\\log(\\frac{4}{9}) - \\frac{5}{9}\\log(\\frac{5}{9}) = 0.99 H(T) = -\\frac{4}{9}\\log(\\frac{4}{9}) - \\frac{5}{9}\\log(\\frac{5}{9}) = 0.99     Now lets consider feature X1, which splits data into subsets  T_1 T_1 ,  T_2 T_2 , and  T_3 T_3  (with X1 value A, B, and C, respectively)    Within  T_1 T_1  there are 3 samples: 2 from C01 and 1 from C02  H(T_1) = -\\frac{2}{3}\\log(\\frac{2}{3}) - \\frac{1}{3}\\log(\\frac{1}{3}) = 0.92 H(T_1) = -\\frac{2}{3}\\log(\\frac{2}{3}) - \\frac{1}{3}\\log(\\frac{1}{3}) = 0.92    Within  T_2 T_2  there are 2 samples: 0 from C01 and 2 from C02  H(T_2) = -\\frac{2}{2}\\log(\\frac{2}{2}) - \\frac{0}{2}\\log(\\frac{0}{2}) = 0.00 H(T_2) = -\\frac{2}{2}\\log(\\frac{2}{2}) - \\frac{0}{2}\\log(\\frac{0}{2}) = 0.00    Within  T_3 T_3  there are 4 samples: 2 from C01 and 2 from C02  H(T_3) = -\\frac{2}{4}\\log(\\frac{2}{4}) - \\frac{2}{4}\\log(\\frac{2}{4}) = 1.00 H(T_3) = -\\frac{2}{4}\\log(\\frac{2}{4}) - \\frac{2}{4}\\log(\\frac{2}{4}) = 1.00    The resulting entropy is  H(T, X1) = \\frac{3}{9}\\cdot H(T_1) + \\frac{2}{9}\\cdot H(T_2) + \\frac{4}{9}\\cdot H(T_3) = 0.75 H(T, X1) = \\frac{3}{9}\\cdot H(T_1) + \\frac{2}{9}\\cdot H(T_2) + \\frac{4}{9}\\cdot H(T_3) = 0.75    Thus, infromation gain if the set is split according to X1  G(X1) = H(T) - H(T, X1) = 0.99 - 0.75 = 0.24 \\mbox{ Sh } G(X1) = H(T) - H(T, X1) = 0.99 - 0.75 = 0.24 \\mbox{ Sh }",
            "title": "Example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#id3-algorithm",
            "text": "For every attribute (feature) calculate the entropy    Split the training set using the one for which information gain is maximum    Continue recursively on subsets using remaining features",
            "title": "ID3 algorithm"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#play-golf-dataset",
            "text": "Popular dataset to explain decision trees    4 features:    outlook :  rainy, overcast, sunny    temperature :  cool, mild, hot    humidity :  normal, high    windy :  false, true      Possible outcomes (play golf?):    false    true      import   pandas   as   pd  # first row = headers  src   =   \"http://chem-eng.utoronto.ca/~datamining/dmc/datasets/weather_nominal.csv\"  golf_data   =   pd . read_csv ( src )   golf_data    \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       Outlook \n       Temperature \n       Humidity \n       Windy \n       Play golf \n     \n   \n   \n     \n       0 \n       Rainy \n       Hot \n       High \n       False \n       No \n     \n     \n       1 \n       Rainy \n       Hot \n       High \n       True \n       No \n     \n     \n       2 \n       Overcast \n       Hot \n       High \n       False \n       Yes \n     \n     \n       3 \n       Sunny \n       Mild \n       High \n       False \n       Yes \n     \n     \n       4 \n       Sunny \n       Cool \n       Normal \n       False \n       Yes \n     \n     \n       5 \n       Sunny \n       Cool \n       Normal \n       True \n       No \n     \n     \n       6 \n       Overcast \n       Cool \n       Normal \n       True \n       Yes \n     \n     \n       7 \n       Rainy \n       Mild \n       High \n       False \n       No \n     \n     \n       8 \n       Rainy \n       Cool \n       Normal \n       False \n       Yes \n     \n     \n       9 \n       Sunny \n       Mild \n       Normal \n       False \n       Yes \n     \n     \n       10 \n       Rainy \n       Mild \n       Normal \n       True \n       Yes \n     \n     \n       11 \n       Overcast \n       Mild \n       High \n       True \n       Yes \n     \n     \n       12 \n       Overcast \n       Hot \n       Normal \n       False \n       Yes \n     \n     \n       13 \n       Sunny \n       Mild \n       High \n       True \n       No",
            "title": "Play Golf dataset"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#play-golf-entropy",
            "text": "entropy ( 9 ,   5 )   0.9402859586706309  | Play golf |\n=============\n| yes | no  |  -> H(T) = 0.94\n-------------\n|  9  |  5  |",
            "title": "Play golf entropy"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#play-golf-vs-outlook",
            "text": "| Play golf |\n                   =============\n                   | yes | no  |\n        ------------------------\n        | sunny    |  3  |  2  |  5\noutlook | overcast |  4  |  0  |  4\n        | rainy    |  2  |  3  |  5\n        ------------------------\n                      9     5  entropy ( 3 ,   2 ),   0 ,   entropy ( 2 ,   3 )   (0.9709505944546686, 0, 0.9709505944546686)   \n\\begin{eqnarray}\n   H(\\mbox{sunny}) & = & 0.97 \\\\\n   H(\\mbox{rainy}) & = & 0.97 \\\\\nH(\\mbox{overcast}) & = & 0\n\\end{eqnarray}     \n\\begin{eqnarray}\nH(T, \\mbox{outlook}) & = & P(\\mbox{sunny})\\cdot H(\\mbox{sunny}) + P(\\mbox{overcast})\\cdot H(\\mbox{overcast}) + P(\\mbox{rainy})\\cdot H(\\mbox{rainy}) \\\\\n                     & = & \\frac{5}{14}\\cdot 0.97 + \\frac{4}{14} \\cdot 0 + \\frac{5}{14}\\cdot 0.97 = 0.69\n\\end{eqnarray}     \n\\begin{eqnarray}\nG(\\mbox{outlook}) & = & H(T) - H(T, \\mbox{outlook}) = 0.94 - 0.69 = 0.25\n\\end{eqnarray}",
            "title": "Play golf vs outlook"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#results-for-all-features",
            "text": "| Play golf |                         | Play golf |\n                    =============                         =============\n                    | yes | no  |                         | yes | no  |\n         ------------------------                 --------------------\n         | sunny    |  3  |  2  |                 | hot   |  2  |  2  |\n outlook | overcast |  4  |  0  |     temperature | mild  |  4  |  2  |\n         | rainy    |  2  |  3  |                 | cool  |  3  |  1  |\n         ------------------------                 --------------------\n            Info. gain = 0.25                       Info gain = 0.03\n\n\n                    | Play golf |                         | Play golf |\n                    =============                         =============\n                    | yes | no  |                         | yes | no  |\n         ------------------------                 --------------------\n         | high     |  3  |  4  |                 | false |  6  |  2  |\nhumidity | normal   |  6  |  1  |           windy | true  |  3  |  3  |\n         ------------------------                 --------------------\n            Info. gain = 0.15                       Info gain = 0.05",
            "title": "Results for all features"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#root-of-the-tree",
            "text": "Start building a tree with the feature with the largest information gain:  outlook    A branch with  entropy 0  is a leaf node:  overcast    Other branches must be spliited using other features     tree   =   Digraph ()  tree . edge ( \"outlook\" ,   \"sunny\" )  tree . edge ( \"outlook\" ,   \"overcast\" )  tree . edge ( \"outlook\" ,   \"rainy\" )  tree . edge ( \"overcast\" ,   \"yes\" )  tree",
            "title": "Root of the tree"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#next-branch",
            "text": "golf_data . loc [ golf_data [ 'Outlook' ]   ==   \"Sunny\" ]    \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       Outlook \n       Temperature \n       Humidity \n       Windy \n       Play golf \n     \n   \n   \n     \n       3 \n       Sunny \n       Mild \n       High \n       False \n       Yes \n     \n     \n       4 \n       Sunny \n       Cool \n       Normal \n       False \n       Yes \n     \n     \n       5 \n       Sunny \n       Cool \n       Normal \n       True \n       No \n     \n     \n       9 \n       Sunny \n       Mild \n       Normal \n       False \n       Yes \n     \n     \n       13 \n       Sunny \n       Mild \n       High \n       True \n       No \n     \n        In general, one should calculate information gain for each feature for this subset    In this case it is clear that we can take  windy      tree . edge ( \"sunny\" ,   \"windy\" )  tree . edge ( \"windy\" ,   \"false\" )  tree . edge ( \"windy\" ,   \"true\" )  tree . edge ( \"false\" ,   \"yes\" )  tree . edge ( \"true\" ,   \"no\" )  tree",
            "title": "Next branch"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#last-branch",
            "text": "golf_data . loc [ golf_data [ 'Outlook' ]   ==   \"Rainy\" ]    \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       Outlook \n       Temperature \n       Humidity \n       Windy \n       Play golf \n     \n   \n   \n     \n       0 \n       Rainy \n       Hot \n       High \n       False \n       No \n     \n     \n       1 \n       Rainy \n       Hot \n       High \n       True \n       No \n     \n     \n       7 \n       Rainy \n       Mild \n       High \n       False \n       No \n     \n     \n       8 \n       Rainy \n       Cool \n       Normal \n       False \n       Yes \n     \n     \n       10 \n       Rainy \n       Mild \n       Normal \n       True \n       Yes \n     \n      tree . edge ( \"rainy\" ,   \"humidity\" )  tree . edge ( \"humidity\" ,   \"high\" )  tree . edge ( \"humidity\" ,   \"normal\" )  tree . edge ( \"normal\" ,   \"yes\" )  tree . edge ( \"high\" ,   \"no \" )  tree",
            "title": "Last branch"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#summary",
            "text": "We got the final tree for Play Golf dataset using ID3 algorithm    We do not even use temperature attribute (for which information gain was 0.03)    The main problem is that the algorithm may overfit easily (tree does not stop growing until the whole training set is classified)    Imagine some crazy guys went playing on a  rainy ,  windy  day with  high humidity , beacaue it was still  hot    With this extra data point we would have to create more branches    Is one unique data sample worth to extend the whole tree?      And there is more disadvantages:    It handles only discrete attributes    There is a strong bias for features with many possible outcomes    And finally, it does not handle missing values",
            "title": "Summary"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#c45-algorithm",
            "text": "C4.5 introduces some improvements to ID3:    continuous values using threshold    tree pruning to avoid overfitting    normalized information gain    missing values",
            "title": "C4.5 algorithm"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#information-gain-ratio",
            "text": "To avoid a bias in favor of features with a lot of different values C4.5 uses information gain ratio instead of information gain    Lets define intrinsic value  V V  of an attribute  X X  as  V(X) = -\\sum\\limits_{i=1}^N \\frac{|T_i|}{|T|}\\cdot\\log(\\frac{|T_i|}{|T|}) V(X) = -\\sum\\limits_{i=1}^N \\frac{|T_i|}{|T|}\\cdot\\log(\\frac{|T_i|}{|T|})    where  T_i T_i  are samples corresponding to  i i -th possible value of  X X  feature    Information gain ratio  R(X) R(X)  is defined as  R(X) = \\frac{G(X)}{V(X)} R(X) = \\frac{G(X)}{V(X)}",
            "title": "Information gain ratio"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#example_2",
            "text": "Lets consider a fake data set    The goal is to determine if someone plays or not video games    We have three features:    name - mostly unique    sex - 50% females and 50% males     age - just old or young      Looking at data we can say that    most young people play video games, why old people don't    sex does not matter    names are almost distinct           name   ||  John  |  Mark  |  Anne  |  Adam  |  John  |  Alex  |  Alex  |  Xena  |  Tina  |  Lucy  |\n--------------------------------------------------------------------------------------------------------\n     sex    ||   M    |   M    |   F    |   M    |   M    |   F    |   M    |   F    |   F    |   F    |\n--------------------------------------------------------------------------------------------------------\n     age    ||  old   | young  |  old   | young  | young  | young  |  old   |  old   | young  | young  |\n========================================================================================================\n play games ||   N    |   Y    |   Y    |   Y    |   Y    |   N    |   N    |   N    |   Y    |   Y    |   Information gain for  name   h   =   entropy ( 4 ,   6 )    # dataset entropy H(T)  # one John plays and the other one doesn't  # in other cases entropy = 0  g_name   =   h   -   2 / 10   *   entropy ( 1 ,   1 )  print ( g_name )   0.7709505944546686   Information gain for  sex   # 5 men - 3 play  # 5 women - 3 play  g_sex   =   h   -   5 / 10   *   entropy ( 2 ,   3 )   -   5 / 10   *   entropy ( 2 ,   3 )  print ( g_sex )   0.0   Information gain for  age   # 4 old people - 1 plays  # 6 young people - 5 play  g_age   =   h   -   4 / 10   *   entropy ( 1 ,   3 )   -   6 / 10   *   entropy ( 1 ,   5 )  print ( g_age )   0.256425891682003    In ID3 a feature with entropy = 0 is always a winner   Imagine having all distinct values (e.g. credit card numbers)     In this case we would choose  name  as the best predictor    Creating a tree with 8 branches (from 10 samples)    Training data would be perfectly classify    But it is unlikely that the algorithm would be able to generalize for unseen data      Lets calculate information gain ratio and see how it changes the choice of the best feature    Information gain ratio for  name    # 2x John, 2x Alex, 6x unique name   g_name   /   entropy ( 2 ,   2 ,   * [ 1 ] * 6 )   0.26384995435159336   Information gain ratio for  sex   # 5 males and 5 females - zero stays zero though  g_sex   /   entropy ( 5 ,   5 )   0.0   Information gain ratio for  age   # 4x old and 6x young  g_age   /   entropy ( 4 ,   6 )   0.26409777505314147    Based on information gain ratio we choose  age  as the best predictor    Because the denominator in a ratio penalizes features with many values    print ( \"Two possible values: \\n \" )  for   i   in   range ( 0 ,   11 ): \n   print ( \" \\t ({}, {}) split -> entropy = {}\" . format ( i ,   10 - i ,   entropy ( i ,   10 - i )))  print ( \" \\n 10 possible values:\" ,   entropy ( * [ 1 ] * 10 ))   Two possible values:\n\n    (0, 10) split -> entropy = 0\n    (1, 9) split -> entropy = 0.4689955935892812\n    (2, 8) split -> entropy = 0.7219280948873623\n    (3, 7) split -> entropy = 0.8812908992306927\n    (4, 6) split -> entropy = 0.9709505944546686\n    (5, 5) split -> entropy = 1.0\n    (6, 4) split -> entropy = 0.9709505944546686\n    (7, 3) split -> entropy = 0.8812908992306927\n    (8, 2) split -> entropy = 0.7219280948873623\n    (9, 1) split -> entropy = 0.4689955935892812\n    (10, 0) split -> entropy = 0\n\n10 possible values: 3.321928094887362   This datset was handcrafted to make a point, but I hope the message is still clear",
            "title": "Example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#continuous-values",
            "text": "Attributes with continuous values must be first discretize    The best way is to find an optimal threshold which splits the set    The optimal threshold is the one which maximize the infromation gain",
            "title": "Continuous values"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#example_3",
            "text": "Lets consider the same example as before    But this time age has numerical values         name   ||  John  |  Mark  |  Anne  |  Adam  |  John  |  Alex  |  Alex  |  Xena  |  Tina  |  Lucy  |\n--------------------------------------------------------------------------------------------------------\n     sex    ||   M    |   M    |   F    |   M    |   M    |   F    |   M    |   F    |   F    |   F    |\n--------------------------------------------------------------------------------------------------------\n     age    ||   50   |   18   |   65   |   24   |   31   |   18   |   50   |   50   |   24   |   31   |\n========================================================================================================\n play games ||   N    |   Y    |   Y    |   Y    |   Y    |   N    |   N    |   N    |   Y    |   Y    |   The possible thesholds are therefore  \\{18, 24, 31, 50\\} \\{18, 24, 31, 50\\}   # calculate entropy for all possible thresholds  e18   =   2 / 10   *   entropy ( 1 ,   1 )   +   8 / 10   *   entropy ( 3 ,   5 )  e24   =   4 / 10   *   entropy ( 1 ,   3 )   +   6 / 10   *   entropy ( 3 ,   3 )  e31   =   6 / 10   *   entropy ( 1 ,   5 )   +   4 / 10   *   entropy ( 3 ,   1 )  e50   =   9 / 10   *   entropy ( 4 ,   5 )   +   1 / 10   *   entropy ( 0 ,   1 )  print ( \"With threshold = {}, entropy = {}\" . format ( 18 ,   e18 ))  print ( \"With threshold = {}, entropy = {}\" . format ( 24 ,   e24 ))  print ( \"With threshold = {}, entropy = {}\" . format ( 31 ,   e31 ))  print ( \"With threshold = {}, entropy = {}\" . format ( 50 ,   e50 ))   With threshold = 18, entropy = 0.963547202339972\nWith threshold = 24, entropy = 0.9245112497836532\nWith threshold = 31, entropy = 0.7145247027726656\nWith threshold = 50, entropy = 0.8919684538544    The best test is  if age > 31   it splits the dataset to 6 samples (with 5 players) and 4 samples (with 3 non-players)     Please note, that the best threshold may change once a node is created",
            "title": "Example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#unknown-parameters",
            "text": "In the case some samples are incomplete one needs to correct the information gain    The information gain is calculated as before for samples with known attributes    But then it is normalized with respect to the probability that the given attribute has known values    Lets define the factor  F F  as the ratio of the number of samples with known value for a given feature to the number of all samples in a dataset    Then information gain is defines as  G(X) = F\\cdot (H(T) - H(T, X)) G(X) = F\\cdot (H(T) - H(T, X))    Please note, that  F = 1 F = 1  if all values are known    Otherwise, information gain is scaled accordingly",
            "title": "Unknown parameters"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#pruning",
            "text": "The algorithm creates as many nodes as needed to classify all test samples    It may lead to overfitting and the resulting tree would fail to classify correctly unseen samples    To avoid this one can prune a tree    pre-pruning (early stopping)    stop building a tree before leaves with few samples are produced    how to decide when it is good time to stop? e.g. using cross-validation on validation set (stop if the error does not increase significantly)    underfitting if stop to early      post-pruning    let a tree grow completely    then go from bottom to top and try to replace a node with a leaf    if there is improvement in accuracy - cut a tree    if the accuracy stays the same - cut a tree (Occam's razor)    otherwise leave a node",
            "title": "Pruning"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#first-example-step-by-step",
            "text": "Lets consider the problem from the beginning of the lecture    Our dataset has 20 blue points and 20 orange points    Each point has two features (both are numerical)    We expect overfitting if pruning is not applied    We will calculate everything step by step (it is boring, but demonstrates how the algorithm works)    # first define some points representing two classes  grid   =   np . mgrid [ 0 : 10 : 2 ,   0 : 10 : 2 ]  set01   =   np . vstack ([ grid [ 0 ] . ravel (),   grid [ 1 ] . ravel ()]) . T  set01   =   np . delete ( set01 ,   [ 17 ,   18 ,   19 ,   22 ,   24 ],   axis = 0 )  grid   =   np . mgrid [ 6 : 16 : 2 ,   0 : 10 : 2 ]  set02   =   np . vstack ([ grid [ 0 ] . ravel (),   grid [ 1 ] . ravel ()]) . T  set02   =   np . delete ( set02 ,   [ 0 ,   1 ,   5 ,   6 ,   8 ],   axis = 0 )  plt . scatter ( * set01 . T )  plt . scatter ( * set02 . T );",
            "title": "First example - step by step"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#validation-set",
            "text": "We will use 10 points from the dataset for validation    This time selected manually to perform by hand calculations    On the plot below X denotes validation samples    # split dataset to training and validation set  # note, we should splt them randomly  # but here we do this by hand  valid_idx   =   [ 3 ,   7 ,   10 ,   14 ,   18 ]  blue_valid   =   set01 [ valid_idx ]  blue_train   =   np . delete ( set01 ,   valid_idx ,   axis = 0 )  orange_valid   =   set02 [ valid_idx ]  orange_train   =   np . delete ( set02 ,   valid_idx ,   axis = 0 )  # circles - training set  # x - validation set  plt . scatter ( * blue_train . T )  plt . scatter ( * blue_valid . T ,   color = 'C0' ,   marker = 'x' )  plt . scatter ( * orange_train . T )  plt . scatter ( * orange_valid . T ,   color = 'C1' ,   marker = 'x' );",
            "title": "Validation set"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#thresholds-finder",
            "text": "When building a tree we need to calculate information gain for every threshold in current subset    Every subset  S S  has  N_b N_b  blue samples and  N_o N_o  orange samples    After split into accoring to some threshold we get two subsets    n_b n_b  of blue points and  n_o n_o  of orange points ( S_1 S_1 )    N_b - n_b N_b - n_b  of blue points and  N_o - n_o N_o - n_o  of orange points ( S_2 S_2 )      def   info_gain ( Nb ,   No ,   nb ,   no ): \n   \"\"\"Calculate information gain for given split\"\"\" \n   h   =   entropy ( Nb ,   No )   # H(S) \n   total   =   Nb   +   No       # total number of samples \n   subtotal   =   nb   +   no    # number of samples in subset \n\n   return   h   -   subtotal   /   total   *   entropy ( nb ,   no )  \\\n            -   ( total   -   subtotal )   /   total   *   entropy ( Nb   -   nb ,   No   -   no )",
            "title": "Thresholds finder"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#feature-x",
            "text": "We need to calculate information gain ratio for the best threshold (the one that maximize information gain)    Possible thresholds  \\{0, 2, 4, 6, 8, 10, 12\\} \\{0, 2, 4, 6, 8, 10, 12\\}    Nb   =   15  No   =   15  splits   =   { \"0\" :   ( 4 ,   0 ),   \"2 \" :   ( 8 ,   0 ),   \"4\" :   ( 11 ,   0 ),   \"6\" :   ( 13 ,   3 ), \n           \"8\" :   ( 15 ,   4 ),   \"10\" :   ( 15 ,   8 ),   \"12\" :   ( 15 ,   11 )}  for   threshold ,   ( no ,   nb )   in   splits . items (): \n   print ( \"Threshold = {} \\t  -> {}\" . format ( threshold ,   info_gain ( Nb ,   No ,   no ,   nb )))   Threshold = 0    -> 0.14818913558232172\nThreshold = 2    -> 0.33824492595034883\nThreshold = 4    -> 0.5297578726233217\nThreshold = 6    -> 0.3525728312615027\nThreshold = 8    -> 0.5297578726233217\nThreshold = 10   -> 0.28538113149388267\nThreshold = 12   -> 0.14818913558232172    We got the same cuts as predicted at the beginning of the lecture:  x > 4 x > 4  or  x > 8 x > 8    Lets choose  x > 4 x > 4  and calculate information gain ratio    # 4 samples with x = 0, 4 samples with x = 2 etc  info_gain ( Nb ,   No ,   * splits [ \"4\" ])   /   entropy ( 4 ,   4 ,   3 ,   5 ,   3 ,   4 ,   3 ,   4 )   0.1779055922617179",
            "title": "Feature X"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#feature-y",
            "text": "Repeat the procedure    This time possible thresholds =  \\{0, 2, 4, 6\\} \\{0, 2, 4, 6\\}    Nb   =   15  No   =   15  splits   =   { \"0\" :   ( 4 ,   2 ),   \"2\" :   ( 8 ,   5 ),   \"4\" :   ( 10 ,   8 ),   \"6\" :   ( 13 ,   11 )}  for   threshold ,   ( no ,   nb )   in   splits . items (): \n   print ( \"Threshold = {} \\t  -> {}\" . format ( threshold ,   info_gain ( Nb ,   No ,   no ,   nb )))   Threshold = 0    -> 0.02035297064032593\nThreshold = 2    -> 0.029594041354123246\nThreshold = 4    -> 0.013406861436605633\nThreshold = 6    -> 0.0203529706403259    The best cut is  y > 2 y > 2  (as predicted before)    Lets calculate information gain ratio    info_gain ( Nb ,   No ,   * splits [ \"2\" ])   /   entropy ( 6 ,   7 ,   5 ,   6 ,   6 )   0.01278981522839263",
            "title": "Feature Y"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#the-root",
            "text": "At the beginning we discussed the choice of  y y  as a root predictor    ID3 and C4.5 are greedy algorithms and select optimal solution at given stage    We can start to build the tree with the first best predictor    tree   =   Digraph ()  tree . edge ( \"x > 4? \\n [15, 15]\" ,   \"blue \\n [11, 0]\" ,   \"No\" )  tree . edge ( \"x > 4? \\n [15, 15]\" ,   \"[4, 15]\" ,   \"Yes\" )  tree",
            "title": "The root"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#branch-x-4",
            "text": "Now we have to repeat the procedure for the branch  [4, 15] [4, 15]    Lets take a look what points are left    plt . xlim ([ 5.5 ,   14.5 ])  plt . scatter ( * blue_train . T )  plt . scatter ( * orange_train . T );     Check  x x  maximum information gain ratio   Nb   =   4  No   =   15  splits   =   { \"6\" :   ( 2 ,   3 ),   \"8\" :   ( 4 ,   4 ),   \"10\" :   ( 4 ,   8 ),   \"12\" :   ( 4 ,   11 )}  for   threshold ,   ( no ,   nb )   in   splits . items (): \n   print ( \"Threshold = {} \\t  -> {}\" . format ( threshold ,   info_gain ( Nb ,   No ,   no ,   nb )))   Threshold = 6    -> 0.051004839414443226\nThreshold = 8    -> 0.32143493796317624\nThreshold = 10   -> 0.16251125329718286\nThreshold = 12   -> 0.08198172064120202  print ( \"Information gain ratio with x > 8:\" , \n       info_gain ( Nb ,   No ,   * splits [ \"8\" ])   /   entropy ( 5 ,   3 ,   4 ,   3 ,   4 ))   Information gain ratio with x > 8: 0.14010311259651076   Check  y y  maximum information gain ratio   Nb   =   4  No   =   15  splits   =   { \"0\" :   ( 2 ,   2 ),   \"2\" :   ( 3 ,   5 ),   \"4\" :   ( 3 ,   6 ),   \"6\" :   ( 4 ,   9 )}  for   threshold ,   ( no ,   nb )   in   splits . items (): \n   print ( \"Threshold = {} \\t  -> {}\" . format ( threshold ,   info_gain ( Nb ,   No ,   no ,   nb )))   Threshold = 0    -> 0.08471690647404045\nThreshold = 2    -> 0.08617499693494635\nThreshold = 4    -> 0.06066554625879636\nThreshold = 6    -> 0.13320381570773476  print ( \"Information gain ratio with y > 6:\" , \n       info_gain ( Nb ,   No ,   * splits [ \"6\" ])   /   entropy ( 4 ,   4 ,   3 ,   4 ,   4 ))   Information gain ratio with y > 6: 0.05757775370755489    Once again  x x  is a winner    And we have a new node    tree   =   Digraph ()  tree . edge ( \"x > 4? \\n [15, 15]\" ,   \"blue \\n [11, 0]\" ,   \"No\" )  tree . edge ( \"x > 4? \\n [15, 15]\" ,   \"x > 8? \\n [4, 15]\" ,   \"Yes\" )  tree . edge ( \"x > 8? \\n [4, 15]\" ,   \"[4, 4]\" ,   \"No\" )  tree . edge ( \"x > 8? \\n [4, 15]\" ,   \"orange \\n [0, 11]\" ,   \"Yes\" )  tree",
            "title": "Branch x &gt; 4"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#branch-x-8",
            "text": "We will continue until the tree is fully grown   plt . xlim ([ 5.5 ,   8.5 ])  plt . scatter ( * blue_train . T )  plt . scatter ( * orange_train . T );     Again, the best cut may be pretty obvious, but lets check the math  We have one possible cut in  x x   Nb   =   4  No   =   4  print ( \"Information gain ratio with x > 6:\" , \n       info_gain ( Nb ,   No ,   2 ,   3 )   /   entropy ( 5 ,   3 ))   Information gain ratio with x > 6: 0.05112447853477686   And usual threshold candidates in  y y   splits   =   { \"0\" :   ( 2 ,   0 ),   \"2\" :   ( 3 ,   0 ),   \"4\" :   ( 3 ,   1 ),   \"6\" :   ( 4 ,   2 )}  for   threshold ,   ( no ,   nb )   in   splits . items (): \n   print ( \"Threshold = {} \\t  -> {}\" . format ( threshold ,   info_gain ( Nb ,   No ,   no ,   nb )))   Threshold = 0    -> 0.31127812445913283\nThreshold = 2    -> 0.5487949406953986\nThreshold = 4    -> 0.1887218755408671\nThreshold = 6    -> 0.31127812445913283  print ( \"Information gain ratio with y > 2:\" , \n       info_gain ( Nb ,   No ,   * splits [ \"2\" ])   /   entropy ( 2 ,   1 ,   1 ,   2 ,   2 ))   Information gain ratio with y > 2: 0.24390886253128827   And the tree is growing   tree   =   Digraph ()  tree . edge ( \"x > 4? \\n [15, 15]\" ,   \"blue \\n [11, 0]\" ,   \"No\" )  tree . edge ( \"x > 4? \\n [15, 15]\" ,   \"x > 8? \\n [4, 15]\" ,   \"Yes\" )  tree . edge ( \"x > 8? \\n [4, 15]\" ,   \"y > 2? \\n [4, 4]\" ,   \"No\" )  tree . edge ( \"x > 8? \\n [4, 15]\" ,   \"orange \\n [0, 11]\" ,   \"Yes\" )  tree . edge ( \"y > 2? \\n [4, 4]\" ,   \"blue \\n [3, 0]\" ,   \"No\" )  tree . edge ( \"y > 2? \\n [4, 4]\" ,   \"[1, 4]\" ,   \"Yes\" )  tree",
            "title": "Branch x&lt;= 8"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#branch-y-2",
            "text": "plt . xlim ([ 5.5 ,   8.5 ])  plt . ylim ([ 3.5 ,   8.5 ])  plt . scatter ( * blue_train . T )  plt . scatter ( * orange_train . T );    Nb   =   1  No   =   4  print ( \"Information gain ratio with x > 6:\" , \n       info_gain ( Nb ,   No ,   0 ,   3 )   /   entropy ( 3 ,   2 ))   Information gain ratio with x > 6: 0.33155970728682876  print ( \"Information gain ratio with y > 4:\" , \n       info_gain ( Nb ,   No ,   0 ,   1 )   /   entropy ( 1 ,   2 ,   2 ))  print ( \"Information gain ratio with y > 6:\" , \n       info_gain ( Nb ,   No ,   1 ,   2 )   /   entropy ( 1 ,   2 ,   2 ))   Information gain ratio with y > 4: 0.047903442721748145\nInformation gain ratio with y > 6: 0.11232501392736344",
            "title": "Branch y &gt; 2"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#the-final-tree",
            "text": "tree   =   Digraph ()  tree . edge ( \"x > 4? \\n [15, 15]\" ,   \"blue \\n [11, 0]\" ,   \"No\" )  tree . edge ( \"x > 4? \\n [15, 15]\" ,   \"x > 8? \\n [4, 15]\" ,   \"Yes\" )  tree . edge ( \"x > 8? \\n [4, 15]\" ,   \"y > 2? \\n [4, 4]\" ,   \"No\" )  tree . edge ( \"x > 8? \\n [4, 15]\" ,   \"orange \\n [0, 11]\" ,   \"Yes\" )  tree . edge ( \"y > 2? \\n [4, 4]\" ,   \"blue \\n [3, 0]\" ,   \"No\" )  tree . edge ( \"y > 2? \\n [4, 4]\" ,   \"x > 6? \\n [1, 4]\" ,   \"Yes\" )  tree . edge ( \"x > 6? \\n [1, 4]\" ,   \"orange \\n [0, 3]\" ,   \"No\" )  tree . edge ( \"x > 6? \\n [1, 4]\" ,   \"y > 6? \\n [1, 1]\" ,   \"Yes\" )  tree . edge ( \"y > 6? \\n [1, 1]\" ,   \"blue \\n [1, 0]\" ,   \"No\" )  tree . edge ( \"y > 6? \\n [1, 1]\" ,   \"orange \\n [0, 1]\" ,   \"Yes\" )  tree      It is likely that this tree is overfitted    We will proceed with pruning as it was explained    But first lets implement decision rules to measure accuracy    def   tree_nominal ( x ,   y ): \n   \"\"\"Implementation of above tree\"\"\" \n   if   x   <=   4 : \n     return   \"blue\" \n   elif   x   >   8 : \n     return   \"orange\" \n   elif   y   <=   2 : \n     return   \"blue\" \n   elif   x   <=   6 : \n     return   \"orange\" \n   else : \n     return   \"orange\"   if   y   >   6   else   \"blue\"",
            "title": "The final tree"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#sanity-check",
            "text": "If the tree is built  correctly  we expect 100% accuracy on training set   for   x ,   y   in   blue_train : \n   print ( tree_nominal ( x ,   y ),   end = ' ' )   blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue  for   x ,   y   in   orange_train : \n   print ( tree_nominal ( x ,   y ),   end = ' ' )    orange orange orange orange orange orange orange orange orange orange orange orange orange orange orange",
            "title": "Sanity check"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#accuracy-before-pruning",
            "text": "def   accuracy ( samples ,   tree ): \n   \"\"\"Just print the result of classification\"\"\" \n   for   x ,   y   in   samples : \n     print ( \"({}, {}) -> {}\" . format ( x ,   y ,   tree ( x ,   y )))   accuracy ( blue_valid ,   tree_nominal )   (0, 6) -> blue\n(2, 4) -> blue\n(4, 0) -> blue\n(4, 8) -> blue\n(8, 2) -> blue  accuracy ( orange_valid ,   tree )   (8, 4) -> blue\n(10, 4) -> orange\n(12, 0) -> orange\n(12, 8) -> orange\n(14, 6) -> orange",
            "title": "Accuracy before pruning"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#pruning-i",
            "text": "We want to prune last decision node  y > 6 y > 6    In general, majority decides about the leaf node class    As it is a tie here, lets check both    def   tree_prune01a ( x ,   y ): \n   \"\"\"Implementation of above tree\"\"\" \n   if   x   <=   4 : \n     return   \"blue\" \n   elif   x   >   8 : \n     return   \"orange\" \n   elif   y   <=   2 : \n     return   \"blue\" \n   elif   x   <=   6 : \n     return   \"orange\" \n   else : \n     return   \"blue\"  def   tree_prune01b ( x ,   y ): \n   \"\"\"Implementation of above tree\"\"\" \n   if   x   <=   4 : \n     return   \"blue\" \n   elif   x   >   8 : \n     return   \"orange\" \n   elif   y   <=   2 : \n     return   \"blue\" \n   else : \n     return   \"orange\"   accuracy ( blue_valid ,   tree_prune01a )   (0, 6) -> blue\n(2, 4) -> blue\n(4, 0) -> blue\n(4, 8) -> blue\n(8, 2) -> blue  accuracy ( orange_valid ,   tree_prune01a )   (8, 4) -> blue\n(10, 4) -> orange\n(12, 0) -> orange\n(12, 8) -> orange\n(14, 6) -> orange    Pruning does not change the accuracy    We always use Occam's razor and  prune01a  is preferred over nominal tree    But lets see how  prune01b  works    accuracy ( blue_valid ,   tree_prune01b )   (0, 6) -> blue\n(2, 4) -> blue\n(4, 0) -> blue\n(4, 8) -> blue\n(8, 2) -> blue  accuracy ( orange_valid ,   tree_prune01b )   (8, 4) -> orange\n(10, 4) -> orange\n(12, 0) -> orange\n(12, 8) -> orange\n(14, 6) -> orange    In this case we even get the increase of the accuracy    We decide to prune a tree by replacing  y > 6 y > 6  decision node with \"orange\" leaf node    Which automatically removes  x > 6 x > 6  decision node    tree   =   Digraph ()  tree . edge ( \"x > 4? \\n [15, 15]\" ,   \"blue \\n [11, 0]\" ,   \"No\" )  tree . edge ( \"x > 4? \\n [15, 15]\" ,   \"x > 8? \\n [4, 15]\" ,   \"Yes\" )  tree . edge ( \"x > 8? \\n [4, 15]\" ,   \"y > 2? \\n [4, 4]\" ,   \"No\" )  tree . edge ( \"x > 8? \\n [4, 15]\" ,   \"orange \\n [0, 11]\" ,   \"Yes\" )  tree . edge ( \"y > 2? \\n [4, 4]\" ,   \"blue \\n [3, 0]\" ,   \"No\" )  tree . edge ( \"y > 2? \\n [4, 4]\" ,   \"orange \\n [1, 4]\" ,   \"Yes\" )  tree",
            "title": "Pruning I"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#pruning-ii",
            "text": "Now, lets see the accuracy after removing  y > 2 y > 2  node    It is once again a tie, so lets check both scenarios    def   tree_prune02a ( x ,   y ): \n   \"\"\"Implementation of above tree\"\"\" \n   if   x   <=   4 : \n     return   \"blue\" \n   else : \n     return   \"orange\"  def   tree_prune02b ( x ,   y ): \n   \"\"\"Implementation of above tree\"\"\" \n   if   x   <=   4 : \n     return   \"blue\" \n   elif   x   >   8 : \n     return   \"orange\" \n   else : \n     return   \"blue\"   accuracy ( blue_valid ,   tree_prune02a )   (0, 6) -> blue\n(2, 4) -> blue\n(4, 0) -> blue\n(4, 8) -> blue\n(8, 2) -> orange  accuracy ( orange_valid ,   tree_prune02a )   (8, 4) -> orange\n(10, 4) -> orange\n(12, 0) -> orange\n(12, 8) -> orange\n(14, 6) -> orange  accuracy ( blue_valid ,   tree_prune02b )   (0, 6) -> blue\n(2, 4) -> blue\n(4, 0) -> blue\n(4, 8) -> blue\n(8, 2) -> blue  accuracy ( orange_valid ,   tree_prune02b )   (8, 4) -> blue\n(10, 4) -> orange\n(12, 0) -> orange\n(12, 8) -> orange\n(14, 6) -> orange    In both cases the error increased    We stop pruning and leave the tree as it is in  prune01b  version",
            "title": "Pruning II"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#summary_1",
            "text": "C4.5 algorithm gives the full and clear prescription for building decision trees    It may look as a long procedure, but it is only because I wanted to show everything step by step and avoid  \"after a few trivial steps...\"    ID3/C4.5/C5.0 are based on information theory    There is alternative procedure based on  gini impurity , which is used by CART",
            "title": "Summary"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#cart",
            "text": "CART stands for Classification and Regression Tree    It was created independently from ID3 (more or less at the same time)    The main differences:    it creates binary trees (each decision node has two branches)    it uses gini impurity instead of information gain    it supports numerical target variables (regression)",
            "title": "CART"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#gini-impurity",
            "text": "Let  T = \\{T_1, T_2, ..., T_n\\} T = \\{T_1, T_2, ..., T_n\\}  be the set of  n n  classes    and  P = \\{p_1, p_2, ..., p_n\\} P = \\{p_1, p_2, ..., p_n\\}  be the probability distribution    where  p_i p_i  is the probability that a sample belongs to class  T_i T_i    and  1 - p_i 1 - p_i  is the probability that it belongs to another class    Gini impurity is defines as  I(P) = \\sum\\limits_{i=1}^n p_i\\cdot (1 - p_i) = \\sum\\limits_{i=1}^n p_i - \\sum\\limits_{i=1}^n p_i^2 = 1 - \\sum\\limits_{i=1}^n p_i^2 I(P) = \\sum\\limits_{i=1}^n p_i\\cdot (1 - p_i) = \\sum\\limits_{i=1}^n p_i - \\sum\\limits_{i=1}^n p_i^2 = 1 - \\sum\\limits_{i=1}^n p_i^2    As before (for entropy), lets consider two case scenario with  P = (p, 1 - p) P = (p, 1 - p) , so gini impurity is given by  I = 1 - p^2 - (1 - p)^2 = -2p(p - 1) I = 1 - p^2 - (1 - p)^2 = -2p(p - 1)    p   =   np . arange ( 0.01 ,   1.0 ,   0.01 )  plt . xlabel ( \"p\" )  plt . ylabel ( \"surprise factor\" )  plt . plot ( p ,   - p   *   np . log2 ( p )   -   ( 1   -   p )   *   np . log2 ( 1   -   p ),   label = \"Entropy\" );  plt . plot ( p ,   - 2 * p * ( p   -   1 ),   label = \"Gini impurity\" )  plt . legend ();",
            "title": "Gini impurity"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#play-golf",
            "text": "Lets consider once again Play Golf dataset   import   pandas   as   pd  # first row = headers  src   =   \"http://chem-eng.utoronto.ca/~datamining/dmc/datasets/weather_nominal.csv\"  golf_data   =   pd . read_csv ( src )   golf_data    \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       Outlook \n       Temperature \n       Humidity \n       Windy \n       Play golf \n     \n   \n   \n     \n       0 \n       Rainy \n       Hot \n       High \n       False \n       No \n     \n     \n       1 \n       Rainy \n       Hot \n       High \n       True \n       No \n     \n     \n       2 \n       Overcast \n       Hot \n       High \n       False \n       Yes \n     \n     \n       3 \n       Sunny \n       Mild \n       High \n       False \n       Yes \n     \n     \n       4 \n       Sunny \n       Cool \n       Normal \n       False \n       Yes \n     \n     \n       5 \n       Sunny \n       Cool \n       Normal \n       True \n       No \n     \n     \n       6 \n       Overcast \n       Cool \n       Normal \n       True \n       Yes \n     \n     \n       7 \n       Rainy \n       Mild \n       High \n       False \n       No \n     \n     \n       8 \n       Rainy \n       Cool \n       Normal \n       False \n       Yes \n     \n     \n       9 \n       Sunny \n       Mild \n       Normal \n       False \n       Yes \n     \n     \n       10 \n       Rainy \n       Mild \n       Normal \n       True \n       Yes \n     \n     \n       11 \n       Overcast \n       Mild \n       High \n       True \n       Yes \n     \n     \n       12 \n       Overcast \n       Hot \n       Normal \n       False \n       Yes \n     \n     \n       13 \n       Sunny \n       Mild \n       High \n       True \n       No",
            "title": "Play Golf"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#gini-impurity_1",
            "text": "We treat all values as they are continues    And consider all possible split    Every split leads to two subsets  S_1 S_1  and  S_2 S_2    And gini impurity for a set  S S  for given split is given by:  I(S) = \\frac{|S_1|}{|S|}\\cdot I(S_1) + \\frac{|S_2|}{|S|}\\cdot I(S_2) I(S) = \\frac{|S_1|}{|S|}\\cdot I(S_1) + \\frac{|S_2|}{|S|}\\cdot I(S_2)    def   gini ( * distribution ): \n   \"\"\"Calculate gini impurity for given ditribution of samples\"\"\" \n   sum2   =   sum ( distribution ) ** 2    # normalization factor \n\n   return   1   -   sum ([ p ** 2   for   p   in   distribution ]) / sum2   def   gini_split ( s1 ,   s2 ,   g1 ,   g2 ): \n   \"\"\"Calcualte impurity for given split    s1 -- the size of S1 subset    s1 -- the size of S2 subset    g1 -- I(S1)    g2 -- I(S2)    \"\"\" \n   s   =   s1   +   s2    # the total set size \n\n   return   s1 / s   *   g1   +   s2 / s   *   g2               | Play golf |\n            =============\n            | yes | no  |\n      -------------------\n      | yes |  2  |  3  | 5\nrainy | no  |  7  |  2  | 9\n      -------------------\n               9     5  gini_split ( 5 ,   9 ,   gini ( 2 ,   3 ),   gini ( 7 ,   2 ))   0.3936507936507937              | Play golf |\n            =============\n            | yes | no  |\n      -------------------\n      | yes |  3  |  2  | 5\nsunny | no  |  6  |  3  | 9\n      -------------------\n               9     5  gini_split ( 5 ,   9 ,   gini ( 3 ,   2 ),   gini ( 6 ,   3 ))   0.45714285714285713                 | Play golf |\n               =============\n               | yes | no  |\n         -------------------\n         | yes |  4  |  0  | 4\novercast | no  |  5  |  5  | 10\n         -------------------\n                  9     5  gini_split ( 4 ,   10 ,   gini ( 4 ,   0 ),   gini ( 5 ,   5 ))   0.35714285714285715    From  Outlook  feature the best choice is  Overcast  as it minimizes impurity    However, we would have to check other features and choose the best predictor from all possibilities    We have one step by step example done though    So lets use some tool",
            "title": "Gini impurity"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#scikit-learn",
            "text": "One step by step example is behind us, so now lets use some tool    CART is implemented in  scikit-learn    However, their implementation takes only numerical values    So we will use  LabelDecoder  to convert all values to numbers    from   sklearn.preprocessing   import   LabelEncoder  # pandas.DataFrame.apply applies a function to given axis (0 by default)  # LabelEncoder encodes class labels with values between 0 and n-1  golf_data_num   =   golf_data . apply ( LabelEncoder () . fit_transform )   golf_data_num    \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       Outlook \n       Temperature \n       Humidity \n       Windy \n       Play golf \n     \n   \n   \n     \n       0 \n       1 \n       1 \n       0 \n       0 \n       0 \n     \n     \n       1 \n       1 \n       1 \n       0 \n       1 \n       0 \n     \n     \n       2 \n       0 \n       1 \n       0 \n       0 \n       1 \n     \n     \n       3 \n       2 \n       2 \n       0 \n       0 \n       1 \n     \n     \n       4 \n       2 \n       0 \n       1 \n       0 \n       1 \n     \n     \n       5 \n       2 \n       0 \n       1 \n       1 \n       0 \n     \n     \n       6 \n       0 \n       0 \n       1 \n       1 \n       1 \n     \n     \n       7 \n       1 \n       2 \n       0 \n       0 \n       0 \n     \n     \n       8 \n       1 \n       0 \n       1 \n       0 \n       1 \n     \n     \n       9 \n       2 \n       2 \n       1 \n       0 \n       1 \n     \n     \n       10 \n       1 \n       2 \n       1 \n       1 \n       1 \n     \n     \n       11 \n       0 \n       2 \n       0 \n       1 \n       1 \n     \n     \n       12 \n       0 \n       1 \n       1 \n       0 \n       1 \n     \n     \n       13 \n       2 \n       2 \n       0 \n       1 \n       0 \n     \n       Now, lets splits our dataset to features and labels   # DataFrame.iloc makes an access thourgh indices  # we want all rows and first 4 columns for features  # and the last column for labels  data   =   np . array ( golf_data_num . iloc [:,   : 4 ])  target   =   np . array ( golf_data_num . iloc [:,   4 ])    Once data is prepared, creating a tree is as easy as 2 + 2 -1   from   sklearn   import   tree  golf_tree   =   tree . DecisionTreeClassifier ()  golf_tree . fit ( data ,   target );    sklearn.tree  supports drawing a tree using  graphviz   import   graphviz  # dot is a graph description language  dot   =   tree . export_graphviz ( golf_tree ,   out_file = None ,  \n                            feature_names = golf_data . columns . values [: 4 ],   \n                            class_names = [ \"no\" ,   \"yes\" ],   \n                            filled = True ,   rounded = True ,   \n                            special_characters = True )   # we create a graph from dot source using graphviz.Source  graph   =   graphviz . Source ( dot )   graph     Please note, that in the case of a real problem we would want to have a validation set and perform a pruning ( scikit-learn  does not support it though)",
            "title": "Scikit learn"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#regression",
            "text": "The difference now is that targets are numerical values (instead of categorical), e.g. in golf data - number of hours played instead of \"yes / no\"    Features may be either discrete or continuous    The idea is the same though - we want to create a binary tree and minimize the error on in each leaf    However, having continuous values as targets we can not simply use entropy or gini    We need to use different measurement - variance  V(X) = \\frac{1}{n}\\sum\\limits_{i=1}^n (x_i - \\bar x)^2 V(X) = \\frac{1}{n}\\sum\\limits_{i=1}^n (x_i - \\bar x)^2    where  X = \\{x_1, ..., x_n\\} X = \\{x_1, ..., x_n\\}  and  \\bar x \\bar x  is the average value    Note, that here  x_i x_i  are equally likely",
            "title": "Regression"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#simple-example",
            "text": "Before we learn how to grow a regression tree, lets take a look how it works on a simple example    Lets consider data distributed according to  x^2 x^2  (with some noise, obviously)    It means with have continuous features ( x x ) and targets ( y y )    We will split by hand the domain in  0.3 0.3  and  0.6 0.6    X   =   np . random . sample ( 50 )  Y   =   np . array ([ x ** 2   +   np . random . normal ( 0 ,   0.05 )   for   x   in   X ])  plt . xlabel ( \"x\" )  plt . ylabel ( \"y\" )  plt . scatter ( X ,   Y ,   color = 'b' )  plt . plot ([ 0.3 ,   0.3 ],   [ - 0.2 ,   1.2 ],   'g--' )  plt . plot ([ 0.6 ,   0.6 ],   [ - 0.2 ,   1.2 ],   'g--' );     The corresponding tree would look like this   tree   =   Digraph ()  tree . edge ( \"x < 0.3?\" ,   \"?\" ,   \"No\" )  tree . edge ( \"x < 0.3?\" ,   \"x < 0.6?\" ,   \"Yes\" )  tree . edge ( \"x < 0.6?\" ,   \"? \" ,   \"No\" )  tree . edge ( \"x < 0.6?\" ,   \"?  \" ,   \"Yes\" )  tree     For each split lets find a value  \\bar y \\bar y   def   avg ( X ,   Y ,   x_min ,   x_max ): \n   \"\"\"Return the average value in (x_min, x_max) range\"\"\" \n   n   =   0      # number of samples in given split  \n   avg   =   0    # average value \n\n   for   x ,   y   in   zip ( X ,   Y ): \n     if   x   >=   x_min   and   x   <   x_max : \n       n   +=   1 \n       avg   +=   y \n\n   return   avg   /   n   plt . scatter ( X ,   Y ,   color = 'b' )  plt . plot ([ 0.3 ,   0.3 ],   [ - 0.2 ,   1.2 ],   'g--' )  plt . plot ([ 0.6 ,   0.6 ],   [ - 0.2 ,   1.2 ],   'g--' )  y   =   avg ( X ,   Y ,   0 ,   0.3 )  plt . plot ([ 0.0 ,   0.3 ],   [ y ,   y ],   'r' )  y   =   avg ( X ,   Y ,   0.3 ,   0.6 )  plt . plot ([ 0.3 ,   0.6 ],   [ y ,   y ],   'r' )  y   =   avg ( X ,   Y ,   0.6 ,   1 )  plt . plot ([ 0.6 ,   1.0 ],   [ y ,   y ],   'r' );     Alternatively, one could do linear regression for split",
            "title": "Simple example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#growing-a-tree",
            "text": "The idea is similar as for numerical values in classification problems    For each feature we check all possible splits and calculate variance    We choose a binary split which minimzes variance    from   sklearn.tree   import   DecisionTreeRegressor  # create a decision tree regressor  fit   =   DecisionTreeRegressor ()  # and grow it (note that X must be reshaped)  fit . fit ( np . reshape ( X ,   ( - 1 ,   1 )),   Y );   # prepare test sample with \"newaxis\" trick  X_test   =   np . arange ( 0.0 ,   1.0 ,   0.01 )[:,   np . newaxis ]  Y_test   =   fit . predict ( X_test )   plt . scatter ( X ,   Y ,   color = 'b' )  plt . plot ( X_test ,   Y_test );      And this is a perfect example of  overfitting    Each point was  classified  as a separate target    Beacause without any stopping criterion the tree is growing until there is a single point in a leaf    There are several strategies to pre-prune a tree:    define a max depth of a tree    define a minimum number of samples in a leaf    define a minimum impurity    define a minimum impurity decrease      Whatever method is chosen you get a hyperparameter    And we already know how to find an optimal hyperparameter: cross-validation",
            "title": "Growing a tree"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#tree-cross-validation",
            "text": "To make it easier to check all possible methods lets create a simple class to do that for us    It takes training data and hyperparameter name (as named in  scikit-learn )    It can change hyperparameter    It can perform a cross-validation for a set of hyperparameter values    It can make accuracy and best fit plots    from   sklearn.model_selection   import   cross_val_score  from   sklearn.tree   import   DecisionTreeRegressor  class   TreeCV : \n   \"\"\"Perform a cross-validation for chosen hyperparameter\"\"\" \n\n   def   __init__ ( self ,   X ,   Y ,   hp = \"max_depth\" ): \n     \"\"\"Save training data\"\"\" \n     self . X   =   X      # features \n     self . Y   =   Y      # targets \n     self . hp   =   hp    # hyperparameter \n\n\n   def   set_method ( self ,   hp ): \n     \"\"\"Set hyperparameter to use\"\"\" \n     self . hp   =   hp \n\n\n   def   cross_me ( self ,   * hp_vals ): \n     \"\"\"Perform cross validation for given hyperparameter values\"\"\" \n     self . scores   =   []    # the accuracy table \n     self . best   =   None    # the best fit \n\n     best_score   =   0 \n\n     for   hp   in   hp_vals : \n       # create a tree with given hyperparameter cut \n       fit   =   DecisionTreeRegressor ( ** { self . hp :   hp }) \n\n       # calculate a cross validation scores and a mean value \n       score   =   cross_val_score ( fit ,   np . reshape ( X ,   ( - 1 ,   1 )),   Y ) . mean () \n\n       # update best fit if necessary \n       if   score   >   best_score : \n         self . best   =   fit \n         best_score   =   score \n\n       self . scores . append ([ hp ,   score ]) \n\n     # train the best fit \n     self . best . fit ( np . reshape ( X ,   ( - 1 ,   1 )),   Y ) \n\n\n   def   plot ( self ): \n     \"\"\"Plot accuracy as a function of hyperparameter values and best fit\"\"\" \n     plt . figure ( figsize = ( 15 ,   5 )) \n\n     plt . subplot ( 1 ,   2 ,   1 ) \n\n     plt . xlabel ( self . hp ) \n     plt . ylabel ( \"accuracy\" ) \n\n     plt . plot ( * zip ( * self . scores )) \n\n     plt . subplot ( 1 ,   2 ,   2 ) \n\n     X_test   =   np . arange ( 0.0 ,   1.0 ,   0.01 )[:,   np . newaxis ] \n     Y_test   =   self . best . predict ( X_test ) \n\n     plt . scatter ( self . X ,   self . Y ,   color = 'b' ,   marker = '.' ,   label = \"Training data\" ) \n     plt . plot ( X_test ,   X_test   *   X_test ,   'g' ,   label = \"True distribution\" )     \n     plt . plot ( X_test ,   Y_test ,   'r' ,   label = \"Decision tree\" ) \n\n     plt . legend ()",
            "title": "Tree: cross-validation"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#traning-dataset",
            "text": "X   =   np . random . sample ( 200 )  Y   =   np . array ([ x ** 2   +   np . random . normal ( 0 ,   0.05 )   for   x   in   X ])",
            "title": "Traning dataset"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#max_depth",
            "text": "tree_handler   =   TreeCV ( X ,   Y )  tree_handler . cross_me ( * range ( 1 ,   10 ))  tree_handler . plot ()",
            "title": "max_depth"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#min_samples_leaf",
            "text": "tree_handler . set_method ( \"min_samples_leaf\" )  tree_handler . cross_me ( * range ( 1 ,   10 ))  tree_handler . plot ()",
            "title": "min_samples_leaf"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#min_impurity_split",
            "text": "# min_impurity_split is depracated so lets disable warnings  import   warnings  warnings . filterwarnings ( \"ignore\" ,   category = DeprecationWarning )   tree_handler . set_method ( \"min_impurity_split\" )  tree_handler . cross_me ( * np . arange ( 0.0 ,   5e-3 ,   1e-4 ))  tree_handler . plot ()",
            "title": "min_impurity_split"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#min_impurity_decrease",
            "text": "tree_handler . set_method ( \"min_impurity_decrease\" )  tree_handler . cross_me ( * np . arange ( 0.0 ,   5e-4 ,   1e-5 ))  tree_handler . plot ()",
            "title": "min_impurity_decrease"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#bias-variance-trade-off",
            "text": "+================+================+\n       /\\                |                /\\\n      /  \\               |               /  \\\n     /    \\              |              /    \\\n    /      \\             |             /      \\\n   /        \\            |            /        \\\n  / variance \\           |           /   bias   \\\n  ^^^^^^^^^^^^           |           ^^^^^^^^^^^^\n                         |\n                         |\n                         |\noverfitting   <----------+--------->   underfitting    Bias is an error coming from wrong model assumptions, which do not allow an algorithm to learn all patterns from training data.    Variance is an error coming from sensivity to features specific for training data.    High bias leads to underfitting and high variance to overfitting.    Total error also depends on irreducible error ( noise  that can not be reduced by algorithm)    Ultmiate goal is to minimize the total error    # fake bias, variance and noise  complexity   =   np . arange ( 1 ,   2 ,   0.1 )  variance   =   np . power ( complexity ,   5 )  bias2   =   variance [:: - 1 ]  irreducible   =   [ 10 * np . random . normal ( abs ( x   -   1.5 ),   0.01 )   for   x   in   complexity ]  # total error = variance + bias^2 + irreducible  total   =   variance   +   bias2   +   irreducible  plt . xticks ([])  plt . yticks ([])  plt . xlabel ( \"Algorithm complexity\" )  plt . ylabel ( \"Error\" )  plt . plot ( complexity ,   variance ,   'C0o-' ,   label = 'Variance' )  plt . plot ( complexity ,   bias2 ,   'C1o-' ,   label = \"Bias^2\" )  plt . plot ( complexity ,   total ,   'C2o-' ,   label = \"Total = Bias^2 + Variance + Irreducible error\" )  plt . plot ([ 1.5 ,   1.5 ],   [ 0 ,   25 ],   'C3--' )  plt . text ( 1.0 ,   7 ,   \"$\\longleftarrow$ better chance of generalizing\" ,   color = 'C0' )  plt . text ( 1.6 ,   7 ,   \"better chance of approximating $\\longrightarrow$\" ,   color = 'C1' )  plt . legend ();      Decision trees are sensitive to splits - small changes in training data may change a tree structure    deep trees tend to have high variance and low bias    shallow trees tend to have low variance and high bias",
            "title": "Bias-Variance trade-off"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#quick-math",
            "text": "",
            "title": "Quick math"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#basic",
            "text": "The general goal of regression is to find how some dependent variable (target,  y y ) is changing when independent variable (feature,  x x ) varies    Lets assume there is some  true  relationship describing this dependence  y = f(x) y = f(x)    We want to find  f(x) f(x)  from observations of  (x, y) (x, y)  pairs    Although, in real life we get some noisy observation, so  y = f(x) + \\epsilon y = f(x) + \\epsilon    As we do not know function  f(x) f(x)  we want to approximate it with some other function  g(x) g(x)  (estimator)    In general,  g(x) g(x)  is a parametrized model which can take many possible functional form   e.g.  g(x) = a\\cdot x^2 + b\\cdot x + c g(x) = a\\cdot x^2 + b\\cdot x + c  can take different coefficients (based on a training dataset)",
            "title": "Basic"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#bias-and-variance",
            "text": "Lets imagine there are  N N  possible training datasets  \\{D_1, D_2, ..., D_N\\} \\{D_1, D_2, ..., D_N\\}    For a given dataset one gets an estimators  g^{(D)}(x) g^{(D)}(x)    Lets denote the expected estimator by  \\mathbf{E}_{D}\\left[g^{(D)}(x)\\right] \\equiv \\bar g(x) \\mathbf{E}_{D}\\left[g^{(D)}(x)\\right] \\equiv \\bar g(x)    If  N N  is large we can approximate it by an average over all datasets (law of large numbers)  \\bar g(x) \\approx \\frac{1}{N}\\sum\\limits_{i=1}^N g^{(D_i)}(x) \\bar g(x) \\approx \\frac{1}{N}\\sum\\limits_{i=1}^N g^{(D_i)}(x)    The  variance  of an estimator tells us how far particular predictions are from the mean value  var = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\bar g(x)\\right)^2\\right] var = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\bar g(x)\\right)^2\\right]    Thus, if the training does not depend on the choice of a dataset the variance is low    The  bias  of an estimator tells us how far the mean value is from the true value  bias = \\bar g(x) - f(x) bias = \\bar g(x) - f(x)    Thus, if the model decribes data accurately the bias is low    Please note the hidden assumption that all possible values of  x x  are equally likely",
            "title": "Bias and variance"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#goodness-of-a-model",
            "text": "The common practice to determine the goodness of a model fit is to calculate mean squared error    The mean squared error is, well, the mean value of error squared:  mse = \\mathbf{E}_{x}\\left[\\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - y\\right)^2\\right]\\right] mse = \\mathbf{E}_{x}\\left[\\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - y\\right)^2\\right]\\right]    Lets consider MSE for a particlar point  x x ,  y = f(x) + \\epsilon y = f(x) + \\epsilon , so  mse = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - y\\right)^2\\right] = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x)\\right)^2\\right]- 2\\cdot\\mathbf{E}_{D}\\left[g^{(D)}(x)\\cdot y\\right] + \\mathbf{E}_{D}\\left[y^2\\right] mse = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - y\\right)^2\\right] = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x)\\right)^2\\right]- 2\\cdot\\mathbf{E}_{D}\\left[g^{(D)}(x)\\cdot y\\right] + \\mathbf{E}_{D}\\left[y^2\\right]    Here, we used the linearity of the expected value operator. Lets use another common property:  \\mathbf{E}\\left[X^2\\right] = \\mathbf{E}\\left[\\left(X - \\mathbf{E}\\left[X\\right]\\right)^2\\right] + \\mathbf{E}\\left[X\\right]^2 \\mathbf{E}\\left[X^2\\right] = \\mathbf{E}\\left[\\left(X - \\mathbf{E}\\left[X\\right]\\right)^2\\right] + \\mathbf{E}\\left[X\\right]^2      So the first term can be rewritten in the form  \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x)\\right)^2\\right] = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\mathbf{E}_{D}\\left[g^{(D)}(x)\\right]\\right)^2\\right] + \\mathbf{E}_{D}\\left[g^{(D)}(x)\\right]^2 = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\bar g(x)\\right)^2\\right] + \\left(\\bar g(x)\\right)^2 \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x)\\right)^2\\right] = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\mathbf{E}_{D}\\left[g^{(D)}(x)\\right]\\right)^2\\right] + \\mathbf{E}_{D}\\left[g^{(D)}(x)\\right]^2 = \\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\bar g(x)\\right)^2\\right] + \\left(\\bar g(x)\\right)^2    And the last term  \\mathbf{E}_{D}\\left[y^2\\right] = \\mathbf{E}_{D}\\left[\\left(y - \\mathbf{E}_{D}\\left[y\\right]\\right)^2\\right] + \\mathbf{E}_{D}\\left[y\\right]^2 = \\mathbf{E}_{D}\\left[\\left(y - f(x)\\right)^2\\right] + \\left(f(x)\\right)^2 \\mathbf{E}_{D}\\left[y^2\\right] = \\mathbf{E}_{D}\\left[\\left(y - \\mathbf{E}_{D}\\left[y\\right]\\right)^2\\right] + \\mathbf{E}_{D}\\left[y\\right]^2 = \\mathbf{E}_{D}\\left[\\left(y - f(x)\\right)^2\\right] + \\left(f(x)\\right)^2    Here, we used the fact that  \\mathbf{E}_{D}\\left[y\\right] = f(x) \\mathbf{E}_{D}\\left[y\\right] = f(x)  (noise would average out when averaging over  infinite  number of datasets)    For the middle term we use the fact that for independent  X X  and  Y Y :  \\mathbf{E}\\left[XY\\right] = \\mathbf{E}\\left[X\\right]\\cdot\\mathbf{E}\\left[Y\\right] \\mathbf{E}\\left[XY\\right] = \\mathbf{E}\\left[X\\right]\\cdot\\mathbf{E}\\left[Y\\right] , so  \\mathbf{E}_{D}\\left[g^{(D)}(x)\\cdot y\\right] = \\bar g(x)\\cdot f(x) \\mathbf{E}_{D}\\left[g^{(D)}(x)\\cdot y\\right] = \\bar g(x)\\cdot f(x)    Taking all together we get  mse = \\underbrace{\\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\bar g(x)\\right)^2\\right]}_{variance} + \\underbrace{\\left(\\bar g(x) - f(x)\\right)^2}_{bias^2} + \\underbrace{\\mathbf{E}_{D}\\left[\\left(y - f(x)\\right)^2\\right]}_{noise} mse = \\underbrace{\\mathbf{E}_{D}\\left[\\left(g^{(D)}(x) - \\bar g(x)\\right)^2\\right]}_{variance} + \\underbrace{\\left(\\bar g(x) - f(x)\\right)^2}_{bias^2} + \\underbrace{\\mathbf{E}_{D}\\left[\\left(y - f(x)\\right)^2\\right]}_{noise}",
            "title": "Goodness of a model"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#example_4",
            "text": "Lets consider  f(x) = \\sin(\\pi x) f(x) = \\sin(\\pi x)    With a noise given by a zero-mean Gaussian with a variance  \\sigma^2 \\sigma^2    So the observation  y = f(x) + \\mathcal{N}(0, \\sigma^2) y = f(x) + \\mathcal{N}(0, \\sigma^2)    from   math   import   sin ,   cos ,   pi ,   exp  def   get_dataset ( N = 20 ,   sigma = 0.1 ): \n   \"\"\"Generate N training samples\"\"\" \n   # X is a set of random points from [-1, 1] \n   X   =   2   *   np . random . sample ( N )   -   1 \n   # Y are corresponding target values (with noise included) \n   Y   =   np . array ([ sin ( pi * x )   +   np . random . normal ( 0 ,   sigma )   for   x   in   X ]) \n\n   return   X ,   Y  # plot a sample  X ,   Y   =   get_dataset ()  x_   =   np . arange ( - 1 ,   1 ,   0.01 )  plt . scatter ( X ,   Y ,   color = 'C1' )  plt . plot ( x_ ,   np . sin ( np . pi   *   x_ ),   'C0--' );      We do not know  f(x) f(x)    We assume it is a polynomial    Lets consider polynomials of orders:  1 - 9 1 - 9    g_1(x) = a_1\\cdot x + a_0 g_1(x) = a_1\\cdot x + a_0    g_2(x) = a_2\\cdot x^2 + \\cdots + a_0 g_2(x) = a_2\\cdot x^2 + \\cdots + a_0    g_3(x) = a_3\\cdot x^3 + \\cdots + a_0 g_3(x) = a_3\\cdot x^3 + \\cdots + a_0    ...      Lets assume we have 100 independent dataset    Each one has 20 points  (x, y = \\sin(\\pi x) + \\mathcal{N}(0, \\sigma^2)) (x, y = \\sin(\\pi x) + \\mathcal{N}(0, \\sigma^2))    # generate 100 datasets with default settings  datasets   =   [ get_dataset ()   for   i   in   range ( 100 )]  # and plot them all together with true signal  for   i   in   range ( 100 ): \n   plt . scatter ( datasets [ i ][ 0 ],   datasets [ i ][ 1 ],   marker = '.' )  plt . plot ( x_ ,   np . sin ( np . pi   *   x_ ),   'C0--' );     Now we need to fit each polynomial to each dataset separately   def   get_fit ( N ,   data ): \n   \"\"\"Find a fit of polynomial of order N to data = (X, Y)\"\"\" \n   return   np . poly1d ( np . polyfit ( data [ 0 ],   data [ 1 ],   N ))  # for the whole range of possible polynomials orders  # create a list of fits to different datasets  fits   =   [[ get_fit ( order ,   data )   for   data   in   datasets ]   for   order   in   range ( 1 ,   10 )]   plt . figure ( figsize = ( 13 ,   10 ))  for   order   in   range ( 1 ,   10 ): \n   plt . subplot ( 3 ,   3 ,   order ) \n   plt . ylim ([ - 1.5 , 1.5 ]) \n\n   for   g   in   fits [ order   -   1 ]: \n     plt . plot ( x_ ,   g ( x_ ),   'C1-' ,   linewidth = 0.1 ) \n\n   plt . plot ( x_ ,   np . sin ( np . pi   *   x_ ),   'C0--' ) \n\n   plt . title ( \"Polynomial of order {}\" . format ( order ));  plt . tight_layout ();",
            "title": "Example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#training-and-test-errors",
            "text": "In real life is impossible (unless one creates data by hand) to calculate true variance and bias    One would need all possible datasets  D D  and all possible input values  x x    Thus, usually one looks at training and test errors    Training error is measured on the data used to make a fit    Test/validation error is measured on unseen data      # fake error  complexity   =   np . arange ( 0.1 ,   2 ,   0.1 )  train_error   =   - np . log ( complexity )  test_error   =   - np . log ( complexity )   +   np . power ( complexity ,   1 )  plt . xticks ([])  plt . yticks ([])  plt . xlabel ( \"Algorithm complexity\" )  plt . ylabel ( \"Error\" )  plt . plot ( complexity ,   train_error ,   'C0o-' ,   label = 'Training error' )  plt . plot ( complexity ,   test_error ,   'C1o-' ,   label = \"Test error\" )  plt . text ( 0.1 ,   0.25 ,   \"$\\longleftarrow$ high bias\" ,   color = 'C0' )  plt . text ( 1.5 ,   0.25 ,   \"high variance $\\longrightarrow$\" ,   color = 'C1' )  plt . legend ();      High training error indicates high bias (which means underfitting)    Training error must decrease with model complexity    If the training error is high:    Use more complex model (or new model architecture)    Use more features - maybe there is just not enough information to make a good prediction    Train longer (if the algorithm is an iterative optimization problem)    Decrease regularization (next lecture)      Test error deacreses with model complexity up to a point when algorithm is to sensitive to features seen in training data    If test error starts to increase it indicates high variance (which means overfitting)    If test error is high:    Use more data - easy to say hard to do...    Use less features    Increase regularization     Use different model",
            "title": "Training and test errors"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#ensemble-learning",
            "text": "Lets first define a  weak learner  as a classifier which is just slighlty better than random guessing    The idea behind ensemble learning is to create a  strong learner  as a combination of many  weak learners    We will discuss two popular ensemble methods:    Bagging ( b ootstrap  agg regat ing ), e.g. random forest    Boosting, e.g. boosted decision tress",
            "title": "Ensemble learning"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#random-forest",
            "text": "Once we know a way to produce a tree we can create a forest    And each tree contributes to a final prediction    It is a  random  forest, because each tree is randomly  incomplete  - trained only on a random subsets of samples and features ( features bagging )    The final prediction of a random forest is an avearge predictions (for regression) or a majority vote (classification)",
            "title": "Random forest"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#intuitive-naive-example",
            "text": "Imagine you want to go to a cinema and need to choose a movie to watch    You can ask a friend about the recommendation    She/he would ask you about movies you watched in the past    and (based on your answers) create a set of rules (a decision tree)    to finally recommend you a movie (make a prediction)      Alternatively, you can ask many friends for an advice    Each friend would ask you random questions to give an answer    At the end, you choose a movie with most votes",
            "title": "Intuitive / naive example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#the-algorithm",
            "text": "Lets imagine we have  N N  samples in our dataset (e.g.  N N  movies you watched)    Each sample has  M M  features (e.g. do you like a movie? do you like the leading actor / actress or director?)    To create a tree take  n n  random samples from the dataset and  at each node  select  m << M m << M  features ( m \\sim \\sqrt M m \\sim \\sqrt M ) to find the best predictor    Repeat the procedure for next trees until you reach desired size of a forest                        +------------+\n                    |            |\n                    |   Dataset  |\n         +----------+            +----------+\n         |          | N features |          |\n         |          |            |          |\n         v          +------------+          v                    Hyperparameters:\n\n+-----------------+                 +-----------------+\n|                 |                 |                 |\n| Random subset 1 |      . . .      | Random subset T |\n|                 |                 |                 |              - the number of trees\n|   N features    |                 |   N features    |              - the size of subsets\n|                 |                 |                 |\n+--------+--------+                 +--------+--------+\n         |                                   |\n         v                                   v\n\n+-----------------+                 +-----------------+\n|                 |                 |                 |\n|     Tree 1      |      . . .      |     Tree T      |\n|                 |                 |                 |\n+--------+--------+                 +-----------------+\n         |\n         |          +-------------+          +--------+\n         |          |             |          |        |\n         +--------> | M1 features +--------> | Node 1 |              - the number of random features\n         |          |             |          |        |              - the size of a single tree\n         |          +-------------+          +--------+\n         |\n         |          +-------------+          +--------+\n         |          |             |          |        |\n         +--------> | M2 features +--------> | Node 2 |\n         |          |             |          |        |\n         |          +-------------+          +--------+\n\n                           .\n                           .\n                           .",
            "title": "The algorithm"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#boosted-trees",
            "text": "The idea is similar to bagging    The key differences are:    Data is reweighted every time a  weak learner  is added (so future learners focus on misclassified samples)    The final prediction is weighted average (better classifiers have higher weights)                                  Bagging (parallel)\n\n       +--------------------+                +----------------+\n       |                    |                |                |\n+----> |      Dataset       +--------------> | Weak learner 1 |\n|      |                    |                |                |\n|      +--------------------+                +----------------+\n|\n|\n|      +--------------------+                +----------------+\n|      |                    |                |                |\n+----> |      Dataset       +------------->  | Weak learner 2 |\n|      |                    |                |                |\n|      +--------------------+       .        +----------------+\n|                                   .\n|                                   .\n|      +--------------------+                +----------------+\n|      |                    |                |                |\n+----> |      Dataset       +------------->  | Weak learner N |\n       |                    |                |                |\n       +--------------------+                +----------------+  \\hspace{140pt}\\text{output} = \\frac{1}{N}\\sum \\text{output}_i \\hspace{140pt}\\text{output} = \\frac{1}{N}\\sum \\text{output}_i                               Boosting (sequential)\n\n      +--------------------+                +----------------+\n      |                    |                |                |\n      |      Dataset       +--------------> | Weak learner 1 |\n      |                    |                |                |\n      +--------------------+                +--------+-------+\n                                                     |\n                +------------------------------------+\n                |\n                v\n\n      +--------------------+                +----------------+\n      |                    |                |                |\n      | Reweighted dataset +------------->  | Weak learner 2 |\n      |                    |                |                |\n      +--------------------+                +--------+-------+\n                                                     |\n                +------------     . . .      --------+\n                |\n                v\n\n      +--------------------+                +----------------+\n      |                    |                |                |\n      | Reweighted dataset +------------->  | Weak learner N |\n      |                    |                |                |\n      +--------------------+                +----------------+  \\hspace{130pt}\\text{output} = \\sum w_i\\cdot \\text{output}_i \\hspace{130pt}\\text{output} = \\sum w_i\\cdot \\text{output}_i",
            "title": "Boosted trees"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#adaboost",
            "text": "AdaBoost is on of the most famous algorithms in machine learning    Y. Freund and R. Schapire got a G\u00f6del Prize for this    Lets consider  N N  labled training examples  (x_1, y_1), \\cdots, (x_N, y_N) (x_1, y_1), \\cdots, (x_N, y_N) , where  x_i \\in X x_i \\in X  and  y_i = \\left\\{-1, 1\\right\\} y_i = \\left\\{-1, 1\\right\\}    The initial distribution is initizlized with  D_1(i) = \\frac{1}{N} D_1(i) = \\frac{1}{N} , where  i = 1, \\cdots, N i = 1, \\cdots, N  (so every sample is equaly likely)    For  t = 1, \\cdots, T t = 1, \\cdots, T :    train a weak learner using  D_t D_t ,  h_t: X \\rightarrow \\left\\{-1, 1\\right\\} h_t: X \\rightarrow \\left\\{-1, 1\\right\\}    choose the one which minimizes the weighted error  \\epsilon_t = \\sum\\limits_{i=1}^{N}D_t(i)\\delta\\left(h_t(x_i)\\neq y_i\\right) \\epsilon_t = \\sum\\limits_{i=1}^{N}D_t(i)\\delta\\left(h_t(x_i)\\neq y_i\\right)    calculate  \\alpha_t \\alpha_t   \\alpha_t = \\frac{1}{2}\\ln\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right) \\alpha_t = \\frac{1}{2}\\ln\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)    For  i = 1, \\cdots, N i = 1, \\cdots, N  update weights according to  D_{t+1} = \\frac{D_t(i)}{Z_t}\\begin{cases}e^{-\\alpha_t} & h_t(x_i) = y_i \\\\ e^{\\alpha_t} & h_t(x_i) \\neq y_i \\end{cases} = \\frac{D_t(i)}{Z_t}e^{-\\alpha_ty_ih_t(x_i)} D_{t+1} = \\frac{D_t(i)}{Z_t}\\begin{cases}e^{-\\alpha_t} & h_t(x_i) = y_i \\\\ e^{\\alpha_t} & h_t(x_i) \\neq y_i \\end{cases} = \\frac{D_t(i)}{Z_t}e^{-\\alpha_ty_ih_t(x_i)}    Z_t Z_t  is a normilization factor so  D_{t+1} D_{t+1}  is a distribution  Z_t = \\sum\\limits_{i=1}^ND_t(i)e^{-\\alpha_ty_ih_t(x_i)} Z_t = \\sum\\limits_{i=1}^ND_t(i)e^{-\\alpha_ty_ih_t(x_i)}      The final hyptohesis  H H  is computes the sign of a weighted combination of weak hypotheses  H(x) = \\text{sign}\\left(\\sum\\limits_{t=1}^T\\alpha_th_t(x)\\right) H(x) = \\text{sign}\\left(\\sum\\limits_{t=1}^T\\alpha_th_t(x)\\right)",
            "title": "AdaBoost"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#out-of-bag-error",
            "text": "Out-of-bag (OOB) error may be used for machine learning models using bootstrap aggregation (like random forest and boosted trees) instead of cross-validation    Bagging involves random sampling with replacement    Some samples are not used in the training process (out-of-bag samples) and therefore can be used to calculate test error    OOB error is the average error over all training samples calculated using predictions from weak classifiers which do not contain particular sample in their bootstrap samples",
            "title": "Out-of-bag error"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#example_5",
            "text": "Lets consider some fake data generated with  make_blobs  from  scikit-learn    and then apply decision trees with different maximum depths    and random forests with different maximum depths",
            "title": "Example"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#dataset",
            "text": "sklearn.datasets.make_blobs  allows to generate random Gaussian blobs    We generate 8 blobs with fixed random generator (just to make sure we get the same set every time)    from   sklearn.datasets   import   make_blobs  # generate 5 blobs with fixed random generator  X ,   Y   =   make_blobs ( n_samples = 500 ,   centers = 8 ,   random_state = 300 )  plt . scatter ( * X . T ,   c = Y ,   marker = '.' ,   cmap = 'Dark2' );",
            "title": "Dataset"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#train-and-visualize",
            "text": "To make our life easier we create a function to    plot training data on existing axes or new one (if not provided)    train given classifier on given dataset    create countours representing predictions of the classifier      def   train_and_look ( classifier ,   X ,   Y ,   ax = None ,   title = '' ,   cmap = 'Dark2' ): \n   \"\"\"Train classifier on (X,Y). Plot data and prediction.\"\"\" \n   # create new axis if not provided \n   ax   =   ax   or   plt . gca (); \n\n   ax . set_title ( title ) \n\n   # plot training data \n   ax . scatter ( * X . T ,   c = Y ,   marker = '.' ,   cmap = cmap ) \n\n   # train a cliassifier \n   classifier . fit ( X ,   Y ) \n\n   # create a grid of testing points \n   x_ ,   y_   =   np . meshgrid ( np . linspace ( * ax . get_xlim (),   num = 200 ), \n                        np . linspace ( * ax . get_ylim (),   num = 200 )) \n\n   # convert to an array of 2D points \n   test_data   =   np . vstack ([ x_ . ravel (),   y_ . ravel ()]) . T \n\n   # make a prediction and reshape to grid structure  \n   z_   =   classifier . predict ( test_data ) . reshape ( x_ . shape ) \n\n   # arange z bins so class labels are in the middle \n   z_levels   =   np . arange ( len ( np . unique ( Y ))   +   1 )   -   0.5 \n\n   # plot contours corresponding to classifier prediction \n   ax . contourf ( x_ ,   y_ ,   z_ ,   alpha = 0.25 ,   cmap = cmap ,   levels = z_levels )    Let check how it works on a decision tree classifier with default  sklearn  setting   from   sklearn.tree   import   DecisionTreeClassifier   as   DT  train_and_look ( DT (),   X ,   Y )",
            "title": "Train and visualize"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#decision-tree",
            "text": "We consider decision trees with fixed maximum depths from 1 to 9   # create a figure with 9 axes 3x3  fig ,   ax   =   plt . subplots ( 3 ,   3 ,   figsize = ( 15 , 15 ))  # train and look at decision trees with different max depth  for   max_depth   in   range ( 0 ,   9 ): \n   train_and_look ( DT ( max_depth = max_depth   +   1 ),   X ,   Y , \n                  ax = ax [ max_depth   //   3 ][ max_depth   %   3 ], \n                  title = \"Max depth = {}\" . format ( max_depth   +   1 ))     max_depth  <= 3 - undefitting  max_depth  <= 6 - quite good  max_depth   > 6 - overfitting",
            "title": "Decision tree"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#random-forest_1",
            "text": "Lets do the same with random forests (100 trees in each forest)   from   sklearn.ensemble   import   RandomForestClassifier   as   RF  # create a figure with 9 axes 3x3  fig ,   ax   =   plt . subplots ( 3 ,   3 ,   figsize = ( 15 , 15 ))  # train and look at decision trees with different max depth  for   max_depth   in   range ( 0 ,   9 ): \n   train_and_look ( RF ( n_estimators = 100 ,   max_depth = max_depth   +   1 ),   X ,   Y , \n                  ax = ax [ max_depth   //   3 ][ max_depth   %   3 ], \n                  title = \"Max depth = {}\" . format ( max_depth   +   1 ))      The combination of shallow trees (weak learners) does a good job    Overfitting is somehow prevented",
            "title": "Random forest"
        },
        {
            "location": "/markdown/introduction_to_machine_learning_02_dt/introduction_to_machine_learning_02_dt/#summary_2",
            "text": "The most important lesson today:  bias-variance trade-off    For the lecture easy examples are chosen so they can be visualize    In real life problems, it is hard / impossible to determine using \"bye eye\" method if the model is underfitted or overfitted    Note, that actually you should never used this method even if you think \"your eye\" is right - you would be surprised how it is not    One needs a way to measure the goodnes of a model - usually mean squared error    In practice, most people use cross-calidation technique       The biggest advantages of decision trees algoritms is that they are east to interpret (it is easy to explain even to non-experts how they work, which is not the case with e.g. deep neural networks)    Usually, decision trees are used as weak learners in ensemble learning    The most famous boosting algorithm is AdaBoost, because it is a good one and the first one. Although, there are many other boosting methods on market right now with XGBoost being one of the most popular one    As for today, deep learning has better publicity, but boosted trees are still one of the most common algorithms used in  Kaggle competitions    Boosted trees are also popular among physicist and used by them as an alternative to neural networks for experimental elementary particle physics in event reconstruction procedures",
            "title": "Summary"
        }
    ]
}